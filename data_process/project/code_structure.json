{
  "securedrop": {
    "rm.py": [
      {
        "type": "function",
        "name": "shred",
        "code": "def shred(path: str, delete: bool = True) -> None:\n    \"\"\"\n    Run shred on the file at the given path.\n\n    Args:\n        path (str): The path to the file to shred.\n        delete (bool): Whether to unlink the file after shredding.\n\n    Returns:\n        None\n\n    Raises:\n        subprocess.CalledProcessError: If shred's return code is not zero.\n        EnvironmentError: If shred is not available.\n    \"\"\"\n\n    if not os.path.exists(path):\n        raise OSError(path)\n\n    if not os.path.isfile(path):\n        raise ValueError(\"The shred function only works on files.\")\n    cmd = [\"shred\", \"-z\", \"-n\", \"30\"]\n    if delete:\n        cmd.append(\"-u\")\n    cmd.append(path)\n    subprocess.check_call(cmd)",
        "file": "rm.py"
      },
      {
        "type": "function",
        "name": "secure_delete",
        "code": "def secure_delete(path: str) -> None:\n    \"\"\"\n    Securely deletes the file at ``path``.\n\n    Args:\n        path (str): The path to the file to delete.\n\n    Returns:\n        str: A string signaling success to rq.\n\n    Raises:\n        subprocess.CalledProcessError: If shred's return code is not zero.\n        EnvironmentError: If shred is not available.\n    \"\"\"\n    path = os.path.abspath(path)\n\n    directories = []\n    targets = []\n    if not os.path.isdir(path):\n        targets.append(path)\n    else:\n        for directory, subdirs, files in os.walk(path):\n            directories.append(directory)\n            directories.extend([os.path.abspath(os.path.join(directory, s)) for s in subdirs])\n            for f in files:\n                targets.append(os.path.abspath(os.path.join(directory, f)))\n\n    for t in targets:\n        shred(t)\n\n    directories_to_remove = set(directories)\n    for d in reversed(sorted(directories_to_remove)):\n        os.rmdir(d)",
        "file": "rm.py"
      },
      {
        "type": "function",
        "name": "check_secure_delete_capability",
        "code": "def check_secure_delete_capability() -> bool:\n    \"\"\"\n    Checks the availability of the program we use for secure deletion.\n\n    Returns:\n        bool: True if the program is available, otherwise False.\n    \"\"\"\n    try:\n        subprocess.check_output([\"shred\", \"--help\"])\n        return True\n    except OSError as e:\n        if e.errno != errno.ENOENT:\n            raise\n        logging.error(\"The shred utility is missing.\")\n    except subprocess.CalledProcessError as e:\n        logging.error(\"The shred utility is broken: %s %s\", e, e.output)\n    return False",
        "file": "rm.py"
      }
    ],
    "request_that_secures_file_uploads.py": [
      {
        "type": "class",
        "name": "RequestThatSecuresFileUploads",
        "code": "class RequestThatSecuresFileUploads(wrappers.Request):\n    def _secure_file_stream(\n        self,\n        total_content_length: Optional[int],\n        content_type: Optional[str],\n        filename: Optional[str] = None,\n        content_length: Optional[int] = None,\n    ) -> IO[bytes]:\n        \"\"\"Storage class for data streamed in from requests.\n\n        If the data is relatively small (512KB), just store it in\n        memory. Otherwise, use the SecureTemporaryFile class to buffer\n        it on disk, encrypted with an ephemeral key to mitigate\n        forensic recovery of the plaintext.\n\n        \"\"\"\n        if total_content_length is None or total_content_length > 1024 * 512:\n            # We don't use `config.TEMP_DIR` here because that\n            # directory is exposed via X-Send-File and there is no\n            # reason for these files to be publicly accessible. See\n            # note in `config.py` for more info. Instead, we just use\n            # `/tmp`, which has the additional benefit of being\n            # automatically cleared on reboot.\n            return SecureTemporaryFile(\"/tmp\")  # noqa: S108\n        return BytesIO()\n\n    def make_form_data_parser(self) -> FormDataParser:\n        return self.form_data_parser_class(\n            self._secure_file_stream,\n            self.charset,\n            self.encoding_errors,\n            self.max_form_memory_size,\n            self.max_content_length,\n            self.parameter_storage_class,\n        )",
        "file": "request_that_secures_file_uploads.py"
      },
      {
        "type": "function",
        "name": "_secure_file_stream",
        "code": "def _secure_file_stream(\n        self,\n        total_content_length: Optional[int],\n        content_type: Optional[str],\n        filename: Optional[str] = None,\n        content_length: Optional[int] = None,\n    ) -> IO[bytes]:\n        \"\"\"Storage class for data streamed in from requests.\n\n        If the data is relatively small (512KB), just store it in\n        memory. Otherwise, use the SecureTemporaryFile class to buffer\n        it on disk, encrypted with an ephemeral key to mitigate\n        forensic recovery of the plaintext.\n\n        \"\"\"\n        if total_content_length is None or total_content_length > 1024 * 512:\n            # We don't use `config.TEMP_DIR` here because that\n            # directory is exposed via X-Send-File and there is no\n            # reason for these files to be publicly accessible. See\n            # note in `config.py` for more info. Instead, we just use\n            # `/tmp`, which has the additional benefit of being\n            # automatically cleared on reboot.\n            return SecureTemporaryFile(\"/tmp\")  # noqa: S108\n        return BytesIO()",
        "file": "request_that_secures_file_uploads.py"
      },
      {
        "type": "function",
        "name": "make_form_data_parser",
        "code": "def make_form_data_parser(self) -> FormDataParser:\n        return self.form_data_parser_class(\n            self._secure_file_stream,\n            self.charset,\n            self.encoding_errors,\n            self.max_form_memory_size,\n            self.max_content_length,\n            self.parameter_storage_class,\n        )",
        "file": "request_that_secures_file_uploads.py"
      }
    ],
    "passphrases.py": [
      {
        "type": "class",
        "name": "InvalidWordListError",
        "code": "class InvalidWordListError(Exception):\n    pass",
        "file": "passphrases.py"
      },
      {
        "type": "class",
        "name": "PassphraseGenerator",
        "code": "class PassphraseGenerator:\n    PASSPHRASE_WORDS_COUNT = 7\n\n    # Enforce a reasonable maximum length for passphrases to avoid DoS\n    MAX_PASSPHRASE_LENGTH = 128\n    MIN_PASSPHRASE_LENGTH = 20\n\n    _WORD_LIST_MINIMUM_SIZE = 7300  # Minimum number of words in any of the word lists\n\n    def __init__(\n        self, language_to_words: Dict[str, List[str]], fallback_language: str = \"en\"\n    ) -> None:\n        # SystemRandom sources from the system rand (e.g. urandom, CryptGenRandom, etc)\n        # It supplies a CSPRNG but with an interface that supports methods like choice\n        self._random_generator = SystemRandom()\n\n        self._fallback_language = fallback_language\n        self._language_to_words = language_to_words\n        if self._fallback_language not in self._language_to_words:\n            raise InvalidWordListError(\n                f\"Missing words list for fallback language '{self._fallback_language}'\"\n            )\n\n        # Validate each words list\n        for language, word_list in self._language_to_words.items():\n            # Ensure that there are enough words in the list\n            word_list_size = len(word_list)\n            if word_list_size < self._WORD_LIST_MINIMUM_SIZE:\n                raise InvalidWordListError(\n                    f\"The word list for language '{language}' only contains {word_list_size}\"\n                    \" long-enough words;\"\n                    f\" minimum required is {self._WORD_LIST_MINIMUM_SIZE} words.\"\n                )\n\n            # Ensure all words are ascii\n            try:\n                \" \".join(word_list).encode(\"ascii\")\n            except UnicodeEncodeError:\n                raise InvalidWordListError(\n                    \"The word list for language '{}' contains non-ASCII words.\"\n                )\n\n            # Ensure that passphrases longer than what's supported can't be generated\n            longest_word = max(word_list, key=len)\n            longest_passphrase_length = len(longest_word) * self.PASSPHRASE_WORDS_COUNT\n            longest_passphrase_length += self.PASSPHRASE_WORDS_COUNT  # One space between each word\n            if longest_passphrase_length >= self.MAX_PASSPHRASE_LENGTH:\n                raise InvalidWordListError(\n                    f\"Passphrases over the maximum length ({self.MAX_PASSPHRASE_LENGTH}) \"\n                    \"may be generated:\"\n                    f\" longest word in word list for language '{language}' is '{longest_word}' \"\n                    \"and number of words per\"\n                    f\" passphrase is {self.PASSPHRASE_WORDS_COUNT}\"\n                )\n\n            # Ensure that passphrases shorter than what's supported can't be generated\n            shortest_word = min(word_list, key=len)\n            shortest_passphrase_length = len(shortest_word) * self.PASSPHRASE_WORDS_COUNT\n            shortest_passphrase_length += self.PASSPHRASE_WORDS_COUNT\n            if shortest_passphrase_length <= self.MIN_PASSPHRASE_LENGTH:\n                raise InvalidWordListError(\n                    f\"Passphrases under the minimum length ({self.MIN_PASSPHRASE_LENGTH}) \"\n                    \"may be generated:\"\n                    f\" shortest word in word list for language '{language}' is '{shortest_word}' \"\n                    \"and number of words per\"\n                    f\" passphrase is {self.PASSPHRASE_WORDS_COUNT}\"\n                )\n\n    @classmethod\n    def get_default(cls) -> \"PassphraseGenerator\":\n        global _default_generator\n        if _default_generator is None:\n            config = SecureDropConfig.get_current()\n            language_to_words = _parse_available_words_list(config.SECUREDROP_ROOT)\n            _default_generator = cls(language_to_words)\n        return _default_generator\n\n    @property\n    def available_languages(self) -> Set[str]:\n        return set(self._language_to_words.keys())\n\n    def generate_passphrase(self, preferred_language: Optional[str] = None) -> DicewarePassphrase:\n        final_language = preferred_language if preferred_language else self._fallback_language\n        try:\n            words_list = self._language_to_words[final_language]\n        except KeyError:\n            # If there is no wordlist for the desired language, fall back to the word list for the\n            # default language\n            words_list = self._language_to_words[self._fallback_language]\n\n        words: List[str] = [\n            self._random_generator.choice(words_list) for _ in range(self.PASSPHRASE_WORDS_COUNT)\n        ]\n        return DicewarePassphrase(\" \".join(words))",
        "file": "passphrases.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(\n        self, language_to_words: Dict[str, List[str]], fallback_language: str = \"en\"\n    ) -> None:\n        # SystemRandom sources from the system rand (e.g. urandom, CryptGenRandom, etc)\n        # It supplies a CSPRNG but with an interface that supports methods like choice\n        self._random_generator = SystemRandom()\n\n        self._fallback_language = fallback_language\n        self._language_to_words = language_to_words\n        if self._fallback_language not in self._language_to_words:\n            raise InvalidWordListError(\n                f\"Missing words list for fallback language '{self._fallback_language}'\"\n            )\n\n        # Validate each words list\n        for language, word_list in self._language_to_words.items():\n            # Ensure that there are enough words in the list\n            word_list_size = len(word_list)\n            if word_list_size < self._WORD_LIST_MINIMUM_SIZE:\n                raise InvalidWordListError(\n                    f\"The word list for language '{language}' only contains {word_list_size}\"\n                    \" long-enough words;\"\n                    f\" minimum required is {self._WORD_LIST_MINIMUM_SIZE} words.\"\n                )\n\n            # Ensure all words are ascii\n            try:\n                \" \".join(word_list).encode(\"ascii\")\n            except UnicodeEncodeError:\n                raise InvalidWordListError(\n                    \"The word list for language '{}' contains non-ASCII words.\"\n                )\n\n            # Ensure that passphrases longer than what's supported can't be generated\n            longest_word = max(word_list, key=len)\n            longest_passphrase_length = len(longest_word) * self.PASSPHRASE_WORDS_COUNT\n            longest_passphrase_length += self.PASSPHRASE_WORDS_COUNT  # One space between each word\n            if longest_passphrase_length >= self.MAX_PASSPHRASE_LENGTH:\n                raise InvalidWordListError(\n                    f\"Passphrases over the maximum length ({self.MAX_PASSPHRASE_LENGTH}) \"\n                    \"may be generated:\"\n                    f\" longest word in word list for language '{language}' is '{longest_word}' \"\n                    \"and number of words per\"\n                    f\" passphrase is {self.PASSPHRASE_WORDS_COUNT}\"\n                )\n\n            # Ensure that passphrases shorter than what's supported can't be generated\n            shortest_word = min(word_list, key=len)\n            shortest_passphrase_length = len(shortest_word) * self.PASSPHRASE_WORDS_COUNT\n            shortest_passphrase_length += self.PASSPHRASE_WORDS_COUNT\n            if shortest_passphrase_length <= self.MIN_PASSPHRASE_LENGTH:\n                raise InvalidWordListError(\n                    f\"Passphrases under the minimum length ({self.MIN_PASSPHRASE_LENGTH}) \"\n                    \"may be generated:\"\n                    f\" shortest word in word list for language '{language}' is '{shortest_word}' \"\n                    \"and number of words per\"\n                    f\" passphrase is {self.PASSPHRASE_WORDS_COUNT}\"\n                )",
        "file": "passphrases.py"
      },
      {
        "type": "function",
        "name": "get_default",
        "code": "def get_default(cls) -> \"PassphraseGenerator\":\n        global _default_generator\n        if _default_generator is None:\n            config = SecureDropConfig.get_current()\n            language_to_words = _parse_available_words_list(config.SECUREDROP_ROOT)\n            _default_generator = cls(language_to_words)\n        return _default_generator",
        "file": "passphrases.py"
      },
      {
        "type": "function",
        "name": "available_languages",
        "code": "def available_languages(self) -> Set[str]:\n        return set(self._language_to_words.keys())",
        "file": "passphrases.py"
      },
      {
        "type": "function",
        "name": "generate_passphrase",
        "code": "def generate_passphrase(self, preferred_language: Optional[str] = None) -> DicewarePassphrase:\n        final_language = preferred_language if preferred_language else self._fallback_language\n        try:\n            words_list = self._language_to_words[final_language]\n        except KeyError:\n            # If there is no wordlist for the desired language, fall back to the word list for the\n            # default language\n            words_list = self._language_to_words[self._fallback_language]\n\n        words: List[str] = [\n            self._random_generator.choice(words_list) for _ in range(self.PASSPHRASE_WORDS_COUNT)\n        ]\n        return DicewarePassphrase(\" \".join(words))",
        "file": "passphrases.py"
      },
      {
        "type": "function",
        "name": "_parse_available_words_list",
        "code": "def _parse_available_words_list(securedrop_root: Path) -> Dict[str, List[str]]:\n    \"\"\"Find all .txt files in the wordlists folder and parse them as words lists.\n\n    This will also ignore words that are too short.\n    \"\"\"\n    language_to_words = {}\n    words_lists_folder = securedrop_root / \"wordlists\"\n    for words_file in words_lists_folder.glob(\"*.txt\"):\n        language = words_file.stem\n        all_words = words_file.read_text().strip().splitlines()\n        words_that_are_long_enough = [word for word in all_words if len(word) >= 2]\n        language_to_words[language] = words_that_are_long_enough\n    return language_to_words",
        "file": "passphrases.py"
      }
    ],
    "source_user.py": [
      {
        "type": "class",
        "name": "SourceUser",
        "code": "class SourceUser:\n    \"\"\"A source user and their associated data derived from their passphrase.\"\"\"\n\n    def __init__(self, db_record: models.Source, filesystem_id: str, gpg_secret: str) -> None:\n        self.gpg_secret = gpg_secret\n        self.filesystem_id = filesystem_id\n        self.db_record_id = db_record.id  # We don't store the actual record to force a refresh\n\n    def get_db_record(self) -> models.Source:\n        return models.Source.query.get(self.db_record_id)",
        "file": "source_user.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(self, db_record: models.Source, filesystem_id: str, gpg_secret: str) -> None:\n        self.gpg_secret = gpg_secret\n        self.filesystem_id = filesystem_id\n        self.db_record_id = db_record.id  # We don't store the actual record to force a refresh",
        "file": "source_user.py"
      },
      {
        "type": "function",
        "name": "get_db_record",
        "code": "def get_db_record(self) -> models.Source:\n        return models.Source.query.get(self.db_record_id)",
        "file": "source_user.py"
      },
      {
        "type": "class",
        "name": "InvalidPassphraseError",
        "code": "class InvalidPassphraseError(Exception):\n    pass",
        "file": "source_user.py"
      },
      {
        "type": "function",
        "name": "authenticate_source_user",
        "code": "def authenticate_source_user(\n    db_session: Session, supplied_passphrase: \"DicewarePassphrase\"\n) -> SourceUser:\n    \"\"\"Try to authenticate a Source user using the passphrase they supplied via the login form.\"\"\"\n    # Validate the passphrase: does it map to an actual Source record in the DB?\n    scrypt_manager = _SourceScryptManager.get_default()\n    source_filesystem_id = scrypt_manager.derive_source_filesystem_id(supplied_passphrase)\n    source_db_record = (\n        db_session.query(models.Source)\n        .filter_by(\n            filesystem_id=source_filesystem_id,\n            deleted_at=None,\n        )\n        .one_or_none()\n    )\n    if source_db_record is None:\n        raise InvalidPassphraseError()\n\n    source_gpg_secret = scrypt_manager.derive_source_gpg_secret(supplied_passphrase)\n    return SourceUser(source_db_record, source_filesystem_id, source_gpg_secret)",
        "file": "source_user.py"
      },
      {
        "type": "class",
        "name": "SourcePassphraseCollisionError",
        "code": "class SourcePassphraseCollisionError(Exception):\n    \"\"\"Tried to create a Source with a passphrase already used by another Source.\"\"\"",
        "file": "source_user.py"
      },
      {
        "type": "class",
        "name": "SourceDesignationCollisionError",
        "code": "class SourceDesignationCollisionError(Exception):\n    \"\"\"Tried to create a Source with a journalist designation already used by another Source.\"\"\"",
        "file": "source_user.py"
      },
      {
        "type": "function",
        "name": "create_source_user",
        "code": "def create_source_user(\n    db_session: Session,\n    source_passphrase: \"DicewarePassphrase\",\n    source_app_storage: \"Storage\",\n) -> SourceUser:\n    # Derive the source's info from their passphrase\n    scrypt_manager = _SourceScryptManager.get_default()\n    filesystem_id = scrypt_manager.derive_source_filesystem_id(source_passphrase)\n    gpg_secret = scrypt_manager.derive_source_gpg_secret(source_passphrase)\n\n    # Create a unique journalist designation for the source\n    # TODO: Add unique=True to models.Source.journalist_designation to enforce uniqueness\n    #  as the logic below has a race condition (time we check VS time when we add to the DB)\n    designation_generation_attempts = 0\n    valid_designation = None\n    designation_generator = _DesignationGenerator.get_default()\n    while designation_generation_attempts < 50:\n        # Generate a designation\n        designation_generation_attempts += 1\n        new_designation = designation_generator.generate_journalist_designation()\n\n        # Check to see if it's already used by an existing source\n        existing_source_with_same_designation = (\n            db_session.query(models.Source)\n            .filter_by(journalist_designation=new_designation)\n            .one_or_none()\n        )\n        if not existing_source_with_same_designation:\n            # The designation is not already used - good to go\n            valid_designation = new_designation\n            break\n\n    if not valid_designation:\n        # Could not generate a designation that is not already used\n        raise SourceDesignationCollisionError()\n\n    # Generate PGP keys\n    public_key, secret_key, fingerprint = redwood.generate_source_key_pair(\n        gpg_secret, filesystem_id\n    )\n\n    # Store the source in the DB\n    source_db_record = models.Source(\n        filesystem_id=filesystem_id,\n        journalist_designation=valid_designation,\n        public_key=public_key,\n        secret_key=secret_key,\n        fingerprint=fingerprint,\n    )\n    db_session.add(source_db_record)\n    try:\n        db_session.commit()\n    except IntegrityError:\n        db_session.rollback()\n        raise SourcePassphraseCollisionError(\n            f\"Passphrase already used by another Source (filesystem_id {filesystem_id})\"\n        )\n\n    # Create the source's folder\n    os.mkdir(source_app_storage.path(filesystem_id))\n\n    # All done\n    return SourceUser(source_db_record, filesystem_id, gpg_secret)",
        "file": "source_user.py"
      },
      {
        "type": "class",
        "name": "_SourceScryptManager",
        "code": "class _SourceScryptManager:\n    def __init__(\n        self,\n        salt_for_gpg_secret: bytes,\n        salt_for_filesystem_id: bytes,\n        scrypt_n: int,\n        scrypt_r: int,\n        scrypt_p: int,\n    ) -> None:\n        if salt_for_gpg_secret == salt_for_filesystem_id:\n            raise ValueError(\"scrypt_id_pepper == scrypt_gpg_pepper\")\n\n        self._salt_for_gpg_secret = salt_for_gpg_secret\n        self._salt_for_filesystem_id = salt_for_filesystem_id\n        self._scrypt_n = scrypt_n\n        self._scrypt_r = scrypt_r\n        self._scrypt_p = scrypt_p\n        self._backend = default_backend()\n\n    # Use @lru_cache to not recompute the same values over and over for the same user\n    @lru_cache  # noqa: B019\n    def derive_source_gpg_secret(self, source_passphrase: \"DicewarePassphrase\") -> str:\n        scrypt_for_gpg_secret = scrypt.Scrypt(\n            length=64,\n            salt=self._salt_for_gpg_secret,\n            n=self._scrypt_n,\n            r=self._scrypt_r,\n            p=self._scrypt_p,\n            backend=self._backend,\n        )\n        hashed_passphrase = scrypt_for_gpg_secret.derive(source_passphrase.encode(\"utf-8\"))\n        return b32encode(hashed_passphrase).decode(\"utf-8\")\n\n    @lru_cache  # noqa: B019\n    def derive_source_filesystem_id(self, source_passphrase: \"DicewarePassphrase\") -> str:\n        scrypt_for_filesystem_id = scrypt.Scrypt(\n            length=64,\n            salt=self._salt_for_filesystem_id,\n            n=self._scrypt_n,\n            r=self._scrypt_r,\n            p=self._scrypt_p,\n            backend=self._backend,\n        )\n        hashed_passphrase = scrypt_for_filesystem_id.derive(source_passphrase.encode(\"utf-8\"))\n        return b32encode(hashed_passphrase).decode(\"utf-8\")\n\n    @classmethod\n    def get_default(cls) -> \"_SourceScryptManager\":\n        global _default_scrypt_mgr\n        if _default_scrypt_mgr is None:\n            config = SecureDropConfig.get_current()\n            _default_scrypt_mgr = cls(\n                salt_for_gpg_secret=config.SCRYPT_GPG_PEPPER.encode(\"utf-8\"),\n                salt_for_filesystem_id=config.SCRYPT_ID_PEPPER.encode(\"utf-8\"),\n                scrypt_n=config.SCRYPT_PARAMS[\"N\"],\n                scrypt_r=config.SCRYPT_PARAMS[\"r\"],\n                scrypt_p=config.SCRYPT_PARAMS[\"p\"],\n            )\n        return _default_scrypt_mgr",
        "file": "source_user.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(\n        self,\n        salt_for_gpg_secret: bytes,\n        salt_for_filesystem_id: bytes,\n        scrypt_n: int,\n        scrypt_r: int,\n        scrypt_p: int,\n    ) -> None:\n        if salt_for_gpg_secret == salt_for_filesystem_id:\n            raise ValueError(\"scrypt_id_pepper == scrypt_gpg_pepper\")\n\n        self._salt_for_gpg_secret = salt_for_gpg_secret\n        self._salt_for_filesystem_id = salt_for_filesystem_id\n        self._scrypt_n = scrypt_n\n        self._scrypt_r = scrypt_r\n        self._scrypt_p = scrypt_p\n        self._backend = default_backend()",
        "file": "source_user.py"
      },
      {
        "type": "function",
        "name": "derive_source_gpg_secret",
        "code": "def derive_source_gpg_secret(self, source_passphrase: \"DicewarePassphrase\") -> str:\n        scrypt_for_gpg_secret = scrypt.Scrypt(\n            length=64,\n            salt=self._salt_for_gpg_secret,\n            n=self._scrypt_n,\n            r=self._scrypt_r,\n            p=self._scrypt_p,\n            backend=self._backend,\n        )\n        hashed_passphrase = scrypt_for_gpg_secret.derive(source_passphrase.encode(\"utf-8\"))\n        return b32encode(hashed_passphrase).decode(\"utf-8\")",
        "file": "source_user.py"
      },
      {
        "type": "function",
        "name": "derive_source_filesystem_id",
        "code": "def derive_source_filesystem_id(self, source_passphrase: \"DicewarePassphrase\") -> str:\n        scrypt_for_filesystem_id = scrypt.Scrypt(\n            length=64,\n            salt=self._salt_for_filesystem_id,\n            n=self._scrypt_n,\n            r=self._scrypt_r,\n            p=self._scrypt_p,\n            backend=self._backend,\n        )\n        hashed_passphrase = scrypt_for_filesystem_id.derive(source_passphrase.encode(\"utf-8\"))\n        return b32encode(hashed_passphrase).decode(\"utf-8\")",
        "file": "source_user.py"
      },
      {
        "type": "function",
        "name": "get_default",
        "code": "def get_default(cls) -> \"_SourceScryptManager\":\n        global _default_scrypt_mgr\n        if _default_scrypt_mgr is None:\n            config = SecureDropConfig.get_current()\n            _default_scrypt_mgr = cls(\n                salt_for_gpg_secret=config.SCRYPT_GPG_PEPPER.encode(\"utf-8\"),\n                salt_for_filesystem_id=config.SCRYPT_ID_PEPPER.encode(\"utf-8\"),\n                scrypt_n=config.SCRYPT_PARAMS[\"N\"],\n                scrypt_r=config.SCRYPT_PARAMS[\"r\"],\n                scrypt_p=config.SCRYPT_PARAMS[\"p\"],\n            )\n        return _default_scrypt_mgr",
        "file": "source_user.py"
      },
      {
        "type": "class",
        "name": "_DesignationGenerator",
        "code": "class _DesignationGenerator:\n    def __init__(self, nouns: List[str], adjectives: List[str]):\n        self._random_generator = SystemRandom()\n\n        # Ensure that there are no empty lists or empty strings\n        if not nouns:\n            raise ValueError(\"Nouns word list is empty\")\n        shortest_noun = min(nouns, key=len)\n        shortest_noun_length = len(shortest_noun)\n        if shortest_noun_length < 1:\n            raise ValueError(\"Nouns word list contains an empty string\")\n\n        if not adjectives:\n            raise ValueError(\"Adjectives word list is empty\")\n        shortest_adjective = min(adjectives, key=len)\n        shortest_adjective_length = len(shortest_adjective)\n        if shortest_adjective_length < 1:\n            raise ValueError(\"Adjectives word list contains an empty string\")\n\n        self._nouns = nouns\n        self._adjectives = adjectives\n\n    def generate_journalist_designation(self) -> str:\n        random_adjective = self._random_generator.choice(self._adjectives)\n        random_noun = self._random_generator.choice(self._nouns)\n        return f\"{random_adjective} {random_noun}\"\n\n    @classmethod\n    def get_default(cls) -> \"_DesignationGenerator\":\n        global _default_designation_generator\n        if _default_designation_generator is None:\n            config = SecureDropConfig.get_current()\n\n            # Parse the nouns and adjectives files from the config\n            nouns = Path(config.NOUNS).read_text().strip().splitlines()\n            adjectives = Path(config.ADJECTIVES).read_text().strip().splitlines()\n\n            # Create the generator\n            _default_designation_generator = cls(nouns=nouns, adjectives=adjectives)\n\n        return _default_designation_generator",
        "file": "source_user.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(self, nouns: List[str], adjectives: List[str]):\n        self._random_generator = SystemRandom()\n\n        # Ensure that there are no empty lists or empty strings\n        if not nouns:\n            raise ValueError(\"Nouns word list is empty\")\n        shortest_noun = min(nouns, key=len)\n        shortest_noun_length = len(shortest_noun)\n        if shortest_noun_length < 1:\n            raise ValueError(\"Nouns word list contains an empty string\")\n\n        if not adjectives:\n            raise ValueError(\"Adjectives word list is empty\")\n        shortest_adjective = min(adjectives, key=len)\n        shortest_adjective_length = len(shortest_adjective)\n        if shortest_adjective_length < 1:\n            raise ValueError(\"Adjectives word list contains an empty string\")\n\n        self._nouns = nouns\n        self._adjectives = adjectives",
        "file": "source_user.py"
      },
      {
        "type": "function",
        "name": "generate_journalist_designation",
        "code": "def generate_journalist_designation(self) -> str:\n        random_adjective = self._random_generator.choice(self._adjectives)\n        random_noun = self._random_generator.choice(self._nouns)\n        return f\"{random_adjective} {random_noun}\"",
        "file": "source_user.py"
      },
      {
        "type": "function",
        "name": "get_default",
        "code": "def get_default(cls) -> \"_DesignationGenerator\":\n        global _default_designation_generator\n        if _default_designation_generator is None:\n            config = SecureDropConfig.get_current()\n\n            # Parse the nouns and adjectives files from the config\n            nouns = Path(config.NOUNS).read_text().strip().splitlines()\n            adjectives = Path(config.ADJECTIVES).read_text().strip().splitlines()\n\n            # Create the generator\n            _default_designation_generator = cls(nouns=nouns, adjectives=adjectives)\n\n        return _default_designation_generator",
        "file": "source_user.py"
      }
    ],
    "manage.py": [
      {
        "type": "function",
        "name": "obtain_input",
        "code": "def obtain_input(text: str) -> str:\n    \"\"\"Wrapper for testability as suggested in\n    https://github.com/pytest-dev/pytest/issues/1598#issuecomment-224761877\"\"\"\n    return input(text)",
        "file": "manage.py"
      },
      {
        "type": "function",
        "name": "reset",
        "code": "def reset(\n    args: argparse.Namespace,\n    alembic_ini_path: Path = Path(\"alembic.ini\"),\n    context: Optional[AppContext] = None,\n) -> int:\n    \"\"\"Clears the SecureDrop development applications' state, restoring them to\n    the way they were immediately after running `setup_dev.sh`. This command:\n    1. Erases the development sqlite database file.\n    2. Regenerates the database.\n    3. Erases stored submissions and replies from the store dir.\n    \"\"\"\n    config = SecureDropConfig.get_current()\n\n    # Erase the development db file\n    if not hasattr(config, \"DATABASE_FILE\"):\n        raise Exception(\n            \"./manage.py doesn't know how to clear the db \" \"if the backend is not sqlite\"\n        )\n\n    # we need to save some data about the old DB file so we can recreate it\n    # with the same state\n    try:\n        stat_res = os.stat(config.DATABASE_FILE)\n        uid = stat_res.st_uid\n        gid = stat_res.st_gid\n    except OSError:\n        uid = os.getuid()\n        gid = os.getgid()\n\n    try:\n        os.remove(config.DATABASE_FILE)\n    except OSError:\n        pass\n\n    # Regenerate the database\n    # 1. Create it\n    subprocess.check_call([\"sqlite3\", config.DATABASE_FILE, \".databases\"])\n    # 2. Set permissions on it\n    os.chown(config.DATABASE_FILE, uid, gid)\n    os.chmod(config.DATABASE_FILE, 0o0640)\n\n    if os.environ.get(\"SECUREDROP_ENV\") == \"dev\":\n        # 3. Create the DB from the metadata directly when in 'dev' so\n        # developers can test application changes without first writing\n        # alembic migration.\n        with context or app_context():\n            db.create_all()\n    else:\n        # 3. Migrate it to 'head'\n        subprocess.check_call([\"alembic\", \"upgrade\", \"head\"], cwd=alembic_ini_path.parent)\n\n    # Clear submission/reply storage\n    try:\n        os.stat(args.store_dir)\n    except OSError:\n        pass\n    else:\n        for source_dir in os.listdir(args.store_dir):\n            try:\n                # Each entry in STORE_DIR is a directory corresponding\n                # to a source\n                shutil.rmtree(os.path.join(args.store_dir, source_dir))\n            except OSError:\n                pass\n    return 0",
        "file": "manage.py"
      },
      {
        "type": "function",
        "name": "add_admin",
        "code": "def add_admin(args: argparse.Namespace) -> int:\n    return _add_user(is_admin=True)",
        "file": "manage.py"
      },
      {
        "type": "function",
        "name": "add_journalist",
        "code": "def add_journalist(args: argparse.Namespace) -> int:\n    return _add_user()",
        "file": "manage.py"
      },
      {
        "type": "function",
        "name": "_get_username",
        "code": "def _get_username() -> str:\n    while True:\n        username = obtain_input(\"Username: \")\n        try:\n            Journalist.check_username_acceptable(username)\n        except InvalidUsernameException as e:\n            print(\"Invalid username: \" + str(e))\n        else:\n            return username",
        "file": "manage.py"
      },
      {
        "type": "function",
        "name": "_get_first_name",
        "code": "def _get_first_name() -> Optional[str]:\n    while True:\n        first_name = obtain_input(\"First name: \")\n        if not first_name:\n            return None\n        try:\n            Journalist.check_name_acceptable(first_name)\n            return first_name\n        except FirstOrLastNameError as e:\n            print(\"Invalid name: \" + str(e))",
        "file": "manage.py"
      },
      {
        "type": "function",
        "name": "_get_last_name",
        "code": "def _get_last_name() -> Optional[str]:\n    while True:\n        last_name = obtain_input(\"Last name: \")\n        if not last_name:\n            return None\n        try:\n            Journalist.check_name_acceptable(last_name)\n            return last_name\n        except FirstOrLastNameError as e:\n            print(\"Invalid name: \" + str(e))",
        "file": "manage.py"
      },
      {
        "type": "function",
        "name": "_get_yubikey_usage",
        "code": "def _get_yubikey_usage() -> bool:\n    \"\"\"Function used to allow for test suite mocking\"\"\"\n    while True:\n        answer = (\n            obtain_input(\"Will this user be using a YubiKey [HOTP]? \" \"(y/N): \").lower().strip()\n        )\n        if answer in (\"y\", \"yes\"):\n            return True\n        elif answer in (\"\", \"n\", \"no\"):\n            return False\n        else:\n            print('Invalid answer. Please type \"y\" or \"n\"')",
        "file": "manage.py"
      },
      {
        "type": "function",
        "name": "_add_user",
        "code": "def _add_user(is_admin: bool = False, context: Optional[AppContext] = None) -> int:\n    with context or app_context():\n        username = _get_username()\n        first_name = _get_first_name()\n        last_name = _get_last_name()\n\n        print(\"Note: Passwords are now autogenerated.\")\n        password = PassphraseGenerator.get_default().generate_passphrase()\n        print(f\"This user's password is: {password}\")\n\n        is_hotp = _get_yubikey_usage()\n        otp_secret = None\n        if is_hotp:\n            while True:\n                otp_secret = obtain_input(\n                    \"Please configure this user's YubiKey and enter the \" \"secret: \"\n                )\n                if otp_secret:\n                    tmp_str = otp_secret.replace(\" \", \"\")\n                    if len(tmp_str) != 40:\n                        print(\n                            \"The length of the secret is not correct. \"\n                            f\"Expected 40 characters, but received {len(tmp_str)}. \"\n                            \"Try again.\"\n                        )\n                        continue\n                if otp_secret:\n                    break\n\n        try:\n            user = Journalist(\n                username=username,\n                first_name=first_name,\n                last_name=last_name,\n                password=password,\n                is_admin=is_admin,\n                otp_secret=otp_secret,\n            )\n            db.session.add(user)\n            db.session.commit()\n        except Exception as exc:\n            db.session.rollback()\n            if \"UNIQUE constraint failed: journalists.username\" in str(exc):\n                print(\"ERROR: That username is already taken!\")\n            else:\n                exc_type, exc_value, exc_traceback = sys.exc_info()\n                print(repr(traceback.format_exception(exc_type, exc_value, exc_traceback)))\n            return 1\n        else:\n            print(f'User \"{username}\" successfully added')\n            if not otp_secret:\n                # Print the QR code for FreeOTP\n                print(\"\\nScan the QR code below with FreeOTP:\\n\")\n                uri = user.totp.get_provisioning_uri(username)\n                qr = qrcode.QRCode()\n                qr.add_data(uri)\n                qr.print_ascii(tty=sys.stdout.isatty())\n                print(\n                    \"\\nIf the barcode does not render correctly, try \"\n                    \"changing your terminal's font (Monospace for Linux, \"\n                    \"Menlo for OS X). If you are using iTerm on Mac OS X, \"\n                    'you will need to change the \"Non-ASCII Font\", which '\n                    \"is your profile's Text settings.\\n\\nCan't scan the \"\n                    \"barcode? Enter following shared secret manually:\"\n                    f\"\\n{user.formatted_otp_secret}\\n\"\n                )\n        return 0",
        "file": "manage.py"
      },
      {
        "type": "function",
        "name": "_get_username_to_delete",
        "code": "def _get_username_to_delete() -> str:\n    return obtain_input(\"Username to delete: \")",
        "file": "manage.py"
      },
      {
        "type": "function",
        "name": "_get_delete_confirmation",
        "code": "def _get_delete_confirmation(username: str) -> bool:\n    confirmation = obtain_input(\"Are you sure you want to delete user \" f'\"{username}\" (y/n)?')\n    if confirmation.lower() != \"y\":\n        print(f'Confirmation not received: user \"{username}\" was NOT ' \"deleted\")\n        return False\n    return True",
        "file": "manage.py"
      },
      {
        "type": "function",
        "name": "delete_user",
        "code": "def delete_user(args: argparse.Namespace, context: Optional[AppContext] = None) -> int:\n    \"\"\"Deletes a journalist or admin from the application.\"\"\"\n    with context or app_context():\n        username = _get_username_to_delete()\n        try:\n            selected_user = Journalist.query.filter_by(username=username).one()\n        except NoResultFound:\n            print(\"ERROR: That user was not found!\")\n            return 0\n\n        # Confirm deletion if user is found\n        if not _get_delete_confirmation(selected_user.username):\n            return 0\n\n        # Try to delete user from the database\n        try:\n            db.session.delete(selected_user)\n            db.session.commit()\n        except Exception as e:\n            # If the user was deleted between the user selection and\n            # confirmation, (e.g., through the web app), we don't report any\n            # errors. If the user is still there, but there was a error\n            # deleting them from the database, we do report it.\n            try:\n                Journalist.query.filter_by(username=username).one()\n            except NoResultFound:\n                pass\n            else:\n                raise e\n\n        print(f'User \"{username}\" successfully deleted')\n    return 0",
        "file": "manage.py"
      },
      {
        "type": "function",
        "name": "clean_tmp",
        "code": "def clean_tmp(args: argparse.Namespace) -> int:\n    \"\"\"Cleanup the SecureDrop temp directory.\"\"\"\n    if not os.path.exists(args.directory):\n        log.debug(f\"{args.directory} does not exist, do nothing\")\n        return 0\n\n    def listdir_fullpath(d: str) -> List[str]:\n        return [os.path.join(d, f) for f in os.listdir(d)]\n\n    too_old = args.days * 24 * 60 * 60\n    for path in listdir_fullpath(args.directory):\n        if time.time() - os.stat(path).st_mtime > too_old:\n            os.remove(path)\n            log.debug(f\"{path} removed\")\n        else:\n            log.debug(f\"{path} modified less than {args.days} days ago\")\n\n    return 0",
        "file": "manage.py"
      },
      {
        "type": "function",
        "name": "listdir_fullpath",
        "code": "def listdir_fullpath(d: str) -> List[str]:\n        return [os.path.join(d, f) for f in os.listdir(d)]",
        "file": "manage.py"
      },
      {
        "type": "function",
        "name": "init_db",
        "code": "def init_db(args: argparse.Namespace) -> None:\n    config = SecureDropConfig.get_current()\n    user = pwd.getpwnam(args.user)\n    subprocess.check_call([\"sqlite3\", config.DATABASE_FILE, \".databases\"])\n    os.chown(config.DATABASE_FILE, user.pw_uid, user.pw_gid)\n    os.chmod(config.DATABASE_FILE, 0o0640)\n    subprocess.check_call([\"alembic\", \"upgrade\", \"head\"])",
        "file": "manage.py"
      },
      {
        "type": "function",
        "name": "get_args",
        "code": "def get_args() -> argparse.ArgumentParser:\n    config = SecureDropConfig.get_current()\n    parser = argparse.ArgumentParser(\n        prog=__file__, description=\"Management \" \"and testing utility for SecureDrop.\"\n    )\n    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\")\n    parser.add_argument(\n        \"--data-root\",\n        default=config.SECUREDROP_DATA_ROOT,\n        help=(\"directory in which the securedrop \" \"data is stored\"),\n    )\n    parser.add_argument(\n        \"--store-dir\",\n        default=config.STORE_DIR,\n        help=(\"directory in which the files are stored\"),\n    )\n    subps = parser.add_subparsers()\n    # Add/remove journalists + admins\n    admin_subp = subps.add_parser(\"add-admin\", help=\"Add an admin to the \" \"application.\")\n    admin_subp.set_defaults(func=add_admin)\n    admin_subp_a = subps.add_parser(\"add_admin\", help=\"^\")\n    admin_subp_a.set_defaults(func=add_admin)\n    journalist_subp = subps.add_parser(\n        \"add-journalist\", help=\"Add a \" \"journalist to the application.\"\n    )\n    journalist_subp.set_defaults(func=add_journalist)\n    journalist_subp_a = subps.add_parser(\"add_journalist\", help=\"^\")\n    journalist_subp_a.set_defaults(func=add_journalist)\n    delete_user_subp = subps.add_parser(\n        \"delete-user\", help=\"Delete a user \" \"from the application.\"\n    )\n    delete_user_subp.set_defaults(func=delete_user)\n    delete_user_subp_a = subps.add_parser(\"delete_user\", help=\"^\")\n    delete_user_subp_a.set_defaults(func=delete_user)\n\n    remove_pending_sources_subp = subps.add_parser(\n        \"remove-pending-sources\", help=\"Remove pending sources from the server.\"\n    )\n    remove_pending_sources_subp.add_argument(\n        \"--keep-most-recent\",\n        default=100,\n        type=int,\n        help=\"how many of the most recent pending sources to keep\",\n    )\n    remove_pending_sources_subp.set_defaults(func=remove_pending_sources)\n\n    add_check_db_disconnect_parser(subps)\n    add_check_fs_disconnect_parser(subps)\n    add_delete_db_disconnect_parser(subps)\n    add_delete_fs_disconnect_parser(subps)\n    add_list_db_disconnect_parser(subps)\n    add_list_fs_disconnect_parser(subps)\n\n    # Cleanup the SD temp dir\n    set_clean_tmp_parser(subps, \"clean-tmp\")\n    set_clean_tmp_parser(subps, \"clean_tmp\")\n\n    init_db_subp = subps.add_parser(\"init-db\", help=\"Initialize the database.\\n\")\n    init_db_subp.add_argument(\"-u\", \"--user\", help=\"Unix user for the DB\", required=True)\n    init_db_subp.set_defaults(func=init_db)\n\n    add_were_there_submissions_today(subps)\n\n    # Run WSGI app\n    run_subp = subps.add_parser(\n        \"run\",\n        help=\"DANGER!!! ONLY FOR DEVELOPMENT \"\n        \"USE. DO NOT USE IN PRODUCTION. Run the \"\n        \"Werkzeug source and journalist WSGI apps.\\n\",\n    )\n    run_subp.set_defaults(func=run)\n\n    # Reset application state\n    reset_subp = subps.add_parser(\n        \"reset\",\n        help=\"DANGER!!! ONLY FOR DEVELOPMENT \"\n        \"USE. DO NOT USE IN PRODUCTION. Clear the \"\n        \"SecureDrop application's state.\\n\",\n    )\n    reset_subp.set_defaults(func=reset)\n    return parser",
        "file": "manage.py"
      },
      {
        "type": "function",
        "name": "set_clean_tmp_parser",
        "code": "def set_clean_tmp_parser(subps: _SubParsersAction, name: str) -> None:\n    config = SecureDropConfig.get_current()\n\n    parser = subps.add_parser(name, help=\"Cleanup the \" \"SecureDrop temp directory.\")\n    default_days = 7\n    parser.add_argument(\n        \"--days\",\n        default=default_days,\n        type=int,\n        help=(\n            \"remove files not modified in a given number of DAYS \" f\"(default {default_days} days)\"\n        ),\n    )\n    parser.add_argument(\n        \"--directory\",\n        default=config.TEMP_DIR,\n        help=(\"remove old files from DIRECTORY \" f\"(default {config.TEMP_DIR})\"),\n    )\n    parser.set_defaults(func=clean_tmp)",
        "file": "manage.py"
      },
      {
        "type": "function",
        "name": "setup_verbosity",
        "code": "def setup_verbosity(args: argparse.Namespace) -> None:\n    if args.verbose:\n        logging.getLogger(__name__).setLevel(logging.DEBUG)\n    else:\n        logging.getLogger(__name__).setLevel(logging.INFO)",
        "file": "manage.py"
      },
      {
        "type": "function",
        "name": "_run_from_commandline",
        "code": "def _run_from_commandline() -> None:  # pragma: no cover\n    try:\n        parser = get_args()\n        args = parser.parse_args()\n        setup_verbosity(args)\n        try:\n            rc = args.func(args)\n            sys.exit(rc)\n        except AttributeError:\n            parser.print_help()\n            parser.exit()\n    except KeyboardInterrupt:\n        sys.exit(signal.SIGINT)",
        "file": "manage.py"
      }
    ],
    "two_factor.py": [
      {
        "type": "function",
        "name": "random_base32",
        "code": "def random_base32(length: int = 32) -> str:\n    if length < 32:\n        raise ValueError(\"Secrets should be at least 160 bits\")\n\n    chars_to_use = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ234567\")\n    return \"\".join(secrets.choice(chars_to_use) for _ in range(length))",
        "file": "two_factor.py"
      },
      {
        "type": "function",
        "name": "format_secret",
        "code": "def format_secret(sec: str) -> str:\n    \"\"\"The OTP secret is easier to read and manually enter if it is all\n    lowercase and split into four groups of four characters. The secret is\n    base32-encoded, so it is case insensitive.\"\"\"\n    chunks = [sec[i : i + 4] for i in range(0, len(sec), 4)]\n    return \" \".join(chunks).lower()",
        "file": "two_factor.py"
      },
      {
        "type": "class",
        "name": "OtpSecretInvalid",
        "code": "class OtpSecretInvalid(Exception):\n    \"\"\"Raised when a user's OTP secret is invalid - for example, too short\"\"\"",
        "file": "two_factor.py"
      },
      {
        "type": "class",
        "name": "OtpTokenInvalid",
        "code": "class OtpTokenInvalid(Exception):\n    pass",
        "file": "two_factor.py"
      },
      {
        "type": "class",
        "name": "HOTP",
        "code": "class HOTP:\n    # Current parameters for HOTP\n    _LENGTH = 6\n\n    # nosemgrep: python.cryptography.security.insecure-hash-algorithms.insecure-hash-algorithm-sha1\n    _ALGORITHM = SHA1()  # noqa: S303\n\n    _LOOK_AHEAD_WINDOW_SIZE = 20\n\n    _SECRET_BASE32_LENGTH = 32  # 160 bits == 40 hex digits (== 32 ascii-encoded chars in db)\n    SECRET_HEX_LENGTH = 40  # Required length for hex-format HOTP secrets as input by users\n\n    def __init__(self, secret_as_base32: str) -> None:\n        if len(secret_as_base32) != self._SECRET_BASE32_LENGTH:\n            raise OtpSecretInvalid(\"Invalid secret length\")\n\n        try:\n            otp_secret_as_bytes = base64.b32decode(\n                # Need casefold=True because the base32 secret we receive from the UI might be\n                # lowercase\n                secret_as_base32,\n                casefold=True,\n            )\n        except binascii.Error:\n            raise OtpSecretInvalid(\"Secret is not base32-encoded\")\n\n        self._hotp = hotp.HOTP(\n            key=otp_secret_as_bytes,\n            length=self._LENGTH,\n            algorithm=self._ALGORITHM,\n        )\n\n    def generate(self, counter: int) -> str:\n        return self._hotp.generate(counter).decode(\"ascii\")\n\n    def verify(self, token: str, counter: int) -> int:\n        \"\"\"Validate an HOTP-generated token and return the counter value that succeeded.\"\"\"\n        counter_value_that_succeeded: Optional[int] = None\n        for counter_value in range(counter, counter + self._LOOK_AHEAD_WINDOW_SIZE):\n            try:\n                self._hotp.verify(token.encode(\"ascii\"), counter_value)\n                counter_value_that_succeeded = counter_value\n                break\n            except InvalidToken:\n                pass\n\n        if counter_value_that_succeeded is None:\n            raise OtpTokenInvalid(\"Token verification failed\")\n\n        return counter_value_that_succeeded",
        "file": "two_factor.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(self, secret_as_base32: str) -> None:\n        if len(secret_as_base32) != self._SECRET_BASE32_LENGTH:\n            raise OtpSecretInvalid(\"Invalid secret length\")\n\n        try:\n            otp_secret_as_bytes = base64.b32decode(\n                # Need casefold=True because the base32 secret we receive from the UI might be\n                # lowercase\n                secret_as_base32,\n                casefold=True,\n            )\n        except binascii.Error:\n            raise OtpSecretInvalid(\"Secret is not base32-encoded\")\n\n        self._hotp = hotp.HOTP(\n            key=otp_secret_as_bytes,\n            length=self._LENGTH,\n            algorithm=self._ALGORITHM,\n        )",
        "file": "two_factor.py"
      },
      {
        "type": "function",
        "name": "generate",
        "code": "def generate(self, counter: int) -> str:\n        return self._hotp.generate(counter).decode(\"ascii\")",
        "file": "two_factor.py"
      },
      {
        "type": "function",
        "name": "verify",
        "code": "def verify(self, token: str, counter: int) -> int:\n        \"\"\"Validate an HOTP-generated token and return the counter value that succeeded.\"\"\"\n        counter_value_that_succeeded: Optional[int] = None\n        for counter_value in range(counter, counter + self._LOOK_AHEAD_WINDOW_SIZE):\n            try:\n                self._hotp.verify(token.encode(\"ascii\"), counter_value)\n                counter_value_that_succeeded = counter_value\n                break\n            except InvalidToken:\n                pass\n\n        if counter_value_that_succeeded is None:\n            raise OtpTokenInvalid(\"Token verification failed\")\n\n        return counter_value_that_succeeded",
        "file": "two_factor.py"
      },
      {
        "type": "class",
        "name": "TOTP",
        "code": "class TOTP:\n    # Current parameters for TOTP\n    _LENGTH = 6\n    _TIME_STEP = 30\n\n    # nosemgrep: python.cryptography.security.insecure-hash-algorithms.insecure-hash-algorithm-sha1\n    _ALGORITHM = SHA1()  # noqa: S303\n\n    # Minimum length for ascii-encoded OTP secrets - by default, secrets are now 160-bit (32 chars)\n    # but existing Journalist users may still have 80-bit (16-char) secrets\n    _SECRET_MIN_BASE32_LENGTH = 16  # 80 bits == 40 hex digits (== 16 ascii-encoded chars in db)\n\n    def __init__(self, secret_as_base32: str) -> None:\n        if len(secret_as_base32) < self._SECRET_MIN_BASE32_LENGTH:\n            raise OtpSecretInvalid(\"Invalid secret length\")\n\n        try:\n            otp_secret_as_bytes = base64.b32decode(\n                # Need casefold=True because the base32 secret we receive from the UI might be\n                # lowercase\n                secret_as_base32,\n                casefold=True,\n            )\n        except binascii.Error:\n            raise OtpSecretInvalid(\"Secret is not base32-encoded\")\n\n        self._totp = totp.TOTP(\n            key=otp_secret_as_bytes,\n            length=self._LENGTH,\n            algorithm=self._ALGORITHM,\n            time_step=self._TIME_STEP,\n            # Existing Journalist users may still have 80-bit (16-char) secrets\n            enforce_key_length=False,\n        )\n\n    def generate(self, time: datetime) -> str:\n        return self._totp.generate(time.timestamp()).decode(\"ascii\")\n\n    def now(self) -> str:\n        return self._totp.generate(datetime.utcnow().timestamp()).decode(\"ascii\")\n\n    def verify(self, token: str, time: datetime) -> None:\n        # Also check the given token against the previous and next valid tokens, to compensate\n        # for potential time skew between the client and the server. The total valid window is 1:30s\n        token_verification_succeeded = False\n        for index_for_time_skew in [-1, 0, 1]:\n            time_for_time_skew = int(time.timestamp()) + self._TIME_STEP * index_for_time_skew\n            try:\n                self._totp.verify(token.encode(\"ascii\"), time_for_time_skew)\n                token_verification_succeeded = True\n                break\n            except InvalidToken:\n                pass\n\n        if not token_verification_succeeded:\n            raise OtpTokenInvalid(\"Token verification failed\")\n\n    def get_provisioning_uri(self, account_name: str) -> str:\n        return self._totp.get_provisioning_uri(account_name=account_name, issuer=\"SecureDrop\")\n\n    def qrcode_svg(self, account_name: str) -> bytes:\n        uri = self.get_provisioning_uri(account_name)\n\n        qr = qrcode.QRCode(box_size=15, image_factory=qrcode.image.svg.SvgPathImage)\n        qr.add_data(uri)\n        img = qr.make_image()\n\n        svg_out = BytesIO()\n        img.save(svg_out)\n        return svg_out.getvalue()",
        "file": "two_factor.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(self, secret_as_base32: str) -> None:\n        if len(secret_as_base32) < self._SECRET_MIN_BASE32_LENGTH:\n            raise OtpSecretInvalid(\"Invalid secret length\")\n\n        try:\n            otp_secret_as_bytes = base64.b32decode(\n                # Need casefold=True because the base32 secret we receive from the UI might be\n                # lowercase\n                secret_as_base32,\n                casefold=True,\n            )\n        except binascii.Error:\n            raise OtpSecretInvalid(\"Secret is not base32-encoded\")\n\n        self._totp = totp.TOTP(\n            key=otp_secret_as_bytes,\n            length=self._LENGTH,\n            algorithm=self._ALGORITHM,\n            time_step=self._TIME_STEP,\n            # Existing Journalist users may still have 80-bit (16-char) secrets\n            enforce_key_length=False,\n        )",
        "file": "two_factor.py"
      },
      {
        "type": "function",
        "name": "generate",
        "code": "def generate(self, time: datetime) -> str:\n        return self._totp.generate(time.timestamp()).decode(\"ascii\")",
        "file": "two_factor.py"
      },
      {
        "type": "function",
        "name": "now",
        "code": "def now(self) -> str:\n        return self._totp.generate(datetime.utcnow().timestamp()).decode(\"ascii\")",
        "file": "two_factor.py"
      },
      {
        "type": "function",
        "name": "verify",
        "code": "def verify(self, token: str, time: datetime) -> None:\n        # Also check the given token against the previous and next valid tokens, to compensate\n        # for potential time skew between the client and the server. The total valid window is 1:30s\n        token_verification_succeeded = False\n        for index_for_time_skew in [-1, 0, 1]:\n            time_for_time_skew = int(time.timestamp()) + self._TIME_STEP * index_for_time_skew\n            try:\n                self._totp.verify(token.encode(\"ascii\"), time_for_time_skew)\n                token_verification_succeeded = True\n                break\n            except InvalidToken:\n                pass\n\n        if not token_verification_succeeded:\n            raise OtpTokenInvalid(\"Token verification failed\")",
        "file": "two_factor.py"
      },
      {
        "type": "function",
        "name": "get_provisioning_uri",
        "code": "def get_provisioning_uri(self, account_name: str) -> str:\n        return self._totp.get_provisioning_uri(account_name=account_name, issuer=\"SecureDrop\")",
        "file": "two_factor.py"
      },
      {
        "type": "function",
        "name": "qrcode_svg",
        "code": "def qrcode_svg(self, account_name: str) -> bytes:\n        uri = self.get_provisioning_uri(account_name)\n\n        qr = qrcode.QRCode(box_size=15, image_factory=qrcode.image.svg.SvgPathImage)\n        qr.add_data(uri)\n        img = qr.make_image()\n\n        svg_out = BytesIO()\n        img.save(svg_out)\n        return svg_out.getvalue()",
        "file": "two_factor.py"
      }
    ],
    "encryption.py": [
      {
        "type": "class",
        "name": "GpgKeyNotFoundError",
        "code": "class GpgKeyNotFoundError(Exception):\n    pass",
        "file": "encryption.py"
      },
      {
        "type": "class",
        "name": "GpgEncryptError",
        "code": "class GpgEncryptError(Exception):\n    pass",
        "file": "encryption.py"
      },
      {
        "type": "class",
        "name": "GpgDecryptError",
        "code": "class GpgDecryptError(Exception):\n    pass",
        "file": "encryption.py"
      },
      {
        "type": "class",
        "name": "EncryptionManager",
        "code": "class EncryptionManager:\n    \"\"\"EncryptionManager provides a high-level interface for each PGP operation we do\"\"\"\n\n    REDIS_FINGERPRINT_HASH = \"sd/crypto-util/fingerprints\"\n    REDIS_KEY_HASH = \"sd/crypto-util/keys\"\n\n    SOURCE_KEY_UID_RE = re.compile(r\"(Source|Autogenerated) Key <([-A-Za-z0-9+/=_]+)>\")\n\n    def __init__(self, gpg_key_dir: Path, journalist_pub_key: Path, redis: Redis) -> None:\n        self._gpg_key_dir = gpg_key_dir\n        self.journalist_pub_key = journalist_pub_key\n        if not self.journalist_pub_key.exists():\n            raise RuntimeError(\n                f\"The journalist public key does not exist at {self.journalist_pub_key}\"\n            )\n        self._redis = redis\n\n        # Instantiate the \"main\" GPG binary\n        self._gpg = None\n\n        # Instantiate the GPG binary to be used for key deletion: always delete keys without\n        # invoking pinentry-mode=loopback\n        # see: https://lists.gnupg.org/pipermail/gnupg-users/2016-May/055965.html\n        self._gpg_for_key_deletion = None\n\n    def gpg(self, for_deletion: Optional[bool] = False) -> gnupg.GPG:\n        if for_deletion:\n            if self._gpg_for_key_deletion is None:\n                # GPG binary to be used for key deletion: always delete keys without\n                # invoking pinentry-mode=loopback\n                # see: https://lists.gnupg.org/pipermail/gnupg-users/2016-May/055965.html\n                self._gpg_for_key_deletion = gnupg.GPG(\n                    binary=\"gpg2\",\n                    homedir=str(self._gpg_key_dir),\n                    options=[\"--yes\", \"--trust-model direct\"],\n                )\n            return self._gpg_for_key_deletion\n        else:\n            if self._gpg is None:\n                self._gpg = gnupg.GPG(\n                    binary=\"gpg2\",\n                    homedir=str(self._gpg_key_dir),\n                    options=[\"--pinentry-mode loopback\", \"--trust-model direct\"],\n                )\n            return self._gpg\n\n    @classmethod\n    def get_default(cls) -> \"EncryptionManager\":\n        global _default_encryption_mgr\n        if _default_encryption_mgr is None:\n            config = SecureDropConfig.get_current()\n            _default_encryption_mgr = cls(\n                gpg_key_dir=config.GPG_KEY_DIR,\n                journalist_pub_key=(config.SECUREDROP_DATA_ROOT / \"journalist.pub\"),\n                redis=Redis(decode_responses=True, **config.REDIS_KWARGS),\n            )\n        return _default_encryption_mgr\n\n    def delete_source_key_pair(self, source_filesystem_id: str) -> None:\n        \"\"\"\n        Try to delete the source's key from the filesystem.  If it's not found, either:\n        (a) it doesn't exist or\n        (b) the source is Sequoia-based and has its key stored in Source.pgp_public_key,\n            which will be deleted when the Source instance itself is deleted.\n        \"\"\"\n        try:\n            source_key_fingerprint = self.get_source_key_fingerprint(source_filesystem_id)\n        except GpgKeyNotFoundError:\n            # If the source is entirely Sequoia-based, there is nothing to delete\n            return\n\n        # The subkeys keyword argument deletes both secret and public keys\n        self.gpg(for_deletion=True).delete_keys(source_key_fingerprint, secret=True, subkeys=True)\n\n        self._redis.hdel(self.REDIS_KEY_HASH, source_key_fingerprint)\n        self._redis.hdel(self.REDIS_FINGERPRINT_HASH, source_filesystem_id)\n\n    def get_journalist_public_key(self) -> str:\n        return self.journalist_pub_key.read_text()\n\n    def get_source_public_key(self, source_filesystem_id: str) -> str:\n        source_key_fingerprint = self.get_source_key_fingerprint(source_filesystem_id)\n        return self._get_public_key(source_key_fingerprint)\n\n    def get_source_key_fingerprint(self, source_filesystem_id: str) -> str:\n        source_key_fingerprint = self._redis.hget(self.REDIS_FINGERPRINT_HASH, source_filesystem_id)\n        if source_key_fingerprint:\n            return source_key_fingerprint\n\n        # If the fingerprint was not in Redis, get it directly from GPG\n        source_key_details = self._get_source_key_details(source_filesystem_id)\n        source_key_fingerprint = source_key_details[\"fingerprint\"]\n        self._save_key_fingerprint_to_redis(source_filesystem_id, source_key_fingerprint)\n        return source_key_fingerprint\n\n    def get_source_secret_key_from_gpg(self, fingerprint: str, passphrase: str) -> str:\n        secret_key = self.gpg().export_keys(fingerprint, secret=True, passphrase=passphrase)\n        if not secret_key:\n            raise GpgKeyNotFoundError()\n        # Verify the secret key we got can be read and decrypted by redwood\n        try:\n            actual_fingerprint = redwood.is_valid_secret_key(secret_key, passphrase)\n        except redwood.RedwoodError:\n            # Either Sequoia can't extract the secret key or the passphrase\n            # is incorrect.\n            raise GpgKeyNotFoundError()\n        if fingerprint != actual_fingerprint:\n            # Somehow we exported the wrong key?\n            raise GpgKeyNotFoundError()\n        return secret_key\n\n    def encrypt_source_message(self, message_in: str, encrypted_message_path_out: Path) -> None:\n        redwood.encrypt_message(\n            # A submission is only encrypted for the journalist key\n            recipients=[self.get_journalist_public_key()],\n            plaintext=message_in,\n            destination=encrypted_message_path_out,\n        )\n\n    def encrypt_source_file(self, file_in: BinaryIO, encrypted_file_path_out: Path) -> None:\n        redwood.encrypt_stream(\n            # A submission is only encrypted for the journalist key\n            recipients=[self.get_journalist_public_key()],\n            plaintext=file_in,\n            destination=encrypted_file_path_out,\n        )\n\n    def encrypt_journalist_reply(\n        self, for_source: \"Source\", reply_in: str, encrypted_reply_path_out: Path\n    ) -> None:\n        redwood.encrypt_message(\n            # A reply is encrypted for both the journalist key and the source key\n            recipients=[for_source.public_key, self.get_journalist_public_key()],\n            plaintext=reply_in,\n            destination=encrypted_reply_path_out,\n        )\n\n    def decrypt_journalist_reply(self, for_source_user: \"SourceUser\", ciphertext_in: bytes) -> str:\n        \"\"\"Decrypt a reply sent by a journalist.\"\"\"\n        # TODO: Avoid making a database query here\n        for_source = for_source_user.get_db_record()\n        if for_source.pgp_secret_key is not None:\n            return redwood.decrypt(\n                ciphertext_in,\n                secret_key=for_source.pgp_secret_key,\n                passphrase=for_source_user.gpg_secret,\n            ).decode()\n        # In practice this should be uncreachable unless the Sequoia secret key migration failed\n        ciphertext_as_stream = BytesIO(ciphertext_in)\n        out = self.gpg().decrypt_file(ciphertext_as_stream, passphrase=for_source_user.gpg_secret)\n        if not out.ok:\n            raise GpgDecryptError(out.stderr)\n\n        return out.data.decode(\"utf-8\")\n\n    def _get_source_key_details(self, source_filesystem_id: str) -> Dict[str, str]:\n        for key in self.gpg().list_keys():\n            for uid in key[\"uids\"]:\n                if source_filesystem_id in uid and self.SOURCE_KEY_UID_RE.match(uid):\n                    return key\n        raise GpgKeyNotFoundError()\n\n    def _save_key_fingerprint_to_redis(\n        self, source_filesystem_id: str, source_key_fingerprint: str\n    ) -> None:\n        self._redis.hset(self.REDIS_FINGERPRINT_HASH, source_filesystem_id, source_key_fingerprint)\n\n    def _get_public_key(self, key_fingerprint: str) -> str:\n        # First try to fetch the public key from Redis\n        public_key = self._redis.hget(self.REDIS_KEY_HASH, key_fingerprint)\n        if public_key:\n            return public_key\n\n        # Then directly from GPG\n        public_key = self.gpg().export_keys(key_fingerprint)\n        if not public_key:\n            raise GpgKeyNotFoundError()\n\n        self._redis.hset(self.REDIS_KEY_HASH, key_fingerprint, public_key)\n        return public_key",
        "file": "encryption.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(self, gpg_key_dir: Path, journalist_pub_key: Path, redis: Redis) -> None:\n        self._gpg_key_dir = gpg_key_dir\n        self.journalist_pub_key = journalist_pub_key\n        if not self.journalist_pub_key.exists():\n            raise RuntimeError(\n                f\"The journalist public key does not exist at {self.journalist_pub_key}\"\n            )\n        self._redis = redis\n\n        # Instantiate the \"main\" GPG binary\n        self._gpg = None\n\n        # Instantiate the GPG binary to be used for key deletion: always delete keys without\n        # invoking pinentry-mode=loopback\n        # see: https://lists.gnupg.org/pipermail/gnupg-users/2016-May/055965.html\n        self._gpg_for_key_deletion = None",
        "file": "encryption.py"
      },
      {
        "type": "function",
        "name": "gpg",
        "code": "def gpg(self, for_deletion: Optional[bool] = False) -> gnupg.GPG:\n        if for_deletion:\n            if self._gpg_for_key_deletion is None:\n                # GPG binary to be used for key deletion: always delete keys without\n                # invoking pinentry-mode=loopback\n                # see: https://lists.gnupg.org/pipermail/gnupg-users/2016-May/055965.html\n                self._gpg_for_key_deletion = gnupg.GPG(\n                    binary=\"gpg2\",\n                    homedir=str(self._gpg_key_dir),\n                    options=[\"--yes\", \"--trust-model direct\"],\n                )\n            return self._gpg_for_key_deletion\n        else:\n            if self._gpg is None:\n                self._gpg = gnupg.GPG(\n                    binary=\"gpg2\",\n                    homedir=str(self._gpg_key_dir),\n                    options=[\"--pinentry-mode loopback\", \"--trust-model direct\"],\n                )\n            return self._gpg",
        "file": "encryption.py"
      },
      {
        "type": "function",
        "name": "get_default",
        "code": "def get_default(cls) -> \"EncryptionManager\":\n        global _default_encryption_mgr\n        if _default_encryption_mgr is None:\n            config = SecureDropConfig.get_current()\n            _default_encryption_mgr = cls(\n                gpg_key_dir=config.GPG_KEY_DIR,\n                journalist_pub_key=(config.SECUREDROP_DATA_ROOT / \"journalist.pub\"),\n                redis=Redis(decode_responses=True, **config.REDIS_KWARGS),\n            )\n        return _default_encryption_mgr",
        "file": "encryption.py"
      },
      {
        "type": "function",
        "name": "delete_source_key_pair",
        "code": "def delete_source_key_pair(self, source_filesystem_id: str) -> None:\n        \"\"\"\n        Try to delete the source's key from the filesystem.  If it's not found, either:\n        (a) it doesn't exist or\n        (b) the source is Sequoia-based and has its key stored in Source.pgp_public_key,\n            which will be deleted when the Source instance itself is deleted.\n        \"\"\"\n        try:\n            source_key_fingerprint = self.get_source_key_fingerprint(source_filesystem_id)\n        except GpgKeyNotFoundError:\n            # If the source is entirely Sequoia-based, there is nothing to delete\n            return\n\n        # The subkeys keyword argument deletes both secret and public keys\n        self.gpg(for_deletion=True).delete_keys(source_key_fingerprint, secret=True, subkeys=True)\n\n        self._redis.hdel(self.REDIS_KEY_HASH, source_key_fingerprint)\n        self._redis.hdel(self.REDIS_FINGERPRINT_HASH, source_filesystem_id)",
        "file": "encryption.py"
      },
      {
        "type": "function",
        "name": "get_journalist_public_key",
        "code": "def get_journalist_public_key(self) -> str:\n        return self.journalist_pub_key.read_text()",
        "file": "encryption.py"
      },
      {
        "type": "function",
        "name": "get_source_public_key",
        "code": "def get_source_public_key(self, source_filesystem_id: str) -> str:\n        source_key_fingerprint = self.get_source_key_fingerprint(source_filesystem_id)\n        return self._get_public_key(source_key_fingerprint)",
        "file": "encryption.py"
      },
      {
        "type": "function",
        "name": "get_source_key_fingerprint",
        "code": "def get_source_key_fingerprint(self, source_filesystem_id: str) -> str:\n        source_key_fingerprint = self._redis.hget(self.REDIS_FINGERPRINT_HASH, source_filesystem_id)\n        if source_key_fingerprint:\n            return source_key_fingerprint\n\n        # If the fingerprint was not in Redis, get it directly from GPG\n        source_key_details = self._get_source_key_details(source_filesystem_id)\n        source_key_fingerprint = source_key_details[\"fingerprint\"]\n        self._save_key_fingerprint_to_redis(source_filesystem_id, source_key_fingerprint)\n        return source_key_fingerprint",
        "file": "encryption.py"
      },
      {
        "type": "function",
        "name": "get_source_secret_key_from_gpg",
        "code": "def get_source_secret_key_from_gpg(self, fingerprint: str, passphrase: str) -> str:\n        secret_key = self.gpg().export_keys(fingerprint, secret=True, passphrase=passphrase)\n        if not secret_key:\n            raise GpgKeyNotFoundError()\n        # Verify the secret key we got can be read and decrypted by redwood\n        try:\n            actual_fingerprint = redwood.is_valid_secret_key(secret_key, passphrase)\n        except redwood.RedwoodError:\n            # Either Sequoia can't extract the secret key or the passphrase\n            # is incorrect.\n            raise GpgKeyNotFoundError()\n        if fingerprint != actual_fingerprint:\n            # Somehow we exported the wrong key?\n            raise GpgKeyNotFoundError()\n        return secret_key",
        "file": "encryption.py"
      },
      {
        "type": "function",
        "name": "encrypt_source_message",
        "code": "def encrypt_source_message(self, message_in: str, encrypted_message_path_out: Path) -> None:\n        redwood.encrypt_message(\n            # A submission is only encrypted for the journalist key\n            recipients=[self.get_journalist_public_key()],\n            plaintext=message_in,\n            destination=encrypted_message_path_out,\n        )",
        "file": "encryption.py"
      },
      {
        "type": "function",
        "name": "encrypt_source_file",
        "code": "def encrypt_source_file(self, file_in: BinaryIO, encrypted_file_path_out: Path) -> None:\n        redwood.encrypt_stream(\n            # A submission is only encrypted for the journalist key\n            recipients=[self.get_journalist_public_key()],\n            plaintext=file_in,\n            destination=encrypted_file_path_out,\n        )",
        "file": "encryption.py"
      },
      {
        "type": "function",
        "name": "encrypt_journalist_reply",
        "code": "def encrypt_journalist_reply(\n        self, for_source: \"Source\", reply_in: str, encrypted_reply_path_out: Path\n    ) -> None:\n        redwood.encrypt_message(\n            # A reply is encrypted for both the journalist key and the source key\n            recipients=[for_source.public_key, self.get_journalist_public_key()],\n            plaintext=reply_in,\n            destination=encrypted_reply_path_out,\n        )",
        "file": "encryption.py"
      },
      {
        "type": "function",
        "name": "decrypt_journalist_reply",
        "code": "def decrypt_journalist_reply(self, for_source_user: \"SourceUser\", ciphertext_in: bytes) -> str:\n        \"\"\"Decrypt a reply sent by a journalist.\"\"\"\n        # TODO: Avoid making a database query here\n        for_source = for_source_user.get_db_record()\n        if for_source.pgp_secret_key is not None:\n            return redwood.decrypt(\n                ciphertext_in,\n                secret_key=for_source.pgp_secret_key,\n                passphrase=for_source_user.gpg_secret,\n            ).decode()\n        # In practice this should be uncreachable unless the Sequoia secret key migration failed\n        ciphertext_as_stream = BytesIO(ciphertext_in)\n        out = self.gpg().decrypt_file(ciphertext_as_stream, passphrase=for_source_user.gpg_secret)\n        if not out.ok:\n            raise GpgDecryptError(out.stderr)\n\n        return out.data.decode(\"utf-8\")",
        "file": "encryption.py"
      },
      {
        "type": "function",
        "name": "_get_source_key_details",
        "code": "def _get_source_key_details(self, source_filesystem_id: str) -> Dict[str, str]:\n        for key in self.gpg().list_keys():\n            for uid in key[\"uids\"]:\n                if source_filesystem_id in uid and self.SOURCE_KEY_UID_RE.match(uid):\n                    return key\n        raise GpgKeyNotFoundError()",
        "file": "encryption.py"
      },
      {
        "type": "function",
        "name": "_save_key_fingerprint_to_redis",
        "code": "def _save_key_fingerprint_to_redis(\n        self, source_filesystem_id: str, source_key_fingerprint: str\n    ) -> None:\n        self._redis.hset(self.REDIS_FINGERPRINT_HASH, source_filesystem_id, source_key_fingerprint)",
        "file": "encryption.py"
      },
      {
        "type": "function",
        "name": "_get_public_key",
        "code": "def _get_public_key(self, key_fingerprint: str) -> str:\n        # First try to fetch the public key from Redis\n        public_key = self._redis.hget(self.REDIS_KEY_HASH, key_fingerprint)\n        if public_key:\n            return public_key\n\n        # Then directly from GPG\n        public_key = self.gpg().export_keys(key_fingerprint)\n        if not public_key:\n            raise GpgKeyNotFoundError()\n\n        self._redis.hset(self.REDIS_KEY_HASH, key_fingerprint, public_key)\n        return public_key",
        "file": "encryption.py"
      }
    ],
    "worker.py": [
      {
        "type": "function",
        "name": "create_queue",
        "code": "def create_queue(name: str, timeout: int = 3600) -> Queue:\n    \"\"\"\n    Create an rq ``Queue`` named ``name`` with default timeout ``timeout``.\n\n    If ``name`` is omitted, ``config.RQ_WORKER_NAME`` is used.\n    \"\"\"\n    config = SecureDropConfig.get_current()\n    return Queue(name=name, connection=Redis(**config.REDIS_KWARGS), default_timeout=timeout)",
        "file": "worker.py"
      },
      {
        "type": "function",
        "name": "rq_workers",
        "code": "def rq_workers(queue: Queue = None) -> List[Worker]:\n    \"\"\"\n    Returns the list of current rq ``Worker``s.\n    \"\"\"\n\n    config = SecureDropConfig.get_current()\n    return Worker.all(connection=Redis(**config.REDIS_KWARGS), queue=queue)",
        "file": "worker.py"
      },
      {
        "type": "function",
        "name": "worker_for_job",
        "code": "def worker_for_job(job_id: str) -> Optional[Worker]:\n    \"\"\"\n    If the job is being run, return its ``Worker``.\n    \"\"\"\n    for worker in rq_workers():\n        # If the worker process no longer exists, skip it. From \"man 2\n        # kill\": \"If sig is 0, then no signal is sent, but existence\n        # and permission checks are still performed; this can be used\n        # to check for the existence of a process ID or process group\n        # ID that the caller is permitted to signal.\"\n        try:\n            os.kill(worker.pid, 0)\n        except OSError:\n            continue\n\n        # If it's running and working on the given job, return it.\n        if worker.state == WorkerStatus.BUSY and job_id == worker.get_current_job_id():\n            return worker\n    return None",
        "file": "worker.py"
      },
      {
        "type": "function",
        "name": "requeue_interrupted_jobs",
        "code": "def requeue_interrupted_jobs(queue_name: str) -> None:\n    \"\"\"\n    Requeues jobs found in the given queue's started job registry.\n\n    Only restarts those that aren't already queued or being run.\n\n    When rq starts a job, it records it in the queue's started job\n    registry. If the server is rebooted before the job completes, the\n    job is not automatically restarted from the information in the\n    registry. For tasks like secure deletion of files, this means that\n    information thought to be deleted is still present in the case of\n    seizure or compromise. We have manage.py tasks to clean such files\n    up, but this utility attempts to reduce the need for manual\n    intervention by automatically resuming interrupted jobs.\n\n    This function is predicated on a risky assumption: that all jobs\n    are idempotent. At time of writing, we use rq for securely\n    deleting submission files and hashing submissions for the ETag\n    header. Both of these can be safely repeated. If we add rq tasks\n    that cannot, this function should be improved to omit those.\n    \"\"\"\n    queue = create_queue(queue_name)\n    started_job_registry = StartedJobRegistry(queue=queue)\n\n    queued_job_ids = queue.get_job_ids()\n    logging.debug(f\"queued jobs: {queued_job_ids}\")\n    started_job_ids = started_job_registry.get_job_ids()\n    logging.debug(f\"started jobs: {started_job_ids}\")\n    job_ids = [j for j in started_job_ids if j not in queued_job_ids]\n    logging.debug(f\"candidate job ids: {job_ids}\")\n\n    if not job_ids:\n        logging.debug(\"No interrupted jobs found in started job registry.\")\n\n    for job_id in job_ids:\n        logging.debug(\"Considering job %s\", job_id)\n        try:\n            job = started_job_registry.job_class.fetch(job_id, started_job_registry.connection)\n        except NoSuchJobError as e:\n            logging.error(\"Could not find details for job %s: %s\", job_id, e)\n            continue\n\n        logging.debug(\n            \"Job %s enqueued at %s, started at %s\", job_id, job.enqueued_at, job.started_at\n        )\n\n        worker = worker_for_job(job_id)\n        if worker:\n            logging.info(\n                \"Skipping job %s, which is already being run by worker %s\", job_id, worker.key\n            )\n            continue\n\n        logging.info(\"Requeuing job %s\", job)\n\n        try:\n            started_job_registry.remove(job)\n        except InvalidJobOperation as e:\n            logging.error(\"Could not remove job %s from started job registry: %s\", job, e)\n            continue\n\n        try:\n            queue.enqueue_job(job)\n            logging.debug(\"Job now enqueued at %s, started at %s\", job.enqueued_at, job.started_at)\n        except Exception as e:\n            logging.error(\"Could not requeue job %s: %s\", job, e)\n            continue",
        "file": "worker.py"
      }
    ],
    "loaddata.py": [
      {
        "type": "function",
        "name": "fraction",
        "code": "def fraction(s: str) -> float:\n    \"\"\"\n    Ensures the string is a float between 0 and 1.\n    \"\"\"\n    f = float(s)\n    if 0 <= f <= 1:\n        return f\n    raise ValueError(f\"{s} should be a float between 0 and 1\")",
        "file": "loaddata.py"
      },
      {
        "type": "function",
        "name": "non_negative_int",
        "code": "def non_negative_int(s: str) -> int:\n    \"\"\"\n    Ensures the string is a non-negative integer.\n    \"\"\"\n    f = float(s)\n    if f.is_integer() and f >= 0:\n        return int(f)\n    raise ValueError(f\"{s} is not a non-negative integer\")",
        "file": "loaddata.py"
      },
      {
        "type": "function",
        "name": "random_bool",
        "code": "def random_bool() -> bool:\n    \"\"\"\n    Flips a coin.\n    \"\"\"\n    return secrets.choice((True, False))",
        "file": "loaddata.py"
      },
      {
        "type": "function",
        "name": "random_chars",
        "code": "def random_chars(count: int, chars: str = string.ascii_letters) -> str:\n    \"\"\"\n    Returns a random string of len characters from the supplied list.\n    \"\"\"\n    return \"\".join([secrets.choice(chars) for _ in range(count)])",
        "file": "loaddata.py"
      },
      {
        "type": "function",
        "name": "random_datetime",
        "code": "def random_datetime(nullable: bool) -> Optional[datetime.datetime]:\n    \"\"\"\n    Returns a random datetime or possibly None if nullable.\n    \"\"\"\n    if nullable and random_bool():\n        return None\n\n    now = datetime.datetime.now()\n    year = random.randint(2013, now.year)\n    max_day = 366 if calendar.isleap(year) else 365\n    day = random.randint(1, max_day)\n\n    # Calculate the month/day given the year\n    date = datetime.date(year, 1, 1) + datetime.timedelta(days=day - 1)\n\n    return datetime.datetime(\n        year=year,\n        month=date.month,\n        day=date.day,\n        hour=random.randint(0, 23),\n        minute=random.randint(0, 59),\n        second=random.randint(0, 59),\n        microsecond=random.randint(0, 1000),\n    )",
        "file": "loaddata.py"
      },
      {
        "type": "function",
        "name": "default_journalist_count",
        "code": "def default_journalist_count() -> str:\n    return os.environ.get(\"NUM_JOURNALISTS\", \"0\")",
        "file": "loaddata.py"
      },
      {
        "type": "function",
        "name": "default_source_count",
        "code": "def default_source_count() -> str:\n    return os.environ.get(\"NUM_SOURCES\", \"3\")",
        "file": "loaddata.py"
      },
      {
        "type": "function",
        "name": "set_source_count",
        "code": "def set_source_count(s: str) -> int:\n    \"\"\"\n    Sets the source count from command line arguments.\n\n    The --source-count argument can be either a positive integer or\n    the special string \"ALL\", which will result in a number of sources\n    that can demonstrate all of the special strings we want to test,\n    if each source uses two of the strings.\n    \"\"\"\n    if s == \"ALL\":\n        return math.ceil(len(strings) / 2)\n    return non_negative_int(s)",
        "file": "loaddata.py"
      },
      {
        "type": "function",
        "name": "add_journalist",
        "code": "def add_journalist(\n    username: str,\n    is_admin: bool = False,\n    first_name: str = \"\",\n    last_name: str = \"\",\n    progress: Optional[Tuple[int, int]] = None,\n) -> Journalist:\n    \"\"\"\n    Adds a single journalist account.\n    \"\"\"\n    test_password = \"correct horse battery staple profanity oil chewy\"\n    test_otp_secret = \"JHCOGO7VCER3EJ4L\"\n\n    journalist = Journalist(\n        username=username,\n        password=test_password,\n        first_name=first_name,\n        last_name=last_name,\n        is_admin=is_admin,\n    )\n    journalist.otp_secret = test_otp_secret\n    if random_bool():\n        # to add legacy passwords back in\n        journalist.passphrase_hash = None\n        salt = random_chars(32).encode(\"utf-8\")\n        journalist.pw_salt = salt\n        journalist.pw_hash = journalist._scrypt_hash(test_password, salt)\n\n    db.session.add(journalist)\n    attempt = JournalistLoginAttempt(journalist)\n    attempt.timestamp = random_datetime(nullable=True)\n    db.session.add(attempt)\n    db.session.commit()\n\n    print(\n        \"Created {}journalist{} (username={}, password={}, otp_secret={}, is_admin={})\".format(\n            \"additional \" if progress else \"\",\n            \" {}/{}\".format(*progress) if progress else \"\",\n            username,\n            test_password,\n            test_otp_secret,\n            is_admin,\n        )\n    )\n    return journalist",
        "file": "loaddata.py"
      },
      {
        "type": "function",
        "name": "record_source_interaction",
        "code": "def record_source_interaction(source: Source) -> None:\n    \"\"\"\n    Updates the source's interaction count, pending status, and timestamp.\n    \"\"\"\n    source.interaction_count += 1\n    source.pending = False\n    source.last_updated = datetime.datetime.utcnow()\n    db.session.flush()",
        "file": "loaddata.py"
      },
      {
        "type": "function",
        "name": "submit_message",
        "code": "def submit_message(source: Source, journalist_who_saw: Optional[Journalist]) -> None:\n    \"\"\"\n    Adds a single message submitted by a source.\n    \"\"\"\n    record_source_interaction(source)\n    fpath = Storage.get_default().save_message_submission(\n        source.filesystem_id,\n        source.interaction_count,\n        source.journalist_filename,\n        next(messages),\n    )\n    submission = Submission(source, fpath, Storage.get_default())\n    db.session.add(submission)\n\n    if journalist_who_saw:\n        seen_message = SeenMessage(message=submission, journalist=journalist_who_saw)\n        db.session.add(seen_message)",
        "file": "loaddata.py"
      },
      {
        "type": "function",
        "name": "submit_file",
        "code": "def submit_file(source: Source, journalist_who_saw: Optional[Journalist], size: int = 0) -> None:\n    \"\"\"\n    Adds a single file submitted by a source.\n    \"\"\"\n    record_source_interaction(source)\n    if not size:\n        file_bytes = b\"This is an example of a plain text file upload\"\n    else:\n        file_bytes = os.urandom(size * 1024)\n\n    fpath = Storage.get_default().save_file_submission(\n        source.filesystem_id,\n        source.interaction_count,\n        source.journalist_filename,\n        \"memo.txt\",\n        io.BytesIO(file_bytes),\n    )\n\n    submission = Submission(source, fpath, Storage.get_default())\n    db.session.add(submission)\n\n    if journalist_who_saw:\n        seen_file = SeenFile(file=submission, journalist=journalist_who_saw)\n        db.session.add(seen_file)",
        "file": "loaddata.py"
      },
      {
        "type": "function",
        "name": "add_reply",
        "code": "def add_reply(\n    source: Source, journalist: Journalist, journalist_who_saw: Optional[Journalist]\n) -> None:\n    \"\"\"\n    Adds a single reply to a source.\n    \"\"\"\n    record_source_interaction(source)\n    fname = f\"{source.interaction_count}-{source.journalist_filename}-reply.gpg\"\n    EncryptionManager.get_default().encrypt_journalist_reply(\n        for_source=source,\n        reply_in=next(replies),\n        encrypted_reply_path_out=Path(Storage.get_default().path(source.filesystem_id, fname)),\n    )\n    reply = Reply(journalist, source, fname, Storage.get_default())\n    db.session.add(reply)\n\n    # Journalist who replied has seen the reply\n    author_seen_reply = SeenReply(reply=reply, journalist=journalist)\n    db.session.add(author_seen_reply)\n\n    if journalist_who_saw:\n        other_seen_reply = SeenReply(reply=reply, journalist=journalist_who_saw)\n        db.session.add(other_seen_reply)\n\n    db.session.commit()",
        "file": "loaddata.py"
      },
      {
        "type": "function",
        "name": "add_source",
        "code": "def add_source(use_gpg: bool = False) -> Tuple[Source, str]:\n    \"\"\"\n    Adds a single source.\n    \"\"\"\n    codename = PassphraseGenerator.get_default().generate_passphrase()\n    source_user = create_source_user(\n        db_session=db.session,\n        source_passphrase=codename,\n        source_app_storage=Storage.get_default(),\n    )\n    source = source_user.get_db_record()\n    if use_gpg:\n        manager = EncryptionManager.get_default()\n        gen_key_input = manager.gpg().gen_key_input(\n            passphrase=source_user.gpg_secret,\n            name_email=source_user.filesystem_id,\n            key_type=\"RSA\",\n            key_length=4096,\n            name_real=\"Source Key\",\n            creation_date=\"2013-05-14\",\n            # '0' is the magic value that tells GPG's batch key generation not\n            # to set an expiration date.\n            expire_date=\"0\",\n        )\n        manager.gpg().gen_key(gen_key_input)\n\n        # Delete the Sequoia-generated keys\n        source.pgp_public_key = None\n        source.pgp_fingerprint = None\n        source.pgp_secret_key = None\n        db.session.add(source)\n    db.session.commit()\n\n    return source, codename",
        "file": "loaddata.py"
      },
      {
        "type": "function",
        "name": "star_source",
        "code": "def star_source(source: Source) -> None:\n    \"\"\"\n    Adds a SourceStar record for the source.\n    \"\"\"\n    star = SourceStar(source, True)\n    db.session.add(star)\n    db.session.commit()",
        "file": "loaddata.py"
      },
      {
        "type": "function",
        "name": "create_default_journalists",
        "code": "def create_default_journalists() -> Tuple[Journalist, ...]:\n    \"\"\"\n    Adds a set of journalists that should always be created.\n    \"\"\"\n    try:\n        default_journalist = add_journalist(\"journalist\", is_admin=True)\n    except IntegrityError as e:\n        db.session.rollback()\n        if \"UNIQUE constraint failed: journalists.\" in str(e):\n            default_journalist = Journalist.query.filter_by(username=\"journalist\").one()\n        else:\n            raise e\n\n    try:\n        dellsberg = add_journalist(\"dellsberg\")\n    except IntegrityError as e:\n        db.session.rollback()\n        if \"UNIQUE constraint failed: journalists.\" in str(e):\n            dellsberg = Journalist.query.filter_by(username=\"dellsberg\").one()\n        else:\n            raise e\n\n    try:\n        journalist_to_be_deleted = add_journalist(\n            username=\"clarkkent\", first_name=\"Clark\", last_name=\"Kent\"\n        )\n    except IntegrityError as e:\n        db.session.rollback()\n        if \"UNIQUE constraint failed: journalists.\" in str(e):\n            journalist_to_be_deleted = Journalist.query.filter_by(username=\"clarkkent\").one()\n        else:\n            raise e\n\n    return default_journalist, dellsberg, journalist_to_be_deleted",
        "file": "loaddata.py"
      },
      {
        "type": "function",
        "name": "add_journalists",
        "code": "def add_journalists(args: argparse.Namespace) -> None:\n    total = args.journalist_count\n    for i in range(1, total + 1):\n        add_journalist(username=f\"journalist{str(i)}\", progress=(i, total))",
        "file": "loaddata.py"
      },
      {
        "type": "function",
        "name": "add_sources",
        "code": "def add_sources(args: argparse.Namespace, journalists: Tuple[Journalist, ...]) -> None:\n    \"\"\"\n    Add sources with submissions and replies.\n    \"\"\"\n    default_journalist, dellsberg, journalist_to_be_deleted = journalists\n\n    starred_sources_count = int(args.source_count * args.source_star_fraction)\n    replied_sources_count = int(args.source_count * args.source_reply_fraction)\n    seen_message_count = max(\n        int(args.source_count * args.messages_per_source * args.seen_message_fraction),\n        1,\n    )\n    seen_file_count = max(\n        int(args.source_count * args.files_per_source * args.seen_file_fraction), 1\n    )\n\n    for i in range(1, args.source_count + 1):\n        source, codename = add_source(use_gpg=args.gpg)\n\n        for _ in range(args.messages_per_source):\n            submit_message(source, secrets.choice(journalists) if seen_message_count > 0 else None)\n            seen_message_count -= 1\n\n        for _ in range(args.files_per_source):\n            submit_file(\n                source,\n                secrets.choice(journalists) if seen_file_count > 0 else None,\n                args.random_file_size,\n            )\n            seen_file_count -= 1\n\n        if i <= starred_sources_count:\n            star_source(source)\n\n        if i <= replied_sources_count:\n            for _ in range(args.replies_per_source):\n                journalist_who_replied = secrets.choice([dellsberg, journalist_to_be_deleted])\n                journalist_who_saw = secrets.choice([default_journalist, None])\n                add_reply(source, journalist_who_replied, journalist_who_saw)\n\n        print(\n            f\"Created source {i}/{args.source_count} (codename: '{codename}', \"\n            f\"journalist designation '{source.journalist_designation}', \"\n            f\"files: {args.files_per_source}, messages: {args.messages_per_source}, \"\n            f\"replies: {args.replies_per_source if i <= replied_sources_count else 0})\"\n        )",
        "file": "loaddata.py"
      },
      {
        "type": "function",
        "name": "load",
        "code": "def load(args: argparse.Namespace) -> None:\n    \"\"\"\n    Populate the database.\n    \"\"\"\n    if args.seed:\n        random.seed(args.seed)\n\n    if not os.environ.get(\"SECUREDROP_ENV\"):\n        os.environ[\"SECUREDROP_ENV\"] = \"dev\"\n\n    config = SecureDropConfig.get_current()\n    app = journalist_app.create_app(config)\n    with app.app_context():\n        journalists = create_default_journalists()\n\n        add_journalists(args)\n\n        add_sources(args, journalists)\n\n        # delete one journalist\n        _, _, journalist_to_be_deleted = journalists\n        journalist_to_be_deleted.delete()\n        db.session.commit()",
        "file": "loaddata.py"
      },
      {
        "type": "function",
        "name": "parse_arguments",
        "code": "def parse_arguments() -> argparse.Namespace:\n    \"\"\"\n    Parses the command line arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        os.path.basename(__file__),\n        description=\"Loads test data into the database\",\n    )\n    parser.add_argument(\n        \"--journalist-count\",\n        type=non_negative_int,\n        default=default_journalist_count(),\n        help=(\"Number of journalists to create in addition to the default accounts\"),\n    )\n    parser.add_argument(\n        \"--source-count\",\n        type=set_source_count,\n        default=default_source_count(),\n        help=(\n            'Number of sources to create, or \"ALL\" to create a number sufficient to '\n            \"demonstrate all of our test strings.\"\n        ),\n    )\n    parser.add_argument(\n        \"--messages-per-source\",\n        type=non_negative_int,\n        default=2,\n        help=(\"Number of submitted messages to create for each source\"),\n    )\n    parser.add_argument(\n        \"--files-per-source\",\n        type=non_negative_int,\n        default=2,\n        help=(\"Number of submitted files to create for each source\"),\n    )\n    parser.add_argument(\n        \"--replies-per-source\",\n        type=non_negative_int,\n        default=2,\n        help=(\"Number of replies to create for any source that receives replies\"),\n    )\n    parser.add_argument(\n        \"--source-star-fraction\",\n        type=fraction,\n        default=0.1,\n        help=(\"Fraction of sources with stars\"),\n    )\n    parser.add_argument(\n        \"--source-reply-fraction\",\n        type=fraction,\n        default=1,\n        help=(\"Fraction of sources with replies\"),\n    )\n    parser.add_argument(\n        \"--seen-message-fraction\",\n        type=fraction,\n        default=0.75,\n        help=(\"Fraction of messages seen by a journalist\"),\n    )\n    parser.add_argument(\n        \"--seen-file-fraction\",\n        type=fraction,\n        default=0.75,\n        help=(\"Fraction of files seen by a journalist\"),\n    )\n    parser.add_argument(\n        \"--seed\",\n        help=(\"Random number seed (for reproducible datasets)\"),\n    )\n    parser.add_argument(\n        \"--gpg\",\n        help=\"Create sources with a key pair stored in GPG\",\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--random-file-size\",\n        help=\"Create random submission files with size specified (in KB)\",\n        type=non_negative_int,\n        default=0,\n    )\n\n    return parser.parse_args()",
        "file": "loaddata.py"
      }
    ],
    "store.py": [
      {
        "type": "class",
        "name": "PathException",
        "code": "class PathException(Exception):\n    \"\"\"An exception raised by `util.verify` when it encounters a bad path. A path\n    can be bad when it is not absolute or not normalized.\n    \"\"\"",
        "file": "store.py"
      },
      {
        "type": "class",
        "name": "TooManyFilesException",
        "code": "class TooManyFilesException(Exception):\n    \"\"\"An exception raised by path_without_filesystem_id when too many\n    files has been found for a given submission or reply.\n    This could be due to a very unlikely collision between\n    journalist_designations.\n    \"\"\"",
        "file": "store.py"
      },
      {
        "type": "class",
        "name": "NoFileFoundException",
        "code": "class NoFileFoundException(Exception):\n    \"\"\"An exception raised by path_without_filesystem_id when a file could\n    not be found for a given submission or reply.\n    This is likely due to an admin manually deleting files from the server.\n    \"\"\"",
        "file": "store.py"
      },
      {
        "type": "class",
        "name": "NotEncrypted",
        "code": "class NotEncrypted(Exception):\n    \"\"\"An exception raised if a file expected to be encrypted client-side\n    is actually plaintext.\n    \"\"\"",
        "file": "store.py"
      },
      {
        "type": "function",
        "name": "safe_renames",
        "code": "def safe_renames(old: str, new: str) -> None:\n    \"\"\"safe_renames(old, new)\n\n    This is a modified version of Python's os.renames that does not\n    prune directories.\n    Super-rename; create directories as necessary without deleting any\n    left empty.  Works like rename, except creation of any intermediate\n    directories needed to make the new pathname good is attempted\n    first.\n    Note: this function can fail with the new directory structure made\n    if you lack permissions needed to unlink the leaf directory or\n    file.\n    \"\"\"\n    head, tail = os.path.split(new)\n    if head and tail and not os.path.exists(head):\n        os.makedirs(head)\n    os.rename(old, new)",
        "file": "store.py"
      },
      {
        "type": "class",
        "name": "Storage",
        "code": "class Storage:\n    def __init__(self, storage_path: str, temp_dir: str) -> None:\n        if not os.path.isabs(storage_path):\n            raise PathException(f\"storage_path {storage_path} is not absolute\")\n        self.__storage_path = storage_path\n\n        if not os.path.isabs(temp_dir):\n            raise PathException(f\"temp_dir {temp_dir} is not absolute\")\n        self.__temp_dir = temp_dir\n\n        # where files and directories are sent to be securely deleted\n        self.__shredder_path = os.path.abspath(os.path.join(self.__storage_path, \"../shredder\"))\n        os.makedirs(self.__shredder_path, mode=0o700, exist_ok=True)\n\n        # crash if we don't have a way to securely remove files\n        if not rm.check_secure_delete_capability():\n            raise AssertionError(\"Secure file deletion is not possible.\")\n\n    @classmethod\n    def get_default(cls) -> \"Storage\":\n        global _default_storage\n        if _default_storage is None:\n            config = SecureDropConfig.get_current()\n            _default_storage = cls(str(config.STORE_DIR), str(config.TEMP_DIR))\n\n        return _default_storage\n\n    @property\n    def storage_path(self) -> str:\n        return self.__storage_path\n\n    @property\n    def shredder_path(self) -> str:\n        return self.__shredder_path\n\n    def shredder_contains(self, path: str) -> bool:\n        \"\"\"\n        Returns True if the fully-resolved path lies within the shredder.\n        \"\"\"\n        common_path = os.path.commonpath((os.path.realpath(path), self.__shredder_path))\n        return common_path == self.__shredder_path\n\n    def store_contains(self, path: str) -> bool:\n        \"\"\"\n        Returns True if the fully-resolved path lies within the store.\n        \"\"\"\n        common_path = os.path.commonpath((os.path.realpath(path), self.__storage_path))\n        return common_path == self.__storage_path\n\n    def verify(self, p: str) -> bool:\n        \"\"\"\n        Verify that a given path is valid for the store.\n        \"\"\"\n\n        if self.store_contains(p):\n            # verifying a hypothetical path\n            if not os.path.exists(p):\n                return True\n\n            # extant paths must be directories or correctly-named plain files\n            if os.path.isdir(p):\n                return True\n\n            if os.path.isfile(p) and VALIDATE_FILENAME(os.path.basename(p)):\n                return True\n\n        raise PathException(f\"Path not valid in store: {p}\")\n\n    def path(self, filesystem_id: str, filename: str = \"\") -> str:\n        \"\"\"\n        Returns the path resolved within `self.__storage_path`.\n\n        Raises PathException if `verify` doesn't like the path.\n        \"\"\"\n        joined = os.path.join(os.path.realpath(self.__storage_path), filesystem_id, filename)\n        absolute = os.path.realpath(joined)\n        if not self.verify(absolute):\n            raise PathException(\n                f'Could not resolve (\"{filesystem_id}\", \"{filename}\") to a path within '\n                \"the store.\"\n            )\n        return absolute\n\n    def path_without_filesystem_id(self, filename: str) -> str:\n        \"\"\"Get the normalized, absolute file path, within\n        `self.__storage_path` for a filename when the filesystem_id\n        is not known.\n        \"\"\"\n\n        joined_paths = []\n        for rootdir, _, files in os.walk(os.path.realpath(self.__storage_path)):\n            for file_ in files:\n                if file_ in filename:\n                    joined_paths.append(os.path.join(rootdir, file_))\n\n        if len(joined_paths) > 1:\n            raise TooManyFilesException(\"Found duplicate files!\")\n        elif len(joined_paths) == 0:\n            raise NoFileFoundException(f\"File not found: {filename}\")\n        else:\n            absolute = joined_paths[0]\n\n        if not self.verify(absolute):\n            raise PathException(f\"\"\"Could not resolve \"{filename}\" to a path within the store.\"\"\")\n        return absolute\n\n    def get_bulk_archive(\n        self, selected_submissions: \"List\", zip_directory: str = \"\"\n    ) -> \"_TemporaryFileWrapper\":\n        \"\"\"Generate a zip file from the selected submissions\"\"\"\n        zip_file = tempfile.NamedTemporaryFile(\n            prefix=\"tmp_securedrop_bulk_dl_\", dir=self.__temp_dir, delete=False\n        )\n        sources = {i.source.journalist_designation for i in selected_submissions}\n        # The below nested for-loops are there to create a more usable\n        # folder structure per #383\n        missing_files = False\n\n        with zipfile.ZipFile(zip_file, \"w\") as zip:\n            for source in sources:\n                fname = \"\"\n                submissions = [\n                    s for s in selected_submissions if s.source.journalist_designation == source\n                ]\n                for submission in submissions:\n                    filename = self.path(submission.source.filesystem_id, submission.filename)\n\n                    if os.path.exists(filename):\n                        document_number = submission.filename.split(\"-\")[0]\n                        if zip_directory == submission.source.journalist_filename:\n                            fname = zip_directory\n                        else:\n                            fname = os.path.join(zip_directory, source)\n                        zip.write(\n                            filename,\n                            arcname=os.path.join(\n                                fname,\n                                f\"{document_number}_{submission.source.last_updated.date()}\",\n                                os.path.basename(filename),\n                            ),\n                        )\n                    else:\n                        missing_files = True\n                        current_app.logger.error(f\"File {filename} not found\")\n\n        if missing_files:\n            raise FileNotFoundError\n        else:\n            return zip_file\n\n    def move_to_shredder(self, path: str) -> None:\n        \"\"\"\n        Moves content from the store to the shredder for secure deletion.\n\n        Python's safe_renames (and the underlying rename(2) calls) will\n        silently overwrite content, which could bypass secure\n        deletion, so we create a temporary directory under the\n        shredder directory and move the specified content there.\n\n        This function is intended to be atomic and quick, for use in\n        deletions via the UI and API. The actual secure deletion is\n        performed by an asynchronous process that monitors the\n        shredder directory.\n        \"\"\"\n        if not self.verify(path):\n            raise ValueError(f\"\"\"Path is not within the store: \"{path}\" \"\"\")\n\n        if not os.path.exists(path):\n            raise ValueError(f\"\"\"Path does not exist: \"{path}\" \"\"\")\n\n        relpath = os.path.relpath(path, start=self.storage_path)\n        dest = os.path.join(tempfile.mkdtemp(dir=self.__shredder_path), relpath)\n        current_app.logger.info(f\"Moving {path} to shredder: {dest}\")\n        safe_renames(path, dest)\n\n    def clear_shredder(self) -> None:\n        current_app.logger.info(\"Clearing shredder\")\n        directories = []\n        targets = []\n        for directory, subdirs, files in os.walk(self.shredder_path):\n            for subdir in subdirs:\n                real_subdir = os.path.realpath(os.path.join(directory, subdir))\n                if self.shredder_contains(real_subdir):\n                    directories.append(real_subdir)\n            for f in files:\n                abs_file = os.path.abspath(os.path.join(directory, f))\n                if os.path.islink(abs_file):\n                    # Somehow, a symbolic link was created in the\n                    # store. This shouldn't happen in normal\n                    # operations. Just remove the link; don't try to\n                    # shred its target. Note that we only have special\n                    # handling for symlinks. Hard links -- which\n                    # again, shouldn't occur in the store -- will\n                    # result in the file data being shredded once for\n                    # each link.\n                    current_app.logger.info(f\"Deleting link {abs_file} to {os.readlink(abs_file)}\")\n                    os.unlink(abs_file)\n                    continue\n                if self.shredder_contains(abs_file):\n                    targets.append(abs_file)\n\n        target_count = len(targets)\n        current_app.logger.info(f\"Files to delete: {target_count}\")\n        for i, t in enumerate(targets, 1):\n            current_app.logger.info(f\"Securely deleting file {i}/{target_count}: {t}\")\n            rm.secure_delete(t)\n            current_app.logger.info(f\"Securely deleted file {i}/{target_count}: {t}\")\n\n        directories_to_remove = set(directories)\n        dir_count = len(directories_to_remove)\n        for i, d in enumerate(reversed(sorted(directories_to_remove)), 1):\n            current_app.logger.debug(f\"Removing directory {i}/{dir_count}: {d}\")\n            os.rmdir(d)\n            current_app.logger.debug(f\"Removed directory {i}/{dir_count}: {d}\")\n\n    def save_file_submission(\n        self,\n        filesystem_id: str,\n        count: int,\n        journalist_filename: str,\n        filename: Optional[str],\n        stream: BinaryIO,\n    ) -> str:\n        if filename is not None:\n            sanitized_filename = secure_filename(filename)\n        else:\n            sanitized_filename = secure_filename(\"unknown.file\")\n\n        # We store file submissions in a .gz file for two reasons:\n        #\n        # 1. Downloading large files over Tor is very slow. If we can\n        # compress the file, we can speed up future downloads.\n        #\n        # 2. We want to record the original filename because it might be\n        # useful, either for context about the content of the submission\n        # or for figuring out which application should be used to open\n        # it. However, we'd like to encrypt that info and have the\n        # decrypted file automatically have the name of the original\n        # file. Given various usability constraints in GPG and Tails, this\n        # is the most user-friendly way we have found to do this.\n\n        encrypted_file_name = f\"{count}-{journalist_filename}-doc.gz.gpg\"\n        encrypted_file_path = self.path(filesystem_id, encrypted_file_name)\n        with SecureTemporaryFile(\"/tmp\") as stf:  # noqa: S108\n            with gzip.GzipFile(filename=sanitized_filename, mode=\"wb\", fileobj=stf, mtime=0) as gzf:\n                # Buffer the stream into the gzip file to avoid excessive\n                # memory consumption\n                while True:\n                    buf = stream.read(1024 * 8)\n                    if not buf:\n                        break\n                    gzf.write(buf)\n\n            EncryptionManager.get_default().encrypt_source_file(\n                file_in=stf,\n                encrypted_file_path_out=Path(encrypted_file_path),\n            )\n\n        return encrypted_file_name\n\n    def save_pre_encrypted_reply(\n        self, filesystem_id: str, count: int, journalist_filename: str, content: str\n    ) -> str:\n        if \"-----BEGIN PGP MESSAGE-----\" not in content.split(\"\\n\")[0]:\n            raise NotEncrypted\n\n        encrypted_file_name = f\"{count}-{journalist_filename}-reply.gpg\"\n        encrypted_file_path = self.path(filesystem_id, encrypted_file_name)\n\n        with open(encrypted_file_path, \"w\") as fh:\n            fh.write(content)\n\n        return encrypted_file_path\n\n    def save_message_submission(\n        self, filesystem_id: str, count: int, journalist_filename: str, message: str\n    ) -> str:\n        filename = f\"{count}-{journalist_filename}-msg.gpg\"\n        msg_loc = self.path(filesystem_id, filename)\n        EncryptionManager.get_default().encrypt_source_message(\n            message_in=message,\n            encrypted_message_path_out=Path(msg_loc),\n        )\n        return filename",
        "file": "store.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(self, storage_path: str, temp_dir: str) -> None:\n        if not os.path.isabs(storage_path):\n            raise PathException(f\"storage_path {storage_path} is not absolute\")\n        self.__storage_path = storage_path\n\n        if not os.path.isabs(temp_dir):\n            raise PathException(f\"temp_dir {temp_dir} is not absolute\")\n        self.__temp_dir = temp_dir\n\n        # where files and directories are sent to be securely deleted\n        self.__shredder_path = os.path.abspath(os.path.join(self.__storage_path, \"../shredder\"))\n        os.makedirs(self.__shredder_path, mode=0o700, exist_ok=True)\n\n        # crash if we don't have a way to securely remove files\n        if not rm.check_secure_delete_capability():\n            raise AssertionError(\"Secure file deletion is not possible.\")",
        "file": "store.py"
      },
      {
        "type": "function",
        "name": "get_default",
        "code": "def get_default(cls) -> \"Storage\":\n        global _default_storage\n        if _default_storage is None:\n            config = SecureDropConfig.get_current()\n            _default_storage = cls(str(config.STORE_DIR), str(config.TEMP_DIR))\n\n        return _default_storage",
        "file": "store.py"
      },
      {
        "type": "function",
        "name": "storage_path",
        "code": "def storage_path(self) -> str:\n        return self.__storage_path",
        "file": "store.py"
      },
      {
        "type": "function",
        "name": "shredder_path",
        "code": "def shredder_path(self) -> str:\n        return self.__shredder_path",
        "file": "store.py"
      },
      {
        "type": "function",
        "name": "shredder_contains",
        "code": "def shredder_contains(self, path: str) -> bool:\n        \"\"\"\n        Returns True if the fully-resolved path lies within the shredder.\n        \"\"\"\n        common_path = os.path.commonpath((os.path.realpath(path), self.__shredder_path))\n        return common_path == self.__shredder_path",
        "file": "store.py"
      },
      {
        "type": "function",
        "name": "store_contains",
        "code": "def store_contains(self, path: str) -> bool:\n        \"\"\"\n        Returns True if the fully-resolved path lies within the store.\n        \"\"\"\n        common_path = os.path.commonpath((os.path.realpath(path), self.__storage_path))\n        return common_path == self.__storage_path",
        "file": "store.py"
      },
      {
        "type": "function",
        "name": "verify",
        "code": "def verify(self, p: str) -> bool:\n        \"\"\"\n        Verify that a given path is valid for the store.\n        \"\"\"\n\n        if self.store_contains(p):\n            # verifying a hypothetical path\n            if not os.path.exists(p):\n                return True\n\n            # extant paths must be directories or correctly-named plain files\n            if os.path.isdir(p):\n                return True\n\n            if os.path.isfile(p) and VALIDATE_FILENAME(os.path.basename(p)):\n                return True\n\n        raise PathException(f\"Path not valid in store: {p}\")",
        "file": "store.py"
      },
      {
        "type": "function",
        "name": "path",
        "code": "def path(self, filesystem_id: str, filename: str = \"\") -> str:\n        \"\"\"\n        Returns the path resolved within `self.__storage_path`.\n\n        Raises PathException if `verify` doesn't like the path.\n        \"\"\"\n        joined = os.path.join(os.path.realpath(self.__storage_path), filesystem_id, filename)\n        absolute = os.path.realpath(joined)\n        if not self.verify(absolute):\n            raise PathException(\n                f'Could not resolve (\"{filesystem_id}\", \"{filename}\") to a path within '\n                \"the store.\"\n            )\n        return absolute",
        "file": "store.py"
      },
      {
        "type": "function",
        "name": "path_without_filesystem_id",
        "code": "def path_without_filesystem_id(self, filename: str) -> str:\n        \"\"\"Get the normalized, absolute file path, within\n        `self.__storage_path` for a filename when the filesystem_id\n        is not known.\n        \"\"\"\n\n        joined_paths = []\n        for rootdir, _, files in os.walk(os.path.realpath(self.__storage_path)):\n            for file_ in files:\n                if file_ in filename:\n                    joined_paths.append(os.path.join(rootdir, file_))\n\n        if len(joined_paths) > 1:\n            raise TooManyFilesException(\"Found duplicate files!\")\n        elif len(joined_paths) == 0:\n            raise NoFileFoundException(f\"File not found: {filename}\")\n        else:\n            absolute = joined_paths[0]\n\n        if not self.verify(absolute):\n            raise PathException(f\"\"\"Could not resolve \"{filename}\" to a path within the store.\"\"\")\n        return absolute",
        "file": "store.py"
      },
      {
        "type": "function",
        "name": "get_bulk_archive",
        "code": "def get_bulk_archive(\n        self, selected_submissions: \"List\", zip_directory: str = \"\"\n    ) -> \"_TemporaryFileWrapper\":\n        \"\"\"Generate a zip file from the selected submissions\"\"\"\n        zip_file = tempfile.NamedTemporaryFile(\n            prefix=\"tmp_securedrop_bulk_dl_\", dir=self.__temp_dir, delete=False\n        )\n        sources = {i.source.journalist_designation for i in selected_submissions}\n        # The below nested for-loops are there to create a more usable\n        # folder structure per #383\n        missing_files = False\n\n        with zipfile.ZipFile(zip_file, \"w\") as zip:\n            for source in sources:\n                fname = \"\"\n                submissions = [\n                    s for s in selected_submissions if s.source.journalist_designation == source\n                ]\n                for submission in submissions:\n                    filename = self.path(submission.source.filesystem_id, submission.filename)\n\n                    if os.path.exists(filename):\n                        document_number = submission.filename.split(\"-\")[0]\n                        if zip_directory == submission.source.journalist_filename:\n                            fname = zip_directory\n                        else:\n                            fname = os.path.join(zip_directory, source)\n                        zip.write(\n                            filename,\n                            arcname=os.path.join(\n                                fname,\n                                f\"{document_number}_{submission.source.last_updated.date()}\",\n                                os.path.basename(filename),\n                            ),\n                        )\n                    else:\n                        missing_files = True\n                        current_app.logger.error(f\"File {filename} not found\")\n\n        if missing_files:\n            raise FileNotFoundError\n        else:\n            return zip_file",
        "file": "store.py"
      },
      {
        "type": "function",
        "name": "move_to_shredder",
        "code": "def move_to_shredder(self, path: str) -> None:\n        \"\"\"\n        Moves content from the store to the shredder for secure deletion.\n\n        Python's safe_renames (and the underlying rename(2) calls) will\n        silently overwrite content, which could bypass secure\n        deletion, so we create a temporary directory under the\n        shredder directory and move the specified content there.\n\n        This function is intended to be atomic and quick, for use in\n        deletions via the UI and API. The actual secure deletion is\n        performed by an asynchronous process that monitors the\n        shredder directory.\n        \"\"\"\n        if not self.verify(path):\n            raise ValueError(f\"\"\"Path is not within the store: \"{path}\" \"\"\")\n\n        if not os.path.exists(path):\n            raise ValueError(f\"\"\"Path does not exist: \"{path}\" \"\"\")\n\n        relpath = os.path.relpath(path, start=self.storage_path)\n        dest = os.path.join(tempfile.mkdtemp(dir=self.__shredder_path), relpath)\n        current_app.logger.info(f\"Moving {path} to shredder: {dest}\")\n        safe_renames(path, dest)",
        "file": "store.py"
      },
      {
        "type": "function",
        "name": "clear_shredder",
        "code": "def clear_shredder(self) -> None:\n        current_app.logger.info(\"Clearing shredder\")\n        directories = []\n        targets = []\n        for directory, subdirs, files in os.walk(self.shredder_path):\n            for subdir in subdirs:\n                real_subdir = os.path.realpath(os.path.join(directory, subdir))\n                if self.shredder_contains(real_subdir):\n                    directories.append(real_subdir)\n            for f in files:\n                abs_file = os.path.abspath(os.path.join(directory, f))\n                if os.path.islink(abs_file):\n                    # Somehow, a symbolic link was created in the\n                    # store. This shouldn't happen in normal\n                    # operations. Just remove the link; don't try to\n                    # shred its target. Note that we only have special\n                    # handling for symlinks. Hard links -- which\n                    # again, shouldn't occur in the store -- will\n                    # result in the file data being shredded once for\n                    # each link.\n                    current_app.logger.info(f\"Deleting link {abs_file} to {os.readlink(abs_file)}\")\n                    os.unlink(abs_file)\n                    continue\n                if self.shredder_contains(abs_file):\n                    targets.append(abs_file)\n\n        target_count = len(targets)\n        current_app.logger.info(f\"Files to delete: {target_count}\")\n        for i, t in enumerate(targets, 1):\n            current_app.logger.info(f\"Securely deleting file {i}/{target_count}: {t}\")\n            rm.secure_delete(t)\n            current_app.logger.info(f\"Securely deleted file {i}/{target_count}: {t}\")\n\n        directories_to_remove = set(directories)\n        dir_count = len(directories_to_remove)\n        for i, d in enumerate(reversed(sorted(directories_to_remove)), 1):\n            current_app.logger.debug(f\"Removing directory {i}/{dir_count}: {d}\")\n            os.rmdir(d)\n            current_app.logger.debug(f\"Removed directory {i}/{dir_count}: {d}\")",
        "file": "store.py"
      },
      {
        "type": "function",
        "name": "save_file_submission",
        "code": "def save_file_submission(\n        self,\n        filesystem_id: str,\n        count: int,\n        journalist_filename: str,\n        filename: Optional[str],\n        stream: BinaryIO,\n    ) -> str:\n        if filename is not None:\n            sanitized_filename = secure_filename(filename)\n        else:\n            sanitized_filename = secure_filename(\"unknown.file\")\n\n        # We store file submissions in a .gz file for two reasons:\n        #\n        # 1. Downloading large files over Tor is very slow. If we can\n        # compress the file, we can speed up future downloads.\n        #\n        # 2. We want to record the original filename because it might be\n        # useful, either for context about the content of the submission\n        # or for figuring out which application should be used to open\n        # it. However, we'd like to encrypt that info and have the\n        # decrypted file automatically have the name of the original\n        # file. Given various usability constraints in GPG and Tails, this\n        # is the most user-friendly way we have found to do this.\n\n        encrypted_file_name = f\"{count}-{journalist_filename}-doc.gz.gpg\"\n        encrypted_file_path = self.path(filesystem_id, encrypted_file_name)\n        with SecureTemporaryFile(\"/tmp\") as stf:  # noqa: S108\n            with gzip.GzipFile(filename=sanitized_filename, mode=\"wb\", fileobj=stf, mtime=0) as gzf:\n                # Buffer the stream into the gzip file to avoid excessive\n                # memory consumption\n                while True:\n                    buf = stream.read(1024 * 8)\n                    if not buf:\n                        break\n                    gzf.write(buf)\n\n            EncryptionManager.get_default().encrypt_source_file(\n                file_in=stf,\n                encrypted_file_path_out=Path(encrypted_file_path),\n            )\n\n        return encrypted_file_name",
        "file": "store.py"
      },
      {
        "type": "function",
        "name": "save_pre_encrypted_reply",
        "code": "def save_pre_encrypted_reply(\n        self, filesystem_id: str, count: int, journalist_filename: str, content: str\n    ) -> str:\n        if \"-----BEGIN PGP MESSAGE-----\" not in content.split(\"\\n\")[0]:\n            raise NotEncrypted\n\n        encrypted_file_name = f\"{count}-{journalist_filename}-reply.gpg\"\n        encrypted_file_path = self.path(filesystem_id, encrypted_file_name)\n\n        with open(encrypted_file_path, \"w\") as fh:\n            fh.write(content)\n\n        return encrypted_file_path",
        "file": "store.py"
      },
      {
        "type": "function",
        "name": "save_message_submission",
        "code": "def save_message_submission(\n        self, filesystem_id: str, count: int, journalist_filename: str, message: str\n    ) -> str:\n        filename = f\"{count}-{journalist_filename}-msg.gpg\"\n        msg_loc = self.path(filesystem_id, filename)\n        EncryptionManager.get_default().encrypt_source_message(\n            message_in=message,\n            encrypted_message_path_out=Path(msg_loc),\n        )\n        return filename",
        "file": "store.py"
      },
      {
        "type": "function",
        "name": "async_add_checksum_for_file",
        "code": "def async_add_checksum_for_file(db_obj: \"Union[Submission, Reply]\", storage: Storage) -> Job:\n    config = SecureDropConfig.get_current()\n    return create_queue(config.RQ_WORKER_NAME).enqueue(\n        queued_add_checksum_for_file,\n        type(db_obj),\n        db_obj.id,\n        storage.path(db_obj.source.filesystem_id, db_obj.filename),\n        current_app.config[\"SQLALCHEMY_DATABASE_URI\"],\n    )",
        "file": "store.py"
      },
      {
        "type": "function",
        "name": "queued_add_checksum_for_file",
        "code": "def queued_add_checksum_for_file(\n    db_model: \"Union[Type[Submission], Type[Reply]]\", model_id: int, file_path: str, db_uri: str\n) -> str:\n    # we have to create our own DB session because there is no app context\n    session = sessionmaker(bind=create_engine(db_uri))()\n    db_obj = session.query(db_model).filter_by(id=model_id).one()\n    add_checksum_for_file(session, db_obj, file_path)\n    # We need to return a non-`None` value so the rq worker writes this back to Redis\n    return \"success\"",
        "file": "store.py"
      },
      {
        "type": "function",
        "name": "add_checksum_for_file",
        "code": "def add_checksum_for_file(\n    session: \"Session\", db_obj: \"Union[Submission, Reply]\", file_path: str\n) -> None:\n    hasher = sha256()\n    with open(file_path, \"rb\") as f:\n        while True:\n            read_bytes = f.read(4096)\n            if not read_bytes:\n                break\n            hasher.update(read_bytes)\n\n    digest = binascii.hexlify(hasher.digest()).decode(\"utf-8\")\n    digest_str = \"sha256:\" + digest\n    db_obj.checksum = digest_str\n\n    session.add(db_obj)\n    session.commit()",
        "file": "store.py"
      }
    ],
    "template_filters.py": [
      {
        "type": "function",
        "name": "rel_datetime_format",
        "code": "def rel_datetime_format(dt: datetime, fmt: str = \"long\", relative: bool = False) -> str:\n    \"\"\"Template filter for readable formatting of datetime.datetime\"\"\"\n    if relative:\n        time = dates.format_timedelta(datetime.utcnow() - dt, locale=get_locale())\n        return gettext(\"{time} ago\").format(time=time)\n    else:\n        return dates.format_datetime(dt, fmt, locale=get_locale())",
        "file": "template_filters.py"
      },
      {
        "type": "function",
        "name": "nl2br",
        "code": "def nl2br(context: EvalContext, value: str) -> str:\n    formatted = \"<br>\\n\".join(escape(value).split(\"\\n\"))\n    if context.autoescape:\n        formatted = Markup(formatted)\n    return formatted",
        "file": "template_filters.py"
      },
      {
        "type": "function",
        "name": "filesizeformat",
        "code": "def filesizeformat(value: int) -> str:\n    prefixes = [\n        \"digital-kilobyte\",\n        \"digital-megabyte\",\n        \"digital-gigabyte\",\n        \"digital-terabyte\",\n    ]\n    locale = get_locale()\n    base = 1024\n    #\n    # we are using the long length because the short length has no\n    # plural variant and it reads like a bug instead of something\n    # on purpose\n    #\n    if value < base:\n        return units.format_unit(value, \"byte\", locale=locale, length=\"long\")\n    else:\n        i = min(int(math.log(value, base)), len(prefixes)) - 1\n        prefix = prefixes[i]\n        bytes = float(value) / base ** (i + 1)\n        return units.format_unit(bytes, prefix, locale=locale, length=\"short\")",
        "file": "template_filters.py"
      },
      {
        "type": "function",
        "name": "html_datetime_format",
        "code": "def html_datetime_format(dt: datetime) -> str:\n    \"\"\"Return a datetime string that will pass HTML validation\"\"\"\n    return dates.format_datetime(dt, \"yyyy-MM-dd HH:mm:ss.SSS\")",
        "file": "template_filters.py"
      }
    ],
    "execution.py": [
      {
        "type": "function",
        "name": "asynchronous",
        "code": "def asynchronous(f):  # type: ignore\n    \"\"\"\n    Wraps a\n    \"\"\"\n\n    def wrapper(*args, **kwargs):  # type: ignore\n        thread = Thread(target=f, args=args, kwargs=kwargs)\n        thread.start()\n\n    return wrapper",
        "file": "execution.py"
      },
      {
        "type": "function",
        "name": "wrapper",
        "code": "def wrapper(*args, **kwargs):  # type: ignore\n        thread = Thread(target=f, args=args, kwargs=kwargs)\n        thread.start()",
        "file": "execution.py"
      }
    ],
    "sdconfig.py": [
      {
        "type": "class",
        "name": "_FlaskAppConfig",
        "code": "class _FlaskAppConfig:\n    \"\"\"Config fields that are common to the Journalist and Source interfaces.\"\"\"\n\n    SESSION_COOKIE_NAME: str\n    SECRET_KEY: str\n\n    DEBUG: bool\n    TESTING: bool\n    WTF_CSRF_ENABLED: bool\n\n    # Use MAX_CONTENT_LENGTH to mimic the behavior of Apache's LimitRequestBody\n    # in the development environment. See #1714.\n    MAX_CONTENT_LENGTH: int\n\n    # This is recommended for performance, and also resolves #369\n    USE_X_SENDFILE: bool",
        "file": "sdconfig.py"
      },
      {
        "type": "class",
        "name": "JournalistInterfaceConfig",
        "code": "class JournalistInterfaceConfig(_FlaskAppConfig):\n    # Additional config for JI Redis sessions\n    SESSION_SIGNER_SALT: str = \"js_session\"\n    SESSION_KEY_PREFIX: str = \"js_session:\"\n    SESSION_LIFETIME: int = 2 * 60 * 60\n    SESSION_RENEW_COUNT: int = 5",
        "file": "sdconfig.py"
      },
      {
        "type": "class",
        "name": "SourceInterfaceConfig",
        "code": "class SourceInterfaceConfig(_FlaskAppConfig):\n    pass",
        "file": "sdconfig.py"
      },
      {
        "type": "class",
        "name": "SecureDropConfig",
        "code": "class SecureDropConfig:\n    JOURNALIST_APP_FLASK_CONFIG_CLS: JournalistInterfaceConfig\n    SOURCE_APP_FLASK_CONFIG_CLS: SourceInterfaceConfig\n\n    GPG_KEY_DIR: Path\n    JOURNALIST_KEY: str\n    SCRYPT_GPG_PEPPER: str\n    SCRYPT_ID_PEPPER: str\n    SCRYPT_PARAMS: Dict[str, int]\n\n    SECUREDROP_DATA_ROOT: Path\n\n    DATABASE_FILE: Path  # Path to the sqlite DB file\n\n    SECUREDROP_ROOT: Path\n    STATIC_DIR: Path\n    TRANSLATION_DIRS: Path\n    SOURCE_TEMPLATES_DIR: Path\n    JOURNALIST_TEMPLATES_DIR: Path\n    NOUNS: Path\n    ADJECTIVES: Path\n\n    DEFAULT_LOCALE: str\n    SUPPORTED_LOCALES: List[str]\n\n    SESSION_EXPIRATION_MINUTES: float\n\n    RQ_WORKER_NAME: str\n\n    REDIS_PASSWORD: str\n\n    env: str = \"prod\"\n\n    @property\n    def TEMP_DIR(self) -> Path:\n        # We use a directory under the SECUREDROP_DATA_ROOT instead of `/tmp` because\n        # we need to expose this directory via X-Send-File, and want to minimize the\n        # potential for exposing unintended files.\n        return self.SECUREDROP_DATA_ROOT / \"tmp\"\n\n    @property\n    def STORE_DIR(self) -> Path:\n        return self.SECUREDROP_DATA_ROOT / \"store\"\n\n    @property\n    def DATABASE_URI(self) -> str:\n        return f\"sqlite:///{self.DATABASE_FILE}\"\n\n    @property\n    def REDIS_KWARGS(self) -> Dict[str, str]:\n        \"\"\"kwargs to pass to `redis.Redis` constructor\"\"\"\n        return {\"password\": self.REDIS_PASSWORD}\n\n    @classmethod\n    def get_current(cls) -> \"SecureDropConfig\":\n        global _current_config\n        if _current_config is None:\n            # Retrieve the config by parsing it from ./config.py\n            _current_config = _parse_config_from_file(config_module_name=\"config\")\n        return _current_config",
        "file": "sdconfig.py"
      },
      {
        "type": "function",
        "name": "TEMP_DIR",
        "code": "def TEMP_DIR(self) -> Path:\n        # We use a directory under the SECUREDROP_DATA_ROOT instead of `/tmp` because\n        # we need to expose this directory via X-Send-File, and want to minimize the\n        # potential for exposing unintended files.\n        return self.SECUREDROP_DATA_ROOT / \"tmp\"",
        "file": "sdconfig.py"
      },
      {
        "type": "function",
        "name": "STORE_DIR",
        "code": "def STORE_DIR(self) -> Path:\n        return self.SECUREDROP_DATA_ROOT / \"store\"",
        "file": "sdconfig.py"
      },
      {
        "type": "function",
        "name": "DATABASE_URI",
        "code": "def DATABASE_URI(self) -> str:\n        return f\"sqlite:///{self.DATABASE_FILE}\"",
        "file": "sdconfig.py"
      },
      {
        "type": "function",
        "name": "REDIS_KWARGS",
        "code": "def REDIS_KWARGS(self) -> Dict[str, str]:\n        \"\"\"kwargs to pass to `redis.Redis` constructor\"\"\"\n        return {\"password\": self.REDIS_PASSWORD}",
        "file": "sdconfig.py"
      },
      {
        "type": "function",
        "name": "get_current",
        "code": "def get_current(cls) -> \"SecureDropConfig\":\n        global _current_config\n        if _current_config is None:\n            # Retrieve the config by parsing it from ./config.py\n            _current_config = _parse_config_from_file(config_module_name=\"config\")\n        return _current_config",
        "file": "sdconfig.py"
      },
      {
        "type": "function",
        "name": "_parse_config_from_file",
        "code": "def _parse_config_from_file(config_module_name: str) -> SecureDropConfig:\n    \"\"\"Parse the config from a config.py file.\"\"\"\n    config_from_local_file = import_module(config_module_name)\n\n    # Parse the local config; as there are SD instances with very old config files\n    # the parsing logic here has to assume some values might be missing, and hence\n    # set default values for such config entries\n    final_default_locale = getattr(config_from_local_file, \"DEFAULT_LOCALE\", FALLBACK_LOCALE)\n    final_supp_locales = getattr(config_from_local_file, \"SUPPORTED_LOCALES\", [FALLBACK_LOCALE])\n    final_sess_expiration_mins = getattr(config_from_local_file, \"SESSION_EXPIRATION_MINUTES\", 120)\n\n    final_worker_name = getattr(config_from_local_file, \"RQ_WORKER_NAME\", \"default\")\n\n    final_scrypt_params = getattr(config_from_local_file, \"SCRYPT_PARAMS\", dict(N=2**14, r=8, p=1))\n    final_redis_password = getattr(config_from_local_file, \"REDIS_PASSWORD\", None)\n    if final_redis_password is None:\n        raise RuntimeError(\"REDIS_PASSWORD must be set in config.py\")\n\n    env = getattr(config_from_local_file, \"env\", \"prod\")\n\n    try:\n        final_securedrop_root = Path(config_from_local_file.SECUREDROP_ROOT)\n    except AttributeError:\n        final_securedrop_root = DEFAULT_SECUREDROP_ROOT\n\n    try:\n        final_securedrop_data_root = Path(config_from_local_file.SECUREDROP_DATA_ROOT)\n    except AttributeError:\n        final_securedrop_data_root = Path(\"/var/lib/securedrop\")\n\n    try:\n        final_db_file = Path(config_from_local_file.DATABASE_FILE)\n    except AttributeError:\n        final_db_file = final_securedrop_data_root / \"db.sqlite\"\n\n    try:\n        final_gpg_key_dir = Path(config_from_local_file.GPG_KEY_DIR)\n    except AttributeError:\n        final_gpg_key_dir = final_securedrop_data_root / \"keys\"\n\n    try:\n        final_nouns = Path(config_from_local_file.NOUNS)\n    except AttributeError:\n        final_nouns = final_securedrop_root / \"dictionaries\" / \"nouns.txt\"\n\n    try:\n        final_adjectives = Path(config_from_local_file.ADJECTIVES)\n    except AttributeError:\n        final_adjectives = final_securedrop_root / \"dictionaries\" / \"adjectives.txt\"\n\n    try:\n        final_static_dir = Path(config_from_local_file.STATIC_DIR)  # type: ignore\n    except AttributeError:\n        final_static_dir = final_securedrop_root / \"static\"\n\n    try:\n        final_transl_dir = Path(config_from_local_file.TRANSLATION_DIRS)  # type: ignore\n    except AttributeError:\n        final_transl_dir = final_securedrop_root / \"translations\"\n\n    try:\n        final_source_tmpl_dir = Path(config_from_local_file.SOURCE_TEMPLATES_DIR)\n    except AttributeError:\n        final_source_tmpl_dir = final_securedrop_root / \"source_templates\"\n\n    try:\n        final_journ_tmpl_dir = Path(config_from_local_file.JOURNALIST_TEMPLATES_DIR)\n    except AttributeError:\n        final_journ_tmpl_dir = final_securedrop_root / \"journalist_templates\"\n\n    # Parse the Flask configurations\n    journ_flask_config = config_from_local_file.JournalistInterfaceFlaskConfig\n    parsed_journ_flask_config = JournalistInterfaceConfig(\n        SECRET_KEY=journ_flask_config.SECRET_KEY,\n        SESSION_COOKIE_NAME=getattr(journ_flask_config, \"SESSION_COOKIE_NAME\", \"js\"),\n        DEBUG=getattr(journ_flask_config, \"DEBUG\", False),\n        TESTING=getattr(journ_flask_config, \"TESTING\", False),\n        WTF_CSRF_ENABLED=getattr(journ_flask_config, \"WTF_CSRF_ENABLED\", True),\n        MAX_CONTENT_LENGTH=getattr(journ_flask_config, \"MAX_CONTENT_LENGTH\", 524288000),\n        USE_X_SENDFILE=getattr(journ_flask_config, \"USE_X_SENDFILE\", False),\n    )\n    source_flask_config = config_from_local_file.SourceInterfaceFlaskConfig\n    parsed_source_flask_config = SourceInterfaceConfig(\n        SECRET_KEY=source_flask_config.SECRET_KEY,\n        SESSION_COOKIE_NAME=getattr(journ_flask_config, \"SESSION_COOKIE_NAME\", \"ss\"),\n        DEBUG=getattr(journ_flask_config, \"DEBUG\", False),\n        TESTING=getattr(journ_flask_config, \"TESTING\", False),\n        WTF_CSRF_ENABLED=getattr(journ_flask_config, \"WTF_CSRF_ENABLED\", True),\n        MAX_CONTENT_LENGTH=getattr(journ_flask_config, \"MAX_CONTENT_LENGTH\", 524288000),\n        USE_X_SENDFILE=getattr(journ_flask_config, \"USE_X_SENDFILE\", False),\n    )\n\n    return SecureDropConfig(\n        env=env,\n        JOURNALIST_APP_FLASK_CONFIG_CLS=parsed_journ_flask_config,\n        SOURCE_APP_FLASK_CONFIG_CLS=parsed_source_flask_config,\n        GPG_KEY_DIR=final_gpg_key_dir,\n        JOURNALIST_KEY=config_from_local_file.JOURNALIST_KEY,\n        SCRYPT_GPG_PEPPER=config_from_local_file.SCRYPT_GPG_PEPPER,\n        SCRYPT_ID_PEPPER=config_from_local_file.SCRYPT_ID_PEPPER,\n        SCRYPT_PARAMS=final_scrypt_params,\n        SECUREDROP_DATA_ROOT=final_securedrop_data_root,\n        SECUREDROP_ROOT=final_securedrop_root,\n        DATABASE_FILE=final_db_file,\n        STATIC_DIR=final_static_dir,\n        TRANSLATION_DIRS=final_transl_dir,\n        SOURCE_TEMPLATES_DIR=final_source_tmpl_dir,\n        JOURNALIST_TEMPLATES_DIR=final_journ_tmpl_dir,\n        NOUNS=final_nouns,\n        ADJECTIVES=final_adjectives,\n        DEFAULT_LOCALE=final_default_locale,\n        SUPPORTED_LOCALES=final_supp_locales,\n        SESSION_EXPIRATION_MINUTES=final_sess_expiration_mins,\n        RQ_WORKER_NAME=final_worker_name,\n        REDIS_PASSWORD=final_redis_password,\n    )",
        "file": "sdconfig.py"
      }
    ],
    "secure_tempfile.py": [
      {
        "type": "class",
        "name": "SecureTemporaryFile",
        "code": "class SecureTemporaryFile(_TemporaryFileWrapper):\n    \"\"\"Temporary file that provides on-the-fly encryption.\n\n    Buffering large submissions in memory as they come in requires too\n    much memory for too long a period. By writing the file to disk as it\n    comes in using a stream cipher, we are able to minimize memory usage\n    as submissions come in, while minimizing the chances of plaintext\n    recovery through forensic disk analysis. They key used to encrypt\n    each secure temporary file is also ephemeral, and is only stored in\n    memory only for as long as needed.\n\n    Adapted from Globaleaks' GLSecureTemporaryFile:\n    https://github.com/globaleaks/GlobaLeaks/blob/master/backend/globaleaks/security.py#L35\n\n    WARNING: you can't use this like a normal file object. It supports\n    being appended to however many times you wish (although content may not be\n    overwritten), and then it's contents may be read only once (although it may\n    be done in chunks) and only after it's been written to.\n    \"\"\"\n\n    AES_key_size = 256\n    AES_block_size = 128\n\n    def __init__(self, store_dir: str) -> None:\n        \"\"\"Generates an AES key and an initialization vector, and opens\n        a file in the `store_dir` directory with a\n        pseudorandomly-generated filename.\n\n        Args:\n            store_dir (str): the directory to create the secure\n                temporary file under.\n\n        Returns: self\n        \"\"\"\n        self.last_action = \"init\"\n        self.create_key()\n\n        data = base64.urlsafe_b64encode(os.urandom(32))\n        self.tmp_file_id = data.decode(\"utf-8\").strip(\"=\")\n\n        self.filepath = os.path.join(store_dir, f\"{self.tmp_file_id}.aes\")\n        self.file = open(self.filepath, \"w+b\")\n        super().__init__(self.file, self.filepath)\n\n    def create_key(self) -> None:\n        \"\"\"Generates a unique, pseudorandom AES key, stored ephemerally in\n        memory as an instance attribute. Its destruction is ensured by the\n        automatic nightly reboots of the SecureDrop application server combined\n        with the freed memory-overwriting PAX_MEMORY_SANITIZE feature of the\n        grsecurity-patched kernel it uses (for further details consult\n        https://github.com/freedomofpress/securedrop/pull/477#issuecomment-168445450).\n        \"\"\"\n        self.key = os.urandom(self.AES_key_size // 8)\n        self.iv = os.urandom(self.AES_block_size // 8)\n        self.initialize_cipher()\n\n    def initialize_cipher(self) -> None:\n        \"\"\"Creates the cipher-related objects needed for AES-CTR\n        encryption and decryption.\n        \"\"\"\n        self.cipher = Cipher(AES(self.key), CTR(self.iv), default_backend())\n        self.encryptor = self.cipher.encryptor()\n        self.decryptor = self.cipher.decryptor()\n\n    def write(self, data: Union[bytes, str]) -> int:\n        \"\"\"Write `data` to the secure temporary file. This method may be\n        called any number of times following instance initialization,\n        but after calling :meth:`read`, you cannot write to the file\n        again.\n        \"\"\"\n        if self.last_action == \"read\":\n            raise AssertionError(\"You cannot write after reading!\")\n        self.last_action = \"write\"\n\n        if isinstance(data, str):\n            data_as_bytes = data.encode(\"utf-8\")\n        else:\n            data_as_bytes = data\n\n        return self.file.write(self.encryptor.update(data_as_bytes))\n\n    def read(self, count: Optional[int] = None) -> bytes:\n        \"\"\"Read `data` from the secure temporary file. This method may\n        be called any number of times following instance initialization\n        and once :meth:`write has been called at least once, but not\n        before.\n\n        Before the first read operation, `seek(0, 0)` is called. So\n        while you can call this method any number of times, the full\n        contents of the file can only be read once. Additional calls to\n        read will return an empty str, which is desired behavior in that\n        it matches :class:`file` and because other modules depend on\n        this behavior to let them know they've reached the end of the\n        file.\n\n        Args:\n            count (int): the number of bytes to try to read from the\n                file from the current position.\n        \"\"\"\n        if self.last_action == \"init\":\n            raise AssertionError(\"You must write before reading!\")\n        if self.last_action == \"write\":\n            self.seek(0, 0)\n            self.last_action = \"read\"\n\n        if count:\n            return self.decryptor.update(self.file.read(count))\n        else:\n            return self.decryptor.update(self.file.read())\n\n    def close(self) -> None:\n        \"\"\"The __del__ method in tempfile._TemporaryFileWrapper (which\n        SecureTemporaryFile class inherits from) calls close() when the\n        temporary file is deleted.\n        \"\"\"\n        try:\n            self.decryptor.finalize()\n        except AlreadyFinalized:\n            pass\n\n        # Since tempfile._TemporaryFileWrapper.close() does other cleanup,\n        # (i.e. deleting the temp file on disk), we need to call it also.\n        super().close()",
        "file": "secure_tempfile.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(self, store_dir: str) -> None:\n        \"\"\"Generates an AES key and an initialization vector, and opens\n        a file in the `store_dir` directory with a\n        pseudorandomly-generated filename.\n\n        Args:\n            store_dir (str): the directory to create the secure\n                temporary file under.\n\n        Returns: self\n        \"\"\"\n        self.last_action = \"init\"\n        self.create_key()\n\n        data = base64.urlsafe_b64encode(os.urandom(32))\n        self.tmp_file_id = data.decode(\"utf-8\").strip(\"=\")\n\n        self.filepath = os.path.join(store_dir, f\"{self.tmp_file_id}.aes\")\n        self.file = open(self.filepath, \"w+b\")\n        super().__init__(self.file, self.filepath)",
        "file": "secure_tempfile.py"
      },
      {
        "type": "function",
        "name": "create_key",
        "code": "def create_key(self) -> None:\n        \"\"\"Generates a unique, pseudorandom AES key, stored ephemerally in\n        memory as an instance attribute. Its destruction is ensured by the\n        automatic nightly reboots of the SecureDrop application server combined\n        with the freed memory-overwriting PAX_MEMORY_SANITIZE feature of the\n        grsecurity-patched kernel it uses (for further details consult\n        https://github.com/freedomofpress/securedrop/pull/477#issuecomment-168445450).\n        \"\"\"\n        self.key = os.urandom(self.AES_key_size // 8)\n        self.iv = os.urandom(self.AES_block_size // 8)\n        self.initialize_cipher()",
        "file": "secure_tempfile.py"
      },
      {
        "type": "function",
        "name": "initialize_cipher",
        "code": "def initialize_cipher(self) -> None:\n        \"\"\"Creates the cipher-related objects needed for AES-CTR\n        encryption and decryption.\n        \"\"\"\n        self.cipher = Cipher(AES(self.key), CTR(self.iv), default_backend())\n        self.encryptor = self.cipher.encryptor()\n        self.decryptor = self.cipher.decryptor()",
        "file": "secure_tempfile.py"
      },
      {
        "type": "function",
        "name": "write",
        "code": "def write(self, data: Union[bytes, str]) -> int:\n        \"\"\"Write `data` to the secure temporary file. This method may be\n        called any number of times following instance initialization,\n        but after calling :meth:`read`, you cannot write to the file\n        again.\n        \"\"\"\n        if self.last_action == \"read\":\n            raise AssertionError(\"You cannot write after reading!\")\n        self.last_action = \"write\"\n\n        if isinstance(data, str):\n            data_as_bytes = data.encode(\"utf-8\")\n        else:\n            data_as_bytes = data\n\n        return self.file.write(self.encryptor.update(data_as_bytes))",
        "file": "secure_tempfile.py"
      },
      {
        "type": "function",
        "name": "read",
        "code": "def read(self, count: Optional[int] = None) -> bytes:\n        \"\"\"Read `data` from the secure temporary file. This method may\n        be called any number of times following instance initialization\n        and once :meth:`write has been called at least once, but not\n        before.\n\n        Before the first read operation, `seek(0, 0)` is called. So\n        while you can call this method any number of times, the full\n        contents of the file can only be read once. Additional calls to\n        read will return an empty str, which is desired behavior in that\n        it matches :class:`file` and because other modules depend on\n        this behavior to let them know they've reached the end of the\n        file.\n\n        Args:\n            count (int): the number of bytes to try to read from the\n                file from the current position.\n        \"\"\"\n        if self.last_action == \"init\":\n            raise AssertionError(\"You must write before reading!\")\n        if self.last_action == \"write\":\n            self.seek(0, 0)\n            self.last_action = \"read\"\n\n        if count:\n            return self.decryptor.update(self.file.read(count))\n        else:\n            return self.decryptor.update(self.file.read())",
        "file": "secure_tempfile.py"
      },
      {
        "type": "function",
        "name": "close",
        "code": "def close(self) -> None:\n        \"\"\"The __del__ method in tempfile._TemporaryFileWrapper (which\n        SecureTemporaryFile class inherits from) calls close() when the\n        temporary file is deleted.\n        \"\"\"\n        try:\n            self.decryptor.finalize()\n        except AlreadyFinalized:\n            pass\n\n        # Since tempfile._TemporaryFileWrapper.close() does other cleanup,\n        # (i.e. deleting the temp file on disk), we need to call it also.\n        super().close()",
        "file": "secure_tempfile.py"
      }
    ],
    "startup.py": [
      {
        "type": "function",
        "name": "validate_journalist_key",
        "code": "def validate_journalist_key(app: Optional[Flask] = None) -> bool:\n    \"\"\"Verify the journalist PGP key is valid\"\"\"\n    encryption_mgr = EncryptionManager.get_default()\n    # First check that we can read it\n    try:\n        journalist_key = encryption_mgr.get_journalist_public_key()\n    except Exception as e:\n        if app:\n            print(f\"ERROR: Unable to read journalist public key: {e}\", file=sys.stderr)\n            app.logger.error(f\"ERROR: Unable to read journalist public key: {e}\")\n        return False\n    # And then what we read is valid\n    try:\n        redwood.is_valid_public_key(journalist_key)\n    except redwood.RedwoodError as e:\n        if app:\n            print(f\"ERROR: Journalist public key is not valid: {e}\", file=sys.stderr)\n            app.logger.error(f\"ERROR: Journalist public key is not valid: {e}\")\n        return False\n\n    return True",
        "file": "startup.py"
      }
    ],
    "upload-screenshots.py": [
      {
        "type": "function",
        "name": "main",
        "code": "def main() -> None:\n    \"\"\"\n    Uses the generic WeblateUploader class below to run a SecureDrop screenshot\n    upload.\n    \"\"\"\n    token = os.getenv(\"WEBLATE_API_TOKEN\", None)\n    base_url = os.getenv(\"WEBLATE_BASE_URL\", DEFAULT_BASE_URL)\n\n    if token is None:\n        raise BadOrMissingTokenError(\n            \"No token provided via WEBLATE_API_TOKEN environment variable.\", base_url\n        )\n\n    screenshot_files = glob(os.path.join(SCREENSHOTS_DIRECTORY, SCREENSHOTS_GLOB))\n    if len(screenshot_files) == 0:\n        print(\"Page layout test results not found. Run this command from the SecureDrop\")\n        print(\"base directory to generate the English language screenshots:\\n\")\n        print(\"  LOCALES=en_US make translation-test\")\n        print(\"\\nThis will take several minutes to complete.\")\n        sys.exit(1)\n\n    uploader = WeblateUploader(\n        token=token,\n        base_url=base_url,\n        project=PROJECT_SLUG,\n        component=COMPONENT_SLUG,\n        files=screenshot_files,\n        request_limit=REQUEST_LIMIT,\n        canonicalization_rules=CANONICALIZATION_RULES,\n    )\n    uploader.upload()",
        "file": "upload-screenshots.py"
      },
      {
        "type": "class",
        "name": "WeblateUploader",
        "code": "class WeblateUploader:\n    \"\"\"\n    Manages Weblate screenshot batch uploads, matching filenames against\n    titles of existing screenshots to create/update as appropriate.\n    \"\"\"\n\n    def __init__(\n        self,\n        token: str,\n        base_url: str,\n        project: str,\n        component: str,\n        files: List[str],\n        request_limit: int,\n        canonicalization_rules: List[Tuple[str, str]],\n    ) -> None:\n        if len(token) != 40:\n            raise BadOrMissingTokenError(\n                \"API token is not in expected 40 character format.\", base_url\n            )\n\n        self.base_url = base_url\n        self.screenshots_endpoint = urljoin(base_url, \"/api/screenshots/\")\n        self.project = project\n        self.component = component\n        self.files = files\n        self.request_limit = request_limit\n        self.canonicalization_rules = canonicalization_rules\n        self.user_agent = \"Python Weblate Uploader V1.0\"\n\n        # While not all requests require authentication, any useful operation of this\n        # script does, and providing a token for all requests ensures we avoid hitting\n        # the rate limit for unauthenticated users. See:\n        # https://docs.weblate.org/en/latest/api.html#rate-limiting\n        self.session = requests.Session()\n        headers = {\n            \"User-Agent\": self.user_agent,\n            \"Authorization\": f\"Token {token}\",\n        }\n        self.session.headers.update(headers)\n\n    def get_existing_screenshots(self) -> List[Dict[str, str]]:\n        \"\"\"\n        Obtains a list of all existing screenshots, and returns it as a list\n        in the API's format. Paginates up to the request limit.\n        \"\"\"\n        next_screenshots_url = self.screenshots_endpoint\n\n        # API results are paginated, so we must loop through a set of results and\n        # concatenate them.\n        screenshots: List[Dict[str, str]] = []\n        request_count = 0\n        while next_screenshots_url is not None:\n            response = self.session.get(next_screenshots_url)\n            response.raise_for_status()\n            screenshots_page = response.json()\n            next_screenshots_url = screenshots_page[\"next\"]\n            screenshots += screenshots_page[\"results\"]\n            request_count += 1\n            if request_count >= self.request_limit:\n                msg = f\"Request limit of {self.request_limit} exceeded. Aborting.\"\n                raise RequestLimitError(msg)\n        return screenshots\n\n    def _canonicalize(self, filename: str) -> str:\n        \"\"\"\n        Derives a human-readable title from a filename using the defined\n        canonicalization rules, if any. This is used to later update the\n        screenshot.\n        \"\"\"\n        for pattern, repl in self.canonicalization_rules:\n            filename = re.sub(pattern, repl, filename)\n        return filename\n\n    def upload(self, check_existing_screenshots: bool = True) -> None:\n        \"\"\"\n        Uploads all files using the screenshots endpoint. Optionally, checks\n        files against a list of existing screenshots and replaces them rather\n        than creating new uploads.\n        \"\"\"\n        if check_existing_screenshots is True:\n            existing_screenshots = self.get_existing_screenshots()\n        else:\n            existing_screenshots = []\n\n        for file in self.files:\n            basename = os.path.basename(file)\n            canonical_name = self._canonicalize(basename)\n            existing_screenshot_url = None\n\n            for screenshot in existing_screenshots:\n                if screenshot[\"name\"] == canonical_name:\n                    existing_screenshot_url = screenshot[\"file_url\"]\n                    break\n\n            image = {\"image\": open(file, \"rb\")}\n\n            if existing_screenshot_url is not None:\n                print(f\"Replacing existing screenshot {basename}\")\n                response = self.session.post(existing_screenshot_url, files=image)\n                response.raise_for_status()\n            else:\n                fields = {\n                    \"name\": canonical_name,\n                    \"project_slug\": \"securedrop\",\n                    \"component_slug\": \"securedrop\",\n                }\n                print(f\"Uploading new screenshot {basename}\")\n                response = self.session.post(self.screenshots_endpoint, files=image, data=fields)\n                response.raise_for_status()\n\n        result_url = urljoin(self.base_url, f\"screenshots/{self.project}/{self.component}\")\n        print(f\"Upload complete. Visit {result_url} to review the results.\")",
        "file": "upload-screenshots.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(\n        self,\n        token: str,\n        base_url: str,\n        project: str,\n        component: str,\n        files: List[str],\n        request_limit: int,\n        canonicalization_rules: List[Tuple[str, str]],\n    ) -> None:\n        if len(token) != 40:\n            raise BadOrMissingTokenError(\n                \"API token is not in expected 40 character format.\", base_url\n            )\n\n        self.base_url = base_url\n        self.screenshots_endpoint = urljoin(base_url, \"/api/screenshots/\")\n        self.project = project\n        self.component = component\n        self.files = files\n        self.request_limit = request_limit\n        self.canonicalization_rules = canonicalization_rules\n        self.user_agent = \"Python Weblate Uploader V1.0\"\n\n        # While not all requests require authentication, any useful operation of this\n        # script does, and providing a token for all requests ensures we avoid hitting\n        # the rate limit for unauthenticated users. See:\n        # https://docs.weblate.org/en/latest/api.html#rate-limiting\n        self.session = requests.Session()\n        headers = {\n            \"User-Agent\": self.user_agent,\n            \"Authorization\": f\"Token {token}\",\n        }\n        self.session.headers.update(headers)",
        "file": "upload-screenshots.py"
      },
      {
        "type": "function",
        "name": "get_existing_screenshots",
        "code": "def get_existing_screenshots(self) -> List[Dict[str, str]]:\n        \"\"\"\n        Obtains a list of all existing screenshots, and returns it as a list\n        in the API's format. Paginates up to the request limit.\n        \"\"\"\n        next_screenshots_url = self.screenshots_endpoint\n\n        # API results are paginated, so we must loop through a set of results and\n        # concatenate them.\n        screenshots: List[Dict[str, str]] = []\n        request_count = 0\n        while next_screenshots_url is not None:\n            response = self.session.get(next_screenshots_url)\n            response.raise_for_status()\n            screenshots_page = response.json()\n            next_screenshots_url = screenshots_page[\"next\"]\n            screenshots += screenshots_page[\"results\"]\n            request_count += 1\n            if request_count >= self.request_limit:\n                msg = f\"Request limit of {self.request_limit} exceeded. Aborting.\"\n                raise RequestLimitError(msg)\n        return screenshots",
        "file": "upload-screenshots.py"
      },
      {
        "type": "function",
        "name": "_canonicalize",
        "code": "def _canonicalize(self, filename: str) -> str:\n        \"\"\"\n        Derives a human-readable title from a filename using the defined\n        canonicalization rules, if any. This is used to later update the\n        screenshot.\n        \"\"\"\n        for pattern, repl in self.canonicalization_rules:\n            filename = re.sub(pattern, repl, filename)\n        return filename",
        "file": "upload-screenshots.py"
      },
      {
        "type": "function",
        "name": "upload",
        "code": "def upload(self, check_existing_screenshots: bool = True) -> None:\n        \"\"\"\n        Uploads all files using the screenshots endpoint. Optionally, checks\n        files against a list of existing screenshots and replaces them rather\n        than creating new uploads.\n        \"\"\"\n        if check_existing_screenshots is True:\n            existing_screenshots = self.get_existing_screenshots()\n        else:\n            existing_screenshots = []\n\n        for file in self.files:\n            basename = os.path.basename(file)\n            canonical_name = self._canonicalize(basename)\n            existing_screenshot_url = None\n\n            for screenshot in existing_screenshots:\n                if screenshot[\"name\"] == canonical_name:\n                    existing_screenshot_url = screenshot[\"file_url\"]\n                    break\n\n            image = {\"image\": open(file, \"rb\")}\n\n            if existing_screenshot_url is not None:\n                print(f\"Replacing existing screenshot {basename}\")\n                response = self.session.post(existing_screenshot_url, files=image)\n                response.raise_for_status()\n            else:\n                fields = {\n                    \"name\": canonical_name,\n                    \"project_slug\": \"securedrop\",\n                    \"component_slug\": \"securedrop\",\n                }\n                print(f\"Uploading new screenshot {basename}\")\n                response = self.session.post(self.screenshots_endpoint, files=image, data=fields)\n                response.raise_for_status()\n\n        result_url = urljoin(self.base_url, f\"screenshots/{self.project}/{self.component}\")\n        print(f\"Upload complete. Visit {result_url} to review the results.\")",
        "file": "upload-screenshots.py"
      },
      {
        "type": "class",
        "name": "BadOrMissingTokenError",
        "code": "class BadOrMissingTokenError(Exception):\n    def __init__(self, reason: str, base_url: str) -> None:\n        reason += \" Obtain token via {}\".format(urljoin(base_url, \"accounts/profile/#api\"))\n        super().__init__(reason)",
        "file": "upload-screenshots.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(self, reason: str, base_url: str) -> None:\n        reason += \" Obtain token via {}\".format(urljoin(base_url, \"accounts/profile/#api\"))\n        super().__init__(reason)",
        "file": "upload-screenshots.py"
      },
      {
        "type": "class",
        "name": "RequestLimitError",
        "code": "class RequestLimitError(Exception):\n    pass",
        "file": "upload-screenshots.py"
      }
    ],
    "journalist.py": [
      {
        "type": "function",
        "name": "prime_keycache",
        "code": "def prime_keycache() -> None:\n    \"\"\"Pre-load the source public keys into Redis.\"\"\"\n    with app.app_context():\n        encryption_mgr = EncryptionManager.get_default()\n        for source in Source.query.filter_by(pending=False, deleted_at=None).all():\n            try:\n                encryption_mgr.get_source_public_key(source.filesystem_id)\n            except GpgKeyNotFoundError:\n                pass",
        "file": "journalist.py"
      }
    ],
    "models.py": [
      {
        "type": "function",
        "name": "get_one_or_else",
        "code": "def get_one_or_else(\n    query: Query, logger: \"Logger\", failure_method: \"Callable[[int], None]\"\n) -> db.Model:\n    try:\n        return query.one()\n    except MultipleResultsFound as e:\n        logger.error(f\"Found multiple while executing {query} when one was expected: {e}\")\n        failure_method(500)\n    except NoResultFound as e:\n        logger.error(f\"Found none when one was expected: {e}\")\n        failure_method(404)",
        "file": "models.py"
      },
      {
        "type": "class",
        "name": "Source",
        "code": "class Source(db.Model):\n    __tablename__ = \"sources\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    filesystem_id = Column(String(96), unique=True, nullable=False)\n    journalist_designation = Column(String(255), nullable=False)\n    last_updated = Column(DateTime)\n    star = relationship(\"SourceStar\", uselist=False, backref=\"source\")\n\n    # sources are \"pending\" and don't get displayed to journalists until they\n    # submit something\n    pending = Column(Boolean, default=True)\n\n    # keep track of how many interactions have happened, for filenames\n    interaction_count = Column(Integer, default=0, nullable=False)\n\n    # when deletion of the source was requested\n    deleted_at = Column(DateTime)\n\n    # PGP key material\n    pgp_public_key = Column(Text, nullable=True)\n    pgp_secret_key = Column(Text, nullable=True)\n    pgp_fingerprint = Column(String(40), nullable=True)\n\n    def __init__(\n        self,\n        filesystem_id: str,\n        journalist_designation: str,\n        public_key: str,\n        secret_key: str,\n        fingerprint: str,\n    ) -> None:\n        self.filesystem_id = filesystem_id\n        self.journalist_designation = journalist_designation\n        self.pgp_public_key = public_key\n        self.pgp_secret_key = secret_key\n        self.pgp_fingerprint = fingerprint\n        self.uuid = str(uuid.uuid4())\n\n    def __repr__(self) -> str:\n        return f\"<Source {self.journalist_designation!r}>\"\n\n    @property\n    def journalist_filename(self) -> str:\n        valid_chars = \"abcdefghijklmnopqrstuvwxyz1234567890-_\"\n        return \"\".join(\n            [c for c in self.journalist_designation.lower().replace(\" \", \"_\") if c in valid_chars]\n        )\n\n    def documents_messages_count(self) -> \"Dict[str, int]\":\n        self.docs_msgs_count = {\"messages\": 0, \"documents\": 0}\n        for submission in self.submissions:\n            if submission.is_message:\n                self.docs_msgs_count[\"messages\"] += 1\n            elif submission.is_file:\n                self.docs_msgs_count[\"documents\"] += 1\n        return self.docs_msgs_count\n\n    @property\n    def collection(self) -> \"List[Union[Submission, Reply]]\":\n        \"\"\"Return the list of submissions and replies for this source, sorted\n        in ascending order by the filename/interaction count.\"\"\"\n        collection: List[Union[Submission, Reply]] = []\n        collection.extend(self.submissions)\n        collection.extend(self.replies)\n        collection.sort(key=lambda x: int(x.filename.split(\"-\")[0]))\n        return collection\n\n    @property\n    def fingerprint(self) -> Optional[str]:\n        if self.pgp_fingerprint is not None:\n            return self.pgp_fingerprint\n        try:\n            return EncryptionManager.get_default().get_source_key_fingerprint(self.filesystem_id)\n        except GpgKeyNotFoundError:\n            return None\n\n    @property\n    def public_key(self) -> Optional[str]:\n        if self.pgp_public_key:\n            return self.pgp_public_key\n        try:\n            return EncryptionManager.get_default().get_source_public_key(self.filesystem_id)\n        except GpgKeyNotFoundError:\n            return None\n\n    def to_json(self) -> \"Dict[str, object]\":\n        docs_msg_count = self.documents_messages_count()\n\n        if self.last_updated:\n            last_updated = self.last_updated\n        else:\n            last_updated = datetime.datetime.now(tz=datetime.timezone.utc)\n\n        if self.star and self.star.starred:\n            starred = True\n        else:\n            starred = False\n\n        return {\n            \"uuid\": self.uuid,\n            \"url\": url_for(\"api.single_source\", source_uuid=self.uuid),\n            \"journalist_designation\": self.journalist_designation,\n            \"is_flagged\": False,\n            \"is_starred\": starred,\n            \"last_updated\": last_updated,\n            \"interaction_count\": self.interaction_count,\n            \"key\": {\n                \"type\": \"PGP\",\n                \"public\": self.public_key,\n                \"fingerprint\": self.fingerprint,\n            },\n            \"number_of_documents\": docs_msg_count[\"documents\"],\n            \"number_of_messages\": docs_msg_count[\"messages\"],\n            \"submissions_url\": url_for(\"api.all_source_submissions\", source_uuid=self.uuid),\n            \"add_star_url\": url_for(\"api.add_star\", source_uuid=self.uuid),\n            \"remove_star_url\": url_for(\"api.remove_star\", source_uuid=self.uuid),\n            \"replies_url\": url_for(\"api.all_source_replies\", source_uuid=self.uuid),\n        }",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(\n        self,\n        filesystem_id: str,\n        journalist_designation: str,\n        public_key: str,\n        secret_key: str,\n        fingerprint: str,\n    ) -> None:\n        self.filesystem_id = filesystem_id\n        self.journalist_designation = journalist_designation\n        self.pgp_public_key = public_key\n        self.pgp_secret_key = secret_key\n        self.pgp_fingerprint = fingerprint\n        self.uuid = str(uuid.uuid4())",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "__repr__",
        "code": "def __repr__(self) -> str:\n        return f\"<Source {self.journalist_designation!r}>\"",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "journalist_filename",
        "code": "def journalist_filename(self) -> str:\n        valid_chars = \"abcdefghijklmnopqrstuvwxyz1234567890-_\"\n        return \"\".join(\n            [c for c in self.journalist_designation.lower().replace(\" \", \"_\") if c in valid_chars]\n        )",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "documents_messages_count",
        "code": "def documents_messages_count(self) -> \"Dict[str, int]\":\n        self.docs_msgs_count = {\"messages\": 0, \"documents\": 0}\n        for submission in self.submissions:\n            if submission.is_message:\n                self.docs_msgs_count[\"messages\"] += 1\n            elif submission.is_file:\n                self.docs_msgs_count[\"documents\"] += 1\n        return self.docs_msgs_count",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "collection",
        "code": "def collection(self) -> \"List[Union[Submission, Reply]]\":\n        \"\"\"Return the list of submissions and replies for this source, sorted\n        in ascending order by the filename/interaction count.\"\"\"\n        collection: List[Union[Submission, Reply]] = []\n        collection.extend(self.submissions)\n        collection.extend(self.replies)\n        collection.sort(key=lambda x: int(x.filename.split(\"-\")[0]))\n        return collection",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "fingerprint",
        "code": "def fingerprint(self) -> Optional[str]:\n        if self.pgp_fingerprint is not None:\n            return self.pgp_fingerprint\n        try:\n            return EncryptionManager.get_default().get_source_key_fingerprint(self.filesystem_id)\n        except GpgKeyNotFoundError:\n            return None",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "public_key",
        "code": "def public_key(self) -> Optional[str]:\n        if self.pgp_public_key:\n            return self.pgp_public_key\n        try:\n            return EncryptionManager.get_default().get_source_public_key(self.filesystem_id)\n        except GpgKeyNotFoundError:\n            return None",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "to_json",
        "code": "def to_json(self) -> \"Dict[str, object]\":\n        docs_msg_count = self.documents_messages_count()\n\n        if self.last_updated:\n            last_updated = self.last_updated\n        else:\n            last_updated = datetime.datetime.now(tz=datetime.timezone.utc)\n\n        if self.star and self.star.starred:\n            starred = True\n        else:\n            starred = False\n\n        return {\n            \"uuid\": self.uuid,\n            \"url\": url_for(\"api.single_source\", source_uuid=self.uuid),\n            \"journalist_designation\": self.journalist_designation,\n            \"is_flagged\": False,\n            \"is_starred\": starred,\n            \"last_updated\": last_updated,\n            \"interaction_count\": self.interaction_count,\n            \"key\": {\n                \"type\": \"PGP\",\n                \"public\": self.public_key,\n                \"fingerprint\": self.fingerprint,\n            },\n            \"number_of_documents\": docs_msg_count[\"documents\"],\n            \"number_of_messages\": docs_msg_count[\"messages\"],\n            \"submissions_url\": url_for(\"api.all_source_submissions\", source_uuid=self.uuid),\n            \"add_star_url\": url_for(\"api.add_star\", source_uuid=self.uuid),\n            \"remove_star_url\": url_for(\"api.remove_star\", source_uuid=self.uuid),\n            \"replies_url\": url_for(\"api.all_source_replies\", source_uuid=self.uuid),\n        }",
        "file": "models.py"
      },
      {
        "type": "class",
        "name": "Submission",
        "code": "class Submission(db.Model):\n    MAX_MESSAGE_LEN = 100000\n\n    __tablename__ = \"submissions\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    source_id = Column(Integer, ForeignKey(\"sources.id\"))\n    source = relationship(\"Source\", backref=backref(\"submissions\", order_by=id, cascade=\"delete\"))\n\n    filename = Column(String(255), nullable=False)\n    size = Column(Integer, nullable=False)\n    downloaded = Column(Boolean, default=False)\n    \"\"\"\n    The checksum of the encrypted file on disk.\n    Format: $hash_name:$hex_encoded_hash_value\n    Example: sha256:05fa5efd7d1b608ac1fbdf19a61a5a439d05b05225e81faa63fdd188296b614a\n    \"\"\"\n    checksum = Column(String(255))\n\n    def __init__(self, source: Source, filename: str, storage: Storage) -> None:\n        self.source_id = source.id\n        self.filename = filename\n        self.uuid = str(uuid.uuid4())\n        self.size = os.stat(storage.path(source.filesystem_id, filename)).st_size\n\n    def __repr__(self) -> str:\n        return f\"<Submission {self.filename!r}>\"\n\n    @property\n    def is_file(self) -> bool:\n        return self.filename.endswith(\"doc.gz.gpg\") or self.filename.endswith(\"doc.zip.gpg\")\n\n    @property\n    def is_message(self) -> bool:\n        return self.filename.endswith(\"msg.gpg\")\n\n    def to_json(self) -> \"Dict[str, Any]\":\n        seen_by = {\n            f.journalist.uuid\n            for f in SeenFile.query.filter(SeenFile.file_id == self.id)\n            if f.journalist\n        }\n        seen_by.update(\n            {\n                m.journalist.uuid\n                for m in SeenMessage.query.filter(SeenMessage.message_id == self.id)\n                if m.journalist\n            }\n        )\n        return {\n            \"source_url\": (\n                url_for(\"api.single_source\", source_uuid=self.source.uuid) if self.source else None\n            ),\n            \"submission_url\": (\n                url_for(\n                    \"api.single_submission\",\n                    source_uuid=self.source.uuid,\n                    submission_uuid=self.uuid,\n                )\n                if self.source\n                else None\n            ),\n            \"filename\": self.filename,\n            \"size\": self.size,\n            \"is_file\": self.is_file,\n            \"is_message\": self.is_message,\n            \"is_read\": self.seen,\n            \"uuid\": self.uuid,\n            \"download_url\": (\n                url_for(\n                    \"api.download_submission\",\n                    source_uuid=self.source.uuid,\n                    submission_uuid=self.uuid,\n                )\n                if self.source\n                else None\n            ),\n            \"seen_by\": list(seen_by),\n        }\n\n    @property\n    def seen(self) -> bool:\n        \"\"\"\n        If the submission has been downloaded or seen by any journalist, then the submission is\n        considered seen.\n        \"\"\"\n        return bool(self.downloaded or self.seen_files.count() or self.seen_messages.count())",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(self, source: Source, filename: str, storage: Storage) -> None:\n        self.source_id = source.id\n        self.filename = filename\n        self.uuid = str(uuid.uuid4())\n        self.size = os.stat(storage.path(source.filesystem_id, filename)).st_size",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "__repr__",
        "code": "def __repr__(self) -> str:\n        return f\"<Submission {self.filename!r}>\"",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "is_file",
        "code": "def is_file(self) -> bool:\n        return self.filename.endswith(\"doc.gz.gpg\") or self.filename.endswith(\"doc.zip.gpg\")",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "is_message",
        "code": "def is_message(self) -> bool:\n        return self.filename.endswith(\"msg.gpg\")",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "to_json",
        "code": "def to_json(self) -> \"Dict[str, Any]\":\n        seen_by = {\n            f.journalist.uuid\n            for f in SeenFile.query.filter(SeenFile.file_id == self.id)\n            if f.journalist\n        }\n        seen_by.update(\n            {\n                m.journalist.uuid\n                for m in SeenMessage.query.filter(SeenMessage.message_id == self.id)\n                if m.journalist\n            }\n        )\n        return {\n            \"source_url\": (\n                url_for(\"api.single_source\", source_uuid=self.source.uuid) if self.source else None\n            ),\n            \"submission_url\": (\n                url_for(\n                    \"api.single_submission\",\n                    source_uuid=self.source.uuid,\n                    submission_uuid=self.uuid,\n                )\n                if self.source\n                else None\n            ),\n            \"filename\": self.filename,\n            \"size\": self.size,\n            \"is_file\": self.is_file,\n            \"is_message\": self.is_message,\n            \"is_read\": self.seen,\n            \"uuid\": self.uuid,\n            \"download_url\": (\n                url_for(\n                    \"api.download_submission\",\n                    source_uuid=self.source.uuid,\n                    submission_uuid=self.uuid,\n                )\n                if self.source\n                else None\n            ),\n            \"seen_by\": list(seen_by),\n        }",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "seen",
        "code": "def seen(self) -> bool:\n        \"\"\"\n        If the submission has been downloaded or seen by any journalist, then the submission is\n        considered seen.\n        \"\"\"\n        return bool(self.downloaded or self.seen_files.count() or self.seen_messages.count())",
        "file": "models.py"
      },
      {
        "type": "class",
        "name": "Reply",
        "code": "class Reply(db.Model):\n    __tablename__ = \"replies\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n\n    journalist_id = Column(Integer, ForeignKey(\"journalists.id\"), nullable=False)\n    journalist = relationship(\"Journalist\", backref=backref(\"replies\", order_by=id))\n\n    source_id = Column(Integer, ForeignKey(\"sources.id\"))\n    source = relationship(\"Source\", backref=backref(\"replies\", order_by=id, cascade=\"delete\"))\n\n    filename = Column(String(255), nullable=False)\n    size = Column(Integer, nullable=False)\n    \"\"\"\n    The checksum of the encrypted file on disk.\n    Format: $hash_name:$hex_encoded_hash_value\n    Example: sha256:05fa5efd7d1b608ac1fbdf19a61a5a439d05b05225e81faa63fdd188296b614a\n    \"\"\"\n    checksum = Column(String(255))\n\n    deleted_by_source = Column(Boolean, default=False, nullable=False)\n\n    def __init__(\n        self, journalist: \"Journalist\", source: Source, filename: str, storage: Storage\n    ) -> None:\n        self.journalist = journalist\n        self.source_id = source.id\n        self.uuid = str(uuid.uuid4())\n        self.filename = filename\n        self.size = os.stat(storage.path(source.filesystem_id, filename)).st_size\n\n    def __repr__(self) -> str:\n        return f\"<Reply {self.filename!r}>\"\n\n    def to_json(self) -> \"Dict[str, Any]\":\n        seen_by = [r.journalist.uuid for r in SeenReply.query.filter(SeenReply.reply_id == self.id)]\n        return {\n            \"source_url\": (\n                url_for(\"api.single_source\", source_uuid=self.source.uuid) if self.source else None\n            ),\n            \"reply_url\": (\n                url_for(\"api.single_reply\", source_uuid=self.source.uuid, reply_uuid=self.uuid)\n                if self.source\n                else None\n            ),\n            \"filename\": self.filename,\n            \"size\": self.size,\n            \"journalist_username\": self.journalist.username,\n            \"journalist_first_name\": self.journalist.first_name or \"\",\n            \"journalist_last_name\": self.journalist.last_name or \"\",\n            \"journalist_uuid\": self.journalist.uuid,\n            \"uuid\": self.uuid,\n            \"is_deleted_by_source\": self.deleted_by_source,\n            \"seen_by\": seen_by,\n        }",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(\n        self, journalist: \"Journalist\", source: Source, filename: str, storage: Storage\n    ) -> None:\n        self.journalist = journalist\n        self.source_id = source.id\n        self.uuid = str(uuid.uuid4())\n        self.filename = filename\n        self.size = os.stat(storage.path(source.filesystem_id, filename)).st_size",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "__repr__",
        "code": "def __repr__(self) -> str:\n        return f\"<Reply {self.filename!r}>\"",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "to_json",
        "code": "def to_json(self) -> \"Dict[str, Any]\":\n        seen_by = [r.journalist.uuid for r in SeenReply.query.filter(SeenReply.reply_id == self.id)]\n        return {\n            \"source_url\": (\n                url_for(\"api.single_source\", source_uuid=self.source.uuid) if self.source else None\n            ),\n            \"reply_url\": (\n                url_for(\"api.single_reply\", source_uuid=self.source.uuid, reply_uuid=self.uuid)\n                if self.source\n                else None\n            ),\n            \"filename\": self.filename,\n            \"size\": self.size,\n            \"journalist_username\": self.journalist.username,\n            \"journalist_first_name\": self.journalist.first_name or \"\",\n            \"journalist_last_name\": self.journalist.last_name or \"\",\n            \"journalist_uuid\": self.journalist.uuid,\n            \"uuid\": self.uuid,\n            \"is_deleted_by_source\": self.deleted_by_source,\n            \"seen_by\": seen_by,\n        }",
        "file": "models.py"
      },
      {
        "type": "class",
        "name": "SourceStar",
        "code": "class SourceStar(db.Model):\n    __tablename__ = \"source_stars\"\n    id = Column(\"id\", Integer, primary_key=True)\n    source_id = Column(\"source_id\", Integer, ForeignKey(\"sources.id\"))\n    starred = Column(\"starred\", Boolean, default=True)\n\n    def __eq__(self, other: object) -> bool:\n        if isinstance(other, SourceStar):\n            return (\n                self.source_id == other.source_id\n                and self.id == other.id\n                and self.starred == other.starred\n            )\n        return False\n\n    def __init__(self, source: Source, starred: bool = True) -> None:\n        self.source_id = source.id\n        self.starred = starred",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "__eq__",
        "code": "def __eq__(self, other: object) -> bool:\n        if isinstance(other, SourceStar):\n            return (\n                self.source_id == other.source_id\n                and self.id == other.id\n                and self.starred == other.starred\n            )\n        return False",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(self, source: Source, starred: bool = True) -> None:\n        self.source_id = source.id\n        self.starred = starred",
        "file": "models.py"
      },
      {
        "type": "class",
        "name": "InvalidUsernameException",
        "code": "class InvalidUsernameException(Exception):\n    \"\"\"Raised when a user logs in with an invalid username\"\"\"",
        "file": "models.py"
      },
      {
        "type": "class",
        "name": "FirstOrLastNameError",
        "code": "class FirstOrLastNameError(Exception):\n    \"\"\"Generic error for names that are invalid.\"\"\"\n\n    def __init__(self, msg: str) -> None:\n        super().__init__(msg)",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(self, msg: str) -> None:\n        super().__init__(msg)",
        "file": "models.py"
      },
      {
        "type": "class",
        "name": "InvalidNameLength",
        "code": "class InvalidNameLength(FirstOrLastNameError):\n    \"\"\"Raised when attempting to create a Journalist with an invalid name length.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__(gettext(\"Name too long\"))",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(self) -> None:\n        super().__init__(gettext(\"Name too long\"))",
        "file": "models.py"
      },
      {
        "type": "class",
        "name": "LoginThrottledException",
        "code": "class LoginThrottledException(Exception):\n    \"\"\"Raised when a user attempts to log in\n    too many times in a given time period\"\"\"",
        "file": "models.py"
      },
      {
        "type": "class",
        "name": "WrongPasswordException",
        "code": "class WrongPasswordException(Exception):\n    \"\"\"Raised when a user logs in with an incorrect password\"\"\"",
        "file": "models.py"
      },
      {
        "type": "class",
        "name": "PasswordError",
        "code": "class PasswordError(Exception):\n    \"\"\"Generic error for passwords that are invalid.\"\"\"",
        "file": "models.py"
      },
      {
        "type": "class",
        "name": "InvalidPasswordLength",
        "code": "class InvalidPasswordLength(PasswordError):\n    \"\"\"Raised when attempting to create a Journalist or log in with an invalid\n    password length.\n    \"\"\"\n\n    def __init__(self, passphrase: str) -> None:\n        self.passphrase_len = len(passphrase)\n\n    def __str__(self) -> str:\n        if self.passphrase_len > Journalist.MAX_PASSWORD_LEN:\n            return \"Password is too long.\"\n        if self.passphrase_len < Journalist.MIN_PASSWORD_LEN:\n            return \"Password is too short.\"\n        return \"\"  # return empty string that can be appended harmlessly",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(self, passphrase: str) -> None:\n        self.passphrase_len = len(passphrase)",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "__str__",
        "code": "def __str__(self) -> str:\n        if self.passphrase_len > Journalist.MAX_PASSWORD_LEN:\n            return \"Password is too long.\"\n        if self.passphrase_len < Journalist.MIN_PASSWORD_LEN:\n            return \"Password is too short.\"\n        return \"\"  # return empty string that can be appended harmlessly",
        "file": "models.py"
      },
      {
        "type": "class",
        "name": "NonDicewarePassword",
        "code": "class NonDicewarePassword(PasswordError):\n    \"\"\"Raised when attempting to validate a password that is not diceware-like\"\"\"",
        "file": "models.py"
      },
      {
        "type": "class",
        "name": "Journalist",
        "code": "class Journalist(db.Model):\n    __tablename__ = \"journalists\"\n    id = Column(Integer, primary_key=True)\n    uuid = Column(String(36), unique=True, nullable=False)\n    username = Column(String(255), nullable=False, unique=True)\n    first_name = Column(String(255), nullable=True)\n    last_name = Column(String(255), nullable=True)\n    pw_salt = Column(LargeBinary(32), nullable=True)\n    pw_hash = Column(LargeBinary(256), nullable=True)\n    is_admin = Column(Boolean)\n\n    otp_secret = Column(String(32), default=two_factor.random_base32)\n    is_totp = Column(Boolean, default=True)\n    hotp_counter = Column(Integer, default=0)\n    last_token = Column(String(6))\n\n    created_on = Column(DateTime, default=datetime.datetime.utcnow)\n    last_access = Column(DateTime)\n    passphrase_hash = Column(String(256))\n\n    login_attempts = relationship(\n        \"JournalistLoginAttempt\", backref=\"journalist\", cascade=\"all, delete\"\n    )\n\n    MIN_USERNAME_LEN = 3\n    MIN_NAME_LEN = 0\n    MAX_NAME_LEN = 100\n    INVALID_USERNAMES = [\"deleted\"]\n\n    def __init__(\n        self,\n        username: str,\n        password: str,\n        first_name: \"Optional[str]\" = None,\n        last_name: \"Optional[str]\" = None,\n        is_admin: bool = False,\n        otp_secret: \"Optional[str]\" = None,\n    ) -> None:\n        self.check_username_acceptable(username)\n        self.username = username\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n        # nosemgrep: python.django.security.audit.unvalidated-password.unvalidated-password\n        self.set_password(password)\n        self.is_admin = is_admin\n        self.uuid = str(uuid.uuid4())\n\n        if otp_secret:\n            self.set_hotp_secret(otp_secret)\n\n    def __repr__(self) -> str:\n        return \"<Journalist {}{}>\".format(self.username, \" [admin]\" if self.is_admin else \"\")\n\n    def _scrypt_hash(self, password: str, salt: bytes) -> bytes:\n        backend = default_backend()\n        scrypt_instance = scrypt.Scrypt(\n            length=64,\n            salt=salt,\n            n=2**14,\n            r=8,\n            p=1,\n            backend=backend,\n        )\n        return scrypt_instance.derive(password.encode(\"utf-8\"))\n\n    MAX_PASSWORD_LEN = 128\n    MIN_PASSWORD_LEN = 14\n\n    def set_password(self, passphrase: \"Optional[str]\") -> None:\n        if passphrase is None:\n            raise PasswordError()\n\n        self.check_password_acceptable(passphrase)\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        # \"migrate\" from the legacy case\n        if not self.passphrase_hash:\n            self.passphrase_hash = hasher.hash(passphrase)\n            # argon2 creates one merged field that embeds randomly generated\n            # salt in the output like $alg$salt$hash\n            self.pw_hash = None\n            self.pw_salt = None\n\n        # Don't do anything if user's password hasn't changed.\n        if self.passphrase_hash and self.valid_password(passphrase):\n            return\n\n        self.passphrase_hash = hasher.hash(passphrase)\n\n    def set_name(self, first_name: Optional[str], last_name: Optional[str]) -> None:\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n\n    @classmethod\n    def check_username_acceptable(cls, username: str) -> None:\n        if len(username) < cls.MIN_USERNAME_LEN:\n            raise InvalidUsernameException(\n                ngettext(\n                    \"Must be at least {num} character long.\",\n                    \"Must be at least {num} characters long.\",\n                    cls.MIN_USERNAME_LEN,\n                ).format(num=cls.MIN_USERNAME_LEN)\n            )\n        if username in cls.INVALID_USERNAMES:\n            raise InvalidUsernameException(\n                gettext(\n                    \"This username is invalid because it is reserved \"\n                    \"for internal use by the software.\"\n                )\n            )\n\n    @classmethod\n    def check_name_acceptable(cls, name: str) -> None:\n        # Enforce a reasonable maximum length for names\n        if len(name) > cls.MAX_NAME_LEN:\n            raise InvalidNameLength()\n\n    @classmethod\n    def check_password_acceptable(cls, password: str) -> None:\n        # Enforce a reasonable maximum length for passwords to avoid DoS\n        if len(password) > cls.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Enforce a reasonable minimum length for new passwords\n        if len(password) < cls.MIN_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Ensure all passwords are \"diceware-like\"\n        if len(password.split()) < 7:\n            raise NonDicewarePassword()\n\n    def valid_password(self, passphrase: \"Optional[str]\") -> bool:\n        if not passphrase:\n            return False\n\n        # Avoid hashing passwords that are over the maximum length\n        if len(passphrase) > self.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(passphrase)\n\n        # No check on minimum password length here because some passwords\n        # may have been set prior to setting the minimum password length.\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        if self.passphrase_hash:\n            # default case\n            try:\n                is_valid = hasher.verify(self.passphrase_hash, passphrase)\n            except argon2.exceptions.VerificationError:\n                is_valid = False\n        else:\n            # legacy support\n            if self.pw_salt is None:\n                raise ValueError(\n                    f\"Should never happen: pw_salt is none for legacy Journalist {self.id}\"\n                )\n\n            # For type checking\n            if not isinstance(self.pw_hash, bytes):\n                raise RuntimeError(\"self.pw_hash isn't bytes\")\n\n            is_valid = compare_digest(self._scrypt_hash(passphrase, self.pw_salt), self.pw_hash)\n\n        # If the passphrase isn't valid, bail out now\n        if not is_valid:\n            return False\n        # From here on we can assume the passphrase was valid\n\n        # Perform migration checks\n        needs_update = False\n        if self.passphrase_hash:\n            # Check if the hash needs an update\n            if hasher.check_needs_rehash(self.passphrase_hash):\n                self.passphrase_hash = hasher.hash(passphrase)\n                needs_update = True\n        else:\n            # Migrate to an argon2 hash, which creates one merged field\n            # that embeds randomly generated salt in the output like\n            # $alg$salt$hash\n            self.passphrase_hash = hasher.hash(passphrase)\n            self.pw_salt = None\n            self.pw_hash = None\n            needs_update = True\n\n        if needs_update:\n            db.session.add(self)\n            db.session.commit()\n\n        return is_valid\n\n    def regenerate_totp_shared_secret(self) -> None:\n        self.otp_secret = two_factor.random_base32()\n\n    def set_hotp_secret(self, otp_secret: str) -> None:\n        self.otp_secret = base64.b32encode(binascii.unhexlify(otp_secret.replace(\" \", \"\"))).decode(\n            \"ascii\"\n        )\n        self.is_totp = False\n        self.hotp_counter = 0\n\n    @property\n    def totp(self) -> two_factor.TOTP:\n        if self.is_totp:\n            return two_factor.TOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using TOTP\")\n\n    @property\n    def hotp(self) -> two_factor.HOTP:\n        if not self.is_totp:\n            return two_factor.HOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using HOTP\")\n\n    @property\n    def formatted_otp_secret(self) -> str:\n        return two_factor.format_secret(self.otp_secret)\n\n    def verify_2fa_token(self, token: Optional[str]) -> str:\n        if not token:\n            raise two_factor.OtpTokenInvalid()\n\n        # Strip from 2fa tokens the whitespace that many clients add for readability\n        sanitized_token = \"\".join(token.split())\n\n        # Reject OTP tokens that have already been used\n        if self.last_token is not None and self.last_token == sanitized_token:\n            raise two_factor.OtpTokenInvalid(\"Token already used\")\n\n        if self.is_totp:\n            # TOTP\n            self.totp.verify(sanitized_token, datetime.datetime.utcnow())\n        else:\n            # HOTP\n            successful_counter_value = self.hotp.verify(sanitized_token, self.hotp_counter)\n            self.hotp_counter = successful_counter_value + 1\n            db.session.commit()\n\n        return sanitized_token\n\n    _LOGIN_ATTEMPT_PERIOD = 60  # seconds\n    _MAX_LOGIN_ATTEMPTS_PER_PERIOD = 5\n\n    @classmethod\n    def throttle_login(cls, user: \"Journalist\") -> None:\n        # Record the login attempt...\n        login_attempt = JournalistLoginAttempt(user)\n        db.session.add(login_attempt)\n        db.session.commit()\n\n        # ...and reject it if they have exceeded the threshold\n        login_attempt_period = datetime.datetime.utcnow() - datetime.timedelta(\n            seconds=cls._LOGIN_ATTEMPT_PERIOD\n        )\n        attempts_within_period = (\n            JournalistLoginAttempt.query.filter(JournalistLoginAttempt.journalist_id == user.id)\n            .filter(JournalistLoginAttempt.timestamp > login_attempt_period)\n            .all()\n        )\n        if len(attempts_within_period) > cls._MAX_LOGIN_ATTEMPTS_PER_PERIOD:\n            raise LoginThrottledException(\n                f\"throttled ({len(attempts_within_period)} attempts in last \"\n                f\"{cls._LOGIN_ATTEMPT_PERIOD} seconds)\"\n            )\n\n    @classmethod\n    def login(\n        cls,\n        username: str,\n        password: Optional[str],\n        token: Optional[str],\n    ) -> \"Journalist\":\n        try:\n            user = Journalist.query.filter_by(username=username).one()\n        except NoResultFound:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        if user.username in Journalist.INVALID_USERNAMES:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        # Login hardening measures\n        cls.throttle_login(user)\n\n        sanitized_token = user.verify_2fa_token(token)\n\n        # If it is valid, store the token that to prevent OTP token reuse\n        user.last_token = sanitized_token\n        db.session.commit()\n\n        if not user.valid_password(password):\n            raise WrongPasswordException(\"invalid password\")\n\n        return user\n\n    def to_json(self, all_info: bool = True) -> Dict[str, Any]:\n        \"\"\"Returns a JSON representation of the journalist user. If all_info is\n        False, potentially sensitive or extraneous fields are excluded. Note\n        that both representations do NOT include credentials.\"\"\"\n        json_user: Dict[str, Any] = {\n            \"username\": self.username,\n            \"uuid\": self.uuid,\n            \"first_name\": self.first_name,\n            \"last_name\": self.last_name,\n        }\n\n        if all_info is True:\n            json_user[\"is_admin\"] = self.is_admin\n            if self.last_access:\n                json_user[\"last_login\"] = self.last_access\n            else:\n                json_user[\"last_login\"] = None\n\n        return json_user\n\n    def is_deleted_user(self) -> bool:\n        \"\"\"Is this the special \"deleted\" user managed by the system?\"\"\"\n        return self.username == \"deleted\"\n\n    @classmethod\n    def get_deleted(cls) -> \"Journalist\":\n        \"\"\"Get a system user that represents deleted journalists for referential integrity\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = Journalist.query.filter_by(username=\"deleted\").one_or_none()\n        if deleted is None:\n            # Lazily create\n            deleted = cls(\n                # Use a placeholder username to bypass validation that would reject\n                # \"deleted\" as unusable\n                username=\"placeholder\",\n                # We store a randomly generated passphrase for this account that is\n                # never revealed to anyone.\n                password=PassphraseGenerator.get_default().generate_passphrase(),\n            )\n            deleted.username = \"deleted\"\n            db.session.add(deleted)\n        return deleted\n\n    def delete(self) -> None:\n        \"\"\"delete a journalist, migrating some data over to the \"deleted\" journalist\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = self.get_deleted()\n        # All replies should be reassociated with the \"deleted\" journalist\n        for reply in Reply.query.filter_by(journalist_id=self.id).all():\n            reply.journalist_id = deleted.id\n            db.session.add(reply)\n\n        # For seen indicators, we need to make sure one doesn't already exist\n        # otherwise it'll hit a unique key conflict\n        already_seen_files = {\n            file.file_id for file in SeenFile.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for file in SeenFile.query.filter_by(journalist_id=self.id).all():\n            if file.file_id in already_seen_files:\n                db.session.delete(file)\n            else:\n                file.journalist_id = deleted.id\n                db.session.add(file)\n\n        already_seen_messages = {\n            message.message_id\n            for message in SeenMessage.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for message in SeenMessage.query.filter_by(journalist_id=self.id).all():\n            if message.message_id in already_seen_messages:\n                db.session.delete(message)\n            else:\n                message.journalist_id = deleted.id\n                db.session.add(message)\n\n        already_seen_replies = {\n            reply.reply_id for reply in SeenReply.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for reply in SeenReply.query.filter_by(journalist_id=self.id).all():\n            if reply.reply_id in already_seen_replies:\n                db.session.delete(reply)\n            else:\n                reply.journalist_id = deleted.id\n                db.session.add(reply)\n\n        # For the rest of the associated data we rely on cascading deletions\n        db.session.delete(self)",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(\n        self,\n        username: str,\n        password: str,\n        first_name: \"Optional[str]\" = None,\n        last_name: \"Optional[str]\" = None,\n        is_admin: bool = False,\n        otp_secret: \"Optional[str]\" = None,\n    ) -> None:\n        self.check_username_acceptable(username)\n        self.username = username\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name\n        # nosemgrep: python.django.security.audit.unvalidated-password.unvalidated-password\n        self.set_password(password)\n        self.is_admin = is_admin\n        self.uuid = str(uuid.uuid4())\n\n        if otp_secret:\n            self.set_hotp_secret(otp_secret)",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "__repr__",
        "code": "def __repr__(self) -> str:\n        return \"<Journalist {}{}>\".format(self.username, \" [admin]\" if self.is_admin else \"\")",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "_scrypt_hash",
        "code": "def _scrypt_hash(self, password: str, salt: bytes) -> bytes:\n        backend = default_backend()\n        scrypt_instance = scrypt.Scrypt(\n            length=64,\n            salt=salt,\n            n=2**14,\n            r=8,\n            p=1,\n            backend=backend,\n        )\n        return scrypt_instance.derive(password.encode(\"utf-8\"))",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "set_password",
        "code": "def set_password(self, passphrase: \"Optional[str]\") -> None:\n        if passphrase is None:\n            raise PasswordError()\n\n        self.check_password_acceptable(passphrase)\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        # \"migrate\" from the legacy case\n        if not self.passphrase_hash:\n            self.passphrase_hash = hasher.hash(passphrase)\n            # argon2 creates one merged field that embeds randomly generated\n            # salt in the output like $alg$salt$hash\n            self.pw_hash = None\n            self.pw_salt = None\n\n        # Don't do anything if user's password hasn't changed.\n        if self.passphrase_hash and self.valid_password(passphrase):\n            return\n\n        self.passphrase_hash = hasher.hash(passphrase)",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "set_name",
        "code": "def set_name(self, first_name: Optional[str], last_name: Optional[str]) -> None:\n        if first_name:\n            self.check_name_acceptable(first_name)\n            self.first_name = first_name\n        if last_name:\n            self.check_name_acceptable(last_name)\n            self.last_name = last_name",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "check_username_acceptable",
        "code": "def check_username_acceptable(cls, username: str) -> None:\n        if len(username) < cls.MIN_USERNAME_LEN:\n            raise InvalidUsernameException(\n                ngettext(\n                    \"Must be at least {num} character long.\",\n                    \"Must be at least {num} characters long.\",\n                    cls.MIN_USERNAME_LEN,\n                ).format(num=cls.MIN_USERNAME_LEN)\n            )\n        if username in cls.INVALID_USERNAMES:\n            raise InvalidUsernameException(\n                gettext(\n                    \"This username is invalid because it is reserved \"\n                    \"for internal use by the software.\"\n                )\n            )",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "check_name_acceptable",
        "code": "def check_name_acceptable(cls, name: str) -> None:\n        # Enforce a reasonable maximum length for names\n        if len(name) > cls.MAX_NAME_LEN:\n            raise InvalidNameLength()",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "check_password_acceptable",
        "code": "def check_password_acceptable(cls, password: str) -> None:\n        # Enforce a reasonable maximum length for passwords to avoid DoS\n        if len(password) > cls.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Enforce a reasonable minimum length for new passwords\n        if len(password) < cls.MIN_PASSWORD_LEN:\n            raise InvalidPasswordLength(password)\n\n        # Ensure all passwords are \"diceware-like\"\n        if len(password.split()) < 7:\n            raise NonDicewarePassword()",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "valid_password",
        "code": "def valid_password(self, passphrase: \"Optional[str]\") -> bool:\n        if not passphrase:\n            return False\n\n        # Avoid hashing passwords that are over the maximum length\n        if len(passphrase) > self.MAX_PASSWORD_LEN:\n            raise InvalidPasswordLength(passphrase)\n\n        # No check on minimum password length here because some passwords\n        # may have been set prior to setting the minimum password length.\n\n        hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n        if self.passphrase_hash:\n            # default case\n            try:\n                is_valid = hasher.verify(self.passphrase_hash, passphrase)\n            except argon2.exceptions.VerificationError:\n                is_valid = False\n        else:\n            # legacy support\n            if self.pw_salt is None:\n                raise ValueError(\n                    f\"Should never happen: pw_salt is none for legacy Journalist {self.id}\"\n                )\n\n            # For type checking\n            if not isinstance(self.pw_hash, bytes):\n                raise RuntimeError(\"self.pw_hash isn't bytes\")\n\n            is_valid = compare_digest(self._scrypt_hash(passphrase, self.pw_salt), self.pw_hash)\n\n        # If the passphrase isn't valid, bail out now\n        if not is_valid:\n            return False\n        # From here on we can assume the passphrase was valid\n\n        # Perform migration checks\n        needs_update = False\n        if self.passphrase_hash:\n            # Check if the hash needs an update\n            if hasher.check_needs_rehash(self.passphrase_hash):\n                self.passphrase_hash = hasher.hash(passphrase)\n                needs_update = True\n        else:\n            # Migrate to an argon2 hash, which creates one merged field\n            # that embeds randomly generated salt in the output like\n            # $alg$salt$hash\n            self.passphrase_hash = hasher.hash(passphrase)\n            self.pw_salt = None\n            self.pw_hash = None\n            needs_update = True\n\n        if needs_update:\n            db.session.add(self)\n            db.session.commit()\n\n        return is_valid",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "regenerate_totp_shared_secret",
        "code": "def regenerate_totp_shared_secret(self) -> None:\n        self.otp_secret = two_factor.random_base32()",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "set_hotp_secret",
        "code": "def set_hotp_secret(self, otp_secret: str) -> None:\n        self.otp_secret = base64.b32encode(binascii.unhexlify(otp_secret.replace(\" \", \"\"))).decode(\n            \"ascii\"\n        )\n        self.is_totp = False\n        self.hotp_counter = 0",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "totp",
        "code": "def totp(self) -> two_factor.TOTP:\n        if self.is_totp:\n            return two_factor.TOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using TOTP\")",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "hotp",
        "code": "def hotp(self) -> two_factor.HOTP:\n        if not self.is_totp:\n            return two_factor.HOTP(self.otp_secret)\n        else:\n            raise ValueError(f\"{self} is not using HOTP\")",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "formatted_otp_secret",
        "code": "def formatted_otp_secret(self) -> str:\n        return two_factor.format_secret(self.otp_secret)",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "verify_2fa_token",
        "code": "def verify_2fa_token(self, token: Optional[str]) -> str:\n        if not token:\n            raise two_factor.OtpTokenInvalid()\n\n        # Strip from 2fa tokens the whitespace that many clients add for readability\n        sanitized_token = \"\".join(token.split())\n\n        # Reject OTP tokens that have already been used\n        if self.last_token is not None and self.last_token == sanitized_token:\n            raise two_factor.OtpTokenInvalid(\"Token already used\")\n\n        if self.is_totp:\n            # TOTP\n            self.totp.verify(sanitized_token, datetime.datetime.utcnow())\n        else:\n            # HOTP\n            successful_counter_value = self.hotp.verify(sanitized_token, self.hotp_counter)\n            self.hotp_counter = successful_counter_value + 1\n            db.session.commit()\n\n        return sanitized_token",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "throttle_login",
        "code": "def throttle_login(cls, user: \"Journalist\") -> None:\n        # Record the login attempt...\n        login_attempt = JournalistLoginAttempt(user)\n        db.session.add(login_attempt)\n        db.session.commit()\n\n        # ...and reject it if they have exceeded the threshold\n        login_attempt_period = datetime.datetime.utcnow() - datetime.timedelta(\n            seconds=cls._LOGIN_ATTEMPT_PERIOD\n        )\n        attempts_within_period = (\n            JournalistLoginAttempt.query.filter(JournalistLoginAttempt.journalist_id == user.id)\n            .filter(JournalistLoginAttempt.timestamp > login_attempt_period)\n            .all()\n        )\n        if len(attempts_within_period) > cls._MAX_LOGIN_ATTEMPTS_PER_PERIOD:\n            raise LoginThrottledException(\n                f\"throttled ({len(attempts_within_period)} attempts in last \"\n                f\"{cls._LOGIN_ATTEMPT_PERIOD} seconds)\"\n            )",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "login",
        "code": "def login(\n        cls,\n        username: str,\n        password: Optional[str],\n        token: Optional[str],\n    ) -> \"Journalist\":\n        try:\n            user = Journalist.query.filter_by(username=username).one()\n        except NoResultFound:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        if user.username in Journalist.INVALID_USERNAMES:\n            raise InvalidUsernameException(gettext(\"Invalid username\"))\n\n        # Login hardening measures\n        cls.throttle_login(user)\n\n        sanitized_token = user.verify_2fa_token(token)\n\n        # If it is valid, store the token that to prevent OTP token reuse\n        user.last_token = sanitized_token\n        db.session.commit()\n\n        if not user.valid_password(password):\n            raise WrongPasswordException(\"invalid password\")\n\n        return user",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "to_json",
        "code": "def to_json(self, all_info: bool = True) -> Dict[str, Any]:\n        \"\"\"Returns a JSON representation of the journalist user. If all_info is\n        False, potentially sensitive or extraneous fields are excluded. Note\n        that both representations do NOT include credentials.\"\"\"\n        json_user: Dict[str, Any] = {\n            \"username\": self.username,\n            \"uuid\": self.uuid,\n            \"first_name\": self.first_name,\n            \"last_name\": self.last_name,\n        }\n\n        if all_info is True:\n            json_user[\"is_admin\"] = self.is_admin\n            if self.last_access:\n                json_user[\"last_login\"] = self.last_access\n            else:\n                json_user[\"last_login\"] = None\n\n        return json_user",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "is_deleted_user",
        "code": "def is_deleted_user(self) -> bool:\n        \"\"\"Is this the special \"deleted\" user managed by the system?\"\"\"\n        return self.username == \"deleted\"",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "get_deleted",
        "code": "def get_deleted(cls) -> \"Journalist\":\n        \"\"\"Get a system user that represents deleted journalists for referential integrity\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = Journalist.query.filter_by(username=\"deleted\").one_or_none()\n        if deleted is None:\n            # Lazily create\n            deleted = cls(\n                # Use a placeholder username to bypass validation that would reject\n                # \"deleted\" as unusable\n                username=\"placeholder\",\n                # We store a randomly generated passphrase for this account that is\n                # never revealed to anyone.\n                password=PassphraseGenerator.get_default().generate_passphrase(),\n            )\n            deleted.username = \"deleted\"\n            db.session.add(deleted)\n        return deleted",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "delete",
        "code": "def delete(self) -> None:\n        \"\"\"delete a journalist, migrating some data over to the \"deleted\" journalist\n\n        Callers must commit the session themselves\n        \"\"\"\n        deleted = self.get_deleted()\n        # All replies should be reassociated with the \"deleted\" journalist\n        for reply in Reply.query.filter_by(journalist_id=self.id).all():\n            reply.journalist_id = deleted.id\n            db.session.add(reply)\n\n        # For seen indicators, we need to make sure one doesn't already exist\n        # otherwise it'll hit a unique key conflict\n        already_seen_files = {\n            file.file_id for file in SeenFile.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for file in SeenFile.query.filter_by(journalist_id=self.id).all():\n            if file.file_id in already_seen_files:\n                db.session.delete(file)\n            else:\n                file.journalist_id = deleted.id\n                db.session.add(file)\n\n        already_seen_messages = {\n            message.message_id\n            for message in SeenMessage.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for message in SeenMessage.query.filter_by(journalist_id=self.id).all():\n            if message.message_id in already_seen_messages:\n                db.session.delete(message)\n            else:\n                message.journalist_id = deleted.id\n                db.session.add(message)\n\n        already_seen_replies = {\n            reply.reply_id for reply in SeenReply.query.filter_by(journalist_id=deleted.id).all()\n        }\n        for reply in SeenReply.query.filter_by(journalist_id=self.id).all():\n            if reply.reply_id in already_seen_replies:\n                db.session.delete(reply)\n            else:\n                reply.journalist_id = deleted.id\n                db.session.add(reply)\n\n        # For the rest of the associated data we rely on cascading deletions\n        db.session.delete(self)",
        "file": "models.py"
      },
      {
        "type": "class",
        "name": "SeenFile",
        "code": "class SeenFile(db.Model):\n    __tablename__ = \"seen_files\"\n    __table_args__ = (db.UniqueConstraint(\"file_id\", \"journalist_id\"),)\n    id = Column(Integer, primary_key=True)\n    file_id = Column(Integer, ForeignKey(\"submissions.id\"), nullable=False)\n    journalist_id = Column(Integer, ForeignKey(\"journalists.id\"), nullable=False)\n    file = relationship(\n        \"Submission\",\n        backref=backref(\"seen_files\", lazy=\"dynamic\", cascade=\"all,delete\"),\n    )\n    journalist = relationship(\"Journalist\", backref=backref(\"seen_files\"))",
        "file": "models.py"
      },
      {
        "type": "class",
        "name": "SeenMessage",
        "code": "class SeenMessage(db.Model):\n    __tablename__ = \"seen_messages\"\n    __table_args__ = (db.UniqueConstraint(\"message_id\", \"journalist_id\"),)\n    id = Column(Integer, primary_key=True)\n    message_id = Column(Integer, ForeignKey(\"submissions.id\"), nullable=False)\n    journalist_id = Column(Integer, ForeignKey(\"journalists.id\"), nullable=False)\n    message = relationship(\n        \"Submission\",\n        backref=backref(\"seen_messages\", lazy=\"dynamic\", cascade=\"all,delete\"),\n    )\n    journalist = relationship(\"Journalist\", backref=backref(\"seen_messages\"))",
        "file": "models.py"
      },
      {
        "type": "class",
        "name": "SeenReply",
        "code": "class SeenReply(db.Model):\n    __tablename__ = \"seen_replies\"\n    __table_args__ = (db.UniqueConstraint(\"reply_id\", \"journalist_id\"),)\n    id = Column(Integer, primary_key=True)\n    reply_id = Column(Integer, ForeignKey(\"replies.id\"), nullable=False)\n    journalist_id = Column(Integer, ForeignKey(\"journalists.id\"), nullable=False)\n    reply = relationship(\"Reply\", backref=backref(\"seen_replies\", cascade=\"all,delete\"))\n    journalist = relationship(\"Journalist\", backref=backref(\"seen_replies\"))",
        "file": "models.py"
      },
      {
        "type": "class",
        "name": "JournalistLoginAttempt",
        "code": "class JournalistLoginAttempt(db.Model):\n    \"\"\"This model keeps track of journalist's login attempts so we can\n    rate limit them in order to prevent attackers from brute forcing\n    passwords or two-factor tokens.\"\"\"\n\n    __tablename__ = \"journalist_login_attempt\"\n    id = Column(Integer, primary_key=True)\n    timestamp = Column(DateTime, default=datetime.datetime.utcnow, nullable=True)\n    journalist_id = Column(Integer, ForeignKey(\"journalists.id\"), nullable=False)\n\n    def __init__(self, journalist: Journalist) -> None:\n        self.journalist = journalist",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(self, journalist: Journalist) -> None:\n        self.journalist = journalist",
        "file": "models.py"
      },
      {
        "type": "class",
        "name": "InstanceConfig",
        "code": "class InstanceConfig(db.Model):\n    \"\"\"Versioned key-value store of settings configurable from the journalist\n    interface.  The current version has valid_until=0 (unix epoch start)\n    \"\"\"\n\n    # Limits length of org name used in SI and JI titles, image alt texts etc.\n    MAX_ORG_NAME_LEN = 64\n\n    __tablename__ = \"instance_config\"\n    version = Column(Integer, primary_key=True)\n    valid_until = Column(\n        DateTime,\n        default=datetime.datetime.fromtimestamp(0),\n        nullable=False,\n        unique=True,\n    )\n    allow_document_uploads = Column(Boolean, default=True)\n    organization_name = Column(String(255), nullable=True, default=\"SecureDrop\")\n    initial_message_min_len = Column(Integer, nullable=False, default=0, server_default=\"0\")\n    reject_message_with_codename = Column(\n        Boolean, nullable=False, default=False, server_default=\"0\"\n    )\n\n    # Columns not listed here will be included by InstanceConfig.copy() when\n    # updating the configuration.\n    metadata_cols = [\"version\", \"valid_until\"]\n\n    def __repr__(self) -> str:\n        return (\n            f\"<InstanceConfig(version={self.version}, valid_until={self.valid_until}, \"\n            f\"allow_document_uploads={self.allow_document_uploads}, \"\n            f\"organization_name={self.organization_name}, \"\n            f\"initial_message_min_len={self.initial_message_min_len}, \"\n            f\"reject_message_with_codename={self.reject_message_with_codename})>\"\n        )\n\n    def copy(self) -> \"InstanceConfig\":\n        \"\"\"Make a copy of only the configuration columns of the given\n        InstanceConfig object: i.e., excluding metadata_cols.\n        \"\"\"\n\n        new = type(self)()\n        for col in self.__table__.columns:\n            if col.name in self.metadata_cols:\n                continue\n\n            setattr(new, col.name, getattr(self, col.name))\n\n        return new\n\n    @classmethod\n    def get_default(cls, refresh: bool = False) -> \"InstanceConfig\":\n        global _default_instance_config\n        if (_default_instance_config is None) or (refresh is True):\n            _default_instance_config = InstanceConfig.get_current()\n        return _default_instance_config\n\n    @classmethod\n    def get_current(cls) -> \"InstanceConfig\":\n        \"\"\"If the database was created via db.create_all(), data migrations\n        weren't run, and the \"instance_config\" table is empty.  In this case,\n        save and return a base configuration derived from each setting's\n        column-level default.\n        \"\"\"\n\n        try:\n            return cls.query.filter(cls.valid_until == datetime.datetime.fromtimestamp(0)).one()\n        except NoResultFound:\n            try:\n                current = cls()\n                db.session.add(current)\n                db.session.commit()\n                return current\n            except IntegrityError:\n                return cls.query.filter(cls.valid_until == datetime.datetime.fromtimestamp(0)).one()\n\n    @classmethod\n    def check_name_acceptable(cls, name: str) -> None:\n        # Enforce a reasonable maximum length for names\n        if name is None or len(name) == 0:\n            raise InvalidNameLength()\n        if len(name) > cls.MAX_ORG_NAME_LEN:\n            raise InvalidNameLength()\n\n    @classmethod\n    def set_organization_name(cls, name: str) -> None:\n        \"\"\"Invalidate the current configuration and append a new one with the\n        new organization name.\n        \"\"\"\n\n        old = cls.get_current()\n        old.valid_until = datetime.datetime.utcnow()\n        db.session.add(old)\n\n        new = old.copy()\n        cls.check_name_acceptable(name)\n        new.organization_name = name\n        db.session.add(new)\n\n        db.session.commit()\n\n    @classmethod\n    def update_submission_prefs(\n        cls, allow_uploads: bool, min_length: int, reject_codenames: bool\n    ) -> None:\n        \"\"\"Invalidate the current configuration and append a new one with the\n        updated submission preferences.\n        \"\"\"\n\n        old = cls.get_current()\n        old.valid_until = datetime.datetime.utcnow()\n        db.session.add(old)\n\n        new = old.copy()\n        new.allow_document_uploads = allow_uploads\n        new.initial_message_min_len = min_length\n        new.reject_message_with_codename = reject_codenames\n        db.session.add(new)\n\n        db.session.commit()",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "__repr__",
        "code": "def __repr__(self) -> str:\n        return (\n            f\"<InstanceConfig(version={self.version}, valid_until={self.valid_until}, \"\n            f\"allow_document_uploads={self.allow_document_uploads}, \"\n            f\"organization_name={self.organization_name}, \"\n            f\"initial_message_min_len={self.initial_message_min_len}, \"\n            f\"reject_message_with_codename={self.reject_message_with_codename})>\"\n        )",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "copy",
        "code": "def copy(self) -> \"InstanceConfig\":\n        \"\"\"Make a copy of only the configuration columns of the given\n        InstanceConfig object: i.e., excluding metadata_cols.\n        \"\"\"\n\n        new = type(self)()\n        for col in self.__table__.columns:\n            if col.name in self.metadata_cols:\n                continue\n\n            setattr(new, col.name, getattr(self, col.name))\n\n        return new",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "get_default",
        "code": "def get_default(cls, refresh: bool = False) -> \"InstanceConfig\":\n        global _default_instance_config\n        if (_default_instance_config is None) or (refresh is True):\n            _default_instance_config = InstanceConfig.get_current()\n        return _default_instance_config",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "get_current",
        "code": "def get_current(cls) -> \"InstanceConfig\":\n        \"\"\"If the database was created via db.create_all(), data migrations\n        weren't run, and the \"instance_config\" table is empty.  In this case,\n        save and return a base configuration derived from each setting's\n        column-level default.\n        \"\"\"\n\n        try:\n            return cls.query.filter(cls.valid_until == datetime.datetime.fromtimestamp(0)).one()\n        except NoResultFound:\n            try:\n                current = cls()\n                db.session.add(current)\n                db.session.commit()\n                return current\n            except IntegrityError:\n                return cls.query.filter(cls.valid_until == datetime.datetime.fromtimestamp(0)).one()",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "check_name_acceptable",
        "code": "def check_name_acceptable(cls, name: str) -> None:\n        # Enforce a reasonable maximum length for names\n        if name is None or len(name) == 0:\n            raise InvalidNameLength()\n        if len(name) > cls.MAX_ORG_NAME_LEN:\n            raise InvalidNameLength()",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "set_organization_name",
        "code": "def set_organization_name(cls, name: str) -> None:\n        \"\"\"Invalidate the current configuration and append a new one with the\n        new organization name.\n        \"\"\"\n\n        old = cls.get_current()\n        old.valid_until = datetime.datetime.utcnow()\n        db.session.add(old)\n\n        new = old.copy()\n        cls.check_name_acceptable(name)\n        new.organization_name = name\n        db.session.add(new)\n\n        db.session.commit()",
        "file": "models.py"
      },
      {
        "type": "function",
        "name": "update_submission_prefs",
        "code": "def update_submission_prefs(\n        cls, allow_uploads: bool, min_length: int, reject_codenames: bool\n    ) -> None:\n        \"\"\"Invalidate the current configuration and append a new one with the\n        updated submission preferences.\n        \"\"\"\n\n        old = cls.get_current()\n        old.valid_until = datetime.datetime.utcnow()\n        db.session.add(old)\n\n        new = old.copy()\n        new.allow_document_uploads = allow_uploads\n        new.initial_message_min_len = min_length\n        new.reject_message_with_codename = reject_codenames\n        db.session.add(new)\n\n        db.session.commit()",
        "file": "models.py"
      }
    ],
    "i18n.py": [
      {
        "type": "class",
        "name": "RequestLocaleInfo",
        "code": "class RequestLocaleInfo:\n    \"\"\"\n    Convenience wrapper around a babel.core.Locale.\n    \"\"\"\n\n    def __init__(self, locale: str):\n        self.locale = Locale.parse(locale)\n\n        # This attribute can be set to `True` to differentiate multiple\n        # locales currently available (supported) for the same language.\n        self.use_display_name = False\n\n    def __str__(self) -> str:\n        \"\"\"\n        The Babel string representation of the locale.\n        \"\"\"\n        return str(self.locale)\n\n    @property\n    def display_name(self) -> str:\n        \"\"\"\n        Give callers (i.e., templates) the `Locale` object's display name when\n        such resolution is warranted, otherwise the language name---as\n        determined by `map_locale_display_names()`.\n        \"\"\"\n        if self.use_display_name:\n            return self.locale.display_name\n        return self.locale.language_name\n\n    @property\n    def text_direction(self) -> str:\n        \"\"\"\n        The Babel text direction: ltr or rtl.\n\n        Used primarily to set text direction in HTML via the \"dir\"\n        attribute.\n        \"\"\"\n        return self.locale.text_direction\n\n    @property\n    def language(self) -> str:\n        \"\"\"\n        The Babel language name.\n\n        Just the language, without subtag info like region or script.\n        \"\"\"\n        return self.locale.language\n\n    @property\n    def id(self) -> str:\n        \"\"\"\n        The Babel string representation of the locale.\n\n        This should match the name of the directory containing its\n        translations.\n        \"\"\"\n        return str(self.locale)\n\n    @property\n    def language_tag(self) -> str:\n        \"\"\"\n        Returns a BCP47/RFC5646 language tag for the locale.\n\n        Language tags are used in HTTP headers and the HTML lang\n        attribute.\n        \"\"\"\n        return get_locale_identifier(parse_locale(str(self.locale)), sep=\"-\")",
        "file": "i18n.py"
      },
      {
        "type": "function",
        "name": "__init__",
        "code": "def __init__(self, locale: str):\n        self.locale = Locale.parse(locale)\n\n        # This attribute can be set to `True` to differentiate multiple\n        # locales currently available (supported) for the same language.\n        self.use_display_name = False",
        "file": "i18n.py"
      },
      {
        "type": "function",
        "name": "__str__",
        "code": "def __str__(self) -> str:\n        \"\"\"\n        The Babel string representation of the locale.\n        \"\"\"\n        return str(self.locale)",
        "file": "i18n.py"
      },
      {
        "type": "function",
        "name": "display_name",
        "code": "def display_name(self) -> str:\n        \"\"\"\n        Give callers (i.e., templates) the `Locale` object's display name when\n        such resolution is warranted, otherwise the language name---as\n        determined by `map_locale_display_names()`.\n        \"\"\"\n        if self.use_display_name:\n            return self.locale.display_name\n        return self.locale.language_name",
        "file": "i18n.py"
      },
      {
        "type": "function",
        "name": "text_direction",
        "code": "def text_direction(self) -> str:\n        \"\"\"\n        The Babel text direction: ltr or rtl.\n\n        Used primarily to set text direction in HTML via the \"dir\"\n        attribute.\n        \"\"\"\n        return self.locale.text_direction",
        "file": "i18n.py"
      },
      {
        "type": "function",
        "name": "language",
        "code": "def language(self) -> str:\n        \"\"\"\n        The Babel language name.\n\n        Just the language, without subtag info like region or script.\n        \"\"\"\n        return self.locale.language",
        "file": "i18n.py"
      },
      {
        "type": "function",
        "name": "id",
        "code": "def id(self) -> str:\n        \"\"\"\n        The Babel string representation of the locale.\n\n        This should match the name of the directory containing its\n        translations.\n        \"\"\"\n        return str(self.locale)",
        "file": "i18n.py"
      },
      {
        "type": "function",
        "name": "language_tag",
        "code": "def language_tag(self) -> str:\n        \"\"\"\n        Returns a BCP47/RFC5646 language tag for the locale.\n\n        Language tags are used in HTTP headers and the HTML lang\n        attribute.\n        \"\"\"\n        return get_locale_identifier(parse_locale(str(self.locale)), sep=\"-\")",
        "file": "i18n.py"
      },
      {
        "type": "function",
        "name": "configure_babel",
        "code": "def configure_babel(config: SecureDropConfig, app: Flask) -> Babel:\n    \"\"\"\n    Set up Flask-Babel according to the SecureDrop configuration.\n    \"\"\"\n    # Tell Babel where to find our translations.\n    translations_directory = str(config.TRANSLATION_DIRS.absolute())\n    app.config[\"BABEL_TRANSLATION_DIRECTORIES\"] = translations_directory\n\n    # Create the app's Babel instance. Passing the app to the\n    # constructor causes the instance to attach itself to the app.\n    babel = Babel(app)\n\n    # verify that Babel is only using the translations we told it about\n    if list(babel.translation_directories) != [translations_directory]:\n        raise ValueError(\n            f\"Babel translation directories ({babel.translation_directories}) do not match \"\n            f\"SecureDrop configuration ({[translations_directory]})\"\n        )\n\n    # register the function used to determine the locale of a request\n    babel.localeselector(lambda: get_locale(config))\n    return babel",
        "file": "i18n.py"
      },
      {
        "type": "function",
        "name": "parse_locale_set",
        "code": "def parse_locale_set(codes: List[str]) -> Set[Locale]:\n    return {Locale.parse(code) for code in codes}",
        "file": "i18n.py"
      },
      {
        "type": "function",
        "name": "validate_locale_configuration",
        "code": "def validate_locale_configuration(config: SecureDropConfig, babel: Babel) -> Set[Locale]:\n    \"\"\"\n    Check that configured locales are available in the filesystem and therefore usable by\n    Babel.  Warn about configured locales that are not usable, unless we're left with\n    no usable default or fallback locale, in which case raise an exception.\n    \"\"\"\n    # These locales are available and loadable from the filesystem.\n    available = set(babel.list_translations())\n    available.add(Locale.parse(FALLBACK_LOCALE))\n\n    # These locales are supported in the current version of securedrop-app-code.\n    try:\n        with open(I18N_CONF) as i18n_conf_file:\n            i18n_conf = json.load(i18n_conf_file)\n        supported = parse_locale_set(i18n_conf[\"supported_locales\"].keys())\n    # I18N_CONF may not be available under test.\n    except FileNotFoundError:\n        supported = available\n\n    # These locales were configured via \"securedrop-admin sdconfig\", meaning\n    # they were present on the Admin Workstation at \"securedrop-admin\" runtime.\n    configured = parse_locale_set(config.SUPPORTED_LOCALES)\n\n    # The intersection of these sets is the set of locales usable by Babel.\n    usable = available & configured & supported\n\n    missing = configured - usable\n    if missing:\n        babel.app.logger.warning(\n            f\"Configured locales {missing} are not in the set of usable locales {usable}\"\n        )\n\n    defaults = parse_locale_set([config.DEFAULT_LOCALE, FALLBACK_LOCALE])\n    if not defaults & usable:\n        raise ValueError(\n            f\"None of the default locales {defaults} are in the set of usable locales {usable}\"\n        )\n\n    return usable",
        "file": "i18n.py"
      },
      {
        "type": "function",
        "name": "map_locale_display_names",
        "code": "def map_locale_display_names(\n    config: SecureDropConfig, usable_locales: Set[Locale]\n) -> OrderedDict[str, RequestLocaleInfo]:\n    \"\"\"\n    Create a map of locale identifiers to names for display.\n\n    For most of our supported languages, we only provide one\n    translation, so including the full display name is not necessary\n    to distinguish them. For languages with more than one translation,\n    like Chinese, we do need the additional detail.\n    \"\"\"\n\n    # Deduplicate before sorting.\n    supported_locales = sorted(list(set(config.SUPPORTED_LOCALES)))\n\n    language_locale_counts: DefaultDict[str, int] = collections.defaultdict(int)\n    for code in supported_locales:\n        locale = RequestLocaleInfo(code)\n        language_locale_counts[locale.language] += 1\n\n    locale_map = collections.OrderedDict()\n    for code in supported_locales:\n        if Locale.parse(code) not in usable_locales:\n            continue\n\n        locale = RequestLocaleInfo(code)\n        if language_locale_counts[locale.language] > 1:\n            # Disambiguate translations for this language.\n            locale.use_display_name = True\n\n        locale_map[str(locale)] = locale\n\n    return locale_map",
        "file": "i18n.py"
      },
      {
        "type": "function",
        "name": "configure",
        "code": "def configure(config: SecureDropConfig, app: Flask) -> None:\n    babel = configure_babel(config, app)\n    usable_locales = validate_locale_configuration(config, babel)\n    app.config[\"LOCALES\"] = map_locale_display_names(config, usable_locales)",
        "file": "i18n.py"
      },
      {
        "type": "function",
        "name": "get_locale",
        "code": "def get_locale(config: SecureDropConfig) -> str:\n    \"\"\"\n    Return the best supported locale for a request.\n\n    Get the locale as follows, by order of precedence:\n    - l request argument or session['locale']\n    - browser suggested locale, from the Accept-Languages header\n    - config.DEFAULT_LOCALE\n    - config.FALLBACK_LOCALE\n    \"\"\"\n    preferences: List[str] = []\n    if session and session.get(\"locale\"):\n        preferences.append(session[\"locale\"])\n    if request.args.get(\"l\"):\n        preferences.insert(0, request.args[\"l\"])\n    if not preferences:\n        preferences.extend(get_accepted_languages())\n    preferences.append(config.DEFAULT_LOCALE)\n    preferences.append(FALLBACK_LOCALE)\n\n    locales = current_app.config[\"LOCALES\"]\n    negotiated = negotiate_locale(preferences, locales.keys())\n\n    if not negotiated:\n        raise ValueError(\"No usable locale\")\n\n    return negotiated",
        "file": "i18n.py"
      },
      {
        "type": "function",
        "name": "get_accepted_languages",
        "code": "def get_accepted_languages() -> List[str]:\n    \"\"\"\n    Convert a request's list of accepted languages into locale identifiers.\n    \"\"\"\n    accept_languages = []\n    for code in request.accept_languages.values():\n        try:\n            parsed = Locale.parse(code, \"-\")\n            accept_languages.append(str(parsed))\n\n            # We only have two Chinese translations, simplified\n            # and traditional, based on script and not\n            # region. Browsers tend to send identifiers with\n            # region, e.g. zh-CN or zh-TW. Babel can generally\n            # infer the script from those, so we can fabricate a\n            # fallback entry without region, in the hope that it\n            # will match one of our translations and the site will\n            # at least be more legible at first contact than the\n            # probable default locale of English.\n            if parsed.language == \"zh\" and parsed.script:\n                accept_languages.append(str(Locale(language=parsed.language, script=parsed.script)))\n        except (ValueError, UnknownLocaleError):\n            pass\n    return accept_languages",
        "file": "i18n.py"
      },
      {
        "type": "function",
        "name": "set_locale",
        "code": "def set_locale(config: SecureDropConfig) -> None:\n    \"\"\"\n    Update locale info in request and session.\n    \"\"\"\n    locale = get_locale(config)\n    g.localeinfo = RequestLocaleInfo(locale)  # pylint: disable=assigning-non-slot\n    session[\"locale\"] = locale\n    g.locales = current_app.config[\"LOCALES\"]  # pylint: disable=assigning-non-slot",
        "file": "i18n.py"
      }
    ],
    "server_os.py": [
      {
        "type": "function",
        "name": "get_os_release",
        "code": "def get_os_release() -> str:\n    with open(\"/etc/os-release\") as f:\n        os_release = f.readlines()\n        for line in os_release:\n            if line.startswith(\"VERSION_ID=\"):\n                version_id = line.split(\"=\")[1].strip().strip('\"')\n                break\n    return version_id",
        "file": "server_os.py"
      },
      {
        "type": "function",
        "name": "is_os_past_eol",
        "code": "def is_os_past_eol() -> bool:\n    \"\"\"\n    Check if it's focal and if today is past the official EOL date\n    \"\"\"\n    return get_os_release() == FOCAL_VERSION and date.today() >= FOCAL_ENDOFLIFE",
        "file": "server_os.py"
      },
      {
        "type": "function",
        "name": "needs_migration_fixes",
        "code": "def needs_migration_fixes() -> bool:\n    \"\"\"\n    See if the check script has flagged any issues\n    \"\"\"\n    if get_os_release() != FOCAL_VERSION:\n        return False\n    state_path = Path(\"/etc/securedrop-noble-migration.json\")\n    if not state_path.exists():\n        # Script hasn't run yet\n        return False\n    try:\n        contents = json.loads(state_path.read_text())\n    except json.JSONDecodeError:\n        # Invalid output from the script is an error\n        return True\n    if \"error\" in contents:\n        # Something went wrong with the script itself,\n        # it needs manual fixes.\n        return True\n    # True if any of the checks failed\n    return not all(contents.values())",
        "file": "server_os.py"
      }
    ],
    "debian": {
      "ossec-common": {
        "var": {
          "ossec": {
            "checksdconfig.py": [
              {
                "type": "function",
                "name": "list_iptables_rules",
                "code": "def list_iptables_rules() -> dict:\n    result = subprocess.run([\"iptables\", \"-S\"], capture_output=True, check=False)\n    rules = result.stdout.decode(\"utf-8\").splitlines()\n    policies = [r for r in rules if r.startswith(\"-P\")]\n    input_rules = [r for r in rules if r.startswith(\"-A INPUT\")]\n    output_rules = [r for r in rules if r.startswith(\"-A OUTPUT\")]\n    logndrop_rules = [r for r in rules if r.startswith(\"-A LOGNDROP\")]\n    return {\n        \"all\": rules,\n        \"policies\": policies,\n        \"input\": input_rules,\n        \"output\": output_rules,\n        \"logndrop\": logndrop_rules,\n    }",
                "file": "checksdconfig.py"
              },
              {
                "type": "function",
                "name": "check_iptables_are_default",
                "code": "def check_iptables_are_default(rules: dict) -> None:\n    if rules[\"all\"] == IPTABLES_RULES_UNCONFIGURED:\n        raise ValueError(\"The iptables rules have not been configured.\")",
                "file": "checksdconfig.py"
              },
              {
                "type": "function",
                "name": "check_iptables_default_drop",
                "code": "def check_iptables_default_drop(rules: dict) -> None:\n    for chain, chain_rules in IPTABLES_RULES_DEFAULT_DROP.items():\n        for i, rule in enumerate(reversed(chain_rules), 1):\n            try:\n                if rules[chain][-i] != rule:\n                    raise ValueError(\"The iptables default drop rules are incorrect.\")\n            except (KeyError, IndexError):\n                raise ValueError(\"The iptables default drop rules are incorrect.\")",
                "file": "checksdconfig.py"
              },
              {
                "type": "function",
                "name": "check_iptables_rules",
                "code": "def check_iptables_rules() -> None:\n    rules = list_iptables_rules()\n    check_iptables_are_default(rules)\n    check_iptables_default_drop(rules)",
                "file": "checksdconfig.py"
              },
              {
                "type": "function",
                "name": "check_system_configuration",
                "code": "def check_system_configuration(args: argparse.Namespace) -> None:\n    print(\"Checking system configuration...\")\n    try:\n        check_iptables_rules()\n    except ValueError as e:\n        print(\"System configuration error:\", e)\n        sys.exit(1)\n    print(\"System configuration checks were successful.\")",
                "file": "checksdconfig.py"
              }
            ]
          }
        }
      },
      "config": {
        "usr": {
          "bin": {
            "securedrop-cleanup-ossec.py": [
              {
                "type": "function",
                "name": "main",
                "code": "def main() -> None:\n    cutoff_date = datetime.now() - timedelta(days=KEEP_DAYS)\n\n    for root, _dirs, files in os.walk(OSSEC_DIFFS):\n        for file in files:\n            if RE_REMOVE.match(file):\n                file_path = os.path.join(root, file)\n                modified_time = os.path.getmtime(file_path)\n                file_modified_date = datetime.fromtimestamp(modified_time)\n                if file_modified_date < cutoff_date:\n                    os.remove(file_path)\n                    print(f\"Deleted file: {file_path} (Last modified: {file_modified_date})\")",
                "file": "securedrop-cleanup-ossec.py"
              }
            ],
            "securedrop-migrate-ssh-group.py": [
              {
                "type": "function",
                "name": "main",
                "code": "def main() -> None:\n    try:\n        grp.getgrnam(DEST_GROUP)\n        print(f\"Group {DEST_GROUP} already exists\")\n    except KeyError:\n        print(f\"Creating group {DEST_GROUP}\")\n        subprocess.run([\"groupadd\", DEST_GROUP], check=True)\n\n    try:\n        source_group_info = grp.getgrnam(SOURCE_GROUP)\n    except KeyError:\n        # Source group doesn't exist, probably a new install.\n        print(f\"Group {SOURCE_GROUP} does not exist; stopping migration\")\n        return\n    source_users = source_group_info.gr_mem\n    print(f\"Need to migrate: {source_users}\")\n\n    for username in source_users:\n        # Add user to new group while preserving other group memberships\n        subprocess.run([\"usermod\", \"-a\", \"-G\", DEST_GROUP, username], check=True)\n        print(f\"Added {username} to {DEST_GROUP}\")\n        # can't use usermod -r here since focal doesn't support it\n        subprocess.run([\"gpasswd\", \"-d\", username, SOURCE_GROUP], check=True)\n        print(f\"Removed {username} from {SOURCE_GROUP}\")\n    print(\"User migration complete\")\n\n    # Now update sshd_config\n    sshd_config = Path(\"/etc/ssh/sshd_config\")\n    text = sshd_config.read_text()\n    if f\"AllowGroups {SOURCE_GROUP}\\n\" in text:\n        # Update the AllowGroups stanza\n        text = text.replace(f\"AllowGroups {SOURCE_GROUP}\\n\", f\"AllowGroups {DEST_GROUP}\\n\")\n        # And the comment that precedes it\n        text = text.replace(f\"in the {SOURCE_GROUP} group\", f\"in the {DEST_GROUP} group\")\n        sshd_config.write_text(text)\n        print(\"Updated /etc/ssh/sshd_config\")\n        # n.b. we don't restart sshd here, we'll let it take effect on boot\n\n    # Now update iptables rules\n    iptables = Path(\"/etc/iptables/rules.v4\")\n    text = iptables.read_text()\n    if f\"--gid-owner {SOURCE_GROUP} -j LOGNDROP\" in text:\n        # Update the --gid-owner stanza\n        text = text.replace(\n            f\"--gid-owner {SOURCE_GROUP} -j LOGNDROP\", f\"--gid-owner {DEST_GROUP} -j LOGNDROP\"\n        )\n        # And the comment that precedes it\n        text = text.replace(\n            f\"for users in the {SOURCE_GROUP} group\", f\"for users in the {DEST_GROUP} group\"\n        )\n        iptables.write_text(text)\n        print(\"Updated /etc/iptables/rules.v4\")\n\n    print(\"Done!\")",
                "file": "securedrop-migrate-ssh-group.py"
              }
            ]
          }
        }
      },
      "app-code": {
        "usr": {
          "bin": {
            "securedrop-app-backup.py": [
              {
                "type": "function",
                "name": "parse_args",
                "code": "def parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"Create backup\")\n    parser.add_argument(\n        \"--dest\",\n        type=Path,\n        default=os.getcwd(),\n        help=\"Destination folder for backup\",\n    )\n    return parser.parse_args()",
                "file": "securedrop-app-backup.py"
              },
              {
                "type": "function",
                "name": "main",
                "code": "def main() -> None:\n    args = parse_args()\n    if not args.dest.exists():\n        print(f\"Error: {args.dest} does not exist\")\n        sys.exit(1)\n\n    backup_filename = \"sd-backup-{}.tar.gz\".format(datetime.utcnow().strftime(\"%Y-%m-%d--%H-%M-%S\"))\n\n    # This code assumes everything is in the default locations.\n    sd_data = \"/var/lib/securedrop\"\n\n    sd_code = \"/var/www/securedrop\"\n    sd_config = os.path.join(sd_code, \"config.py\")\n    sd_custom_logo = os.path.join(sd_code, \"static/i/custom_logo.png\")\n\n    tor_hidden_services = \"/var/lib/tor/services\"\n    torrc = \"/etc/tor/torrc\"\n\n    with tarfile.open(os.path.join(args.dest, backup_filename), \"w:gz\") as backup:\n        backup.add(sd_config)\n\n        # If no custom logo has been configured, the file will not exist\n        if os.path.exists(sd_custom_logo):\n            backup.add(sd_custom_logo)\n        backup.add(sd_data)\n        backup.add(tor_hidden_services)\n        backup.add(torrc)\n\n    print(backup_filename)",
                "file": "securedrop-app-backup.py"
              }
            ],
            "securedrop-set-redis-auth.py": [
              {
                "type": "function",
                "name": "read_python_file",
                "code": "def read_python_file(path: Path) -> Optional[str]:\n    \"\"\"Extract the password from a Python file\"\"\"\n    if not path.exists():\n        # rq_config.py might not exist yet\n        return None\n    contents = path.read_text()\n    # Read in reverse because we want to look for the last matching line\n    # since it'll take precedence in Python\n    for line in contents.splitlines()[::-1]:\n        match = PYTHON_RE.match(line)\n        if match:\n            return match.group(1)\n    # Nothing found\n    return None",
                "file": "securedrop-set-redis-auth.py"
              },
              {
                "type": "function",
                "name": "write_python_file",
                "code": "def write_python_file(path: Path, password: str) -> None:\n    \"\"\"Set the new password in a Python file, removing any existing passwords\"\"\"\n    if path.exists():\n        lines = path.read_text().splitlines()\n    else:\n        lines = []\n    # Take the existing file, remove any matching password lines, and then add\n    # our new password at the end\n    lines = [line for line in lines if not PYTHON_RE.match(line)]\n    lines.append(f\"REDIS_PASSWORD = '{password}'\")\n    if path == RQ_CONFIG_PY and not path.exists():\n        # Ensure rq_config.py is created with the correct permissions\n        path.write_text(\"\")\n        path.chmod(0o640)\n        shutil.chown(path, \"root\", \"www-data\")\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\")",
                "file": "securedrop-set-redis-auth.py"
              },
              {
                "type": "function",
                "name": "read_redis_conf",
                "code": "def read_redis_conf(path: Path) -> Optional[str]:\n    \"\"\"Extract the password from our redis.conf file\"\"\"\n    # n.b. we assume redis.conf exists, since the package should already\n    # be installed\n    contents = path.read_text()\n    # Read in reverse because we want to look for the last matching line\n    # since redis uses the last requirepass stanza\n    for line in contents.splitlines()[::-1]:\n        match = REDIS_CONF_RE.match(line)\n        if match:\n            return match.group(1)\n    # Nothing found\n    return None",
                "file": "securedrop-set-redis-auth.py"
              },
              {
                "type": "function",
                "name": "write_redis_conf",
                "code": "def write_redis_conf(path: Path, password: str) -> None:\n    \"\"\"Set the new password in a redis.conf file, removing any existing passwords\"\"\"\n    if not path.exists():\n        raise RuntimeError(\"redis.conf does not already exist\")\n    lines = path.read_text().splitlines()\n    # Take the existing file, remove any matching password lines, and then add\n    # our new password at the end\n    lines = [line for line in lines if not REDIS_CONF_RE.match(line)]\n    lines.append(f\"requirepass {password}\")\n    path.write_text(\"\\n\".join(lines) + \"\\n\")",
                "file": "securedrop-set-redis-auth.py"
              },
              {
                "type": "function",
                "name": "generate_password",
                "code": "def generate_password() -> str:\n    \"\"\"\n    Generate a base64-encoded, 32-byte random string\n\n    This is roughly equivalent to `head -c 32 /dev/urandom | base64`\n    \"\"\"\n    return secrets.token_urlsafe(32)",
                "file": "securedrop-set-redis-auth.py"
              },
              {
                "type": "function",
                "name": "check",
                "code": "def check() -> bool:\n    config_py = read_python_file(CONFIG_PY)\n    rq_config_py = read_python_file(RQ_CONFIG_PY)\n    redis_conf = read_redis_conf(REDIS_CONF)\n    all_passwords = {config_py, rq_config_py, redis_conf}\n    # If any are None, then we don't have the password set yet\n    if None in all_passwords:\n        return False\n    # True if there's only one unique password\n    return len(all_passwords) == 1",
                "file": "securedrop-set-redis-auth.py"
              },
              {
                "type": "function",
                "name": "reset",
                "code": "def reset() -> None:\n    password = generate_password()\n    write_python_file(CONFIG_PY, password)\n    write_python_file(RQ_CONFIG_PY, password)\n    write_redis_conf(REDIS_CONF, password)\n    print(\"Redis password has been reset; please restart redis/apache2\")",
                "file": "securedrop-set-redis-auth.py"
              },
              {
                "type": "function",
                "name": "main",
                "code": "def main() -> None:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"mode\", choices=[\"check\", \"reset\", \"reset-if-needed\"])\n    args = parser.parse_args()\n\n    if args.mode == \"check\":\n        if check():\n            print(\"Yay, all three passwords are the same!\")\n        else:\n            print(\"Error: Passwords are not all the same!\")\n            sys.exit(1)\n    elif args.mode == \"reset\":\n        reset()\n    elif args.mode == \"reset-if-needed\":\n        if not check():\n            reset()\n        else:\n            print(\"All three passwords are the same; nothing changed\")\n    else:\n        # should be unreachable, but just in case\n        raise RuntimeError(f\"unknown mode: {args.mode}\")",
                "file": "securedrop-set-redis-auth.py"
              }
            ]
          }
        }
      }
    },
    "pretty_bad_protocol": {
      "gnupg.py": [
        {
          "type": "class",
          "name": "GPG",
          "code": "class GPG(GPGBase):\n    \"\"\"Python interface for handling interactions with GnuPG, including keyfile\n    generation, keyring maintainance, import and export, encryption and\n    decryption, sending to and recieving from keyservers, and signing and\n    verification.\n    \"\"\"\n\n    #: The number of simultaneous keyids we should list operations like\n    #: '--list-sigs' to:\n    _batch_limit = 25\n\n    def __init__(  # type: ignore[no-untyped-def]\n        self,\n        binary=None,\n        homedir=None,\n        verbose=False,\n        use_agent=False,\n        keyring=None,\n        secring=None,\n        ignore_homedir_permissions=False,\n        options=None,\n    ):\n        \"\"\"Initialize a GnuPG process wrapper.\n\n        :param str binary: Name for GnuPG binary executable. If the absolute\n                           path is not given, the environment variable\n                           ``$PATH`` is searched for the executable and\n                           checked that the real uid/gid of the user has\n                           sufficient permissions.\n\n        :param str homedir: Full pathname to directory containing the public\n                            and private keyrings. Default is\n                            `~/.config/python-gnupg`.\n\n        :type ignore_homedir_permissions: :obj:`bool`\n        :param ignore_homedir_permissions: If true, bypass check that homedir\n                                           be writable.\n\n        :type verbose: :obj:`str` or :obj:`int` or :obj:`bool`\n        :param verbose: String or numeric value to pass to GnuPG's\n                        ``--debug-level`` option. See the GnuPG man page for\n                        the list of valid options. If False, debug output is\n                        not generated by the GnuPG binary. If True, defaults\n                        to ``--debug-level basic.``\n\n        :param str keyring: Name of keyring file containing public key data.\n                            If unspecified, defaults to :file:`pubring.gpg` in\n                            the **homedir** directory.\n\n        :param str secring: Name of alternative secret keyring file to use. If\n                            left unspecified, this will default to using\n                            :file:`secring.gpg` in the **homedir** directory,\n                            and create that file if it does not exist.\n\n        :param list options: A list of additional options to pass to the GnuPG\n                             binary.\n\n        :raises: A :exc:`~exceptions.RuntimeError` with explanation message\n                 if there is a problem invoking GnuPG.\n\n        Example:\n\n        >>> import gnupg\n        GnuPG logging disabled...\n        >>> gpg = gnupg.GPG(homedir='doctests')\n        >>> gpg.keyring\n        './doctests/pubring.gpg'\n        >>> gpg.secring\n        './doctests/secring.gpg'\n        >>> gpg.use_agent\n        False\n        >>> gpg.binary\n        '/usr/bin/gpg'\n        \"\"\"\n\n        super().__init__(\n            binary=binary,\n            home=homedir,\n            keyring=keyring,\n            secring=secring,\n            options=options,\n            verbose=verbose,\n            use_agent=use_agent,\n            ignore_homedir_permissions=ignore_homedir_permissions,\n        )\n\n        log.info(\n            textwrap.dedent(\n                f\"\"\"\n        Initialised settings:\n        binary: {self.binary}\n        binary version: {self.binary_version}\n        homedir: {self.homedir}\n        ignore_homedir_permissions: {self.ignore_homedir_permissions}\n        keyring: {self.keyring}\n        secring: {self.secring}\n        default_preference_list: {self.default_preference_list}\n        keyserver: {self.keyserver}\n        options: {self.options}\n        verbose: {str(self.verbose)}\n        use_agent: {str(self.use_agent)}\n        \"\"\"\n            )\n        )\n\n        self._batch_dir = os.path.join(self.homedir, \"batch-files\")\n        self._key_dir = os.path.join(self.homedir, \"generated-keys\")\n\n        #: The keyring used in the most recently created batch file\n        self.temp_keyring = None\n        #: The secring used in the most recently created batch file\n        self.temp_secring = None\n\n        # Make sure that the trustdb exists, or else GnuPG will exit with a\n        # fatal error (at least it does with GnuPG>=2.0.0):\n        self.create_trustdb()\n\n        # The --no-use-agent and --use-agent options were deprecated in GnuPG\n        # 2.x, so we should set use_agent to None here to avoid having\n        # GPGBase._make_args() add either one.\n        self.use_agent = None\n\n    @functools.wraps(_trust._create_trustdb)\n    def create_trustdb(self):  # type: ignore[no-untyped-def]\n        _trust._create_trustdb(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _create_trustdb = create_trustdb\n\n    @functools.wraps(_trust.fix_trustdb)\n    def fix_trustdb(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.fix_trustdb(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _fix_trustdb = fix_trustdb\n\n    @functools.wraps(_trust.import_ownertrust)\n    def import_ownertrust(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.import_ownertrust(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _import_ownertrust = import_ownertrust\n\n    @functools.wraps(_trust.export_ownertrust)\n    def export_ownertrust(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.export_ownertrust(self)\n\n    # For backward compatibility with python-gnupg<=1.3.1:\n    _export_ownertrust = export_ownertrust\n\n    def sign(self, data, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Create a signature for a message string or file.\n\n        Note that this method is not for signing other keys. (In GnuPG's\n        terms, what we all usually call 'keysigning' is actually termed\n        'certification'...) Even though they are cryptographically the same\n        operation, GnuPG differentiates between them, presumedly because these\n        operations are also the same as the decryption operation. If the\n        ``key_usage``s ``C (certification)``, ``S (sign)``, and ``E\n        (encrypt)``, were all the same key, the key would \"wear down\" through\n        frequent signing usage -- since signing data is usually done often --\n        meaning that the secret portion of the keypair, also used for\n        decryption in this scenario, would have a statistically higher\n        probability of an adversary obtaining an oracle for it (or for a\n        portion of the rounds in the cipher algorithm, depending on the family\n        of cryptanalytic attack used).\n\n        In simpler terms: this function isn't for signing your friends' keys,\n        it's for something like signing an email.\n\n        :type data: :obj:`str` or :obj:`file`\n        :param data: A string or file stream to sign.\n        :param str default_key: The key to sign with.\n        :param str passphrase: The passphrase to pipe to stdin.\n        :param bool clearsign: If True, create a cleartext signature.\n        :param bool detach: If True, create a detached signature.\n        :param bool binary: If True, do not ascii armour the output.\n        :param str digest_algo: The hash digest to use. Again, to see which\n            hashes your GnuPG is capable of using, do:\n            :command:`$ gpg --with-colons --list-config digestname`.\n            The default, if unspecified, is ``'SHA512'``.\n        \"\"\"\n        if \"default_key\" in kwargs:\n            log.info(\"Signing message '{!r}' with keyid: {}\".format(data, kwargs[\"default_key\"]))\n        else:\n            log.warn(\"No 'default_key' given! Using first key on secring.\")\n\n        if hasattr(data, \"read\"):\n            result = self._sign_file(data, **kwargs)\n        elif not _is_stream(data):\n            stream = _make_binary_stream(data, self._encoding)\n            result = self._sign_file(stream, **kwargs)\n            stream.close()\n        else:\n            log.warn(f\"Unable to sign message '{data}' with type {type(data)}\")\n            result = None\n        return result\n\n    def verify(self, data):  # type: ignore[no-untyped-def]\n        \"\"\"Verify the signature on the contents of the string ``data``.\n\n        >>> gpg = GPG(homedir=\"doctests\")\n        >>> input = gpg.gen_key_input(Passphrase='foo')\n        >>> key = gpg.gen_key(input)\n        >>> assert key\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='bar')\n        >>> assert not sig\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='foo')\n        >>> assert sig\n        >>> verify = gpg.verify(sig.data)\n        >>> assert verify\n\n        \"\"\"\n        f = _make_binary_stream(data, self._encoding)\n        result = self.verify_file(f)\n        f.close()\n        return result\n\n    def verify_file(self, file, sig_file=None):  # type: ignore[no-untyped-def]\n        \"\"\"Verify the signature on the contents of a file or file-like\n        object. Can handle embedded signatures as well as detached\n        signatures. If using detached signatures, the file containing the\n        detached signature should be specified as the ``sig_file``.\n\n        :param file file: A file descriptor object.\n\n        :param str sig_file: A file containing the GPG signature data for\n            ``file``. If given, ``file`` is verified via this detached\n            signature. Its type will be checked with :func:`_util._is_file`.\n        \"\"\"\n\n        result = self._result_map[\"verify\"](self)\n\n        if sig_file is None:\n            log.debug(\"verify_file(): Handling embedded signature\")\n            args = [\"--verify\"]\n            proc = self._open_subprocess(args)\n            writer = _util._threaded_copy_data(file, proc.stdin)\n            self._collect_output(proc, result, writer, stdin=proc.stdin)\n        else:\n            if not _util._is_file(sig_file):\n                log.debug(\"verify_file(): '%r' is not a file\" % sig_file)\n                return result\n            log.debug(\"verify_file(): Handling detached verification\")\n            sig_fh = None\n            try:\n                sig_fh = open(sig_file, \"rb\")\n                args = [\"--verify %s -\" % sig_fh.name]\n                proc = self._open_subprocess(args)\n                writer = _util._threaded_copy_data(file, proc.stdin)\n                self._collect_output(proc, result, writer, stdin=proc.stdin)\n            finally:\n                if sig_fh and not sig_fh.closed:\n                    sig_fh.close()\n        return result\n\n    def import_keys(self, key_data):  # type: ignore[no-untyped-def]\n        \"\"\"\n        Import the key_data into our keyring.\n\n        >>> import shutil\n        >>> shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> inpt = gpg.gen_key_input()\n        >>> key1 = gpg.gen_key(inpt)\n        >>> print1 = str(key1.fingerprint)\n        >>> pubkey1 = gpg.export_keys(print1)\n        >>> seckey1 = gpg.export_keys(print1,secret=True)\n        >>> key2 = gpg.gen_key(inpt)\n        >>> print2 = key2.fingerprint\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> pubkeys = gpg.list_keys()\n        >>> assert print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> str(gpg.delete_keys(print1))\n        'Must delete secret key first'\n        >>> str(gpg.delete_keys(print1,secret=True))\n        'ok'\n        >>> str(gpg.delete_keys(print1))\n        'ok'\n        >>> pubkeys = gpg.list_keys()\n        >>> assert not print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(pubkey1)\n        >>> pubkeys = gpg.list_keys()\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert not print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(seckey1)\n        >>> assert result\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert print1 in seckeys.fingerprints\n        \"\"\"\n        # xxx need way to validate that key_data is actually a valid GPG key\n        #     it might be possible to use --list-packets and parse the output\n\n        result = self._result_map[\"import\"](self)\n        log.info(\"Importing: %r\", key_data[:256])\n        data = _make_binary_stream(key_data, self._encoding)\n        self._handle_io([\"--import\"], data, result, binary=True)\n        data.close()\n        return result\n\n    def recv_keys(self, *keyids, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Import keys from a keyserver.\n\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key = gpg.recv_keys('3FF0DB166A7476EA', keyserver='hkp://pgp.mit.edu')\n        >>> assert key\n\n        :param str keyids: Each ``keyids`` argument should be a string\n             containing a keyid to request.\n        :param str keyserver: The keyserver to request the ``keyids`` from;\n             defaults to `gnupg.GPG.keyserver`.\n        \"\"\"\n        if keyids:\n            keys = \" \".join([key for key in keyids])\n            return self._recv_keys(keys, **kwargs)\n        else:\n            log.error(\"No keyids requested for --recv-keys!\")\n\n    def delete_keys(self, fingerprints, secret=False, subkeys=False):  # type: ignore[no-untyped-def]\n        \"\"\"Delete a key, or list of keys, from the current keyring.\n\n        The keys must be referred to by their full fingerprints for GnuPG to\n        delete them. If ``secret=True``, the corresponding secret keyring will\n        be deleted from :obj:`.secring`.\n\n        :type fingerprints: :obj:`str` or :obj:`list` or :obj:`tuple`\n        :param fingerprints: A string, or a list/tuple of strings,\n                             representing the fingerprint(s) for the key(s)\n                             to delete.\n\n        :param bool secret: If True, delete the corresponding secret key(s)\n                            also. (default: False)\n\n        :param bool subkeys: If True, delete the secret subkey first, then the\n                             public key. (default: False) Same as:\n                             :command:`$gpg --delete-secret-and-public-key 0x12345678`.\n        \"\"\"\n        which = \"keys\"\n        if secret:\n            which = \"secret-keys\"\n        if subkeys:\n            which = \"secret-and-public-keys\"\n\n        if _is_list_or_tuple(fingerprints):\n            fingerprints = \" \".join(fingerprints)\n\n        args = [\"--batch\"]\n        args.append(f\"--delete-{which} {fingerprints}\")\n\n        result = self._result_map[\"delete\"](self)\n        p = self._open_subprocess(args)\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def export_keys(self, keyids, secret=False, subkeys=False, passphrase=None):  # type: ignore[no-untyped-def]\n        \"\"\"Export the indicated ``keyids``.\n\n        :param str keyids: A keyid or fingerprint in any format that GnuPG will\n            accept.\n        :param bool secret: If True, export only the secret key.\n        :param bool subkeys: If True, export the secret subkeys.\n        :param Optional[str] passphrase: if exporting secret keys, use this\n            passphrase to authenticate\n        \"\"\"\n        which = \"\"\n        if subkeys:\n            which = \"-secret-subkeys\"\n        elif secret:\n            which = \"-secret-keys\"\n        else:\n            # No secret key material, ignore passphrase arg\n            passphrase = None\n\n        if _is_list_or_tuple(keyids):\n            keyids = \" \".join([\"%s\" % k for k in keyids])\n\n        args = [\"--armor\", f\"--export{which} {keyids}\"]\n        result = self._result_map[\"export\"](self)\n\n        # Yes we need to pass in an empty temporary file here,\n        # please don't ask me why. I can't get it to work otherwise.\n        with tempfile.NamedTemporaryFile() as tmp:\n            self._handle_io(args, tmp.name, result, passphrase, binary=True)\n\n        log.debug(f\"Exported:{os.linesep}{result.fingerprints!r}\")\n        return result.data.decode(self._encoding, self._decode_errors)\n\n    def list_keys(self, secret=False):  # type: ignore[no-untyped-def]\n        \"\"\"List the keys currently in the keyring.\n\n        The GnuPG option '--show-photos', according to the GnuPG manual, \"does\n        not work with --with-colons\", but since we can't rely on all versions\n        of GnuPG to explicitly handle this correctly, we should probably\n        include it in the args.\n\n        >>> import shutil\n        >>> shutil.rmtree(\"doctests\")\n        >>> gpg = GPG(homedir=\"doctests\")\n        >>> input = gpg.gen_key_input()\n        >>> result = gpg.gen_key(input)\n        >>> print1 = result.fingerprint\n        >>> result = gpg.gen_key(input)\n        >>> print2 = result.fingerprint\n        >>> pubkeys = gpg.list_keys()\n        >>> assert print1 in pubkeys.fingerprints\n        >>> assert print2 in pubkeys.fingerprints\n        \"\"\"\n        which = \"public-keys\"\n        if secret:\n            which = \"secret-keys\"\n\n        args = []\n        args.append(\"--fixed-list-mode\")\n        args.append(\"--fingerprint\")\n        args.append(\"--with-colons\")\n        args.append(\"--list-options no-show-photos\")\n        args.append(\"--list-%s\" % (which))\n\n        p = self._open_subprocess(args)\n\n        # there might be some status thingumy here I should handle... (amk)\n        # ...nope, unless you care about expired sigs or keys (stevegt)\n\n        # Get the response information\n        result = self._result_map[\"list\"](self)\n        self._collect_output(p, result, stdin=p.stdin)\n        self._parse_keys(result)\n        return result\n\n    def list_packets(self, raw_data):  # type: ignore[no-untyped-def]\n        \"\"\"List the packet contents of a file.\"\"\"\n        args = [\"--list-packets\"]\n        result = self._result_map[\"packets\"](self)\n        self._handle_io(args, _make_binary_stream(raw_data, self._encoding), result)\n        return result\n\n    def sign_key(self, keyid, default_key=None, passphrase=None):  # type: ignore[no-untyped-def]\n        \"\"\"sign (an imported) public key - keyid, with default secret key\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> gpg.sign_key(key['fingerprint'])\n        >>> gpg.list_sigs(key['fingerprint'])\n\n        :param str keyid: key shortID, longID, fingerprint or email_address\n        :param str passphrase: passphrase used when creating the key, leave None otherwise\n\n        :returns: The result giving status of the key signing...\n                    success can be verified by gpg.list_sigs(keyid)\n        \"\"\"\n\n        args = []\n        input_command = \"\"\n        if passphrase:\n            passphrase_arg = \"--passphrase-fd 0\"\n            input_command = \"%s\\n\" % passphrase\n            args.append(passphrase_arg)\n\n        if default_key:\n            args.append(str(\"--default-key %s\" % default_key))\n\n        args.extend([\"--command-fd 0\", \"--sign-key %s\" % keyid])\n\n        p = self._open_subprocess(args)\n        result = self._result_map[\"signing\"](self)\n        confirm_command = \"%sy\\n\" % input_command\n        p.stdin.write(confirm_command.encode())\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def list_sigs(self, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Get the signatures for each of the ``keyids``.\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> assert key.fingerprint\n\n        :rtype: dict\n        :returns: res.sigs is a dictionary whose keys are the uids and whose\n                values are a set of signature keyids.\n        \"\"\"\n        return self._process_keys(keyids)\n\n    def check_sigs(self, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Validate the signatures for each of the ``keyids``.\n\n        :rtype: dict\n        :returns: res.certs is a dictionary whose keys are the uids and whose\n                values are a set of signature keyids.\n        \"\"\"\n        return self._process_keys(keyids, check_sig=True)\n\n    def _process_keys(self, keyids, check_sig=False):  # type: ignore[no-untyped-def]\n        if len(keyids) > self._batch_limit:\n            raise ValueError(\n                \"List signatures is limited to %d keyids simultaneously\" % self._batch_limit\n            )\n\n        args = [\"--with-colons\", \"--fixed-list-mode\"]\n        arg = \"--check-sigs\" if check_sig else \"--list-sigs\"\n\n        if len(keyids):\n            arg += \" \" + \" \".join(keyids)\n\n        args.append(arg)\n\n        proc = self._open_subprocess(args)\n        result = self._result_map[\"list\"](self)\n        self._collect_output(proc, result, stdin=proc.stdin)\n        self._parse_keys(result)\n        return result\n\n    def _parse_keys(self, result):  # type: ignore[no-untyped-def]\n        lines = result.data.decode(self._encoding, self._decode_errors).splitlines()\n        valid_keywords = \"pub uid sec fpr sub sig rev\".split()\n        for line in lines:\n            if self.verbose:\n                print(line)\n            log.debug(\"%r\", line.rstrip())\n            if not line:\n                break\n            L = line.strip().split(\":\")\n            if not L:\n                continue\n            keyword = L[0]\n            if keyword in valid_keywords:\n                getattr(result, keyword)(L)\n\n    def expire(self, keyid, expiration_time=\"1y\", passphrase=None, expire_subkeys=True):  # type: ignore[no-untyped-def]\n        \"\"\"Changes GnuPG key expiration by passing in new time period (from now) through\n            subprocess's stdin\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> gpg.expire(key.fingerprint, '2w', 'good passphrase')\n\n        :param str keyid: key shortID, longID, email_address or fingerprint\n        :param str expiration_time: 0 or number of days (d), or weeks (*w) , or months (*m)\n                or years (*y) for when to expire the key, from today.\n        :param str passphrase: passphrase used when creating the key, leave None otherwise\n        :param bool expire_subkeys: to indicate whether the subkeys will also change the\n                expiration time by the same period -- default is True\n\n        :returns: The result giving status of the change in expiration...\n                the new expiration date can be obtained by .list_keys()\n        \"\"\"\n\n        passphrase = passphrase.encode(self._encoding) if passphrase else passphrase\n\n        try:\n            sub_keys_number = len(self.list_sigs(keyid)[0][\"subkeys\"]) if expire_subkeys else 0\n        except IndexError:\n            sub_keys_number = 0\n\n        expiration_input = KeyExpirationInterface(\n            expiration_time, passphrase\n        ).gpg_interactive_input(sub_keys_number)\n\n        args = [\"--command-fd 0\", \"--edit-key %s\" % keyid]\n        p = self._open_subprocess(args)\n        p.stdin.write(expiration_input.encode())\n\n        result = self._result_map[\"expire\"](self)\n        p.stdin.write(expiration_input.encode())\n\n        self._collect_output(p, result, stdin=p.stdin)\n        return result\n\n    def gen_key(self, input):  # type: ignore[no-untyped-def]\n        \"\"\"Generate a GnuPG key through batch file key generation. See\n        :meth:`GPG.gen_key_input()` for creating the control input.\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> assert key.fingerprint\n\n        :param dict input: A dictionary of parameters and values for the new\n                           key.\n        :returns: The result mapping with details of the new key, which is a\n                  :class:`GenKey <gnupg._parsers.GenKey>` object.\n        \"\"\"\n        args = [\"--gen-key --cert-digest-algo SHA512 --batch\"]\n        key = self._result_map[\"generate\"](self)\n        f = _make_binary_stream(input, self._encoding)\n        self._handle_io(args, f, key, binary=True)\n        f.close()\n\n        fpr = str(key.fingerprint)\n        if len(fpr) == 20:\n            for d in map(lambda x: os.path.dirname(x), [self.temp_keyring, self.temp_secring]):\n                if not os.path.exists(d):\n                    os.makedirs(d)\n\n            if self.temp_keyring and os.path.isfile(self.temp_keyring):\n                prefix = os.path.join(self.temp_keyring, fpr)\n                try:\n                    os.rename(self.temp_keyring, prefix + \".pubring\")\n                except OSError as ose:\n                    log.error(str(ose))\n\n            if self.temp_secring and os.path.isfile(self.temp_secring):\n                prefix = os.path.join(self.temp_secring, fpr)\n                try:\n                    os.rename(self.temp_secring, prefix + \".secring\")\n                except OSError as ose:\n                    log.error(str(ose))\n\n        log.info(\"Key created. Fingerprint: %s\" % fpr)\n        key.keyring = self.temp_keyring\n        key.secring = self.temp_secring\n        self.temp_keyring = None\n        self.temp_secring = None\n\n        return key\n\n    def gen_key_input(self, separate_keyring=False, save_batchfile=False, testing=False, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Generate a batch file for input to :meth:`~gnupg.GPG.gen_key`.\n\n        The GnuPG batch file key generation feature allows unattended key\n        generation by creating a file with special syntax and then providing it\n        to: :command:`gpg --gen-key --batch`. Batch files look like this:\n\n        |  Name-Real: Alice\n        |  Name-Email: alice@inter.net\n        |  Expire-Date: 2014-04-01\n        |  Key-Type: RSA\n        |  Key-Length: 4096\n        |  Key-Usage: cert\n        |  Subkey-Type: RSA\n        |  Subkey-Length: 4096\n        |  Subkey-Usage: encrypt,sign,auth\n        |  Passphrase: sekrit\n        |  %pubring foo.gpg\n        |  %secring sec.gpg\n        |  %commit\n\n        which is what this function creates for you. All of the available,\n        non-control parameters are detailed below (control parameters are the\n        ones which begin with a '%'). For example, to generate the batch file\n        example above, use like this:\n\n        >>> import gnupg\n        GnuPG logging disabled...\n        >>> from __future__ import print_function\n        >>> gpg = gnupg.GPG(homedir='doctests')\n        >>> alice = { 'name_real': 'Alice',\n        ...     'name_email': 'alice@inter.net',\n        ...     'expire_date': '2014-04-01',\n        ...     'key_type': 'RSA',\n        ...     'key_length': 4096,\n        ...     'key_usage': '',\n        ...     'subkey_type': 'RSA',\n        ...     'subkey_length': 4096,\n        ...     'subkey_usage': 'encrypt,sign,auth',\n        ...     'passphrase': 'sekrit'}\n        >>> alice_input = gpg.gen_key_input(**alice)\n        >>> print(alice_input)\n        Key-Type: RSA\n        Subkey-Type: RSA\n        Subkey-Usage: encrypt,sign,auth\n        Expire-Date: 2014-04-01\n        Passphrase: sekrit\n        Name-Real: Alice\n        Name-Email: alice@inter.net\n        Key-Length: 4096\n        Subkey-Length: 4096\n        %pubring ./doctests/alice.pubring.gpg\n        %secring ./doctests/alice.secring.gpg\n        %commit\n        <BLANKLINE>\n        >>> alice_key = gpg.gen_key(alice_input)\n        >>> assert alice_key is not None\n        >>> assert alice_key.fingerprint is not None\n        >>> message = \"no one else can read my sekrit message\"\n        >>> encrypted = gpg.encrypt(message, alice_key.fingerprint)\n        >>> assert isinstance(encrypted.data, str)\n\n        :param bool separate_keyring: Specify for the new key to be written to\n            a separate pubring.gpg and secring.gpg. If True,\n            :meth:`~gnupg.GPG.gen_key` will automatically rename the separate\n            keyring and secring to whatever the fingerprint of the generated\n            key ends up being, suffixed with '.pubring' and '.secring'\n            respectively.\n\n        :param bool save_batchfile: Save a copy of the generated batch file to\n            disk in a file named <name_real>.batch, where <name_real> is the\n            ``name_real`` parameter stripped of punctuation, spaces, and\n            non-ascii characters.\n\n        :param bool testing: Uses a faster, albeit insecure random number\n            generator to create keys. This should only be used for testing\n            purposes, for keys which are going to be created and then soon\n            after destroyed, and never for the generation of actual use keys.\n\n        :param str name_real: The name field of the UID in the generated key.\n        :param str name_comment: The comment in the UID of the generated key.\n\n        :param str name_email: The email in the UID of the generated key.\n            (default: ``$USER`` @ :command:`hostname` ) Remember to use UTF-8\n            encoding for the entirety of the UID. At least one of\n            ``name_real``, ``name_comment``, or ``name_email`` must be\n            provided, or else no user ID is created.\n\n        :param str key_type: One of 'RSA', 'DSA', 'ELG-E', or 'default'.\n            (default: 'RSA', if using GnuPG v1.x, otherwise 'default') Starts\n            a new parameter block by giving the type of the primary key. The\n            algorithm must be capable of signing. This is a required\n            parameter. The algorithm may either be an OpenPGP algorithm number\n            or a string with the algorithm name. The special value ‘default’\n            may be used for algo to create the default key type; in this case\n            a ``key_usage`` should not be given and 'default' must also be\n            used for ``subkey_type``.\n\n        :param int key_length: The requested length of the generated key in\n            bits. (Default: 4096)\n\n        :param str key_grip: hexstring This is an optional hexidecimal string\n            which is used to generate a CSR or certificate for an already\n            existing key. ``key_length`` will be ignored if this parameter\n            is given.\n\n        :param str key_usage: Space or comma delimited string of key\n            usages. Allowed values are ‘encrypt’, ‘sign’, and ‘auth’. This is\n            used to generate the key flags. Please make sure that the\n            algorithm is capable of this usage. Note that OpenPGP requires\n            that all primary keys are capable of certification, so no matter\n            what usage is given here, the ‘cert’ flag will be on. If no\n            ‘Key-Usage’ is specified and the ‘Key-Type’ is not ‘default’, all\n            allowed usages for that particular algorithm are used; if it is\n            not given but ‘default’ is used the usage will be ‘sign’.\n\n        :param str subkey_type: This generates a secondary key\n            (subkey). Currently only one subkey can be handled. See also\n            ``key_type`` above.\n\n        :param int subkey_length: The length of the secondary subkey in bits.\n\n        :param str subkey_usage: Key usage for a subkey; similar to\n            ``key_usage``.\n\n        :type expire_date: :obj:`int` or :obj:`str`\n        :param expire_date: Can be specified as an iso-date or as\n            <int>[d|w|m|y] Set the expiration date for the key (and the\n            subkey). It may either be entered in ISO date format (2000-08-15)\n            or as number of days, weeks, month or years. The special notation\n            \"seconds=N\" is also allowed to directly give an Epoch\n            value. Without a letter days are assumed. Note that there is no\n            check done on the overflow of the type used by OpenPGP for\n            timestamps. Thus you better make sure that the given value make\n            sense. Although OpenPGP works with time intervals, GnuPG uses an\n            absolute value internally and thus the last year we can represent\n            is 2105.\n\n        :param str creation_date: Set the creation date of the key as stored\n            in the key information and which is also part of the fingerprint\n            calculation. Either a date like \"1986-04-26\" or a full timestamp\n            like \"19860426T042640\" may be used. The time is considered to be\n            UTC. If it is not given the current time is used.\n\n        :param str passphrase: The passphrase for the new key. The default is\n            to not use any passphrase. Note that GnuPG>=2.1.x will not allow\n            you to specify a passphrase for batch key generation -- GnuPG will\n            ignore the **passphrase** parameter, stop, and ask the user for\n            the new passphrase.  However, we can put the command\n            ``%no-protection`` into the batch key generation file to allow a\n            passwordless key to be created, which can then have its passphrase\n            set later with ``--edit-key``.\n\n        :param str preferences: Set the cipher, hash, and compression\n            preference values for this key. This expects the same type of\n            string as the sub-command ‘setpref’ in the --edit-key menu.\n\n        :param str revoker: Should be given as 'algo:fpr' (case sensitive).\n            Add a designated revoker to the generated key. Algo is the public\n            key algorithm of the designated revoker (i.e. RSA=1, DSA=17, etc.)\n            fpr is the fingerprint of the designated revoker. The optional\n            ‘sensitive’ flag marks the designated revoker as sensitive\n            information. Only v4 keys may be designated revokers.\n\n        :param str keyserver: This is an optional parameter that specifies the\n            preferred keyserver URL for the key.\n\n        :param str handle: This is an optional parameter only used with the\n            status lines ``KEY_CREATED`` and ``KEY_NOT_CREATED``. string may\n            be up to 100 characters and should not contain spaces. It is\n            useful for batch key generation to associate a key parameter block\n            with a status line.\n\n        :rtype: str\n        :returns: A suitable input string for the :meth:`GPG.gen_key` method,\n            the latter of which will create the new keypair.\n\n        See `this GnuPG Manual section`__ for more details.\n\n        __ http://www.gnupg.org/documentation/manuals/gnupg-devel/Unattended-GPG-key-generation.html\n        \"\"\"\n        #: A boolean for determining whether to set subkey_type to 'default'\n        default_type = False\n\n        parms = {}\n\n        parms.setdefault(\"Key-Type\", \"default\")\n        log.debug(\n            \"GnuPG v{} detected: setting default key type to {}.\".format(\n                self.binary_version, parms[\"Key-Type\"]\n            )\n        )\n        parms.setdefault(\"Key-Length\", 4096)\n        parms.setdefault(\"Name-Real\", \"Autogenerated Key\")\n        parms.setdefault(\"Expire-Date\", _util._next_year())\n\n        name_email = kwargs.get(\"name_email\")\n        uidemail = _util.create_uid_email(name_email)\n        parms.setdefault(\"Name-Email\", uidemail)\n\n        if testing:\n            # This specific comment string is required by (some? all?)\n            # versions of GnuPG to use the insecure PRNG:\n            parms.setdefault(\"Name-Comment\", \"insecure!\")\n\n        for key, val in list(kwargs.items()):\n            key = key.replace(\"_\", \"-\").title()\n            # to set 'cert', 'Key-Usage' must be blank string\n            if key not in (\"Key-Usage\", \"Subkey-Usage\") and str(val).strip():\n                parms[key] = val\n\n        # if Key-Type is 'default', make Subkey-Type also be 'default'\n        if parms[\"Key-Type\"] == \"default\":\n            default_type = True\n            for field in (\n                \"Key-Usage\",\n                \"Subkey-Usage\",\n            ):\n                try:\n                    parms.pop(field)  # toss these out, handle manually\n                except KeyError:\n                    pass\n\n        # Key-Type must come first, followed by length\n        out = \"Key-Type: %s\\n\" % parms.pop(\"Key-Type\")\n        out += \"Key-Length: %d\\n\" % parms.pop(\"Key-Length\")\n        if \"Subkey-Type\" in parms:\n            out += \"Subkey-Type: %s\\n\" % parms.pop(\"Subkey-Type\")\n        elif default_type:\n            out += \"Subkey-Type: default\\n\"\n        if \"Subkey-Length\" in parms:\n            out += \"Subkey-Length: %s\\n\" % parms.pop(\"Subkey-Length\")\n\n        for key, val in list(parms.items()):\n            out += f\"{key}: {val}\\n\"\n\n        # There is a problem where, in the batch files, if the '%%pubring'\n        # and '%%secring' are given as any static string, i.e. 'pubring.gpg',\n        # that file will always get rewritten without confirmation, killing\n        # off any keys we had before. So in the case where we wish to\n        # generate a bunch of keys and then do stuff with them, we should not\n        # give 'pubring.gpg' as our keyring file, otherwise we will lose any\n        # keys we had previously.\n\n        if separate_keyring:\n            ring = str(uidemail + \"_\" + str(int(time.time())))\n            self.temp_keyring = os.path.join(self.homedir, ring + \".pubring\")\n            self.temp_secring = os.path.join(self.homedir, ring + \".secring\")\n            out += \"%%pubring %s\\n\" % self.temp_keyring\n            out += \"%%secring %s\\n\" % self.temp_secring\n\n        if testing:\n            # see TODO file, tag :compatibility:gen_key_input:\n            #\n            # Add version detection before the '%no-protection' flag.\n            out += \"%no-protection\\n\"\n            out += \"%transient-key\\n\"\n\n        out += \"%commit\\n\"\n\n        # if we've been asked to save a copy of the batch file:\n        if save_batchfile and parms[\"Name-Email\"] != uidemail:\n            asc_uid = encodings.normalize_encoding(parms[\"Name-Email\"])\n            filename = _fix_unsafe(asc_uid) + _util._now() + \".batch\"\n            save_as = os.path.join(self._batch_dir, filename)\n            readme = os.path.join(self._batch_dir, \"README\")\n\n            if not os.path.exists(self._batch_dir):\n                os.makedirs(self._batch_dir)\n\n                # the following pulls the link to GnuPG's online batchfile\n                # documentation from this function's docstring and sticks it\n                # in a README file in the batch directory:\n\n                if getattr(self.gen_key_input, \"__doc__\", None) is not None:\n                    docs = self.gen_key_input.__doc__\n                else:\n                    docs = \"\"  # docstring=None if run with \"python -OO\"\n                links = \"\\n\".join(x.strip() for x in docs.splitlines()[-2:])\n                explain = f\"\"\"\nThis directory was created by python-gnupg, on {_util.now()}, and\nit contains saved batch files, which can be given to GnuPG to automatically\ngenerate keys. Please see\n{links}\"\"\"  # sometimes python is awesome.\n\n                with open(readme, \"a+\") as fh:\n                    [fh.write(line) for line in explain]\n\n            with open(save_as, \"a+\") as batch_file:\n                [batch_file.write(line) for line in out]\n\n        return out\n\n    def encrypt(self, data, *recipients, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Encrypt the message contained in ``data`` to ``recipients``.\n\n        :param str data: The file or bytestream to encrypt.\n\n        :param str recipients: The recipients to encrypt to. Recipients must\n            be specified keyID/fingerprint. Care should be taken in Python2.x\n            to make sure that the given fingerprint is in fact a string and\n            not a unicode object.  Multiple recipients may be specified by\n            doing ``GPG.encrypt(data, fpr1, fpr2, fpr3)`` etc.\n\n        :param str default_key: The keyID/fingerprint of the key to use for\n            signing. If given, ``data`` will be encrypted and signed.\n\n        :param str passphrase: If given, and ``default_key`` is also given,\n            use this passphrase to unlock the secret portion of the\n            ``default_key`` to sign the encrypted ``data``. Otherwise, if\n            ``default_key`` is not given, but ``symmetric=True``, then use\n            this passphrase as the passphrase for symmetric\n            encryption. Signing and symmetric encryption should *not* be\n            combined when sending the ``data`` to other recipients, else the\n            passphrase to the secret key would be shared with them.\n\n        :param bool armor: If True, ascii armor the output; otherwise, the\n            output will be in binary format. (Default: True)\n\n        :param bool encrypt: If True, encrypt the ``data`` using the\n            ``recipients`` public keys. (Default: True)\n\n        :param bool symmetric: If True, encrypt the ``data`` to ``recipients``\n            using a symmetric key. See the ``passphrase`` parameter. Symmetric\n            encryption and public key encryption can be used simultaneously,\n            and will result in a ciphertext which is decryptable with either\n            the symmetric ``passphrase`` or one of the corresponding private\n            keys.\n\n        :param bool always_trust: If True, ignore trust warnings on recipient\n            keys. If False, display trust warnings.  (default: True)\n\n        :param str output: The output file to write to. If not specified, the\n            encrypted output is returned, and thus should be stored as an\n            object in Python. For example:\n\n        >>> import shutil\n        >>> import gnupg\n        >>> if os.path.exists(\"doctests\"):\n        ...     shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_settings = gpg.gen_key_input(key_type='RSA',\n        ...     key_length=1024,\n        ...     key_usage='ESCA',\n        ...     passphrase='foo')\n        >>> key = gpg.gen_key(key_settings)\n        >>> message = \"The crow flies at midnight.\"\n        >>> encrypted = str(gpg.encrypt(message, key.fingerprint))\n        >>> assert encrypted != message\n        >>> assert not encrypted.isspace()\n        >>> decrypted = str(gpg.decrypt(encrypted, passphrase='foo'))\n        >>> assert not decrypted.isspace()\n        >>> decrypted\n        'The crow flies at midnight.'\n\n\n        :param bool throw_keyids: If True, make all **recipients** keyids be\n            zero'd out in packet information. This is the same as using\n            **hidden_recipients** for all **recipients**. (Default: False).\n\n        :param list hidden_recipients: A list of recipients that should have\n            their keyids zero'd out in packet information.\n\n        :param str cipher_algo: The cipher algorithm to use. To see available\n            algorithms with your version of GnuPG, do:\n            :command:`$ gpg --with-colons --list-config ciphername`.\n            The default ``cipher_algo``, if unspecified, is ``'AES256'``.\n\n        :param str digest_algo: The hash digest to use. Again, to see which\n            hashes your GnuPG is capable of using, do:\n            :command:`$ gpg --with-colons --list-config digestname`.\n            The default, if unspecified, is ``'SHA512'``.\n\n        :param str compress_algo: The compression algorithm to use. Can be one\n            of ``'ZLIB'``, ``'BZIP2'``, ``'ZIP'``, or ``'Uncompressed'``.\n\n        .. seealso:: :meth:`._encrypt`\n        \"\"\"\n        if _is_stream(data):\n            stream = data\n        else:\n            stream = _make_binary_stream(data, self._encoding)\n        result = self._encrypt(stream, recipients, **kwargs)\n        stream.close()\n        return result\n\n    def decrypt(self, message, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Decrypt the contents of a string or file-like object ``message``.\n\n        :type message: file or str or :class:`io.BytesIO`\n        :param message: A string or file-like object to decrypt.\n        :param bool always_trust: Instruct GnuPG to ignore trust checks.\n        :param str passphrase: The passphrase for the secret key used for decryption.\n        :param str output: A filename to write the decrypted output to.\n        \"\"\"\n        stream = _make_binary_stream(message, self._encoding)\n        result = self.decrypt_file(stream, **kwargs)\n        stream.close()\n        return result\n\n    def decrypt_file(self, filename, always_trust=False, passphrase=None, output=None):  # type: ignore[no-untyped-def]\n        \"\"\"Decrypt the contents of a file-like object ``filename`` .\n\n        :param str filename: A file-like object to decrypt.\n        :param bool always_trust: Instruct GnuPG to ignore trust checks.\n        :param str passphrase: The passphrase for the secret key used for decryption.\n        :param str output: A filename to write the decrypted output to.\n        \"\"\"\n        args = [\"--decrypt\"]\n        if output:  # write the output to a file with the specified name\n            if os.path.exists(output):\n                os.remove(output)  # to avoid overwrite confirmation message\n            args.append(\"--output %s\" % output)\n        if always_trust:\n            args.append(\"--always-trust\")\n        result = self._result_map[\"crypt\"](self)\n        self._handle_io(args, filename, result, passphrase, binary=True)\n        log.debug(\"decrypt result: %r\", result.data)\n        return result",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(  # type: ignore[no-untyped-def]\n        self,\n        binary=None,\n        homedir=None,\n        verbose=False,\n        use_agent=False,\n        keyring=None,\n        secring=None,\n        ignore_homedir_permissions=False,\n        options=None,\n    ):\n        \"\"\"Initialize a GnuPG process wrapper.\n\n        :param str binary: Name for GnuPG binary executable. If the absolute\n                           path is not given, the environment variable\n                           ``$PATH`` is searched for the executable and\n                           checked that the real uid/gid of the user has\n                           sufficient permissions.\n\n        :param str homedir: Full pathname to directory containing the public\n                            and private keyrings. Default is\n                            `~/.config/python-gnupg`.\n\n        :type ignore_homedir_permissions: :obj:`bool`\n        :param ignore_homedir_permissions: If true, bypass check that homedir\n                                           be writable.\n\n        :type verbose: :obj:`str` or :obj:`int` or :obj:`bool`\n        :param verbose: String or numeric value to pass to GnuPG's\n                        ``--debug-level`` option. See the GnuPG man page for\n                        the list of valid options. If False, debug output is\n                        not generated by the GnuPG binary. If True, defaults\n                        to ``--debug-level basic.``\n\n        :param str keyring: Name of keyring file containing public key data.\n                            If unspecified, defaults to :file:`pubring.gpg` in\n                            the **homedir** directory.\n\n        :param str secring: Name of alternative secret keyring file to use. If\n                            left unspecified, this will default to using\n                            :file:`secring.gpg` in the **homedir** directory,\n                            and create that file if it does not exist.\n\n        :param list options: A list of additional options to pass to the GnuPG\n                             binary.\n\n        :raises: A :exc:`~exceptions.RuntimeError` with explanation message\n                 if there is a problem invoking GnuPG.\n\n        Example:\n\n        >>> import gnupg\n        GnuPG logging disabled...\n        >>> gpg = gnupg.GPG(homedir='doctests')\n        >>> gpg.keyring\n        './doctests/pubring.gpg'\n        >>> gpg.secring\n        './doctests/secring.gpg'\n        >>> gpg.use_agent\n        False\n        >>> gpg.binary\n        '/usr/bin/gpg'\n        \"\"\"\n\n        super().__init__(\n            binary=binary,\n            home=homedir,\n            keyring=keyring,\n            secring=secring,\n            options=options,\n            verbose=verbose,\n            use_agent=use_agent,\n            ignore_homedir_permissions=ignore_homedir_permissions,\n        )\n\n        log.info(\n            textwrap.dedent(\n                f\"\"\"\n        Initialised settings:\n        binary: {self.binary}\n        binary version: {self.binary_version}\n        homedir: {self.homedir}\n        ignore_homedir_permissions: {self.ignore_homedir_permissions}\n        keyring: {self.keyring}\n        secring: {self.secring}\n        default_preference_list: {self.default_preference_list}\n        keyserver: {self.keyserver}\n        options: {self.options}\n        verbose: {str(self.verbose)}\n        use_agent: {str(self.use_agent)}\n        \"\"\"\n            )\n        )\n\n        self._batch_dir = os.path.join(self.homedir, \"batch-files\")\n        self._key_dir = os.path.join(self.homedir, \"generated-keys\")\n\n        #: The keyring used in the most recently created batch file\n        self.temp_keyring = None\n        #: The secring used in the most recently created batch file\n        self.temp_secring = None\n\n        # Make sure that the trustdb exists, or else GnuPG will exit with a\n        # fatal error (at least it does with GnuPG>=2.0.0):\n        self.create_trustdb()\n\n        # The --no-use-agent and --use-agent options were deprecated in GnuPG\n        # 2.x, so we should set use_agent to None here to avoid having\n        # GPGBase._make_args() add either one.\n        self.use_agent = None",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "create_trustdb",
          "code": "def create_trustdb(self):  # type: ignore[no-untyped-def]\n        _trust._create_trustdb(self)",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "fix_trustdb",
          "code": "def fix_trustdb(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.fix_trustdb(self)",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "import_ownertrust",
          "code": "def import_ownertrust(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.import_ownertrust(self)",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "export_ownertrust",
          "code": "def export_ownertrust(self, trustdb=None):  # type: ignore[no-untyped-def]\n        _trust.export_ownertrust(self)",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "sign",
          "code": "def sign(self, data, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Create a signature for a message string or file.\n\n        Note that this method is not for signing other keys. (In GnuPG's\n        terms, what we all usually call 'keysigning' is actually termed\n        'certification'...) Even though they are cryptographically the same\n        operation, GnuPG differentiates between them, presumedly because these\n        operations are also the same as the decryption operation. If the\n        ``key_usage``s ``C (certification)``, ``S (sign)``, and ``E\n        (encrypt)``, were all the same key, the key would \"wear down\" through\n        frequent signing usage -- since signing data is usually done often --\n        meaning that the secret portion of the keypair, also used for\n        decryption in this scenario, would have a statistically higher\n        probability of an adversary obtaining an oracle for it (or for a\n        portion of the rounds in the cipher algorithm, depending on the family\n        of cryptanalytic attack used).\n\n        In simpler terms: this function isn't for signing your friends' keys,\n        it's for something like signing an email.\n\n        :type data: :obj:`str` or :obj:`file`\n        :param data: A string or file stream to sign.\n        :param str default_key: The key to sign with.\n        :param str passphrase: The passphrase to pipe to stdin.\n        :param bool clearsign: If True, create a cleartext signature.\n        :param bool detach: If True, create a detached signature.\n        :param bool binary: If True, do not ascii armour the output.\n        :param str digest_algo: The hash digest to use. Again, to see which\n            hashes your GnuPG is capable of using, do:\n            :command:`$ gpg --with-colons --list-config digestname`.\n            The default, if unspecified, is ``'SHA512'``.\n        \"\"\"\n        if \"default_key\" in kwargs:\n            log.info(\"Signing message '{!r}' with keyid: {}\".format(data, kwargs[\"default_key\"]))\n        else:\n            log.warn(\"No 'default_key' given! Using first key on secring.\")\n\n        if hasattr(data, \"read\"):\n            result = self._sign_file(data, **kwargs)\n        elif not _is_stream(data):\n            stream = _make_binary_stream(data, self._encoding)\n            result = self._sign_file(stream, **kwargs)\n            stream.close()\n        else:\n            log.warn(f\"Unable to sign message '{data}' with type {type(data)}\")\n            result = None\n        return result",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "verify",
          "code": "def verify(self, data):  # type: ignore[no-untyped-def]\n        \"\"\"Verify the signature on the contents of the string ``data``.\n\n        >>> gpg = GPG(homedir=\"doctests\")\n        >>> input = gpg.gen_key_input(Passphrase='foo')\n        >>> key = gpg.gen_key(input)\n        >>> assert key\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='bar')\n        >>> assert not sig\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='foo')\n        >>> assert sig\n        >>> verify = gpg.verify(sig.data)\n        >>> assert verify\n\n        \"\"\"\n        f = _make_binary_stream(data, self._encoding)\n        result = self.verify_file(f)\n        f.close()\n        return result",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "verify_file",
          "code": "def verify_file(self, file, sig_file=None):  # type: ignore[no-untyped-def]\n        \"\"\"Verify the signature on the contents of a file or file-like\n        object. Can handle embedded signatures as well as detached\n        signatures. If using detached signatures, the file containing the\n        detached signature should be specified as the ``sig_file``.\n\n        :param file file: A file descriptor object.\n\n        :param str sig_file: A file containing the GPG signature data for\n            ``file``. If given, ``file`` is verified via this detached\n            signature. Its type will be checked with :func:`_util._is_file`.\n        \"\"\"\n\n        result = self._result_map[\"verify\"](self)\n\n        if sig_file is None:\n            log.debug(\"verify_file(): Handling embedded signature\")\n            args = [\"--verify\"]\n            proc = self._open_subprocess(args)\n            writer = _util._threaded_copy_data(file, proc.stdin)\n            self._collect_output(proc, result, writer, stdin=proc.stdin)\n        else:\n            if not _util._is_file(sig_file):\n                log.debug(\"verify_file(): '%r' is not a file\" % sig_file)\n                return result\n            log.debug(\"verify_file(): Handling detached verification\")\n            sig_fh = None\n            try:\n                sig_fh = open(sig_file, \"rb\")\n                args = [\"--verify %s -\" % sig_fh.name]\n                proc = self._open_subprocess(args)\n                writer = _util._threaded_copy_data(file, proc.stdin)\n                self._collect_output(proc, result, writer, stdin=proc.stdin)\n            finally:\n                if sig_fh and not sig_fh.closed:\n                    sig_fh.close()\n        return result",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "import_keys",
          "code": "def import_keys(self, key_data):  # type: ignore[no-untyped-def]\n        \"\"\"\n        Import the key_data into our keyring.\n\n        >>> import shutil\n        >>> shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> inpt = gpg.gen_key_input()\n        >>> key1 = gpg.gen_key(inpt)\n        >>> print1 = str(key1.fingerprint)\n        >>> pubkey1 = gpg.export_keys(print1)\n        >>> seckey1 = gpg.export_keys(print1,secret=True)\n        >>> key2 = gpg.gen_key(inpt)\n        >>> print2 = key2.fingerprint\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> pubkeys = gpg.list_keys()\n        >>> assert print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> str(gpg.delete_keys(print1))\n        'Must delete secret key first'\n        >>> str(gpg.delete_keys(print1,secret=True))\n        'ok'\n        >>> str(gpg.delete_keys(print1))\n        'ok'\n        >>> pubkeys = gpg.list_keys()\n        >>> assert not print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(pubkey1)\n        >>> pubkeys = gpg.list_keys()\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert not print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(seckey1)\n        >>> assert result\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert print1 in seckeys.fingerprints\n        \"\"\"\n        # xxx need way to validate that key_data is actually a valid GPG key\n        #     it might be possible to use --list-packets and parse the output\n\n        result = self._result_map[\"import\"](self)\n        log.info(\"Importing: %r\", key_data[:256])\n        data = _make_binary_stream(key_data, self._encoding)\n        self._handle_io([\"--import\"], data, result, binary=True)\n        data.close()\n        return result",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "recv_keys",
          "code": "def recv_keys(self, *keyids, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Import keys from a keyserver.\n\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key = gpg.recv_keys('3FF0DB166A7476EA', keyserver='hkp://pgp.mit.edu')\n        >>> assert key\n\n        :param str keyids: Each ``keyids`` argument should be a string\n             containing a keyid to request.\n        :param str keyserver: The keyserver to request the ``keyids`` from;\n             defaults to `gnupg.GPG.keyserver`.\n        \"\"\"\n        if keyids:\n            keys = \" \".join([key for key in keyids])\n            return self._recv_keys(keys, **kwargs)\n        else:\n            log.error(\"No keyids requested for --recv-keys!\")",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "delete_keys",
          "code": "def delete_keys(self, fingerprints, secret=False, subkeys=False):  # type: ignore[no-untyped-def]\n        \"\"\"Delete a key, or list of keys, from the current keyring.\n\n        The keys must be referred to by their full fingerprints for GnuPG to\n        delete them. If ``secret=True``, the corresponding secret keyring will\n        be deleted from :obj:`.secring`.\n\n        :type fingerprints: :obj:`str` or :obj:`list` or :obj:`tuple`\n        :param fingerprints: A string, or a list/tuple of strings,\n                             representing the fingerprint(s) for the key(s)\n                             to delete.\n\n        :param bool secret: If True, delete the corresponding secret key(s)\n                            also. (default: False)\n\n        :param bool subkeys: If True, delete the secret subkey first, then the\n                             public key. (default: False) Same as:\n                             :command:`$gpg --delete-secret-and-public-key 0x12345678`.\n        \"\"\"\n        which = \"keys\"\n        if secret:\n            which = \"secret-keys\"\n        if subkeys:\n            which = \"secret-and-public-keys\"\n\n        if _is_list_or_tuple(fingerprints):\n            fingerprints = \" \".join(fingerprints)\n\n        args = [\"--batch\"]\n        args.append(f\"--delete-{which} {fingerprints}\")\n\n        result = self._result_map[\"delete\"](self)\n        p = self._open_subprocess(args)\n        self._collect_output(p, result, stdin=p.stdin)\n        return result",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "export_keys",
          "code": "def export_keys(self, keyids, secret=False, subkeys=False, passphrase=None):  # type: ignore[no-untyped-def]\n        \"\"\"Export the indicated ``keyids``.\n\n        :param str keyids: A keyid or fingerprint in any format that GnuPG will\n            accept.\n        :param bool secret: If True, export only the secret key.\n        :param bool subkeys: If True, export the secret subkeys.\n        :param Optional[str] passphrase: if exporting secret keys, use this\n            passphrase to authenticate\n        \"\"\"\n        which = \"\"\n        if subkeys:\n            which = \"-secret-subkeys\"\n        elif secret:\n            which = \"-secret-keys\"\n        else:\n            # No secret key material, ignore passphrase arg\n            passphrase = None\n\n        if _is_list_or_tuple(keyids):\n            keyids = \" \".join([\"%s\" % k for k in keyids])\n\n        args = [\"--armor\", f\"--export{which} {keyids}\"]\n        result = self._result_map[\"export\"](self)\n\n        # Yes we need to pass in an empty temporary file here,\n        # please don't ask me why. I can't get it to work otherwise.\n        with tempfile.NamedTemporaryFile() as tmp:\n            self._handle_io(args, tmp.name, result, passphrase, binary=True)\n\n        log.debug(f\"Exported:{os.linesep}{result.fingerprints!r}\")\n        return result.data.decode(self._encoding, self._decode_errors)",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "list_keys",
          "code": "def list_keys(self, secret=False):  # type: ignore[no-untyped-def]\n        \"\"\"List the keys currently in the keyring.\n\n        The GnuPG option '--show-photos', according to the GnuPG manual, \"does\n        not work with --with-colons\", but since we can't rely on all versions\n        of GnuPG to explicitly handle this correctly, we should probably\n        include it in the args.\n\n        >>> import shutil\n        >>> shutil.rmtree(\"doctests\")\n        >>> gpg = GPG(homedir=\"doctests\")\n        >>> input = gpg.gen_key_input()\n        >>> result = gpg.gen_key(input)\n        >>> print1 = result.fingerprint\n        >>> result = gpg.gen_key(input)\n        >>> print2 = result.fingerprint\n        >>> pubkeys = gpg.list_keys()\n        >>> assert print1 in pubkeys.fingerprints\n        >>> assert print2 in pubkeys.fingerprints\n        \"\"\"\n        which = \"public-keys\"\n        if secret:\n            which = \"secret-keys\"\n\n        args = []\n        args.append(\"--fixed-list-mode\")\n        args.append(\"--fingerprint\")\n        args.append(\"--with-colons\")\n        args.append(\"--list-options no-show-photos\")\n        args.append(\"--list-%s\" % (which))\n\n        p = self._open_subprocess(args)\n\n        # there might be some status thingumy here I should handle... (amk)\n        # ...nope, unless you care about expired sigs or keys (stevegt)\n\n        # Get the response information\n        result = self._result_map[\"list\"](self)\n        self._collect_output(p, result, stdin=p.stdin)\n        self._parse_keys(result)\n        return result",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "list_packets",
          "code": "def list_packets(self, raw_data):  # type: ignore[no-untyped-def]\n        \"\"\"List the packet contents of a file.\"\"\"\n        args = [\"--list-packets\"]\n        result = self._result_map[\"packets\"](self)\n        self._handle_io(args, _make_binary_stream(raw_data, self._encoding), result)\n        return result",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "sign_key",
          "code": "def sign_key(self, keyid, default_key=None, passphrase=None):  # type: ignore[no-untyped-def]\n        \"\"\"sign (an imported) public key - keyid, with default secret key\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> gpg.sign_key(key['fingerprint'])\n        >>> gpg.list_sigs(key['fingerprint'])\n\n        :param str keyid: key shortID, longID, fingerprint or email_address\n        :param str passphrase: passphrase used when creating the key, leave None otherwise\n\n        :returns: The result giving status of the key signing...\n                    success can be verified by gpg.list_sigs(keyid)\n        \"\"\"\n\n        args = []\n        input_command = \"\"\n        if passphrase:\n            passphrase_arg = \"--passphrase-fd 0\"\n            input_command = \"%s\\n\" % passphrase\n            args.append(passphrase_arg)\n\n        if default_key:\n            args.append(str(\"--default-key %s\" % default_key))\n\n        args.extend([\"--command-fd 0\", \"--sign-key %s\" % keyid])\n\n        p = self._open_subprocess(args)\n        result = self._result_map[\"signing\"](self)\n        confirm_command = \"%sy\\n\" % input_command\n        p.stdin.write(confirm_command.encode())\n        self._collect_output(p, result, stdin=p.stdin)\n        return result",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "list_sigs",
          "code": "def list_sigs(self, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Get the signatures for each of the ``keyids``.\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> assert key.fingerprint\n\n        :rtype: dict\n        :returns: res.sigs is a dictionary whose keys are the uids and whose\n                values are a set of signature keyids.\n        \"\"\"\n        return self._process_keys(keyids)",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "check_sigs",
          "code": "def check_sigs(self, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Validate the signatures for each of the ``keyids``.\n\n        :rtype: dict\n        :returns: res.certs is a dictionary whose keys are the uids and whose\n                values are a set of signature keyids.\n        \"\"\"\n        return self._process_keys(keyids, check_sig=True)",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "_process_keys",
          "code": "def _process_keys(self, keyids, check_sig=False):  # type: ignore[no-untyped-def]\n        if len(keyids) > self._batch_limit:\n            raise ValueError(\n                \"List signatures is limited to %d keyids simultaneously\" % self._batch_limit\n            )\n\n        args = [\"--with-colons\", \"--fixed-list-mode\"]\n        arg = \"--check-sigs\" if check_sig else \"--list-sigs\"\n\n        if len(keyids):\n            arg += \" \" + \" \".join(keyids)\n\n        args.append(arg)\n\n        proc = self._open_subprocess(args)\n        result = self._result_map[\"list\"](self)\n        self._collect_output(proc, result, stdin=proc.stdin)\n        self._parse_keys(result)\n        return result",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "_parse_keys",
          "code": "def _parse_keys(self, result):  # type: ignore[no-untyped-def]\n        lines = result.data.decode(self._encoding, self._decode_errors).splitlines()\n        valid_keywords = \"pub uid sec fpr sub sig rev\".split()\n        for line in lines:\n            if self.verbose:\n                print(line)\n            log.debug(\"%r\", line.rstrip())\n            if not line:\n                break\n            L = line.strip().split(\":\")\n            if not L:\n                continue\n            keyword = L[0]\n            if keyword in valid_keywords:\n                getattr(result, keyword)(L)",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "expire",
          "code": "def expire(self, keyid, expiration_time=\"1y\", passphrase=None, expire_subkeys=True):  # type: ignore[no-untyped-def]\n        \"\"\"Changes GnuPG key expiration by passing in new time period (from now) through\n            subprocess's stdin\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> gpg.expire(key.fingerprint, '2w', 'good passphrase')\n\n        :param str keyid: key shortID, longID, email_address or fingerprint\n        :param str expiration_time: 0 or number of days (d), or weeks (*w) , or months (*m)\n                or years (*y) for when to expire the key, from today.\n        :param str passphrase: passphrase used when creating the key, leave None otherwise\n        :param bool expire_subkeys: to indicate whether the subkeys will also change the\n                expiration time by the same period -- default is True\n\n        :returns: The result giving status of the change in expiration...\n                the new expiration date can be obtained by .list_keys()\n        \"\"\"\n\n        passphrase = passphrase.encode(self._encoding) if passphrase else passphrase\n\n        try:\n            sub_keys_number = len(self.list_sigs(keyid)[0][\"subkeys\"]) if expire_subkeys else 0\n        except IndexError:\n            sub_keys_number = 0\n\n        expiration_input = KeyExpirationInterface(\n            expiration_time, passphrase\n        ).gpg_interactive_input(sub_keys_number)\n\n        args = [\"--command-fd 0\", \"--edit-key %s\" % keyid]\n        p = self._open_subprocess(args)\n        p.stdin.write(expiration_input.encode())\n\n        result = self._result_map[\"expire\"](self)\n        p.stdin.write(expiration_input.encode())\n\n        self._collect_output(p, result, stdin=p.stdin)\n        return result",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "gen_key",
          "code": "def gen_key(self, input):  # type: ignore[no-untyped-def]\n        \"\"\"Generate a GnuPG key through batch file key generation. See\n        :meth:`GPG.gen_key_input()` for creating the control input.\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> assert key.fingerprint\n\n        :param dict input: A dictionary of parameters and values for the new\n                           key.\n        :returns: The result mapping with details of the new key, which is a\n                  :class:`GenKey <gnupg._parsers.GenKey>` object.\n        \"\"\"\n        args = [\"--gen-key --cert-digest-algo SHA512 --batch\"]\n        key = self._result_map[\"generate\"](self)\n        f = _make_binary_stream(input, self._encoding)\n        self._handle_io(args, f, key, binary=True)\n        f.close()\n\n        fpr = str(key.fingerprint)\n        if len(fpr) == 20:\n            for d in map(lambda x: os.path.dirname(x), [self.temp_keyring, self.temp_secring]):\n                if not os.path.exists(d):\n                    os.makedirs(d)\n\n            if self.temp_keyring and os.path.isfile(self.temp_keyring):\n                prefix = os.path.join(self.temp_keyring, fpr)\n                try:\n                    os.rename(self.temp_keyring, prefix + \".pubring\")\n                except OSError as ose:\n                    log.error(str(ose))\n\n            if self.temp_secring and os.path.isfile(self.temp_secring):\n                prefix = os.path.join(self.temp_secring, fpr)\n                try:\n                    os.rename(self.temp_secring, prefix + \".secring\")\n                except OSError as ose:\n                    log.error(str(ose))\n\n        log.info(\"Key created. Fingerprint: %s\" % fpr)\n        key.keyring = self.temp_keyring\n        key.secring = self.temp_secring\n        self.temp_keyring = None\n        self.temp_secring = None\n\n        return key",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "gen_key_input",
          "code": "def gen_key_input(self, separate_keyring=False, save_batchfile=False, testing=False, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Generate a batch file for input to :meth:`~gnupg.GPG.gen_key`.\n\n        The GnuPG batch file key generation feature allows unattended key\n        generation by creating a file with special syntax and then providing it\n        to: :command:`gpg --gen-key --batch`. Batch files look like this:\n\n        |  Name-Real: Alice\n        |  Name-Email: alice@inter.net\n        |  Expire-Date: 2014-04-01\n        |  Key-Type: RSA\n        |  Key-Length: 4096\n        |  Key-Usage: cert\n        |  Subkey-Type: RSA\n        |  Subkey-Length: 4096\n        |  Subkey-Usage: encrypt,sign,auth\n        |  Passphrase: sekrit\n        |  %pubring foo.gpg\n        |  %secring sec.gpg\n        |  %commit\n\n        which is what this function creates for you. All of the available,\n        non-control parameters are detailed below (control parameters are the\n        ones which begin with a '%'). For example, to generate the batch file\n        example above, use like this:\n\n        >>> import gnupg\n        GnuPG logging disabled...\n        >>> from __future__ import print_function\n        >>> gpg = gnupg.GPG(homedir='doctests')\n        >>> alice = { 'name_real': 'Alice',\n        ...     'name_email': 'alice@inter.net',\n        ...     'expire_date': '2014-04-01',\n        ...     'key_type': 'RSA',\n        ...     'key_length': 4096,\n        ...     'key_usage': '',\n        ...     'subkey_type': 'RSA',\n        ...     'subkey_length': 4096,\n        ...     'subkey_usage': 'encrypt,sign,auth',\n        ...     'passphrase': 'sekrit'}\n        >>> alice_input = gpg.gen_key_input(**alice)\n        >>> print(alice_input)\n        Key-Type: RSA\n        Subkey-Type: RSA\n        Subkey-Usage: encrypt,sign,auth\n        Expire-Date: 2014-04-01\n        Passphrase: sekrit\n        Name-Real: Alice\n        Name-Email: alice@inter.net\n        Key-Length: 4096\n        Subkey-Length: 4096\n        %pubring ./doctests/alice.pubring.gpg\n        %secring ./doctests/alice.secring.gpg\n        %commit\n        <BLANKLINE>\n        >>> alice_key = gpg.gen_key(alice_input)\n        >>> assert alice_key is not None\n        >>> assert alice_key.fingerprint is not None\n        >>> message = \"no one else can read my sekrit message\"\n        >>> encrypted = gpg.encrypt(message, alice_key.fingerprint)\n        >>> assert isinstance(encrypted.data, str)\n\n        :param bool separate_keyring: Specify for the new key to be written to\n            a separate pubring.gpg and secring.gpg. If True,\n            :meth:`~gnupg.GPG.gen_key` will automatically rename the separate\n            keyring and secring to whatever the fingerprint of the generated\n            key ends up being, suffixed with '.pubring' and '.secring'\n            respectively.\n\n        :param bool save_batchfile: Save a copy of the generated batch file to\n            disk in a file named <name_real>.batch, where <name_real> is the\n            ``name_real`` parameter stripped of punctuation, spaces, and\n            non-ascii characters.\n\n        :param bool testing: Uses a faster, albeit insecure random number\n            generator to create keys. This should only be used for testing\n            purposes, for keys which are going to be created and then soon\n            after destroyed, and never for the generation of actual use keys.\n\n        :param str name_real: The name field of the UID in the generated key.\n        :param str name_comment: The comment in the UID of the generated key.\n\n        :param str name_email: The email in the UID of the generated key.\n            (default: ``$USER`` @ :command:`hostname` ) Remember to use UTF-8\n            encoding for the entirety of the UID. At least one of\n            ``name_real``, ``name_comment``, or ``name_email`` must be\n            provided, or else no user ID is created.\n\n        :param str key_type: One of 'RSA', 'DSA', 'ELG-E', or 'default'.\n            (default: 'RSA', if using GnuPG v1.x, otherwise 'default') Starts\n            a new parameter block by giving the type of the primary key. The\n            algorithm must be capable of signing. This is a required\n            parameter. The algorithm may either be an OpenPGP algorithm number\n            or a string with the algorithm name. The special value ‘default’\n            may be used for algo to create the default key type; in this case\n            a ``key_usage`` should not be given and 'default' must also be\n            used for ``subkey_type``.\n\n        :param int key_length: The requested length of the generated key in\n            bits. (Default: 4096)\n\n        :param str key_grip: hexstring This is an optional hexidecimal string\n            which is used to generate a CSR or certificate for an already\n            existing key. ``key_length`` will be ignored if this parameter\n            is given.\n\n        :param str key_usage: Space or comma delimited string of key\n            usages. Allowed values are ‘encrypt’, ‘sign’, and ‘auth’. This is\n            used to generate the key flags. Please make sure that the\n            algorithm is capable of this usage. Note that OpenPGP requires\n            that all primary keys are capable of certification, so no matter\n            what usage is given here, the ‘cert’ flag will be on. If no\n            ‘Key-Usage’ is specified and the ‘Key-Type’ is not ‘default’, all\n            allowed usages for that particular algorithm are used; if it is\n            not given but ‘default’ is used the usage will be ‘sign’.\n\n        :param str subkey_type: This generates a secondary key\n            (subkey). Currently only one subkey can be handled. See also\n            ``key_type`` above.\n\n        :param int subkey_length: The length of the secondary subkey in bits.\n\n        :param str subkey_usage: Key usage for a subkey; similar to\n            ``key_usage``.\n\n        :type expire_date: :obj:`int` or :obj:`str`\n        :param expire_date: Can be specified as an iso-date or as\n            <int>[d|w|m|y] Set the expiration date for the key (and the\n            subkey). It may either be entered in ISO date format (2000-08-15)\n            or as number of days, weeks, month or years. The special notation\n            \"seconds=N\" is also allowed to directly give an Epoch\n            value. Without a letter days are assumed. Note that there is no\n            check done on the overflow of the type used by OpenPGP for\n            timestamps. Thus you better make sure that the given value make\n            sense. Although OpenPGP works with time intervals, GnuPG uses an\n            absolute value internally and thus the last year we can represent\n            is 2105.\n\n        :param str creation_date: Set the creation date of the key as stored\n            in the key information and which is also part of the fingerprint\n            calculation. Either a date like \"1986-04-26\" or a full timestamp\n            like \"19860426T042640\" may be used. The time is considered to be\n            UTC. If it is not given the current time is used.\n\n        :param str passphrase: The passphrase for the new key. The default is\n            to not use any passphrase. Note that GnuPG>=2.1.x will not allow\n            you to specify a passphrase for batch key generation -- GnuPG will\n            ignore the **passphrase** parameter, stop, and ask the user for\n            the new passphrase.  However, we can put the command\n            ``%no-protection`` into the batch key generation file to allow a\n            passwordless key to be created, which can then have its passphrase\n            set later with ``--edit-key``.\n\n        :param str preferences: Set the cipher, hash, and compression\n            preference values for this key. This expects the same type of\n            string as the sub-command ‘setpref’ in the --edit-key menu.\n\n        :param str revoker: Should be given as 'algo:fpr' (case sensitive).\n            Add a designated revoker to the generated key. Algo is the public\n            key algorithm of the designated revoker (i.e. RSA=1, DSA=17, etc.)\n            fpr is the fingerprint of the designated revoker. The optional\n            ‘sensitive’ flag marks the designated revoker as sensitive\n            information. Only v4 keys may be designated revokers.\n\n        :param str keyserver: This is an optional parameter that specifies the\n            preferred keyserver URL for the key.\n\n        :param str handle: This is an optional parameter only used with the\n            status lines ``KEY_CREATED`` and ``KEY_NOT_CREATED``. string may\n            be up to 100 characters and should not contain spaces. It is\n            useful for batch key generation to associate a key parameter block\n            with a status line.\n\n        :rtype: str\n        :returns: A suitable input string for the :meth:`GPG.gen_key` method,\n            the latter of which will create the new keypair.\n\n        See `this GnuPG Manual section`__ for more details.\n\n        __ http://www.gnupg.org/documentation/manuals/gnupg-devel/Unattended-GPG-key-generation.html\n        \"\"\"\n        #: A boolean for determining whether to set subkey_type to 'default'\n        default_type = False\n\n        parms = {}\n\n        parms.setdefault(\"Key-Type\", \"default\")\n        log.debug(\n            \"GnuPG v{} detected: setting default key type to {}.\".format(\n                self.binary_version, parms[\"Key-Type\"]\n            )\n        )\n        parms.setdefault(\"Key-Length\", 4096)\n        parms.setdefault(\"Name-Real\", \"Autogenerated Key\")\n        parms.setdefault(\"Expire-Date\", _util._next_year())\n\n        name_email = kwargs.get(\"name_email\")\n        uidemail = _util.create_uid_email(name_email)\n        parms.setdefault(\"Name-Email\", uidemail)\n\n        if testing:\n            # This specific comment string is required by (some? all?)\n            # versions of GnuPG to use the insecure PRNG:\n            parms.setdefault(\"Name-Comment\", \"insecure!\")\n\n        for key, val in list(kwargs.items()):\n            key = key.replace(\"_\", \"-\").title()\n            # to set 'cert', 'Key-Usage' must be blank string\n            if key not in (\"Key-Usage\", \"Subkey-Usage\") and str(val).strip():\n                parms[key] = val\n\n        # if Key-Type is 'default', make Subkey-Type also be 'default'\n        if parms[\"Key-Type\"] == \"default\":\n            default_type = True\n            for field in (\n                \"Key-Usage\",\n                \"Subkey-Usage\",\n            ):\n                try:\n                    parms.pop(field)  # toss these out, handle manually\n                except KeyError:\n                    pass\n\n        # Key-Type must come first, followed by length\n        out = \"Key-Type: %s\\n\" % parms.pop(\"Key-Type\")\n        out += \"Key-Length: %d\\n\" % parms.pop(\"Key-Length\")\n        if \"Subkey-Type\" in parms:\n            out += \"Subkey-Type: %s\\n\" % parms.pop(\"Subkey-Type\")\n        elif default_type:\n            out += \"Subkey-Type: default\\n\"\n        if \"Subkey-Length\" in parms:\n            out += \"Subkey-Length: %s\\n\" % parms.pop(\"Subkey-Length\")\n\n        for key, val in list(parms.items()):\n            out += f\"{key}: {val}\\n\"\n\n        # There is a problem where, in the batch files, if the '%%pubring'\n        # and '%%secring' are given as any static string, i.e. 'pubring.gpg',\n        # that file will always get rewritten without confirmation, killing\n        # off any keys we had before. So in the case where we wish to\n        # generate a bunch of keys and then do stuff with them, we should not\n        # give 'pubring.gpg' as our keyring file, otherwise we will lose any\n        # keys we had previously.\n\n        if separate_keyring:\n            ring = str(uidemail + \"_\" + str(int(time.time())))\n            self.temp_keyring = os.path.join(self.homedir, ring + \".pubring\")\n            self.temp_secring = os.path.join(self.homedir, ring + \".secring\")\n            out += \"%%pubring %s\\n\" % self.temp_keyring\n            out += \"%%secring %s\\n\" % self.temp_secring\n\n        if testing:\n            # see TODO file, tag :compatibility:gen_key_input:\n            #\n            # Add version detection before the '%no-protection' flag.\n            out += \"%no-protection\\n\"\n            out += \"%transient-key\\n\"\n\n        out += \"%commit\\n\"\n\n        # if we've been asked to save a copy of the batch file:\n        if save_batchfile and parms[\"Name-Email\"] != uidemail:\n            asc_uid = encodings.normalize_encoding(parms[\"Name-Email\"])\n            filename = _fix_unsafe(asc_uid) + _util._now() + \".batch\"\n            save_as = os.path.join(self._batch_dir, filename)\n            readme = os.path.join(self._batch_dir, \"README\")\n\n            if not os.path.exists(self._batch_dir):\n                os.makedirs(self._batch_dir)\n\n                # the following pulls the link to GnuPG's online batchfile\n                # documentation from this function's docstring and sticks it\n                # in a README file in the batch directory:\n\n                if getattr(self.gen_key_input, \"__doc__\", None) is not None:\n                    docs = self.gen_key_input.__doc__\n                else:\n                    docs = \"\"  # docstring=None if run with \"python -OO\"\n                links = \"\\n\".join(x.strip() for x in docs.splitlines()[-2:])\n                explain = f\"\"\"\nThis directory was created by python-gnupg, on {_util.now()}, and\nit contains saved batch files, which can be given to GnuPG to automatically\ngenerate keys. Please see\n{links}\"\"\"  # sometimes python is awesome.\n\n                with open(readme, \"a+\") as fh:\n                    [fh.write(line) for line in explain]\n\n            with open(save_as, \"a+\") as batch_file:\n                [batch_file.write(line) for line in out]\n\n        return out",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "encrypt",
          "code": "def encrypt(self, data, *recipients, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Encrypt the message contained in ``data`` to ``recipients``.\n\n        :param str data: The file or bytestream to encrypt.\n\n        :param str recipients: The recipients to encrypt to. Recipients must\n            be specified keyID/fingerprint. Care should be taken in Python2.x\n            to make sure that the given fingerprint is in fact a string and\n            not a unicode object.  Multiple recipients may be specified by\n            doing ``GPG.encrypt(data, fpr1, fpr2, fpr3)`` etc.\n\n        :param str default_key: The keyID/fingerprint of the key to use for\n            signing. If given, ``data`` will be encrypted and signed.\n\n        :param str passphrase: If given, and ``default_key`` is also given,\n            use this passphrase to unlock the secret portion of the\n            ``default_key`` to sign the encrypted ``data``. Otherwise, if\n            ``default_key`` is not given, but ``symmetric=True``, then use\n            this passphrase as the passphrase for symmetric\n            encryption. Signing and symmetric encryption should *not* be\n            combined when sending the ``data`` to other recipients, else the\n            passphrase to the secret key would be shared with them.\n\n        :param bool armor: If True, ascii armor the output; otherwise, the\n            output will be in binary format. (Default: True)\n\n        :param bool encrypt: If True, encrypt the ``data`` using the\n            ``recipients`` public keys. (Default: True)\n\n        :param bool symmetric: If True, encrypt the ``data`` to ``recipients``\n            using a symmetric key. See the ``passphrase`` parameter. Symmetric\n            encryption and public key encryption can be used simultaneously,\n            and will result in a ciphertext which is decryptable with either\n            the symmetric ``passphrase`` or one of the corresponding private\n            keys.\n\n        :param bool always_trust: If True, ignore trust warnings on recipient\n            keys. If False, display trust warnings.  (default: True)\n\n        :param str output: The output file to write to. If not specified, the\n            encrypted output is returned, and thus should be stored as an\n            object in Python. For example:\n\n        >>> import shutil\n        >>> import gnupg\n        >>> if os.path.exists(\"doctests\"):\n        ...     shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_settings = gpg.gen_key_input(key_type='RSA',\n        ...     key_length=1024,\n        ...     key_usage='ESCA',\n        ...     passphrase='foo')\n        >>> key = gpg.gen_key(key_settings)\n        >>> message = \"The crow flies at midnight.\"\n        >>> encrypted = str(gpg.encrypt(message, key.fingerprint))\n        >>> assert encrypted != message\n        >>> assert not encrypted.isspace()\n        >>> decrypted = str(gpg.decrypt(encrypted, passphrase='foo'))\n        >>> assert not decrypted.isspace()\n        >>> decrypted\n        'The crow flies at midnight.'\n\n\n        :param bool throw_keyids: If True, make all **recipients** keyids be\n            zero'd out in packet information. This is the same as using\n            **hidden_recipients** for all **recipients**. (Default: False).\n\n        :param list hidden_recipients: A list of recipients that should have\n            their keyids zero'd out in packet information.\n\n        :param str cipher_algo: The cipher algorithm to use. To see available\n            algorithms with your version of GnuPG, do:\n            :command:`$ gpg --with-colons --list-config ciphername`.\n            The default ``cipher_algo``, if unspecified, is ``'AES256'``.\n\n        :param str digest_algo: The hash digest to use. Again, to see which\n            hashes your GnuPG is capable of using, do:\n            :command:`$ gpg --with-colons --list-config digestname`.\n            The default, if unspecified, is ``'SHA512'``.\n\n        :param str compress_algo: The compression algorithm to use. Can be one\n            of ``'ZLIB'``, ``'BZIP2'``, ``'ZIP'``, or ``'Uncompressed'``.\n\n        .. seealso:: :meth:`._encrypt`\n        \"\"\"\n        if _is_stream(data):\n            stream = data\n        else:\n            stream = _make_binary_stream(data, self._encoding)\n        result = self._encrypt(stream, recipients, **kwargs)\n        stream.close()\n        return result",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "decrypt",
          "code": "def decrypt(self, message, **kwargs):  # type: ignore[no-untyped-def]\n        \"\"\"Decrypt the contents of a string or file-like object ``message``.\n\n        :type message: file or str or :class:`io.BytesIO`\n        :param message: A string or file-like object to decrypt.\n        :param bool always_trust: Instruct GnuPG to ignore trust checks.\n        :param str passphrase: The passphrase for the secret key used for decryption.\n        :param str output: A filename to write the decrypted output to.\n        \"\"\"\n        stream = _make_binary_stream(message, self._encoding)\n        result = self.decrypt_file(stream, **kwargs)\n        stream.close()\n        return result",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "decrypt_file",
          "code": "def decrypt_file(self, filename, always_trust=False, passphrase=None, output=None):  # type: ignore[no-untyped-def]\n        \"\"\"Decrypt the contents of a file-like object ``filename`` .\n\n        :param str filename: A file-like object to decrypt.\n        :param bool always_trust: Instruct GnuPG to ignore trust checks.\n        :param str passphrase: The passphrase for the secret key used for decryption.\n        :param str output: A filename to write the decrypted output to.\n        \"\"\"\n        args = [\"--decrypt\"]\n        if output:  # write the output to a file with the specified name\n            if os.path.exists(output):\n                os.remove(output)  # to avoid overwrite confirmation message\n            args.append(\"--output %s\" % output)\n        if always_trust:\n            args.append(\"--always-trust\")\n        result = self._result_map[\"crypt\"](self)\n        self._handle_io(args, filename, result, passphrase, binary=True)\n        log.debug(\"decrypt result: %r\", result.data)\n        return result",
          "file": "gnupg.py"
        },
        {
          "type": "class",
          "name": "GPGUtilities",
          "code": "class GPGUtilities:\n    \"\"\"Extra tools for working with GnuPG.\"\"\"\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        \"\"\"Initialise extra utility functions.\"\"\"\n        self._gpg = gpg\n\n    def find_key_by_email(self, email, secret=False):  # type: ignore[no-untyped-def]\n        \"\"\"Find user's key based on their email address.\n\n        :param str email: The email address to search for.\n        :param bool secret: If True, search through secret keyring.\n        \"\"\"\n        for key in self.list_keys(secret=secret):\n            for uid in key[\"uids\"]:\n                if re.search(email, uid):\n                    return key\n        raise LookupError(\"GnuPG public key for email %s not found!\" % email)\n\n    def find_key_by_subkey(self, subkey):  # type: ignore[no-untyped-def]\n        \"\"\"Find a key by a fingerprint of one of its subkeys.\n\n        :param str subkey: The fingerprint of the subkey to search for.\n        \"\"\"\n        for key in self.list_keys():\n            for sub in key[\"subkeys\"]:\n                if sub[0] == subkey:\n                    return key\n        raise LookupError(\"GnuPG public key for subkey %s not found!\" % subkey)\n\n    def send_keys(self, keyserver, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Send keys to a keyserver.\"\"\"\n        result = self._result_map[\"list\"](self)\n        log.debug(\"send_keys: %r\", keyids)\n        data = _util._make_binary_stream(\"\", self._encoding)\n        args = [\"--keyserver\", keyserver, \"--send-keys\"]\n        args.extend(keyids)\n        self._handle_io(args, data, result, binary=True)\n        log.debug(\"send_keys result: %r\", result.__dict__)\n        data.close()\n        return result\n\n    def encrypted_to(self, raw_data):  # type: ignore[no-untyped-def]\n        \"\"\"Return the key to which raw_data is encrypted to.\"\"\"\n        # TODO: make this support multiple keys.\n        result = self._gpg.list_packets(raw_data)\n        if not result.key:\n            raise LookupError(\"Content is not encrypted to a GnuPG key!\")\n        try:\n            return self.find_key_by_keyid(result.key)\n        except:  # noqa: E722\n            return self.find_key_by_subkey(result.key)\n\n    def is_encrypted_sym(self, raw_data):  # type: ignore[no-untyped-def]\n        result = self._gpg.list_packets(raw_data)\n        return bool(result.need_passphrase_sym)\n\n    def is_encrypted_asym(self, raw_data):  # type: ignore[no-untyped-def]\n        result = self._gpg.list_packets(raw_data)\n        return bool(result.key)\n\n    def is_encrypted(self, raw_data):  # type: ignore[no-untyped-def]\n        return self.is_encrypted_asym(raw_data) or self.is_encrypted_sym(raw_data)",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        \"\"\"Initialise extra utility functions.\"\"\"\n        self._gpg = gpg",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "find_key_by_email",
          "code": "def find_key_by_email(self, email, secret=False):  # type: ignore[no-untyped-def]\n        \"\"\"Find user's key based on their email address.\n\n        :param str email: The email address to search for.\n        :param bool secret: If True, search through secret keyring.\n        \"\"\"\n        for key in self.list_keys(secret=secret):\n            for uid in key[\"uids\"]:\n                if re.search(email, uid):\n                    return key\n        raise LookupError(\"GnuPG public key for email %s not found!\" % email)",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "find_key_by_subkey",
          "code": "def find_key_by_subkey(self, subkey):  # type: ignore[no-untyped-def]\n        \"\"\"Find a key by a fingerprint of one of its subkeys.\n\n        :param str subkey: The fingerprint of the subkey to search for.\n        \"\"\"\n        for key in self.list_keys():\n            for sub in key[\"subkeys\"]:\n                if sub[0] == subkey:\n                    return key\n        raise LookupError(\"GnuPG public key for subkey %s not found!\" % subkey)",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "send_keys",
          "code": "def send_keys(self, keyserver, *keyids):  # type: ignore[no-untyped-def]\n        \"\"\"Send keys to a keyserver.\"\"\"\n        result = self._result_map[\"list\"](self)\n        log.debug(\"send_keys: %r\", keyids)\n        data = _util._make_binary_stream(\"\", self._encoding)\n        args = [\"--keyserver\", keyserver, \"--send-keys\"]\n        args.extend(keyids)\n        self._handle_io(args, data, result, binary=True)\n        log.debug(\"send_keys result: %r\", result.__dict__)\n        data.close()\n        return result",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "encrypted_to",
          "code": "def encrypted_to(self, raw_data):  # type: ignore[no-untyped-def]\n        \"\"\"Return the key to which raw_data is encrypted to.\"\"\"\n        # TODO: make this support multiple keys.\n        result = self._gpg.list_packets(raw_data)\n        if not result.key:\n            raise LookupError(\"Content is not encrypted to a GnuPG key!\")\n        try:\n            return self.find_key_by_keyid(result.key)\n        except:  # noqa: E722\n            return self.find_key_by_subkey(result.key)",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "is_encrypted_sym",
          "code": "def is_encrypted_sym(self, raw_data):  # type: ignore[no-untyped-def]\n        result = self._gpg.list_packets(raw_data)\n        return bool(result.need_passphrase_sym)",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "is_encrypted_asym",
          "code": "def is_encrypted_asym(self, raw_data):  # type: ignore[no-untyped-def]\n        result = self._gpg.list_packets(raw_data)\n        return bool(result.key)",
          "file": "gnupg.py"
        },
        {
          "type": "function",
          "name": "is_encrypted",
          "code": "def is_encrypted(self, raw_data):  # type: ignore[no-untyped-def]\n        return self.is_encrypted_asym(raw_data) or self.is_encrypted_sym(raw_data)",
          "file": "gnupg.py"
        }
      ],
      "_trust.py": [
        {
          "type": "function",
          "name": "_create_trustdb",
          "code": "def _create_trustdb(cls):  # type: ignore[no-untyped-def]\n    \"\"\"Create the trustdb file in our homedir, if it doesn't exist.\"\"\"\n    trustdb = os.path.join(cls.homedir, \"trustdb.gpg\")\n    if not os.path.isfile(trustdb):\n        log.info(\n            \"GnuPG complained that your trustdb file was missing. {}\".format(\n                \"This is likely due to changing to a new homedir.\"\n            )\n        )\n        log.info(\"Creating trustdb.gpg file in your GnuPG homedir.\")\n        cls.fix_trustdb(trustdb)",
          "file": "_trust.py"
        },
        {
          "type": "function",
          "name": "export_ownertrust",
          "code": "def export_ownertrust(cls, trustdb=None):  # type: ignore[no-untyped-def]\n    \"\"\"Export ownertrust to a trustdb file.\n\n    If there is already a file named :file:`trustdb.gpg` in the current GnuPG\n    homedir, it will be renamed to :file:`trustdb.gpg.bak`.\n\n    :param string trustdb: The path to the trustdb.gpg file. If not given,\n                           defaults to ``'trustdb.gpg'`` in the current GnuPG\n                           homedir.\n    \"\"\"\n    if trustdb is None:\n        trustdb = os.path.join(cls.homedir, \"trustdb.gpg\")\n\n    try:\n        os.rename(trustdb, trustdb + \".bak\")\n    except OSError as err:\n        log.debug(str(err))\n\n    export_proc = cls._open_subprocess([\"--export-ownertrust\"])\n    tdb = open(trustdb, \"wb\")\n    _util._threaded_copy_data(export_proc.stdout, tdb)\n    export_proc.wait()",
          "file": "_trust.py"
        },
        {
          "type": "function",
          "name": "import_ownertrust",
          "code": "def import_ownertrust(cls, trustdb=None):  # type: ignore[no-untyped-def]\n    \"\"\"Import ownertrust from a trustdb file.\n\n    :param str trustdb: The path to the trustdb.gpg file. If not given,\n                        defaults to :file:`trustdb.gpg` in the current GnuPG\n                        homedir.\n    \"\"\"\n    if trustdb is None:\n        trustdb = os.path.join(cls.homedir, \"trustdb.gpg\")\n\n    import_proc = cls._open_subprocess([\"--import-ownertrust\"])\n\n    try:\n        tdb = open(trustdb, \"rb\")\n    except OSError:\n        log.error(\"trustdb file %s does not exist!\" % trustdb)\n\n    _util._threaded_copy_data(tdb, import_proc.stdin)\n    import_proc.wait()",
          "file": "_trust.py"
        },
        {
          "type": "function",
          "name": "fix_trustdb",
          "code": "def fix_trustdb(cls, trustdb=None):  # type: ignore[no-untyped-def]\n    \"\"\"Attempt to repair a broken trustdb.gpg file.\n\n    GnuPG>=2.0.x has this magical-seeming flag: `--fix-trustdb`. You'd think\n    it would fix the the trustdb. Hah! It doesn't. Here's what it does\n    instead::\n\n      (gpg)~/code/python-gnupg $ gpg2 --fix-trustdb\n      gpg: You may try to re-create the trustdb using the commands:\n      gpg:   cd ~/.gnupg\n      gpg:   gpg2 --export-ownertrust > otrust.tmp\n      gpg:   rm trustdb.gpg\n      gpg:   gpg2 --import-ownertrust < otrust.tmp\n      gpg: If that does not work, please consult the manual\n\n    Brilliant piece of software engineering right there.\n\n    :param str trustdb: The path to the trustdb.gpg file. If not given,\n                        defaults to :file:`trustdb.gpg` in the current GnuPG\n                        homedir.\n    \"\"\"\n    if trustdb is None:\n        trustdb = os.path.join(cls.homedir, \"trustdb.gpg\")\n    export_proc = cls._open_subprocess([\"--export-ownertrust\"])\n    import_proc = cls._open_subprocess([\"--import-ownertrust\"])\n    _util._threaded_copy_data(export_proc.stdout, import_proc.stdin)\n    export_proc.wait()\n    import_proc.wait()",
          "file": "_trust.py"
        }
      ],
      "_parsers.py": [
        {
          "type": "class",
          "name": "ProtectedOption",
          "code": "class ProtectedOption(Exception):\n    \"\"\"Raised when the option passed to GPG is disallowed.\"\"\"",
          "file": "_parsers.py"
        },
        {
          "type": "class",
          "name": "UsageError",
          "code": "class UsageError(Exception):\n    \"\"\"Raised when incorrect usage of the API occurs..\"\"\"",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_check_keyserver",
          "code": "def _check_keyserver(location):  # type: ignore[no-untyped-def]\n    \"\"\"Check that a given keyserver is a known protocol and does not contain\n    shell escape characters.\n\n    :param str location: A string containing the default keyserver. This\n                         should contain the desired keyserver protocol which\n                         is supported by the keyserver, for example, the\n                         default is ``'hkp://wwwkeys .pgp.net'``.\n    :rtype: :obj:`str` or :obj:`None`\n    :returns: A string specifying the protocol and keyserver hostname, if the\n              checks passed. If not, returns None.\n    \"\"\"\n    protocols = [\n        \"hkp://\",\n        \"hkps://\",\n        \"http://\",\n        \"https://\",\n        \"ldap://\",\n        \"mailto:\",\n    ]  # xxx feels like i´m forgetting one...\n    for proto in protocols:\n        if location.startswith(proto):\n            url = location.replace(proto, \"\")\n            host, slash, extra = url.partition(\"/\")\n            if extra:\n                log.warn(f\"URI text for {host}: '{extra}'\")\n            log.debug(\"Got host string for keyserver setting: '%s'\" % host)\n\n            host = _fix_unsafe(host)\n            if host:\n                log.debug(\"Cleaned host string: '%s'\" % host)\n                return proto + host\n            return None",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_check_preferences",
          "code": "def _check_preferences(prefs, pref_type=None):  # type: ignore[no-untyped-def]\n    \"\"\"Check cipher, digest, and compression preference settings.\n\n    MD5 is not allowed. This is `not 1994`__. SHA1 is allowed_ grudgingly_.\n\n    __ http://www.cs.colorado.edu/~jrblack/papers/md5e-full.pdf\n    .. _allowed: http://eprint.iacr.org/2008/469.pdf\n    .. _grudgingly: https://www.schneier.com/blog/archives/2012/10/when_will_we_se.html\n    \"\"\"\n    if prefs is None:\n        return None\n\n    cipher = frozenset(\n        [\"AES256\", \"AES192\", \"AES128\", \"CAMELLIA256\", \"CAMELLIA192\", \"TWOFISH\", \"3DES\"]\n    )\n    digest = frozenset([\"SHA512\", \"SHA384\", \"SHA256\", \"SHA224\", \"RMD160\", \"SHA1\"])\n    compress = frozenset([\"BZIP2\", \"ZLIB\", \"ZIP\", \"Uncompressed\"])\n    trust = frozenset([\"gpg\", \"classic\", \"direct\", \"always\", \"auto\"])\n    pinentry = frozenset([\"loopback\"])\n    all = frozenset([cipher, digest, compress, trust, pinentry])\n\n    if isinstance(prefs, str):\n        prefs = set(prefs.split())\n    elif isinstance(prefs, list):\n        prefs = set(prefs)\n    else:\n        message = \"prefs must be list of strings, or space-separated string\"\n        log.error(\"parsers._check_preferences(): %s\" % message)\n        raise TypeError(message)\n\n    if not pref_type:\n        pref_type = \"all\"\n\n    allowed = \"\"\n\n    if pref_type == \"cipher\":\n        allowed += \" \".join(prefs.intersection(cipher))\n    if pref_type == \"digest\":\n        allowed += \" \".join(prefs.intersection(digest))\n    if pref_type == \"compress\":\n        allowed += \" \".join(prefs.intersection(compress))\n    if pref_type == \"trust\":\n        allowed += \" \".join(prefs.intersection(trust))\n    if pref_type == \"pinentry\":\n        allowed += \" \".join(prefs.intersection(pinentry))\n    if pref_type == \"all\":\n        allowed += \" \".join(prefs.intersection(all))\n\n    return allowed",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_fix_unsafe",
          "code": "def _fix_unsafe(shell_input):  # type: ignore[no-untyped-def]\n    \"\"\"Find characters used to escape from a string into a shell, and wrap them in\n    quotes if they exist. Regex pilfered from Python3 :mod:`shlex` module.\n\n    :param str shell_input: The input intended for the GnuPG process.\n    \"\"\"\n    _unsafe = re.compile(r\"[^\\w@%+=:,./-]\", 256)\n    try:\n        if len(_unsafe.findall(shell_input)) == 0:\n            return shell_input.strip()\n        else:\n            return \"'\" + shell_input.replace(\"'\", \"'\\\"'\\\"'\") + \"'\"\n    except TypeError:\n        return None",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_hyphenate",
          "code": "def _hyphenate(input, add_prefix=False):  # type: ignore[no-untyped-def]\n    \"\"\"Change underscores to hyphens so that object attributes can be easily\n    tranlated to GPG option names.\n\n    :param str input: The attribute to hyphenate.\n    :param bool add_prefix: If True, add leading hyphens to the input.\n    :rtype: str\n    :return: The ``input`` with underscores changed to hyphens.\n    \"\"\"\n    ret = \"--\" if add_prefix else \"\"\n    ret += input.replace(\"_\", \"-\")\n    return ret",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_is_allowed",
          "code": "def _is_allowed(input):  # type: ignore[no-untyped-def]\n    \"\"\"Check that an option or argument given to GPG is in the set of allowed\n    options, the latter being a strict subset of the set of all options known\n    to GPG.\n\n    :param str input: An input meant to be parsed as an option or flag to the\n                      GnuPG process. Should be formatted the same as an option\n                      or flag to the commandline gpg, i.e. \"--encrypt-files\".\n\n    :ivar frozenset gnupg_options: All known GPG options and flags.\n\n    :ivar frozenset allowed: All allowed GPG options and flags, e.g. all GPG\n                             options and flags which we are willing to\n                             acknowledge and parse. If we want to support a\n                             new option, it will need to have its own parsing\n                             class and its name will need to be added to this\n                             set.\n\n    :raises: :exc:`UsageError` if **input** is not a subset of the hard-coded\n             set of all GnuPG options in :func:`_get_all_gnupg_options`.\n\n             :exc:`ProtectedOption` if **input** is not in the set of allowed\n             options.\n\n    :rtype: str\n    :return: The original **input** parameter, unmodified and unsanitized, if\n             no errors occur.\n    \"\"\"\n    gnupg_options = _get_all_gnupg_options()\n    allowed = _get_options_group(\"allowed\")\n\n    # these are the allowed options we will handle so far, all others should\n    # be dropped. this dance is so that when new options are added later, we\n    # merely add the to the _allowed list, and the `` _allowed.issubset``\n    # assertion will check that GPG will recognise them\n    try:\n        # check that allowed is a subset of all gnupg_options\n        assert allowed.issubset(gnupg_options)\n    except AssertionError:\n        raise UsageError(\n            \"'allowed' isn't a subset of known options, diff: %s\"\n            % allowed.difference(gnupg_options)\n        )\n\n    # if we got a list of args, join them\n    #\n    # see TODO file, tag :cleanup:\n    if not isinstance(input, str):\n        input = \" \".join([x for x in input])\n\n    if isinstance(input, str):\n        if input.find(\"_\") > 0:\n            if not input.startswith(\"--\"):\n                hyphenated = _hyphenate(input, add_prefix=True)\n            else:\n                hyphenated = _hyphenate(input)\n        else:\n            hyphenated = input\n            # xxx we probably want to use itertools.dropwhile here\n            try:\n                assert hyphenated in allowed\n            except AssertionError:\n                dropped = _fix_unsafe(hyphenated)\n                log.warn(\"_is_allowed(): Dropping option '%s'...\" % dropped)\n                raise ProtectedOption(\"Option '%s' not supported.\" % dropped)\n            else:\n                return input\n    return None",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_is_hex",
          "code": "def _is_hex(string):  # type: ignore[no-untyped-def]\n    \"\"\"Check that a string is hexadecimal, with alphabetic characters\n    in either upper or lower case and without whitespace.\n\n    :param str string: The string to check.\n    \"\"\"\n    return bool(HEXADECIMAL.match(string))",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_sanitise",
          "code": "def _sanitise(*args):  # type: ignore[no-untyped-def]\n    \"\"\"Take an arg or the key portion of a kwarg and check that it is in the\n    set of allowed GPG options and flags, and that it has the correct\n    type. Then, attempt to escape any unsafe characters. If an option is not\n    allowed, drop it with a logged warning. Returns a dictionary of all\n    sanitised, allowed options.\n\n    Each new option that we support that is not a boolean, but instead has\n    some additional inputs following it, i.e. \"--encrypt-file foo.txt\", will\n    need some basic safety checks added here.\n\n    GnuPG has three-hundred and eighteen commandline flags. Also, not all\n    implementations of OpenPGP parse PGP packets and headers in the same way,\n    so there is added potential there for messing with calls to GPG.\n\n    For information on the PGP message format specification, see\n    :rfc:`1991`.\n\n    If you're asking, \"Is this *really* necessary?\": No, not really -- we could\n    just follow the security precautions recommended by `this xkcd`__.\n\n     __ https://xkcd.com/1181/\n\n    :param str args: (optional) The boolean arguments which will be passed to\n                     the GnuPG process.\n    :rtype: str\n    :returns: ``sanitised``\n    \"\"\"\n\n    # see TODO file, tag :cleanup:sanitise:\n\n    def _check_option(arg, value):  # type: ignore[no-untyped-def]\n        \"\"\"Check that a single ``arg`` is an allowed option.\n\n        If it is allowed, quote out any escape characters in ``value``, and\n        add the pair to :ivar:`sanitised`. Otherwise, drop them.\n\n        :param str arg: The arguments which will be passed to the GnuPG\n                        process, and, optionally their corresponding values.\n                        The values are any additional arguments following the\n                        GnuPG option or flag. For example, if we wanted to\n                        pass ``\"--encrypt --recipient isis@leap.se\"`` to\n                        GnuPG, then ``\"--encrypt\"`` would be an arg without a\n                        value, and ``\"--recipient\"`` would also be an arg,\n                        with a value of ``\"isis@leap.se\"``.\n\n        :ivar list checked: The sanitised, allowed options and values.\n        :rtype: str\n        :returns: A string of the items in ``checked``, delimited by spaces.\n        \"\"\"\n        checked = \"\"\n        none_options = _get_options_group(\"none_options\")\n        hex_options = _get_options_group(\"hex_options\")\n        hex_or_none_options = _get_options_group(\"hex_or_none_options\")\n\n        try:\n            flag = _is_allowed(arg)\n            assert flag is not None, \"_check_option(): got None for flag\"\n        except (AssertionError, ProtectedOption) as error:\n            log.warn(\"_check_option(): %s\" % str(error))\n        else:\n            checked += flag + \" \"\n\n            if isinstance(value, str):\n                values = value.split(\" \")\n                for v in values:\n                    # these can be handled separately, without _fix_unsafe(),\n                    # because they are only allowed if they pass the regex\n                    if (flag in none_options) and (v is None):\n                        continue\n\n                    if flag in hex_options:\n                        if _is_hex(v):\n                            checked += v + \" \"\n                        else:\n                            log.debug(f\"'{flag} {v}' not hex.\")\n                            if (flag in hex_or_none_options) and (v is None):\n                                log.debug(\"Allowing '%s' for all keys\" % flag)\n                        continue\n\n                    elif flag in [\"--keyserver\"]:\n                        host = _check_keyserver(v)\n                        if host:\n                            log.debug(\"Setting keyserver: %s\" % host)\n                            checked += v + \" \"\n                        else:\n                            log.debug(\"Dropping keyserver: %s\" % v)\n                        continue\n\n                    # the rest are strings, filenames, etc, and should be\n                    # shell escaped:\n                    val = _fix_unsafe(v)\n                    try:\n                        assert val is not None\n                        assert not val.isspace()\n                        assert v is not None\n                        assert not v.isspace()\n                    except:  # noqa: E722\n                        log.debug(f\"Dropping {flag} {v}\")\n                        continue\n\n                    if flag in [\n                        \"--encrypt\",\n                        \"--encrypt-files\",\n                        \"--decrypt\",\n                        \"--decrypt-files\",\n                        \"--import\",\n                        \"--verify\",\n                    ]:\n                        if (_util._is_file(val)) or ((flag == \"--verify\") and (val == \"-\")):\n                            checked += val + \" \"\n                        else:\n                            log.debug(f\"{flag} not file: {val}\")\n\n                    elif flag in [\n                        \"--cipher-algo\",\n                        \"--personal-cipher-prefs\",\n                        \"--personal-cipher-preferences\",\n                    ]:\n                        legit_algos = _check_preferences(val, \"cipher\")\n                        if legit_algos:\n                            checked += legit_algos + \" \"\n                        else:\n                            log.debug(\"'%s' is not cipher\" % val)\n\n                    elif flag in [\n                        \"--compress-algo\",\n                        \"--compression-algo\",\n                        \"--personal-compress-prefs\",\n                        \"--personal-compress-preferences\",\n                    ]:\n                        legit_algos = _check_preferences(val, \"compress\")\n                        if legit_algos:\n                            checked += legit_algos + \" \"\n                        else:\n                            log.debug(\"'%s' not compress algo\" % val)\n\n                    elif flag == \"--trust-model\":\n                        legit_models = _check_preferences(val, \"trust\")\n                        if legit_models:\n                            checked += legit_models + \" \"\n                        else:\n                            log.debug(\"%r is not a trust model\", val)\n\n                    elif flag == \"--pinentry-mode\":\n                        legit_modes = _check_preferences(val, \"pinentry\")\n                        if legit_modes:\n                            checked += legit_modes + \" \"\n                        else:\n                            log.debug(\"%r is not a pinentry mode\", val)\n\n                    else:\n                        checked += val + \" \"\n                        log.debug(\"_check_option(): No checks for %s\" % val)\n\n        return checked.rstrip(\" \")\n\n    def is_flag(x):  # type: ignore[no-untyped-def]\n        return x.startswith(\"--\")\n\n    def _make_filo(arg):  # type: ignore[no-untyped-def]\n        filo = arg.split(\" \")\n        filo.reverse()\n        log.debug(\"_make_filo(): Converted to reverse list: %s\" % filo)\n        return filo\n\n    def _make_groups(filo):  # type: ignore[no-untyped-def]\n        groups = {}\n        while len(filo) >= 1:\n            last = filo.pop()\n            if is_flag(last):\n                log.debug(\"Got arg: %s\" % last)\n                if last == \"--verify\":\n                    groups[last] = str(filo.pop())\n                    # accept the read-from-stdin arg:\n                    if len(filo) >= 1 and filo[len(filo) - 1] == \"-\":\n                        groups[last] += \" - \"  # gross hack\n                        filo.pop()\n                else:\n                    groups[last] = \"\"\n                while len(filo) > 1 and not is_flag(filo[len(filo) - 1]):\n                    log.debug(\"Got value: %s\" % filo[len(filo) - 1])\n                    groups[last] += filo.pop() + \" \"\n                if len(filo) == 1 and not is_flag(filo[0]):\n                    log.debug(\"Got value: %s\" % filo[0])\n                    groups[last] += filo.pop()\n            else:\n                log.warn(\"_make_groups(): Got solitary value: %s\" % last)\n                groups[\"xxx\"] = last\n        return groups\n\n    def _check_groups(groups):  # type: ignore[no-untyped-def]\n        log.debug(\"Got groups: %s\" % groups)\n        checked_groups = []\n        for a, v in groups.items():\n            v = None if len(v) == 0 else v\n            safe = _check_option(a, v)\n            if safe is not None and safe.strip() != \"\":\n                log.debug(\"Appending option: %s\" % safe)\n                checked_groups.append(safe)\n            else:\n                log.warn(f\"Dropped option: '{a} {v}'\")\n        return checked_groups\n\n    if args is not None:\n        option_groups = {}\n        for arg in args:\n            # if we're given a string with a bunch of options in it split\n            # them up and deal with them separately\n            if isinstance(arg, str):\n                log.debug(\"Got arg string: %s\" % arg)\n                if arg.find(\" \") > 0:\n                    filo = _make_filo(arg)\n                    option_groups.update(_make_groups(filo))\n                else:\n                    option_groups.update({arg: \"\"})\n            elif isinstance(arg, list):\n                log.debug(\"Got arg list: %s\" % arg)\n                arg.reverse()\n                option_groups.update(_make_groups(arg))\n            else:\n                log.warn(f\"Got non-str/list arg: '{arg}', type '{type(arg)}'\")\n        checked = _check_groups(option_groups)\n        return \" \".join(x for x in checked)\n    else:\n        log.debug(\"Got None for args\")",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_check_option",
          "code": "def _check_option(arg, value):  # type: ignore[no-untyped-def]\n        \"\"\"Check that a single ``arg`` is an allowed option.\n\n        If it is allowed, quote out any escape characters in ``value``, and\n        add the pair to :ivar:`sanitised`. Otherwise, drop them.\n\n        :param str arg: The arguments which will be passed to the GnuPG\n                        process, and, optionally their corresponding values.\n                        The values are any additional arguments following the\n                        GnuPG option or flag. For example, if we wanted to\n                        pass ``\"--encrypt --recipient isis@leap.se\"`` to\n                        GnuPG, then ``\"--encrypt\"`` would be an arg without a\n                        value, and ``\"--recipient\"`` would also be an arg,\n                        with a value of ``\"isis@leap.se\"``.\n\n        :ivar list checked: The sanitised, allowed options and values.\n        :rtype: str\n        :returns: A string of the items in ``checked``, delimited by spaces.\n        \"\"\"\n        checked = \"\"\n        none_options = _get_options_group(\"none_options\")\n        hex_options = _get_options_group(\"hex_options\")\n        hex_or_none_options = _get_options_group(\"hex_or_none_options\")\n\n        try:\n            flag = _is_allowed(arg)\n            assert flag is not None, \"_check_option(): got None for flag\"\n        except (AssertionError, ProtectedOption) as error:\n            log.warn(\"_check_option(): %s\" % str(error))\n        else:\n            checked += flag + \" \"\n\n            if isinstance(value, str):\n                values = value.split(\" \")\n                for v in values:\n                    # these can be handled separately, without _fix_unsafe(),\n                    # because they are only allowed if they pass the regex\n                    if (flag in none_options) and (v is None):\n                        continue\n\n                    if flag in hex_options:\n                        if _is_hex(v):\n                            checked += v + \" \"\n                        else:\n                            log.debug(f\"'{flag} {v}' not hex.\")\n                            if (flag in hex_or_none_options) and (v is None):\n                                log.debug(\"Allowing '%s' for all keys\" % flag)\n                        continue\n\n                    elif flag in [\"--keyserver\"]:\n                        host = _check_keyserver(v)\n                        if host:\n                            log.debug(\"Setting keyserver: %s\" % host)\n                            checked += v + \" \"\n                        else:\n                            log.debug(\"Dropping keyserver: %s\" % v)\n                        continue\n\n                    # the rest are strings, filenames, etc, and should be\n                    # shell escaped:\n                    val = _fix_unsafe(v)\n                    try:\n                        assert val is not None\n                        assert not val.isspace()\n                        assert v is not None\n                        assert not v.isspace()\n                    except:  # noqa: E722\n                        log.debug(f\"Dropping {flag} {v}\")\n                        continue\n\n                    if flag in [\n                        \"--encrypt\",\n                        \"--encrypt-files\",\n                        \"--decrypt\",\n                        \"--decrypt-files\",\n                        \"--import\",\n                        \"--verify\",\n                    ]:\n                        if (_util._is_file(val)) or ((flag == \"--verify\") and (val == \"-\")):\n                            checked += val + \" \"\n                        else:\n                            log.debug(f\"{flag} not file: {val}\")\n\n                    elif flag in [\n                        \"--cipher-algo\",\n                        \"--personal-cipher-prefs\",\n                        \"--personal-cipher-preferences\",\n                    ]:\n                        legit_algos = _check_preferences(val, \"cipher\")\n                        if legit_algos:\n                            checked += legit_algos + \" \"\n                        else:\n                            log.debug(\"'%s' is not cipher\" % val)\n\n                    elif flag in [\n                        \"--compress-algo\",\n                        \"--compression-algo\",\n                        \"--personal-compress-prefs\",\n                        \"--personal-compress-preferences\",\n                    ]:\n                        legit_algos = _check_preferences(val, \"compress\")\n                        if legit_algos:\n                            checked += legit_algos + \" \"\n                        else:\n                            log.debug(\"'%s' not compress algo\" % val)\n\n                    elif flag == \"--trust-model\":\n                        legit_models = _check_preferences(val, \"trust\")\n                        if legit_models:\n                            checked += legit_models + \" \"\n                        else:\n                            log.debug(\"%r is not a trust model\", val)\n\n                    elif flag == \"--pinentry-mode\":\n                        legit_modes = _check_preferences(val, \"pinentry\")\n                        if legit_modes:\n                            checked += legit_modes + \" \"\n                        else:\n                            log.debug(\"%r is not a pinentry mode\", val)\n\n                    else:\n                        checked += val + \" \"\n                        log.debug(\"_check_option(): No checks for %s\" % val)\n\n        return checked.rstrip(\" \")",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "is_flag",
          "code": "def is_flag(x):  # type: ignore[no-untyped-def]\n        return x.startswith(\"--\")",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_make_filo",
          "code": "def _make_filo(arg):  # type: ignore[no-untyped-def]\n        filo = arg.split(\" \")\n        filo.reverse()\n        log.debug(\"_make_filo(): Converted to reverse list: %s\" % filo)\n        return filo",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_make_groups",
          "code": "def _make_groups(filo):  # type: ignore[no-untyped-def]\n        groups = {}\n        while len(filo) >= 1:\n            last = filo.pop()\n            if is_flag(last):\n                log.debug(\"Got arg: %s\" % last)\n                if last == \"--verify\":\n                    groups[last] = str(filo.pop())\n                    # accept the read-from-stdin arg:\n                    if len(filo) >= 1 and filo[len(filo) - 1] == \"-\":\n                        groups[last] += \" - \"  # gross hack\n                        filo.pop()\n                else:\n                    groups[last] = \"\"\n                while len(filo) > 1 and not is_flag(filo[len(filo) - 1]):\n                    log.debug(\"Got value: %s\" % filo[len(filo) - 1])\n                    groups[last] += filo.pop() + \" \"\n                if len(filo) == 1 and not is_flag(filo[0]):\n                    log.debug(\"Got value: %s\" % filo[0])\n                    groups[last] += filo.pop()\n            else:\n                log.warn(\"_make_groups(): Got solitary value: %s\" % last)\n                groups[\"xxx\"] = last\n        return groups",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_check_groups",
          "code": "def _check_groups(groups):  # type: ignore[no-untyped-def]\n        log.debug(\"Got groups: %s\" % groups)\n        checked_groups = []\n        for a, v in groups.items():\n            v = None if len(v) == 0 else v\n            safe = _check_option(a, v)\n            if safe is not None and safe.strip() != \"\":\n                log.debug(\"Appending option: %s\" % safe)\n                checked_groups.append(safe)\n            else:\n                log.warn(f\"Dropped option: '{a} {v}'\")\n        return checked_groups",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_sanitise_list",
          "code": "def _sanitise_list(arg_list):  # type: ignore[no-untyped-def]\n    \"\"\"A generator for iterating through a list of gpg options and sanitising\n    them.\n\n    :param list arg_list: A list of options and flags for GnuPG.\n    :rtype: generator\n    :returns: A generator whose next() method returns each of the items in\n              ``arg_list`` after calling ``_sanitise()`` with that item as a\n              parameter.\n    \"\"\"\n    if isinstance(arg_list, list):\n        for arg in arg_list:\n            safe_arg = _sanitise(arg)\n            if safe_arg != \"\":\n                yield safe_arg",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_get_options_group",
          "code": "def _get_options_group(group=None):  # type: ignore[no-untyped-def]\n    \"\"\"Get a specific group of options which are allowed.\"\"\"\n\n    #: These expect a hexidecimal keyid as their argument, and can be parsed\n    #: with :func:`_is_hex`.\n    hex_options = frozenset(\n        [\n            \"--check-sigs\",\n            \"--default-key\",\n            \"--default-recipient\",\n            \"--delete-keys\",\n            \"--delete-secret-keys\",\n            \"--delete-secret-and-public-keys\",\n            \"--desig-revoke\",\n            \"--export\",\n            \"--export-secret-keys\",\n            \"--export-secret-subkeys\",\n            \"--fingerprint\",\n            \"--gen-revoke\",\n            \"--hidden-encrypt-to\",\n            \"--hidden-recipient\",\n            \"--list-key\",\n            \"--list-keys\",\n            \"--list-public-keys\",\n            \"--list-secret-keys\",\n            \"--list-sigs\",\n            \"--recipient\",\n            \"--recv-keys\",\n            \"--send-keys\",\n            \"--edit-key\",\n            \"--sign-key\",\n        ]\n    )\n    #: These options expect value which are left unchecked, though still run\n    #: through :func:`_fix_unsafe`.\n    unchecked_options = frozenset(\n        [\n            \"--list-options\",\n            \"--passphrase-fd\",\n            \"--status-fd\",\n            \"--verify-options\",\n            \"--command-fd\",\n        ]\n    )\n    #: These have their own parsers and don't really fit into a group\n    other_options = frozenset(\n        [\n            \"--debug-level\",\n            \"--keyserver\",\n        ]\n    )\n    #: These should have a directory for an argument\n    dir_options = frozenset(\n        [\n            \"--homedir\",\n        ]\n    )\n    #: These expect a keyring or keyfile as their argument\n    keyring_options = frozenset(\n        [\n            \"--keyring\",\n            \"--primary-keyring\",\n            \"--secret-keyring\",\n            \"--trustdb-name\",\n        ]\n    )\n    #: These expect a filename (or the contents of a file as a string) or None\n    #: (meaning that they read from stdin)\n    file_or_none_options = frozenset(\n        [\n            \"--decrypt\",\n            \"--decrypt-files\",\n            \"--encrypt\",\n            \"--encrypt-files\",\n            \"--import\",\n            \"--verify\",\n            \"--verify-files\",\n            \"--output\",\n        ]\n    )\n    #: These options expect a string. see :func:`_check_preferences`.\n    pref_options = frozenset(\n        [\n            \"--digest-algo\",\n            \"--cipher-algo\",\n            \"--compress-algo\",\n            \"--compression-algo\",\n            \"--cert-digest-algo\",\n            \"--personal-digest-prefs\",\n            \"--personal-digest-preferences\",\n            \"--personal-cipher-prefs\",\n            \"--personal-cipher-preferences\",\n            \"--personal-compress-prefs\",\n            \"--personal-compress-preferences\",\n            \"--pinentry-mode\",\n            \"--print-md\",\n            \"--trust-model\",\n        ]\n    )\n    #: These options expect no arguments\n    none_options = frozenset(\n        [\n            \"--allow-loopback-pinentry\",\n            \"--always-trust\",\n            \"--armor\",\n            \"--armour\",\n            \"--batch\",\n            \"--check-sigs\",\n            \"--check-trustdb\",\n            \"--clearsign\",\n            \"--debug-all\",\n            \"--default-recipient-self\",\n            \"--detach-sign\",\n            \"--export\",\n            \"--export-ownertrust\",\n            \"--export-secret-keys\",\n            \"--export-secret-subkeys\",\n            \"--fingerprint\",\n            \"--fixed-list-mode\",\n            \"--gen-key\",\n            \"--import-ownertrust\",\n            \"--list-config\",\n            \"--list-key\",\n            \"--list-keys\",\n            \"--list-packets\",\n            \"--list-public-keys\",\n            \"--list-secret-keys\",\n            \"--list-sigs\",\n            \"--lock-multiple\",\n            \"--lock-never\",\n            \"--lock-once\",\n            \"--no-default-keyring\",\n            \"--no-default-recipient\",\n            \"--no-emit-version\",\n            \"--no-options\",\n            \"--no-tty\",\n            \"--no-use-agent\",\n            \"--no-verbose\",\n            \"--print-mds\",\n            \"--quiet\",\n            \"--sign\",\n            \"--symmetric\",\n            \"--throw-keyids\",\n            \"--use-agent\",\n            \"--verbose\",\n            \"--version\",\n            \"--with-colons\",\n            \"--yes\",\n        ]\n    )\n    #: These options expect either None or a hex string\n    hex_or_none_options = hex_options.intersection(none_options)\n    allowed = hex_options.union(\n        unchecked_options,\n        other_options,\n        dir_options,\n        keyring_options,\n        file_or_none_options,\n        pref_options,\n        none_options,\n    )\n\n    if group and group in locals():\n        return locals()[group]",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_get_all_gnupg_options",
          "code": "def _get_all_gnupg_options():  # type: ignore[no-untyped-def]\n    \"\"\"Get all GnuPG options and flags.\n\n    This is hardcoded within a local scope to reduce the chance of a tampered\n    GnuPG binary reporting falsified option sets, i.e. because certain options\n    (namedly the ``--no-options`` option, which prevents the usage of gpg.conf\n    files) are necessary and statically specified in\n    :meth:`gnupg._meta.GPGBase._make_args`, if the inputs into Python are\n    already controlled, and we were to summon the GnuPG binary to ask it for\n    its options, it would be possible to receive a falsified options set\n    missing the ``--no-options`` option in response. This seems unlikely, and\n    the method is stupid and ugly, but at least we'll never have to debug\n    whether or not an option *actually* disappeared in a different GnuPG\n    version, or some funny business is happening.\n\n    These are the options as of GnuPG 1.4.12; the current stable branch of the\n    2.1.x tree contains a few more -- if you need them you'll have to add them\n    in here.\n\n    :type gnupg_options: frozenset\n    :ivar gnupg_options: All known GPG options and flags.\n    :rtype: frozenset\n    :returns: ``gnupg_options``\n    \"\"\"\n    three_hundred_eighteen = (\n        \"\"\"\n--allow-freeform-uid              --multifile\n--allow-multiple-messages         --no\n--allow-multisig-verification     --no-allow-freeform-uid\n--allow-non-selfsigned-uid        --no-allow-multiple-messages\n--allow-secret-key-import         --no-allow-non-selfsigned-uid\n--always-trust                    --no-armor\n--armor                           --no-armour\n--armour                          --no-ask-cert-expire\n--ask-cert-expire                 --no-ask-cert-level\n--ask-cert-level                  --no-ask-sig-expire\n--ask-sig-expire                  --no-auto-check-trustdb\n--attribute-fd                    --no-auto-key-locate\n--attribute-file                  --no-auto-key-retrieve\n--auto-check-trustdb              --no-batch\n--auto-key-locate                 --no-comments\n--auto-key-retrieve               --no-default-keyring\n--batch                           --no-default-recipient\n--bzip2-compress-level            --no-disable-mdc\n--bzip2-decompress-lowmem         --no-emit-version\n--card-edit                       --no-encrypt-to\n--card-status                     --no-escape-from-lines\n--cert-digest-algo                --no-expensive-trust-checks\n--cert-notation                   --no-expert\n--cert-policy-url                 --no-force-mdc\n--change-pin                      --no-force-v3-sigs\n--charset                         --no-force-v4-certs\n--check-sig                       --no-for-your-eyes-only\n--check-sigs                      --no-greeting\n--check-trustdb                   --no-groups\n--cipher-algo                     --no-literal\n--clearsign                       --no-mangle-dos-filenames\n--command-fd                      --no-mdc-warning\n--command-file                    --no-options\n--comment                         --no-permission-warning\n--completes-needed                --no-pgp2\n--compress-algo                   --no-pgp6\n--compression-algo                --no-pgp7\n--compress-keys                   --no-pgp8\n--compress-level                  --no-random-seed-file\n--compress-sigs                   --no-require-backsigs\n--ctapi-driver                    --no-require-cross-certification\n--dearmor                         --no-require-secmem\n--dearmour                        --no-rfc2440-text\n--debug                           --no-secmem-warning\n--debug-all                       --no-show-notation\n--debug-ccid-driver               --no-show-photos\n--debug-level                     --no-show-policy-url\n--decrypt                         --no-sig-cache\n--decrypt-files                   --no-sig-create-check\n--default-cert-check-level        --no-sk-comments\n--default-cert-expire             --no-strict\n--default-cert-level              --notation-data\n--default-comment                 --not-dash-escaped\n--default-key                     --no-textmode\n--default-keyserver-url           --no-throw-keyid\n--default-preference-list         --no-throw-keyids\n--default-recipient               --no-tty\n--default-recipient-self          --no-use-agent\n--default-sig-expire              --no-use-embedded-filename\n--delete-keys                     --no-utf8-strings\n--delete-secret-and-public-keys   --no-verbose\n--delete-secret-keys              --no-version\n--desig-revoke                    --openpgp\n--detach-sign                     --options\n--digest-algo                     --output\n--disable-ccid                    --override-session-key\n--disable-cipher-algo             --passphrase\n--disable-dsa2                    --passphrase-fd\n--disable-mdc                     --passphrase-file\n--disable-pubkey-algo             --passphrase-repeat\n--display                         --pcsc-driver\n--display-charset                 --personal-cipher-preferences\n--dry-run                         --personal-cipher-prefs\n--dump-options                    --personal-compress-preferences\n--edit-key                        --personal-compress-prefs\n--emit-version                    --personal-digest-preferences\n--enable-dsa2                     --personal-digest-prefs\n--enable-progress-filter          --pgp2\n--enable-special-filenames        --pgp6\n--enarmor                         --pgp7\n--enarmour                        --pgp8\n--encrypt                         --photo-viewer\n--encrypt-files                   --pipemode\n--encrypt-to                      --preserve-permissions\n--escape-from-lines               --primary-keyring\n--exec-path                       --print-md\n--exit-on-status-write-error      --print-mds\n--expert                          --quick-random\n--export                          --quiet\n--export-options                  --reader-port\n--export-ownertrust               --rebuild-keydb-caches\n--export-secret-keys              --recipient\n--export-secret-subkeys           --recv-keys\n--fast-import                     --refresh-keys\n--fast-list-mode                  --remote-user\n--fetch-keys                      --require-backsigs\n--fingerprint                     --require-cross-certification\n--fixed-list-mode                 --require-secmem\n--fix-trustdb                     --rfc1991\n--force-mdc                       --rfc2440\n--force-ownertrust                --rfc2440-text\n--force-v3-sigs                   --rfc4880\n--force-v4-certs                  --run-as-shm-coprocess\n--for-your-eyes-only              --s2k-cipher-algo\n--gen-key                         --s2k-count\n--gen-prime                       --s2k-digest-algo\n--gen-random                      --s2k-mode\n--gen-revoke                      --search-keys\n--gnupg                           --secret-keyring\n--gpg-agent-info                  --send-keys\n--gpgconf-list                    --set-filename\n--gpgconf-test                    --set-filesize\n--group                           --set-notation\n--help                            --set-policy-url\n--hidden-encrypt-to               --show-keyring\n--hidden-recipient                --show-notation\n--homedir                         --show-photos\n--honor-http-proxy                --show-policy-url\n--ignore-crc-error                --show-session-key\n--ignore-mdc-error                --sig-keyserver-url\n--ignore-time-conflict            --sign\n--ignore-valid-from               --sign-key\n--import                          --sig-notation\n--import-options                  --sign-with\n--import-ownertrust               --sig-policy-url\n--interactive                     --simple-sk-checksum\n--keyid-format                    --sk-comments\n--keyring                         --skip-verify\n--keyserver                       --status-fd\n--keyserver-options               --status-file\n--lc-ctype                        --store\n--lc-messages                     --strict\n--limit-card-insert-tries         --symmetric\n--list-config                     --temp-directory\n--list-key                        --textmode\n--list-keys                       --throw-keyid\n--list-only                       --throw-keyids\n--list-options                    --trustdb-name\n--list-ownertrust                 --trusted-key\n--list-packets                    --trust-model\n--list-public-keys                --try-all-secrets\n--list-secret-keys                --ttyname\n--list-sig                        --ttytype\n--list-sigs                       --ungroup\n--list-trustdb                    --update-trustdb\n--load-extension                  --use-agent\n--local-user                      --use-embedded-filename\n--lock-multiple                   --user\n--lock-never                      --utf8-strings\n--lock-once                       --verbose\n--logger-fd                       --verify\n--logger-file                     --verify-files\n--lsign-key                       --verify-options\n--mangle-dos-filenames            --version\n--marginals-needed                --warranty\n--max-cert-depth                  --with-colons\n--max-output                      --with-fingerprint\n--merge-only                      --with-key-data\n--min-cert-level                  --yes\n\"\"\"\n    ).split()\n\n    # These are extra options which only exist for GnuPG>=2.0.0\n    three_hundred_eighteen.append(\"--export-ownertrust\")\n    three_hundred_eighteen.append(\"--import-ownertrust\")\n\n    # These are extra options which only exist for GnuPG>=2.1.0\n    three_hundred_eighteen.append(\"--pinentry-mode\")\n    three_hundred_eighteen.append(\"--allow-loopback-pinentry\")\n\n    return frozenset(three_hundred_eighteen)",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "nodata",
          "code": "def nodata(status_code):  # type: ignore[no-untyped-def]\n    \"\"\"Translate NODATA status codes from GnuPG to messages.\"\"\"\n    lookup = {\n        \"1\": \"No armored data.\",\n        \"2\": \"Expected a packet but did not find one.\",\n        \"3\": \"Invalid packet found, this may indicate a non OpenPGP message.\",\n        \"4\": \"Signature expected but not found.\",\n    }\n    for key, value in lookup.items():\n        if str(status_code) == key:\n            return value",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "progress",
          "code": "def progress(status_code):  # type: ignore[no-untyped-def]\n    \"\"\"Translate PROGRESS status codes from GnuPG to messages.\"\"\"\n    lookup = {\n        \"pk_dsa\": \"DSA key generation\",\n        \"pk_elg\": \"Elgamal key generation\",\n        \"primegen\": \"Prime generation\",\n        \"need_entropy\": \"Waiting for new entropy in the RNG\",\n        \"tick\": \"Generic tick without any special meaning - still working.\",\n        \"starting_agent\": \"A gpg-agent was started.\",\n        \"learncard\": \"gpg-agent or gpgsm is learning the smartcard data.\",\n        \"card_busy\": \"A smartcard is still working.\",\n    }\n    for key, value in lookup.items():\n        if str(status_code) == key:\n            return value",
          "file": "_parsers.py"
        },
        {
          "type": "class",
          "name": "KeyExpirationInterface",
          "code": "class KeyExpirationInterface:\n    \"\"\"Interface that guards against misuse of --edit-key combined with --command-fd\"\"\"\n\n    def __init__(self, expiration_time, passphrase=None):  # type: ignore[no-untyped-def]\n        self._passphrase = passphrase\n        self._expiration_time = expiration_time\n        self._clean_key_expiration_option()\n\n    def _clean_key_expiration_option(self):  # type: ignore[no-untyped-def]\n        \"\"\"validates the expiration option supplied\"\"\"\n        allowed_entry = re.findall(r\"^(\\d+)(|w|m|y)$\", self._expiration_time)\n        if not allowed_entry:\n            raise UsageError(\"Key expiration option: %s is not valid\" % self._expiration_time)\n\n    def _input_passphrase(self, _input):  # type: ignore[no-untyped-def]\n        if self._passphrase:\n            return f\"{_input}{self._passphrase}\\n\"\n        return _input\n\n    def _main_key_command(self):  # type: ignore[no-untyped-def]\n        main_key_input = \"expire\\n%s\\n\" % self._expiration_time\n        return self._input_passphrase(main_key_input)\n\n    def _sub_key_command(self, sub_key_number):  # type: ignore[no-untyped-def]\n        sub_key_input = \"key %d\\nexpire\\n%s\\n\" % (sub_key_number, self._expiration_time)\n        return self._input_passphrase(sub_key_input)\n\n    def gpg_interactive_input(self, sub_keys_number):  # type: ignore[no-untyped-def]\n        \"\"\"processes series of inputs normally supplied on --edit-key but passed through stdin\n        this ensures that no other --edit-key command is actually passing through.\n        \"\"\"\n        deselect_sub_key = \"key 0\\n\"\n\n        _input = self._main_key_command()\n        for sub_key_number in range(1, sub_keys_number + 1):\n            _input += self._sub_key_command(sub_key_number) + deselect_sub_key\n        return \"%ssave\\n\" % _input",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, expiration_time, passphrase=None):  # type: ignore[no-untyped-def]\n        self._passphrase = passphrase\n        self._expiration_time = expiration_time\n        self._clean_key_expiration_option()",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_clean_key_expiration_option",
          "code": "def _clean_key_expiration_option(self):  # type: ignore[no-untyped-def]\n        \"\"\"validates the expiration option supplied\"\"\"\n        allowed_entry = re.findall(r\"^(\\d+)(|w|m|y)$\", self._expiration_time)\n        if not allowed_entry:\n            raise UsageError(\"Key expiration option: %s is not valid\" % self._expiration_time)",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_input_passphrase",
          "code": "def _input_passphrase(self, _input):  # type: ignore[no-untyped-def]\n        if self._passphrase:\n            return f\"{_input}{self._passphrase}\\n\"\n        return _input",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_main_key_command",
          "code": "def _main_key_command(self):  # type: ignore[no-untyped-def]\n        main_key_input = \"expire\\n%s\\n\" % self._expiration_time\n        return self._input_passphrase(main_key_input)",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_sub_key_command",
          "code": "def _sub_key_command(self, sub_key_number):  # type: ignore[no-untyped-def]\n        sub_key_input = \"key %d\\nexpire\\n%s\\n\" % (sub_key_number, self._expiration_time)\n        return self._input_passphrase(sub_key_input)",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "gpg_interactive_input",
          "code": "def gpg_interactive_input(self, sub_keys_number):  # type: ignore[no-untyped-def]\n        \"\"\"processes series of inputs normally supplied on --edit-key but passed through stdin\n        this ensures that no other --edit-key command is actually passing through.\n        \"\"\"\n        deselect_sub_key = \"key 0\\n\"\n\n        _input = self._main_key_command()\n        for sub_key_number in range(1, sub_keys_number + 1):\n            _input += self._sub_key_command(sub_key_number) + deselect_sub_key\n        return \"%ssave\\n\" % _input",
          "file": "_parsers.py"
        },
        {
          "type": "class",
          "name": "KeyExpirationResult",
          "code": "class KeyExpirationResult:\n    \"\"\"Handle status messages for key expiry\n    It does not really have a job, but just to conform to the API\n    \"\"\"\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        self._gpg = gpg\n        self.status = \"ok\"\n\n    def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in (\n            \"USERID_HINT\",\n            \"NEED_PASSPHRASE\",\n            \"GET_HIDDEN\",\n            \"SIGEXPIRED\",\n            \"KEYEXPIRED\",\n            \"GOOD_PASSPHRASE\",\n            \"GOT_IT\",\n            \"GET_LINE\",\n        ):\n            pass\n        elif key in (\"BAD_PASSPHRASE\", \"MISSING_PASSPHRASE\"):\n            self.status = key.replace(\"_\", \" \").lower()\n        else:\n            self.status = \"failed\"\n            raise ValueError(\"Unknown status message: %r\" % key)",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        self._gpg = gpg\n        self.status = \"ok\"",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_handle_status",
          "code": "def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in (\n            \"USERID_HINT\",\n            \"NEED_PASSPHRASE\",\n            \"GET_HIDDEN\",\n            \"SIGEXPIRED\",\n            \"KEYEXPIRED\",\n            \"GOOD_PASSPHRASE\",\n            \"GOT_IT\",\n            \"GET_LINE\",\n        ):\n            pass\n        elif key in (\"BAD_PASSPHRASE\", \"MISSING_PASSPHRASE\"):\n            self.status = key.replace(\"_\", \" \").lower()\n        else:\n            self.status = \"failed\"\n            raise ValueError(\"Unknown status message: %r\" % key)",
          "file": "_parsers.py"
        },
        {
          "type": "class",
          "name": "KeySigningResult",
          "code": "class KeySigningResult:\n    \"\"\"Handle status messages for key singing\"\"\"\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        self._gpg = gpg\n        self.status = \"ok\"\n\n    def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in (\n            \"USERID_HINT\",\n            \"NEED_PASSPHRASE\",\n            \"ALREADY_SIGNED\",\n            \"GOOD_PASSPHRASE\",\n            \"GOT_IT\",\n            \"GET_BOOL\",\n        ):\n            pass\n        elif key in (\"BAD_PASSPHRASE\", \"MISSING_PASSPHRASE\"):\n            self.status = \"{}: {}\".format(key.replace(\"_\", \" \").lower(), value)\n        else:\n            self.status = \"failed\"\n            raise ValueError(f\"Key signing, unknown status message: {key!r} ::{value}\")",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        self._gpg = gpg\n        self.status = \"ok\"",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_handle_status",
          "code": "def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in (\n            \"USERID_HINT\",\n            \"NEED_PASSPHRASE\",\n            \"ALREADY_SIGNED\",\n            \"GOOD_PASSPHRASE\",\n            \"GOT_IT\",\n            \"GET_BOOL\",\n        ):\n            pass\n        elif key in (\"BAD_PASSPHRASE\", \"MISSING_PASSPHRASE\"):\n            self.status = \"{}: {}\".format(key.replace(\"_\", \" \").lower(), value)\n        else:\n            self.status = \"failed\"\n            raise ValueError(f\"Key signing, unknown status message: {key!r} ::{value}\")",
          "file": "_parsers.py"
        },
        {
          "type": "class",
          "name": "GenKey",
          "code": "class GenKey:\n    \"\"\"Handle status messages for key generation.\n\n    Calling the ``__str__()`` method of this class will return the generated\n    key's fingerprint, or a status string explaining the results.\n    \"\"\"\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        self._gpg = gpg\n        # this should get changed to something more useful, like 'key_type'\n        #: 'P':= primary, 'S':= subkey, 'B':= both\n        self.type = None\n        self.fingerprint = None\n        #: This will store a string describing the result of this operation.\n        #: Current statuses are:\n        #:     * 'key not created'\n        #:     * 'key created'\n        self.status = \"\"\n        self.subkey_created = False\n        self.primary_created = False\n        #: This will store the key's public keyring filename, if\n        #: :meth:`~gnupg.GPG.gen_key_input` was called with\n        #: ``separate_keyring=True``.\n        self.keyring = None\n        #: This will store the key's secret keyring filename, if :\n        #: :meth:`~gnupg.GPG.gen_key_input` was called with\n        #: ``separate_keyring=True``.\n        self.secring = None\n\n    def __nonzero__(self):  # type: ignore[no-untyped-def]\n        return bool(self.fingerprint)\n\n    __bool__ = __nonzero__\n\n    def __str__(self):  # type: ignore[no-untyped-def]\n        if self.fingerprint:\n            return self.fingerprint\n        elif self.status is not None:\n            return self.status\n        else:\n            # FIXME: we should only be returning a str\n            return False  # noqa: PLE0307\n\n    def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in (\"GOOD_PASSPHRASE\"):\n            pass\n        elif key == \"KEY_CONSIDERED\":\n            self.status = key.replace(\"_\", \" \").lower()\n        elif key == \"KEY_NOT_CREATED\":\n            self.status = \"key not created\"\n        elif key == \"KEY_CREATED\":\n            (self.type, self.fingerprint) = value.split()\n            self.status = \"key created\"\n        elif key == \"NODATA\":\n            self.status = nodata(value)\n        elif key == \"PROGRESS\":\n            self.status = progress(value.split(\" \", 1)[0])\n        elif key == \"PINENTRY_LAUNCHED\":\n            log.warn(\n                \"GnuPG has just attempted to launch whichever pinentry \"\n                \"program you have configured, in order to obtain the \"\n                \"passphrase for this key.  If you did not use the \"\n                \"`passphrase=` parameter, please try doing so.  Otherwise, \"\n                \"see Issues #122 and #137:\"\n                \"\\nhttps://github.com/isislovecruft/python-gnupg/issues/122\"\n                \"\\nhttps://github.com/isislovecruft/python-gnupg/issues/137\"\n            )\n            self.status = \"key not created\"\n        elif key.startswith((\"TRUST_\", \"PKA_TRUST_\")) or key == \"NEWSIG\":\n            pass\n        else:\n            raise ValueError(\"Unknown status message: %r\" % key)\n\n        if self.type in (\"B\", \"P\"):\n            self.primary_created = True\n        if self.type in (\"B\", \"S\"):\n            self.subkey_created = True",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        self._gpg = gpg\n        # this should get changed to something more useful, like 'key_type'\n        #: 'P':= primary, 'S':= subkey, 'B':= both\n        self.type = None\n        self.fingerprint = None\n        #: This will store a string describing the result of this operation.\n        #: Current statuses are:\n        #:     * 'key not created'\n        #:     * 'key created'\n        self.status = \"\"\n        self.subkey_created = False\n        self.primary_created = False\n        #: This will store the key's public keyring filename, if\n        #: :meth:`~gnupg.GPG.gen_key_input` was called with\n        #: ``separate_keyring=True``.\n        self.keyring = None\n        #: This will store the key's secret keyring filename, if :\n        #: :meth:`~gnupg.GPG.gen_key_input` was called with\n        #: ``separate_keyring=True``.\n        self.secring = None",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__nonzero__",
          "code": "def __nonzero__(self):  # type: ignore[no-untyped-def]\n        return bool(self.fingerprint)",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__str__",
          "code": "def __str__(self):  # type: ignore[no-untyped-def]\n        if self.fingerprint:\n            return self.fingerprint\n        elif self.status is not None:\n            return self.status\n        else:\n            # FIXME: we should only be returning a str\n            return False  # noqa: PLE0307",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_handle_status",
          "code": "def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in (\"GOOD_PASSPHRASE\"):\n            pass\n        elif key == \"KEY_CONSIDERED\":\n            self.status = key.replace(\"_\", \" \").lower()\n        elif key == \"KEY_NOT_CREATED\":\n            self.status = \"key not created\"\n        elif key == \"KEY_CREATED\":\n            (self.type, self.fingerprint) = value.split()\n            self.status = \"key created\"\n        elif key == \"NODATA\":\n            self.status = nodata(value)\n        elif key == \"PROGRESS\":\n            self.status = progress(value.split(\" \", 1)[0])\n        elif key == \"PINENTRY_LAUNCHED\":\n            log.warn(\n                \"GnuPG has just attempted to launch whichever pinentry \"\n                \"program you have configured, in order to obtain the \"\n                \"passphrase for this key.  If you did not use the \"\n                \"`passphrase=` parameter, please try doing so.  Otherwise, \"\n                \"see Issues #122 and #137:\"\n                \"\\nhttps://github.com/isislovecruft/python-gnupg/issues/122\"\n                \"\\nhttps://github.com/isislovecruft/python-gnupg/issues/137\"\n            )\n            self.status = \"key not created\"\n        elif key.startswith((\"TRUST_\", \"PKA_TRUST_\")) or key == \"NEWSIG\":\n            pass\n        else:\n            raise ValueError(\"Unknown status message: %r\" % key)\n\n        if self.type in (\"B\", \"P\"):\n            self.primary_created = True\n        if self.type in (\"B\", \"S\"):\n            self.subkey_created = True",
          "file": "_parsers.py"
        },
        {
          "type": "class",
          "name": "DeleteResult",
          "code": "class DeleteResult:\n    \"\"\"Handle status messages for --delete-keys and --delete-secret-keys\"\"\"\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        self._gpg = gpg\n        self.status = \"ok\"\n\n    def __str__(self):  # type: ignore[no-untyped-def]\n        return self.status\n\n    problem_reason = {\n        \"1\": \"No such key\",\n        \"2\": \"Must delete secret key first\",\n        \"3\": \"Ambigious specification\",\n    }\n\n    def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"\n        Parse a status code from the attached GnuPG process.\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in (\"DELETE_PROBLEM\", \"KEY_CONSIDERED\"):\n            self.status = self.problem_reason.get(value, \"Unknown error: %r\" % value)\n        elif key in (\"PINENTRY_LAUNCHED\"):\n            self.status = key.replace(\"_\", \" \").lower()\n        else:\n            raise ValueError(\"Unknown status message: %r\" % key)",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        self._gpg = gpg\n        self.status = \"ok\"",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__str__",
          "code": "def __str__(self):  # type: ignore[no-untyped-def]\n        return self.status",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_handle_status",
          "code": "def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"\n        Parse a status code from the attached GnuPG process.\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in (\"DELETE_PROBLEM\", \"KEY_CONSIDERED\"):\n            self.status = self.problem_reason.get(value, \"Unknown error: %r\" % value)\n        elif key in (\"PINENTRY_LAUNCHED\"):\n            self.status = key.replace(\"_\", \" \").lower()\n        else:\n            raise ValueError(\"Unknown status message: %r\" % key)",
          "file": "_parsers.py"
        },
        {
          "type": "class",
          "name": "Sign",
          "code": "class Sign:\n    \"\"\"Parse GnuPG status messages for signing operations.\n\n    :param gpg: An instance of :class:`gnupg.GPG`.\n    \"\"\"\n\n    #: The type of signature created.\n    sig_type = None\n    #: The algorithm used to create the signature.\n    sig_algo = None\n    #: The hash algorithm used to create the signature.\n    sig_hash_also = None\n    #: The fingerprint of the signing keyid.\n    fingerprint = None\n    #: The timestamp on the signature.\n    timestamp = None\n    #: xxx fill me in\n    what = None\n    status = None\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        self._gpg = gpg\n\n    def __nonzero__(self):  # type: ignore[no-untyped-def]\n        \"\"\"Override the determination for truthfulness evaluation.\n\n        :rtype: bool\n        :returns: True if we have a valid signature, False otherwise.\n        \"\"\"\n        return self.fingerprint is not None\n\n    __bool__ = __nonzero__\n\n    def __str__(self):  # type: ignore[no-untyped-def]\n        return self.data.decode(self._gpg._encoding, self._gpg._decode_errors)\n\n    def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in (\n            \"USERID_HINT\",\n            \"NEED_PASSPHRASE\",\n            \"BAD_PASSPHRASE\",\n            \"GOOD_PASSPHRASE\",\n            \"MISSING_PASSPHRASE\",\n            \"PINENTRY_LAUNCHED\",\n            \"BEGIN_SIGNING\",\n            \"CARDCTRL\",\n            \"INV_SGNR\",\n            \"SIGEXPIRED\",\n            \"KEY_CONSIDERED\",\n        ):\n            self.status = key.replace(\"_\", \" \").lower()\n        elif key == \"SIG_CREATED\":\n            (\n                self.sig_type,\n                self.sig_algo,\n                self.sig_hash_algo,\n                self.what,\n                self.timestamp,\n                self.fingerprint,\n            ) = value.split()\n        elif key == \"KEYEXPIRED\":\n            self.status = \"skipped signing key, key expired\"\n            if (value is not None) and (len(value) > 0):\n                self.status += f\" on {str(value)}\"\n        elif key == \"KEYREVOKED\":\n            self.status = \"skipped signing key, key revoked\"\n            if (value is not None) and (len(value) > 0):\n                self.status += f\" on {str(value)}\"\n        elif key == \"NODATA\":\n            self.status = nodata(value)\n        elif key == \"PROGRESS\":\n            self.status = progress(value.split(\" \", 1)[0])\n        else:\n            raise ValueError(\"Unknown status message: %r\" % key)",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        self._gpg = gpg",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__nonzero__",
          "code": "def __nonzero__(self):  # type: ignore[no-untyped-def]\n        \"\"\"Override the determination for truthfulness evaluation.\n\n        :rtype: bool\n        :returns: True if we have a valid signature, False otherwise.\n        \"\"\"\n        return self.fingerprint is not None",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__str__",
          "code": "def __str__(self):  # type: ignore[no-untyped-def]\n        return self.data.decode(self._gpg._encoding, self._gpg._decode_errors)",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_handle_status",
          "code": "def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in (\n            \"USERID_HINT\",\n            \"NEED_PASSPHRASE\",\n            \"BAD_PASSPHRASE\",\n            \"GOOD_PASSPHRASE\",\n            \"MISSING_PASSPHRASE\",\n            \"PINENTRY_LAUNCHED\",\n            \"BEGIN_SIGNING\",\n            \"CARDCTRL\",\n            \"INV_SGNR\",\n            \"SIGEXPIRED\",\n            \"KEY_CONSIDERED\",\n        ):\n            self.status = key.replace(\"_\", \" \").lower()\n        elif key == \"SIG_CREATED\":\n            (\n                self.sig_type,\n                self.sig_algo,\n                self.sig_hash_algo,\n                self.what,\n                self.timestamp,\n                self.fingerprint,\n            ) = value.split()\n        elif key == \"KEYEXPIRED\":\n            self.status = \"skipped signing key, key expired\"\n            if (value is not None) and (len(value) > 0):\n                self.status += f\" on {str(value)}\"\n        elif key == \"KEYREVOKED\":\n            self.status = \"skipped signing key, key revoked\"\n            if (value is not None) and (len(value) > 0):\n                self.status += f\" on {str(value)}\"\n        elif key == \"NODATA\":\n            self.status = nodata(value)\n        elif key == \"PROGRESS\":\n            self.status = progress(value.split(\" \", 1)[0])\n        else:\n            raise ValueError(\"Unknown status message: %r\" % key)",
          "file": "_parsers.py"
        },
        {
          "type": "class",
          "name": "ListKeys",
          "code": "class ListKeys(list):\n    \"\"\"Handle status messages for --list-keys.\n\n    Handles pub and uid (relating the latter to the former).  Don't care about\n    the following attributes/status messages (from doc/DETAILS):\n\n    |  crt = X.509 certificate\n    |  crs = X.509 certificate and private key available\n    |  ssb = secret subkey (secondary key)\n    |  uat = user attribute (same as user id except for field 10).\n    |  pkd = public key data (special field format, see below)\n    |  grp = reserved for gpgsm\n    |  rvk = revocation key\n    \"\"\"\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        super().__init__()\n        self._gpg = gpg\n        self.curkey = None\n        self.curuid = None\n        self.fingerprints = []\n        self.uids = []\n        self.sigs = {}\n        self.certs = {}\n        self.revs = {}\n\n    def key(self, args):  # type: ignore[no-untyped-def]\n        vars = (\n            \"\"\"\n            type trust length algo keyid date expires dummy ownertrust uid\n        \"\"\"\n        ).split()\n        self.curkey = {}\n        for i in range(len(vars)):\n            self.curkey[vars[i]] = args[i]\n        self.curkey[\"uids\"] = []\n        self.curkey[\"sigs\"] = {}\n        self.curkey[\"rev\"] = {}\n        if self.curkey[\"uid\"]:\n            self.curuid = self.curkey[\"uid\"]\n            self.curkey[\"uids\"].append(self.curuid)\n            self.sigs[self.curuid] = set()\n            self.certs[self.curuid] = set()\n            self.revs[self.curuid] = set()\n            self.curkey[\"sigs\"][self.curuid] = []\n        del self.curkey[\"uid\"]\n        self.curkey[\"subkeys\"] = []\n        self.append(self.curkey)\n\n    pub = sec = key\n\n    def fpr(self, args):  # type: ignore[no-untyped-def]\n        self.curkey[\"fingerprint\"] = args[9]\n        self.fingerprints.append(args[9])\n\n    def uid(self, args):  # type: ignore[no-untyped-def]\n        uid = args[9]\n        uid = ESCAPE_PATTERN.sub(lambda m: chr(int(m.group(1), 16)), uid)\n        self.curkey[\"uids\"].append(uid)\n        self.curuid = uid\n        self.curkey[\"sigs\"][uid] = []\n        self.sigs[uid] = set()\n        self.certs[uid] = set()\n        self.uids.append(uid)\n\n    def sig(self, args):  # type: ignore[no-untyped-def]\n        vars = (\n            \"\"\"\n            type trust length algo keyid date expires dummy ownertrust uid\n        \"\"\"\n        ).split()\n        sig = {}\n        for i in range(len(vars)):\n            sig[vars[i]] = args[i]\n        self.curkey[\"sigs\"][self.curuid].append(sig)\n        self.sigs[self.curuid].add(sig[\"keyid\"])\n        if sig[\"trust\"] == \"!\":\n            self.certs[self.curuid].add(sig[\"keyid\"])\n\n    def sub(self, args):  # type: ignore[no-untyped-def]\n        subkey = [args[4], args[11]]\n        self.curkey[\"subkeys\"].append(subkey)\n\n    def rev(self, args):  # type: ignore[no-untyped-def]\n        self.curkey[\"rev\"] = {\"keyid\": args[4], \"revtime\": args[5], \"uid\": self.curuid}\n\n    def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        pass",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        super().__init__()\n        self._gpg = gpg\n        self.curkey = None\n        self.curuid = None\n        self.fingerprints = []\n        self.uids = []\n        self.sigs = {}\n        self.certs = {}\n        self.revs = {}",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "key",
          "code": "def key(self, args):  # type: ignore[no-untyped-def]\n        vars = (\n            \"\"\"\n            type trust length algo keyid date expires dummy ownertrust uid\n        \"\"\"\n        ).split()\n        self.curkey = {}\n        for i in range(len(vars)):\n            self.curkey[vars[i]] = args[i]\n        self.curkey[\"uids\"] = []\n        self.curkey[\"sigs\"] = {}\n        self.curkey[\"rev\"] = {}\n        if self.curkey[\"uid\"]:\n            self.curuid = self.curkey[\"uid\"]\n            self.curkey[\"uids\"].append(self.curuid)\n            self.sigs[self.curuid] = set()\n            self.certs[self.curuid] = set()\n            self.revs[self.curuid] = set()\n            self.curkey[\"sigs\"][self.curuid] = []\n        del self.curkey[\"uid\"]\n        self.curkey[\"subkeys\"] = []\n        self.append(self.curkey)",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "fpr",
          "code": "def fpr(self, args):  # type: ignore[no-untyped-def]\n        self.curkey[\"fingerprint\"] = args[9]\n        self.fingerprints.append(args[9])",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "uid",
          "code": "def uid(self, args):  # type: ignore[no-untyped-def]\n        uid = args[9]\n        uid = ESCAPE_PATTERN.sub(lambda m: chr(int(m.group(1), 16)), uid)\n        self.curkey[\"uids\"].append(uid)\n        self.curuid = uid\n        self.curkey[\"sigs\"][uid] = []\n        self.sigs[uid] = set()\n        self.certs[uid] = set()\n        self.uids.append(uid)",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "sig",
          "code": "def sig(self, args):  # type: ignore[no-untyped-def]\n        vars = (\n            \"\"\"\n            type trust length algo keyid date expires dummy ownertrust uid\n        \"\"\"\n        ).split()\n        sig = {}\n        for i in range(len(vars)):\n            sig[vars[i]] = args[i]\n        self.curkey[\"sigs\"][self.curuid].append(sig)\n        self.sigs[self.curuid].add(sig[\"keyid\"])\n        if sig[\"trust\"] == \"!\":\n            self.certs[self.curuid].add(sig[\"keyid\"])",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "sub",
          "code": "def sub(self, args):  # type: ignore[no-untyped-def]\n        subkey = [args[4], args[11]]\n        self.curkey[\"subkeys\"].append(subkey)",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "rev",
          "code": "def rev(self, args):  # type: ignore[no-untyped-def]\n        self.curkey[\"rev\"] = {\"keyid\": args[4], \"revtime\": args[5], \"uid\": self.curuid}",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_handle_status",
          "code": "def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        pass",
          "file": "_parsers.py"
        },
        {
          "type": "class",
          "name": "ImportResult",
          "code": "class ImportResult:\n    \"\"\"Parse GnuPG status messages for key import operations.\"\"\"\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        \"\"\"Start parsing the results of a key import operation.\n\n        :type gpg: :class:`gnupg.GPG`\n        :param gpg: An instance of :class:`gnupg.GPG`.\n        \"\"\"\n        self._gpg = gpg\n\n        #: A map from GnuPG codes shown with the ``IMPORT_OK`` status message\n        #: to their human-meaningful English equivalents.\n        self._ok_reason = {\n            \"0\": \"Not actually changed\",\n            \"1\": \"Entirely new key\",\n            \"2\": \"New user IDs\",\n            \"4\": \"New signatures\",\n            \"8\": \"New subkeys\",\n            \"16\": \"Contains private key\",\n            \"17\": \"Contains private key\",\n        }\n\n        #: A map from GnuPG codes shown with the ``IMPORT_PROBLEM`` status\n        #: message to their human-meaningful English equivalents.\n        self._problem_reason = {\n            \"0\": \"No specific reason given\",\n            \"1\": \"Invalid Certificate\",\n            \"2\": \"Issuer Certificate missing\",\n            \"3\": \"Certificate Chain too long\",\n            \"4\": \"Error storing certificate\",\n        }\n\n        #: All the possible status messages pertaining to actions taken while\n        #: importing a key.\n        self._fields = \"\"\"count no_user_id imported imported_rsa unchanged\n        n_uids n_subk n_sigs n_revoc sec_read sec_imported sec_dups\n        not_imported\"\"\".split()\n\n        #: Counts of all the status message results, :data:`_fields` which\n        #: have appeared.\n        self.counts = OrderedDict(zip(self._fields, [0 for x in range(len(self._fields))]))\n\n        #: A list of strings containing the fingerprints of the GnuPG keyIDs\n        #: imported.\n        self.fingerprints = list()\n\n        #: A list containing dictionaries with information gathered on keys\n        #: imported.\n        self.results = list()\n\n    def __nonzero__(self):  # type: ignore[no-untyped-def]\n        \"\"\"Override the determination for truthfulness evaluation.\n\n        :rtype: bool\n        :returns: True if we have imported some keys, False otherwise.\n        \"\"\"\n        if self.counts[\"not_imported\"] > 0:\n            return False\n        return len(self.fingerprints) != 0\n\n    __bool__ = __nonzero__\n\n    def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises ValueError: if the status message is unknown.\n        \"\"\"\n        if key == \"IMPORTED\":\n            # this duplicates info we already see in import_ok & import_problem\n            pass\n        elif key == \"PINENTRY_LAUNCHED\":\n            log.warn(\n                \"GnuPG has just attempted to launch whichever pinentry \"\n                \"program you have configured, in order to obtain the \"\n                \"passphrase for this key.  If you did not use the \"\n                \"`passphrase=` parameter, please try doing so.  Otherwise, \"\n                \"see Issues #122 and #137:\"\n                \"\\nhttps://github.com/isislovecruft/python-gnupg/issues/122\"\n                \"\\nhttps://github.com/isislovecruft/python-gnupg/issues/137\"\n            )\n        elif key == \"KEY_CONSIDERED\":\n            self.results.append(\n                {\n                    \"status\": key.replace(\"_\", \" \").lower(),\n                }\n            )\n        elif key == \"NODATA\":\n            self.results.append({\"fingerprint\": None, \"status\": \"No valid data found\"})\n        elif key == \"IMPORT_OK\":\n            reason, fingerprint = value.split()\n            reasons = []\n            for code, text in self._ok_reason.items():\n                if int(reason) == int(code):\n                    reasons.append(text)\n            reasontext = \"\\n\".join(reasons) + \"\\n\"\n            self.results.append({\"fingerprint\": fingerprint, \"status\": reasontext})\n            self.fingerprints.append(fingerprint)\n        elif key == \"IMPORT_PROBLEM\":\n            try:\n                reason, fingerprint = value.split()\n            except:  # noqa: E722\n                reason = value\n                fingerprint = \"<unknown>\"\n            self.results.append(\n                {\"fingerprint\": fingerprint, \"status\": self._problem_reason[reason]}\n            )\n        elif key == \"IMPORT_RES\":\n            import_res = value.split()\n            for x in self.counts:\n                self.counts[x] = int(import_res.pop(0))\n        elif key == \"KEYEXPIRED\":\n            res = {\"fingerprint\": None, \"status\": \"Key expired\"}\n            self.results.append(res)\n        # Accoring to docs/DETAILS L859, SIGEXPIRED is obsolete:\n        # \"Removed on 2011-02-04. This is deprecated in favor of KEYEXPIRED.\"\n        elif key == \"SIGEXPIRED\":\n            res = {\"fingerprint\": None, \"status\": \"Signature expired\"}\n            self.results.append(res)\n        else:\n            raise ValueError(\"Unknown status message: %r\" % key)\n\n    def summary(self):  # type: ignore[no-untyped-def]\n        l = []  # noqa: E741\n        l.append(\"%d imported\" % self.counts[\"imported\"])\n        if self.counts[\"not_imported\"]:\n            l.append(\"%d not imported\" % self.counts[\"not_imported\"])\n        return \", \".join(l)",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        \"\"\"Start parsing the results of a key import operation.\n\n        :type gpg: :class:`gnupg.GPG`\n        :param gpg: An instance of :class:`gnupg.GPG`.\n        \"\"\"\n        self._gpg = gpg\n\n        #: A map from GnuPG codes shown with the ``IMPORT_OK`` status message\n        #: to their human-meaningful English equivalents.\n        self._ok_reason = {\n            \"0\": \"Not actually changed\",\n            \"1\": \"Entirely new key\",\n            \"2\": \"New user IDs\",\n            \"4\": \"New signatures\",\n            \"8\": \"New subkeys\",\n            \"16\": \"Contains private key\",\n            \"17\": \"Contains private key\",\n        }\n\n        #: A map from GnuPG codes shown with the ``IMPORT_PROBLEM`` status\n        #: message to their human-meaningful English equivalents.\n        self._problem_reason = {\n            \"0\": \"No specific reason given\",\n            \"1\": \"Invalid Certificate\",\n            \"2\": \"Issuer Certificate missing\",\n            \"3\": \"Certificate Chain too long\",\n            \"4\": \"Error storing certificate\",\n        }\n\n        #: All the possible status messages pertaining to actions taken while\n        #: importing a key.\n        self._fields = \"\"\"count no_user_id imported imported_rsa unchanged\n        n_uids n_subk n_sigs n_revoc sec_read sec_imported sec_dups\n        not_imported\"\"\".split()\n\n        #: Counts of all the status message results, :data:`_fields` which\n        #: have appeared.\n        self.counts = OrderedDict(zip(self._fields, [0 for x in range(len(self._fields))]))\n\n        #: A list of strings containing the fingerprints of the GnuPG keyIDs\n        #: imported.\n        self.fingerprints = list()\n\n        #: A list containing dictionaries with information gathered on keys\n        #: imported.\n        self.results = list()",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__nonzero__",
          "code": "def __nonzero__(self):  # type: ignore[no-untyped-def]\n        \"\"\"Override the determination for truthfulness evaluation.\n\n        :rtype: bool\n        :returns: True if we have imported some keys, False otherwise.\n        \"\"\"\n        if self.counts[\"not_imported\"] > 0:\n            return False\n        return len(self.fingerprints) != 0",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_handle_status",
          "code": "def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises ValueError: if the status message is unknown.\n        \"\"\"\n        if key == \"IMPORTED\":\n            # this duplicates info we already see in import_ok & import_problem\n            pass\n        elif key == \"PINENTRY_LAUNCHED\":\n            log.warn(\n                \"GnuPG has just attempted to launch whichever pinentry \"\n                \"program you have configured, in order to obtain the \"\n                \"passphrase for this key.  If you did not use the \"\n                \"`passphrase=` parameter, please try doing so.  Otherwise, \"\n                \"see Issues #122 and #137:\"\n                \"\\nhttps://github.com/isislovecruft/python-gnupg/issues/122\"\n                \"\\nhttps://github.com/isislovecruft/python-gnupg/issues/137\"\n            )\n        elif key == \"KEY_CONSIDERED\":\n            self.results.append(\n                {\n                    \"status\": key.replace(\"_\", \" \").lower(),\n                }\n            )\n        elif key == \"NODATA\":\n            self.results.append({\"fingerprint\": None, \"status\": \"No valid data found\"})\n        elif key == \"IMPORT_OK\":\n            reason, fingerprint = value.split()\n            reasons = []\n            for code, text in self._ok_reason.items():\n                if int(reason) == int(code):\n                    reasons.append(text)\n            reasontext = \"\\n\".join(reasons) + \"\\n\"\n            self.results.append({\"fingerprint\": fingerprint, \"status\": reasontext})\n            self.fingerprints.append(fingerprint)\n        elif key == \"IMPORT_PROBLEM\":\n            try:\n                reason, fingerprint = value.split()\n            except:  # noqa: E722\n                reason = value\n                fingerprint = \"<unknown>\"\n            self.results.append(\n                {\"fingerprint\": fingerprint, \"status\": self._problem_reason[reason]}\n            )\n        elif key == \"IMPORT_RES\":\n            import_res = value.split()\n            for x in self.counts:\n                self.counts[x] = int(import_res.pop(0))\n        elif key == \"KEYEXPIRED\":\n            res = {\"fingerprint\": None, \"status\": \"Key expired\"}\n            self.results.append(res)\n        # Accoring to docs/DETAILS L859, SIGEXPIRED is obsolete:\n        # \"Removed on 2011-02-04. This is deprecated in favor of KEYEXPIRED.\"\n        elif key == \"SIGEXPIRED\":\n            res = {\"fingerprint\": None, \"status\": \"Signature expired\"}\n            self.results.append(res)\n        else:\n            raise ValueError(\"Unknown status message: %r\" % key)",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "summary",
          "code": "def summary(self):  # type: ignore[no-untyped-def]\n        l = []  # noqa: E741\n        l.append(\"%d imported\" % self.counts[\"imported\"])\n        if self.counts[\"not_imported\"]:\n            l.append(\"%d not imported\" % self.counts[\"not_imported\"])\n        return \", \".join(l)",
          "file": "_parsers.py"
        },
        {
          "type": "class",
          "name": "ExportResult",
          "code": "class ExportResult:\n    \"\"\"Parse GnuPG status messages for key export operations.\"\"\"\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        \"\"\"Start parsing the results of a key export operation.\n\n        :type gpg: :class:`gnupg.GPG`\n        :param gpg: An instance of :class:`gnupg.GPG`.\n        \"\"\"\n        self._gpg = gpg\n\n        #: All the possible status messages pertaining to actions taken while\n        #: exporting a key.\n        self._fields = \"count secret_count exported\".split()\n\n        #: Counts of all the status message results, :data:`_fields` which\n        #: have appeared.\n        self.counts = OrderedDict(zip(self._fields, [0 for x in range(len(self._fields))]))\n\n        #: A list of strings containing the fingerprints of the GnuPG keyIDs\n        #: exported.\n        self.fingerprints = list()\n\n    def __nonzero__(self):  # type: ignore[no-untyped-def]\n        \"\"\"Override the determination for truthfulness evaluation.\n\n        :rtype: bool\n        :returns: True if we have exported some keys, False otherwise.\n        \"\"\"\n        if self.counts[\"not_imported\"] > 0:\n            return False\n        return len(self.fingerprints) != 0\n\n    __bool__ = __nonzero__\n\n    def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises ValueError: if the status message is unknown.\n        \"\"\"\n        informational_keys = [\"KEY_CONSIDERED\", \"USERID_HINT\", \"INQUIRE_MAXLEN\"]\n        if key in (\"EXPORTED\",):\n            self.fingerprints.append(value)\n        elif key == \"EXPORT_RES\":\n            export_res = value.split()\n            for x in self.counts:\n                self.counts[x] += int(export_res.pop(0))\n        elif key in (\n            \"NEED_PASSPHRASE\",\n            \"BAD_PASSPHRASE\",\n            \"GOOD_PASSPHRASE\",\n            \"MISSING_PASSPHRASE\",\n        ):\n            self.status = key.replace(\"_\", \" \").lower()\n        elif key not in informational_keys:\n            raise ValueError(\"Unknown status message: %r\" % key)\n\n    def summary(self):  # type: ignore[no-untyped-def]\n        return \"%d exported\" % self.counts[\"exported\"]",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        \"\"\"Start parsing the results of a key export operation.\n\n        :type gpg: :class:`gnupg.GPG`\n        :param gpg: An instance of :class:`gnupg.GPG`.\n        \"\"\"\n        self._gpg = gpg\n\n        #: All the possible status messages pertaining to actions taken while\n        #: exporting a key.\n        self._fields = \"count secret_count exported\".split()\n\n        #: Counts of all the status message results, :data:`_fields` which\n        #: have appeared.\n        self.counts = OrderedDict(zip(self._fields, [0 for x in range(len(self._fields))]))\n\n        #: A list of strings containing the fingerprints of the GnuPG keyIDs\n        #: exported.\n        self.fingerprints = list()",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__nonzero__",
          "code": "def __nonzero__(self):  # type: ignore[no-untyped-def]\n        \"\"\"Override the determination for truthfulness evaluation.\n\n        :rtype: bool\n        :returns: True if we have exported some keys, False otherwise.\n        \"\"\"\n        if self.counts[\"not_imported\"] > 0:\n            return False\n        return len(self.fingerprints) != 0",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_handle_status",
          "code": "def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises ValueError: if the status message is unknown.\n        \"\"\"\n        informational_keys = [\"KEY_CONSIDERED\", \"USERID_HINT\", \"INQUIRE_MAXLEN\"]\n        if key in (\"EXPORTED\",):\n            self.fingerprints.append(value)\n        elif key == \"EXPORT_RES\":\n            export_res = value.split()\n            for x in self.counts:\n                self.counts[x] += int(export_res.pop(0))\n        elif key in (\n            \"NEED_PASSPHRASE\",\n            \"BAD_PASSPHRASE\",\n            \"GOOD_PASSPHRASE\",\n            \"MISSING_PASSPHRASE\",\n        ):\n            self.status = key.replace(\"_\", \" \").lower()\n        elif key not in informational_keys:\n            raise ValueError(\"Unknown status message: %r\" % key)",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "summary",
          "code": "def summary(self):  # type: ignore[no-untyped-def]\n        return \"%d exported\" % self.counts[\"exported\"]",
          "file": "_parsers.py"
        },
        {
          "type": "class",
          "name": "Verify",
          "code": "class Verify:\n    \"\"\"Parser for status messages from GnuPG for certifications and signature\n    verifications.\n\n    People often mix these up, or think that they are the same thing. While it\n    is true that certifications and signatures *are* the same cryptographic\n    operation -- and also true that both are the same as the decryption\n    operation -- a distinction is made for important reasons.\n\n    A certification:\n        * is made on a key,\n        * can help to validate or invalidate the key owner's identity,\n        * can assign trust levels to the key (or to uids and/or subkeys that\n          the key contains),\n        * and can be used in absense of in-person fingerprint checking to try\n          to build a path (through keys whose fingerprints have been checked)\n          to the key, so that the identity of the key's owner can be more\n          reliable without having to actually physically meet in person.\n\n    A signature:\n        * is created for a file or other piece of data,\n        * can help to prove that the data hasn't been altered,\n        * and can help to prove that the data was sent by the person(s) in\n          possession of the private key that created the signature, and for\n          parsing portions of status messages from decryption operations.\n\n    There are probably other things unique to each that have been\n    scatterbrainedly omitted due to the programmer sitting still and staring\n    at GnuPG debugging logs for too long without snacks, but that is the gist\n    of it.\n    \"\"\"\n\n    TRUST_UNDEFINED = 0\n    TRUST_NEVER = 1\n    TRUST_MARGINAL = 2\n    TRUST_FULLY = 3\n    TRUST_ULTIMATE = 4\n    DECRYPTION_COMPLIANCE_MODE = 23\n\n    TRUST_LEVELS = {\n        \"TRUST_UNDEFINED\": TRUST_UNDEFINED,\n        \"TRUST_NEVER\": TRUST_NEVER,\n        \"TRUST_MARGINAL\": TRUST_MARGINAL,\n        \"TRUST_FULLY\": TRUST_FULLY,\n        \"TRUST_ULTIMATE\": TRUST_ULTIMATE,\n        # To fix https://github.com/isislovecruft/python-gnupg/issues/250 with Focal gnupg\n        \"DECRYPTION_COMPLIANCE_MODE\": DECRYPTION_COMPLIANCE_MODE,\n    }\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        \"\"\"Create a parser for verification and certification commands.\n\n        :param gpg: An instance of :class:`gnupg.GPG`.\n        \"\"\"\n        self._gpg = gpg\n        #: True if the signature is valid, False otherwise.\n        self.valid = False\n        #: A string describing the status of the signature verification.\n        #: Can be one of ``signature bad``, ``signature good``,\n        #: ``signature valid``, ``signature error``, ``decryption failed``,\n        #: ``no public key``, ``key exp``, or ``key rev``.\n        self.status = None\n        #: The fingerprint of the signing keyid.\n        self.fingerprint = None\n        #: The fingerprint of the corresponding public key, which may be\n        #: different if the signature was created with a subkey.\n        self.pubkey_fingerprint = None\n        #: The keyid of the signing key.\n        self.key_id = None\n        #: The id of the signature itself.\n        self.signature_id = None\n        #: The creation date of the signing key.\n        self.creation_date = None\n        #: The timestamp of the purported signature, if we are unable to parse\n        #: and/or validate it.\n        self.timestamp = None\n        #: The timestamp for when the valid signature was created.\n        self.sig_timestamp = None\n        #: The userid of the signing key which was used to create the\n        #: signature.\n        self.username = None\n        #: When the signing key is due to expire.\n        self.expire_timestamp = None\n        #: An integer 0-4 describing the trust level of the signature.\n        self.trust_level = None\n        #: The string corresponding to the ``trust_level`` number.\n        self.trust_text = None\n        #: The subpackets. These are stored as a dictionary, in the following\n        #: form:\n        #:     Verify.subpackets = {'SUBPACKET_NUMBER': {'flags': FLAGS,\n        #:                                               'length': LENGTH,\n        #:                                               'data': DATA},\n        #:                          'ANOTHER_SUBPACKET_NUMBER': {...}}\n        self.subpackets = {}\n        #: The signature or key notations. These are also stored as a\n        #: dictionary, in the following form:\n        #:\n        #:     Verify.notations = {NOTATION_NAME: NOTATION_DATA}\n        #:\n        #: For example, the Bitcoin core developer, Peter Todd, encodes in\n        #: every signature the header of the latest block on the Bitcoin\n        #: blockchain (to prove that a GnuPG signature that Peter made was made\n        #: *after* a specific point in time). These look like:\n        #:\n        #: gpg: Signature notation: \\\n        #: blockhash@bitcoin.org=000000000000000006f793d4461ee3e756ff04cc62581c96a42ed67dc233da3a\n        #:\n        #: Which python-gnupg would store as:\n        #:\n        #:     Verify.notations['blockhash@bitcoin.org'] = \\\n        #:      '000000000000000006f793d4461ee3e756ff04cc62581c96a42ed67dc233da3a'\n        self.notations = {}\n\n        #: This will be a str or None. If not None, it is the last\n        #: ``NOTATION_NAME`` we stored in the ``notations`` dict. Because we're\n        #: not assured that a ``NOTATION_DATA`` status will arrive *immediately*\n        #: after its corresponding ``NOTATION_NAME``, we store the latest\n        #: ``NOTATION_NAME`` here until we get its corresponding\n        #: ``NOTATION_DATA``.\n        self._last_notation_name = None\n\n    def __nonzero__(self):  # type: ignore[no-untyped-def]\n        \"\"\"Override the determination for truthfulness evaluation.\n\n        :rtype: bool\n        :returns: True if we have a valid signature, False otherwise.\n        \"\"\"\n        return self.valid\n\n    __bool__ = __nonzero__\n\n    def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in self.TRUST_LEVELS:\n            self.trust_text = key\n            self.trust_level = self.TRUST_LEVELS[key]\n        elif key in (\n            \"RSA_OR_IDEA\",\n            \"NODATA\",\n            \"IMPORT_RES\",\n            \"PLAINTEXT\",\n            \"PLAINTEXT_LENGTH\",\n            \"POLICY_URL\",\n            \"DECRYPTION_INFO\",\n            \"DECRYPTION_KEY\",\n            \"DECRYPTION_OKAY\",\n            \"INV_SGNR\",\n            \"PROGRESS\",\n            \"PINENTRY_LAUNCHED\",\n            \"SUCCESS\",\n            \"UNEXPECTED\",\n            \"ENCRYPTION_COMPLIANCE_MODE\",\n            \"VERIFICATION_COMPLIANCE_MODE\",\n        ):\n            pass\n        elif key == \"KEY_CONSIDERED\":\n            self.status = \"\\n\".join([self.status, \"key considered\"])\n        elif key == \"NEWSIG\":\n            # Reset\n            self.status = None\n            self.valid = False\n            self.key_id, self.username = None, None\n        elif key == \"BADSIG\":\n            self.valid = False\n            self.status = \"signature bad\"\n            self.key_id, self.username = value.split(None, 1)\n        elif key == \"GOODSIG\":\n            self.valid = True\n            self.status = \"signature good\"\n            self.key_id, self.username = value.split(None, 1)\n        elif key == \"VALIDSIG\":\n            self.valid = True\n            (\n                self.fingerprint,\n                self.creation_date,\n                self.sig_timestamp,\n                self.expire_timestamp,\n            ) = value.split()[:4]\n            # may be different if signature is made with a subkey\n            self.pubkey_fingerprint = value.split()[-1]\n            self.status = \"signature valid\"\n        elif key == \"SIG_ID\":\n            (self.signature_id, self.creation_date, self.timestamp) = value.split()\n        elif key == \"ERRSIG\":\n            self.valid = False\n            (self.key_id, algo, hash_algo, cls, self.timestamp) = value.split()[:5]\n            self.status = \"signature error\"\n        elif key == \"DECRYPTION_FAILED\":\n            self.valid = False\n            self.key_id = value\n            self.status = \"decryption failed\"\n        elif key in (\"WARNING\", \"ERROR\", \"FAILURE\"):\n            if key in (\"ERROR\", \"FAILURE\"):\n                self.valid = False\n            # The status will hold a (rather indecipherable and bad\n            # design, imho) location (in the GnuPG C code), GnuPG\n            # error_code, e.g. \"151011327_EOF\", and (possibly, in the\n            # case of WARNING or ERROR) additional text.\n            # Have fun figuring out what it means.\n            self.status = value\n            log.warn(f\"{key} status emitted from gpg process: {value}\")\n        elif key == \"NO_PUBKEY\":\n            self.valid = False\n            self.key_id = value\n            self.status = \"no public key\"\n        # These are useless in Verify, since they are spit out for any\n        # pub/subkeys on the key, not just the one doing the signing.\n        # if we want to check for signatures make with expired key,\n        # the relevant flags are REVKEYSIG and KEYREVOKED.\n        elif key in (\"KEYEXPIRED\", \"SIGEXPIRED\"):\n            pass\n        # The signature has an expiration date which has already passed\n        # (EXPKEYSIG), or the signature has been revoked (REVKEYSIG):\n        elif key in (\"EXPKEYSIG\", \"REVKEYSIG\"):\n            self.valid = False\n            self.key_id = value.split()[0]\n            self.status = (f\"{key[:3]} {key[3:]}\").lower()\n        # This is super annoying, and bad design on the part of GnuPG, in my\n        # opinion.\n        #\n        # This flag can get triggered if a valid signature is made, and then\n        # later the key (or subkey) which created the signature is\n        # revoked. When this happens, GnuPG will output:\n        #\n        # REVKEYSIG 075BFD18B365D34C Test Expired Key <test@python-gnupg.git>\n        # VALIDSIG DAB69B05F591640B7F4DCBEA075BFD18B365D34C 2014-09-26 1411700539 0 4 0 1 2 00 \\\n        #   4BA800F77452A6C29447FF20F4AF76ACBBE22CE2\n        # KEYREVOKED\n        #\n        # Meaning that we have a timestamp for when the signature was created,\n        # and we know that the signature is valid, but since GnuPG gives us no\n        # timestamp for when the key was revoked... we have no ability to\n        # determine if the valid signature was made *before* the signing key\n        # was revoked or *after*. Meaning that if you are like me and you sign\n        # all your software releases and git commits, and you also practice\n        # good opsec by doing regular key rotations, your old signatures made\n        # by your expired/revoked keys (even though they were created when the\n        # key was still good) are considered bad because GnuPG is a\n        # braindamaged piece of shit.\n        #\n        # Software engineering, motherfuckers, DO YOU SPEAK IT?\n        #\n        # The signing key which created the signature has since been revoked\n        # (KEYREVOKED), and we're going to ignore it (but add something to the\n        # status message):\n        elif key in (\"KEYREVOKED\"):\n            self.status = \"\\n\".join([self.status, \"key revoked\"])\n        # SIG_SUBPACKET <type> <flags> <len> <data>\n        # This indicates that a signature subpacket was seen.  The format is\n        # the same as the \"spk\" record above.\n        #\n        # [...]\n        #\n        # SPK - Signature subpacket records\n        #\n        # - Field 2 :: Subpacket number as per RFC-4880 and later.\n        # - Field 3 :: Flags in hex.  Currently the only two bits assigned\n        #              are 1, to indicate that the subpacket came from the\n        #              hashed part of the signature, and 2, to indicate the\n        #              subpacket was marked critical.\n        # - Field 4 :: Length of the subpacket.  Note that this is the\n        #              length of the subpacket, and not the length of field\n        #              5 below.  Due to the need for %-encoding, the length\n        #              of field 5 may be up to 3x this value.\n        # - Field 5 :: The subpacket data.  Printable ASCII is shown as\n        #              ASCII, but other values are rendered as %XX where XX\n        #              is the hex value for the byte.\n        elif key in (\"SIG_SUBPACKET\"):\n            fields = value.split()\n            try:\n                subpacket_number = fields[0]\n                self.subpackets[subpacket_number] = {\"flags\": None, \"length\": None, \"data\": None}\n            except IndexError:\n                # We couldn't parse the subpacket type (an RFC4880\n                # identifier), so we shouldn't continue parsing.\n                pass\n            else:\n                # Pull as much data as we can parse out of the subpacket:\n                try:\n                    self.subpackets[subpacket_number][\"flags\"] = fields[1]\n                    self.subpackets[subpacket_number][\"length\"] = fields[2]\n                    self.subpackets[subpacket_number][\"data\"] = fields[3]\n                except IndexError:\n                    pass\n        # NOTATION_\n        # There are actually two related status codes to convey notation\n        # data:\n        #\n        # - NOTATION_NAME <name>\n        # - NOTATION_DATA <string>\n        #\n        # <name> and <string> are %XX escaped; the data may be split among\n        # several NOTATION_DATA lines.\n        elif key.startswith(\"NOTATION_\"):\n            if key.endswith(\"NAME\"):\n                self.notations[value] = \"\"\n                self._last_notation_name = value\n            elif key.endswith(\"DATA\"):\n                if self._last_notation_name is not None:\n                    # Append the NOTATION_DATA to any previous data we\n                    # received for that NOTATION_NAME:\n                    self.notations[self._last_notation_name] += value\n                else:\n                    pass\n        else:\n            raise ValueError(f\"Unknown status message: {key!r} {value!r}\")",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        \"\"\"Create a parser for verification and certification commands.\n\n        :param gpg: An instance of :class:`gnupg.GPG`.\n        \"\"\"\n        self._gpg = gpg\n        #: True if the signature is valid, False otherwise.\n        self.valid = False\n        #: A string describing the status of the signature verification.\n        #: Can be one of ``signature bad``, ``signature good``,\n        #: ``signature valid``, ``signature error``, ``decryption failed``,\n        #: ``no public key``, ``key exp``, or ``key rev``.\n        self.status = None\n        #: The fingerprint of the signing keyid.\n        self.fingerprint = None\n        #: The fingerprint of the corresponding public key, which may be\n        #: different if the signature was created with a subkey.\n        self.pubkey_fingerprint = None\n        #: The keyid of the signing key.\n        self.key_id = None\n        #: The id of the signature itself.\n        self.signature_id = None\n        #: The creation date of the signing key.\n        self.creation_date = None\n        #: The timestamp of the purported signature, if we are unable to parse\n        #: and/or validate it.\n        self.timestamp = None\n        #: The timestamp for when the valid signature was created.\n        self.sig_timestamp = None\n        #: The userid of the signing key which was used to create the\n        #: signature.\n        self.username = None\n        #: When the signing key is due to expire.\n        self.expire_timestamp = None\n        #: An integer 0-4 describing the trust level of the signature.\n        self.trust_level = None\n        #: The string corresponding to the ``trust_level`` number.\n        self.trust_text = None\n        #: The subpackets. These are stored as a dictionary, in the following\n        #: form:\n        #:     Verify.subpackets = {'SUBPACKET_NUMBER': {'flags': FLAGS,\n        #:                                               'length': LENGTH,\n        #:                                               'data': DATA},\n        #:                          'ANOTHER_SUBPACKET_NUMBER': {...}}\n        self.subpackets = {}\n        #: The signature or key notations. These are also stored as a\n        #: dictionary, in the following form:\n        #:\n        #:     Verify.notations = {NOTATION_NAME: NOTATION_DATA}\n        #:\n        #: For example, the Bitcoin core developer, Peter Todd, encodes in\n        #: every signature the header of the latest block on the Bitcoin\n        #: blockchain (to prove that a GnuPG signature that Peter made was made\n        #: *after* a specific point in time). These look like:\n        #:\n        #: gpg: Signature notation: \\\n        #: blockhash@bitcoin.org=000000000000000006f793d4461ee3e756ff04cc62581c96a42ed67dc233da3a\n        #:\n        #: Which python-gnupg would store as:\n        #:\n        #:     Verify.notations['blockhash@bitcoin.org'] = \\\n        #:      '000000000000000006f793d4461ee3e756ff04cc62581c96a42ed67dc233da3a'\n        self.notations = {}\n\n        #: This will be a str or None. If not None, it is the last\n        #: ``NOTATION_NAME`` we stored in the ``notations`` dict. Because we're\n        #: not assured that a ``NOTATION_DATA`` status will arrive *immediately*\n        #: after its corresponding ``NOTATION_NAME``, we store the latest\n        #: ``NOTATION_NAME`` here until we get its corresponding\n        #: ``NOTATION_DATA``.\n        self._last_notation_name = None",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__nonzero__",
          "code": "def __nonzero__(self):  # type: ignore[no-untyped-def]\n        \"\"\"Override the determination for truthfulness evaluation.\n\n        :rtype: bool\n        :returns: True if we have a valid signature, False otherwise.\n        \"\"\"\n        return self.valid",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_handle_status",
          "code": "def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in self.TRUST_LEVELS:\n            self.trust_text = key\n            self.trust_level = self.TRUST_LEVELS[key]\n        elif key in (\n            \"RSA_OR_IDEA\",\n            \"NODATA\",\n            \"IMPORT_RES\",\n            \"PLAINTEXT\",\n            \"PLAINTEXT_LENGTH\",\n            \"POLICY_URL\",\n            \"DECRYPTION_INFO\",\n            \"DECRYPTION_KEY\",\n            \"DECRYPTION_OKAY\",\n            \"INV_SGNR\",\n            \"PROGRESS\",\n            \"PINENTRY_LAUNCHED\",\n            \"SUCCESS\",\n            \"UNEXPECTED\",\n            \"ENCRYPTION_COMPLIANCE_MODE\",\n            \"VERIFICATION_COMPLIANCE_MODE\",\n        ):\n            pass\n        elif key == \"KEY_CONSIDERED\":\n            self.status = \"\\n\".join([self.status, \"key considered\"])\n        elif key == \"NEWSIG\":\n            # Reset\n            self.status = None\n            self.valid = False\n            self.key_id, self.username = None, None\n        elif key == \"BADSIG\":\n            self.valid = False\n            self.status = \"signature bad\"\n            self.key_id, self.username = value.split(None, 1)\n        elif key == \"GOODSIG\":\n            self.valid = True\n            self.status = \"signature good\"\n            self.key_id, self.username = value.split(None, 1)\n        elif key == \"VALIDSIG\":\n            self.valid = True\n            (\n                self.fingerprint,\n                self.creation_date,\n                self.sig_timestamp,\n                self.expire_timestamp,\n            ) = value.split()[:4]\n            # may be different if signature is made with a subkey\n            self.pubkey_fingerprint = value.split()[-1]\n            self.status = \"signature valid\"\n        elif key == \"SIG_ID\":\n            (self.signature_id, self.creation_date, self.timestamp) = value.split()\n        elif key == \"ERRSIG\":\n            self.valid = False\n            (self.key_id, algo, hash_algo, cls, self.timestamp) = value.split()[:5]\n            self.status = \"signature error\"\n        elif key == \"DECRYPTION_FAILED\":\n            self.valid = False\n            self.key_id = value\n            self.status = \"decryption failed\"\n        elif key in (\"WARNING\", \"ERROR\", \"FAILURE\"):\n            if key in (\"ERROR\", \"FAILURE\"):\n                self.valid = False\n            # The status will hold a (rather indecipherable and bad\n            # design, imho) location (in the GnuPG C code), GnuPG\n            # error_code, e.g. \"151011327_EOF\", and (possibly, in the\n            # case of WARNING or ERROR) additional text.\n            # Have fun figuring out what it means.\n            self.status = value\n            log.warn(f\"{key} status emitted from gpg process: {value}\")\n        elif key == \"NO_PUBKEY\":\n            self.valid = False\n            self.key_id = value\n            self.status = \"no public key\"\n        # These are useless in Verify, since they are spit out for any\n        # pub/subkeys on the key, not just the one doing the signing.\n        # if we want to check for signatures make with expired key,\n        # the relevant flags are REVKEYSIG and KEYREVOKED.\n        elif key in (\"KEYEXPIRED\", \"SIGEXPIRED\"):\n            pass\n        # The signature has an expiration date which has already passed\n        # (EXPKEYSIG), or the signature has been revoked (REVKEYSIG):\n        elif key in (\"EXPKEYSIG\", \"REVKEYSIG\"):\n            self.valid = False\n            self.key_id = value.split()[0]\n            self.status = (f\"{key[:3]} {key[3:]}\").lower()\n        # This is super annoying, and bad design on the part of GnuPG, in my\n        # opinion.\n        #\n        # This flag can get triggered if a valid signature is made, and then\n        # later the key (or subkey) which created the signature is\n        # revoked. When this happens, GnuPG will output:\n        #\n        # REVKEYSIG 075BFD18B365D34C Test Expired Key <test@python-gnupg.git>\n        # VALIDSIG DAB69B05F591640B7F4DCBEA075BFD18B365D34C 2014-09-26 1411700539 0 4 0 1 2 00 \\\n        #   4BA800F77452A6C29447FF20F4AF76ACBBE22CE2\n        # KEYREVOKED\n        #\n        # Meaning that we have a timestamp for when the signature was created,\n        # and we know that the signature is valid, but since GnuPG gives us no\n        # timestamp for when the key was revoked... we have no ability to\n        # determine if the valid signature was made *before* the signing key\n        # was revoked or *after*. Meaning that if you are like me and you sign\n        # all your software releases and git commits, and you also practice\n        # good opsec by doing regular key rotations, your old signatures made\n        # by your expired/revoked keys (even though they were created when the\n        # key was still good) are considered bad because GnuPG is a\n        # braindamaged piece of shit.\n        #\n        # Software engineering, motherfuckers, DO YOU SPEAK IT?\n        #\n        # The signing key which created the signature has since been revoked\n        # (KEYREVOKED), and we're going to ignore it (but add something to the\n        # status message):\n        elif key in (\"KEYREVOKED\"):\n            self.status = \"\\n\".join([self.status, \"key revoked\"])\n        # SIG_SUBPACKET <type> <flags> <len> <data>\n        # This indicates that a signature subpacket was seen.  The format is\n        # the same as the \"spk\" record above.\n        #\n        # [...]\n        #\n        # SPK - Signature subpacket records\n        #\n        # - Field 2 :: Subpacket number as per RFC-4880 and later.\n        # - Field 3 :: Flags in hex.  Currently the only two bits assigned\n        #              are 1, to indicate that the subpacket came from the\n        #              hashed part of the signature, and 2, to indicate the\n        #              subpacket was marked critical.\n        # - Field 4 :: Length of the subpacket.  Note that this is the\n        #              length of the subpacket, and not the length of field\n        #              5 below.  Due to the need for %-encoding, the length\n        #              of field 5 may be up to 3x this value.\n        # - Field 5 :: The subpacket data.  Printable ASCII is shown as\n        #              ASCII, but other values are rendered as %XX where XX\n        #              is the hex value for the byte.\n        elif key in (\"SIG_SUBPACKET\"):\n            fields = value.split()\n            try:\n                subpacket_number = fields[0]\n                self.subpackets[subpacket_number] = {\"flags\": None, \"length\": None, \"data\": None}\n            except IndexError:\n                # We couldn't parse the subpacket type (an RFC4880\n                # identifier), so we shouldn't continue parsing.\n                pass\n            else:\n                # Pull as much data as we can parse out of the subpacket:\n                try:\n                    self.subpackets[subpacket_number][\"flags\"] = fields[1]\n                    self.subpackets[subpacket_number][\"length\"] = fields[2]\n                    self.subpackets[subpacket_number][\"data\"] = fields[3]\n                except IndexError:\n                    pass\n        # NOTATION_\n        # There are actually two related status codes to convey notation\n        # data:\n        #\n        # - NOTATION_NAME <name>\n        # - NOTATION_DATA <string>\n        #\n        # <name> and <string> are %XX escaped; the data may be split among\n        # several NOTATION_DATA lines.\n        elif key.startswith(\"NOTATION_\"):\n            if key.endswith(\"NAME\"):\n                self.notations[value] = \"\"\n                self._last_notation_name = value\n            elif key.endswith(\"DATA\"):\n                if self._last_notation_name is not None:\n                    # Append the NOTATION_DATA to any previous data we\n                    # received for that NOTATION_NAME:\n                    self.notations[self._last_notation_name] += value\n                else:\n                    pass\n        else:\n            raise ValueError(f\"Unknown status message: {key!r} {value!r}\")",
          "file": "_parsers.py"
        },
        {
          "type": "class",
          "name": "Crypt",
          "code": "class Crypt(Verify):\n    \"\"\"Parser for internal status messages from GnuPG for ``--encrypt``,\n    ``--decrypt``, and ``--decrypt-files``.\n    \"\"\"\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        Verify.__init__(self, gpg)\n        self._gpg = gpg\n        #: A string containing the encrypted or decrypted data.\n        self.data = \"\"\n        #: True if the decryption/encryption process turned out okay.\n        self.ok = False\n        #: A string describing the current processing status, or error, if one\n        #: has occurred.\n        self.status = None\n        self.data_format = None\n        self.data_timestamp = None\n        self.data_filename = None\n\n    def __nonzero__(self):  # type: ignore[no-untyped-def]\n        return self.ok\n\n    __bool__ = __nonzero__\n\n    def __str__(self):  # type: ignore[no-untyped-def]\n        \"\"\"The str() method for a :class:`Crypt` object will automatically return the\n        decoded data string, which stores the encryped or decrypted data.\n\n        In other words, these two statements are equivalent:\n\n        >>> assert decrypted.data == str(decrypted)\n\n        \"\"\"\n        return self.data.decode(self._gpg._encoding, self._gpg._decode_errors)\n\n    def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in (\n            \"ENC_TO\",\n            \"USERID_HINT\",\n            \"GOODMDC\",\n            \"END_DECRYPTION\",\n            \"BEGIN_SIGNING\",\n            \"NO_SECKEY\",\n            \"ERROR\",\n            \"NODATA\",\n            \"CARDCTRL\",\n        ):\n            # in the case of ERROR, this is because a more specific error\n            # message will have come first\n            pass\n        elif key in (\n            \"NEED_PASSPHRASE\",\n            \"BAD_PASSPHRASE\",\n            \"GOOD_PASSPHRASE\",\n            \"MISSING_PASSPHRASE\",\n            \"DECRYPTION_FAILED\",\n            \"KEY_NOT_CREATED\",\n            \"KEY_CONSIDERED\",\n        ):\n            self.status = key.replace(\"_\", \" \").lower()\n        elif key == \"NEED_TRUSTDB\":\n            self._gpg._create_trustdb()\n        elif key == \"NEED_PASSPHRASE_SYM\":\n            self.status = \"need symmetric passphrase\"\n        elif key == \"BEGIN_DECRYPTION\":\n            self.status = \"decryption incomplete\"\n        elif key == \"BEGIN_ENCRYPTION\":\n            self.status = \"encryption incomplete\"\n        elif key == \"DECRYPTION_OKAY\":\n            self.status = \"decryption ok\"\n            self.ok = True\n        elif key == \"END_ENCRYPTION\":\n            self.status = \"encryption ok\"\n            self.ok = True\n        elif key == \"INV_RECP\":\n            self.status = \"invalid recipient\"\n        elif key == \"KEYEXPIRED\":\n            self.status = \"key expired\"\n        elif key == \"KEYREVOKED\":\n            self.status = \"key revoked\"\n        elif key == \"SIG_CREATED\":\n            self.status = \"sig created\"\n        elif key == \"SIGEXPIRED\":\n            self.status = \"sig expired\"\n        elif key == \"PLAINTEXT\":\n            fmt, dts = value.split(\" \", 1)\n            if dts.find(\" \") > 0:\n                self.data_timestamp, self.data_filename = dts.split(\" \", 1)\n            else:\n                self.data_timestamp = dts\n            # GnuPG gives us a hex byte for an ascii char corresponding to\n            # the data format of the resulting plaintext,\n            # i.e. '62'→'b':= binary data\n            self.data_format = chr(int(str(fmt), 16))\n        else:\n            super()._handle_status(key, value)",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        Verify.__init__(self, gpg)\n        self._gpg = gpg\n        #: A string containing the encrypted or decrypted data.\n        self.data = \"\"\n        #: True if the decryption/encryption process turned out okay.\n        self.ok = False\n        #: A string describing the current processing status, or error, if one\n        #: has occurred.\n        self.status = None\n        self.data_format = None\n        self.data_timestamp = None\n        self.data_filename = None",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__nonzero__",
          "code": "def __nonzero__(self):  # type: ignore[no-untyped-def]\n        return self.ok",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__str__",
          "code": "def __str__(self):  # type: ignore[no-untyped-def]\n        \"\"\"The str() method for a :class:`Crypt` object will automatically return the\n        decoded data string, which stores the encryped or decrypted data.\n\n        In other words, these two statements are equivalent:\n\n        >>> assert decrypted.data == str(decrypted)\n\n        \"\"\"\n        return self.data.decode(self._gpg._encoding, self._gpg._decode_errors)",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_handle_status",
          "code": "def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in (\n            \"ENC_TO\",\n            \"USERID_HINT\",\n            \"GOODMDC\",\n            \"END_DECRYPTION\",\n            \"BEGIN_SIGNING\",\n            \"NO_SECKEY\",\n            \"ERROR\",\n            \"NODATA\",\n            \"CARDCTRL\",\n        ):\n            # in the case of ERROR, this is because a more specific error\n            # message will have come first\n            pass\n        elif key in (\n            \"NEED_PASSPHRASE\",\n            \"BAD_PASSPHRASE\",\n            \"GOOD_PASSPHRASE\",\n            \"MISSING_PASSPHRASE\",\n            \"DECRYPTION_FAILED\",\n            \"KEY_NOT_CREATED\",\n            \"KEY_CONSIDERED\",\n        ):\n            self.status = key.replace(\"_\", \" \").lower()\n        elif key == \"NEED_TRUSTDB\":\n            self._gpg._create_trustdb()\n        elif key == \"NEED_PASSPHRASE_SYM\":\n            self.status = \"need symmetric passphrase\"\n        elif key == \"BEGIN_DECRYPTION\":\n            self.status = \"decryption incomplete\"\n        elif key == \"BEGIN_ENCRYPTION\":\n            self.status = \"encryption incomplete\"\n        elif key == \"DECRYPTION_OKAY\":\n            self.status = \"decryption ok\"\n            self.ok = True\n        elif key == \"END_ENCRYPTION\":\n            self.status = \"encryption ok\"\n            self.ok = True\n        elif key == \"INV_RECP\":\n            self.status = \"invalid recipient\"\n        elif key == \"KEYEXPIRED\":\n            self.status = \"key expired\"\n        elif key == \"KEYREVOKED\":\n            self.status = \"key revoked\"\n        elif key == \"SIG_CREATED\":\n            self.status = \"sig created\"\n        elif key == \"SIGEXPIRED\":\n            self.status = \"sig expired\"\n        elif key == \"PLAINTEXT\":\n            fmt, dts = value.split(\" \", 1)\n            if dts.find(\" \") > 0:\n                self.data_timestamp, self.data_filename = dts.split(\" \", 1)\n            else:\n                self.data_timestamp = dts\n            # GnuPG gives us a hex byte for an ascii char corresponding to\n            # the data format of the resulting plaintext,\n            # i.e. '62'→'b':= binary data\n            self.data_format = chr(int(str(fmt), 16))\n        else:\n            super()._handle_status(key, value)",
          "file": "_parsers.py"
        },
        {
          "type": "class",
          "name": "ListPackets",
          "code": "class ListPackets:\n    \"\"\"Handle status messages for --list-packets.\"\"\"\n\n    def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        self._gpg = gpg\n        #: A string describing the current processing status, or error, if one\n        #: has occurred.\n        self.status = None\n        #: True if the passphrase to a public/private keypair is required.\n        self.need_passphrase = None\n        #: True if a passphrase for a symmetric key is required.\n        self.need_passphrase_sym = None\n        #: The keyid and uid which this data is encrypted to.\n        self.userid_hint = None\n        #: The first key that we detected that a message was encrypted\n        #: to. This is provided for backwards compatibility. As of Issue #77_,\n        #: the ``encrypted_to`` attribute should be used instead.\n        self.key = None\n        #: A list of keyid's that the message has been encrypted to.\n        self.encrypted_to = []\n\n    def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in (\n            \"NO_SECKEY\",\n            \"BEGIN_DECRYPTION\",\n            \"DECRYPTION_FAILED\",\n            \"END_DECRYPTION\",\n            \"GOOD_PASSPHRASE\",\n            \"BAD_PASSPHRASE\",\n            \"KEY_CONSIDERED\",\n        ):\n            pass\n        elif key == \"NODATA\":\n            self.status = nodata(value)\n        elif key == \"ENC_TO\":\n            key, _, _ = value.split()\n            if not self.key:\n                self.key = key\n            self.encrypted_to.append(key)\n        elif key in (\"NEED_PASSPHRASE\", \"MISSING_PASSPHRASE\"):\n            self.need_passphrase = True\n        elif key == \"NEED_PASSPHRASE_SYM\":\n            self.need_passphrase_sym = True\n        elif key == \"USERID_HINT\":\n            self.userid_hint = value.strip().split()\n        else:\n            raise ValueError(\"Unknown status message: %r\" % key)",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, gpg):  # type: ignore[no-untyped-def]\n        self._gpg = gpg\n        #: A string describing the current processing status, or error, if one\n        #: has occurred.\n        self.status = None\n        #: True if the passphrase to a public/private keypair is required.\n        self.need_passphrase = None\n        #: True if a passphrase for a symmetric key is required.\n        self.need_passphrase_sym = None\n        #: The keyid and uid which this data is encrypted to.\n        self.userid_hint = None\n        #: The first key that we detected that a message was encrypted\n        #: to. This is provided for backwards compatibility. As of Issue #77_,\n        #: the ``encrypted_to`` attribute should be used instead.\n        self.key = None\n        #: A list of keyid's that the message has been encrypted to.\n        self.encrypted_to = []",
          "file": "_parsers.py"
        },
        {
          "type": "function",
          "name": "_handle_status",
          "code": "def _handle_status(self, key, value):  # type: ignore[no-untyped-def]\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in (\n            \"NO_SECKEY\",\n            \"BEGIN_DECRYPTION\",\n            \"DECRYPTION_FAILED\",\n            \"END_DECRYPTION\",\n            \"GOOD_PASSPHRASE\",\n            \"BAD_PASSPHRASE\",\n            \"KEY_CONSIDERED\",\n        ):\n            pass\n        elif key == \"NODATA\":\n            self.status = nodata(value)\n        elif key == \"ENC_TO\":\n            key, _, _ = value.split()\n            if not self.key:\n                self.key = key\n            self.encrypted_to.append(key)\n        elif key in (\"NEED_PASSPHRASE\", \"MISSING_PASSPHRASE\"):\n            self.need_passphrase = True\n        elif key == \"NEED_PASSPHRASE_SYM\":\n            self.need_passphrase_sym = True\n        elif key == \"USERID_HINT\":\n            self.userid_hint = value.strip().split()\n        else:\n            raise ValueError(\"Unknown status message: %r\" % key)",
          "file": "_parsers.py"
        }
      ],
      "_logger.py": [
        {
          "type": "function",
          "name": "status",
          "code": "def status(self, message, *args, **kwargs):  # type: ignore[no-untyped-def]\n    \"\"\"LogRecord for GnuPG internal status messages.\"\"\"\n    if self.isEnabledFor(GNUPG_STATUS_LEVEL):\n        self._log(GNUPG_STATUS_LEVEL, message, args, **kwargs)",
          "file": "_logger.py"
        },
        {
          "type": "function",
          "name": "create_logger",
          "code": "def create_logger(level=logging.NOTSET):  # type: ignore[no-untyped-def]\n    \"\"\"Create a logger for python-gnupg at a specific message level.\n\n    :type level: :obj:`int` or :obj:`str`\n    :param level: A string or an integer for the lowest level to include in\n                  logs.\n\n    **Available levels:**\n\n    ==== ======== ========================================\n    int   str     description\n    ==== ======== ========================================\n    0    NOTSET   Disable all logging.\n    9    GNUPG    Log GnuPG's internal status messages.\n    10   DEBUG    Log module level debuging messages.\n    20   INFO     Normal user-level messages.\n    30   WARN     Warning messages.\n    40   ERROR    Error messages and tracebacks.\n    50   CRITICAL Unhandled exceptions and tracebacks.\n    ==== ======== ========================================\n    \"\"\"\n    # Add the GNUPG_STATUS_LEVEL LogRecord to all Loggers in the module:\n    logging.addLevelName(GNUPG_STATUS_LEVEL, \"GNUPG\")\n    logging.Logger.status = status\n\n    handler = NullHandler()\n\n    log = logging.getLogger(\"gnupg\")\n    log.addHandler(handler)\n    log.setLevel(level)\n    log.info(\"Log opened: %s UTC\" % datetime.ctime(datetime.utcnow()))\n    return log",
          "file": "_logger.py"
        }
      ],
      "_util.py": [
        {
          "type": "class",
          "name": "GnuPGVersionError",
          "code": "class GnuPGVersionError(ValueError):\n    \"\"\"Raised when we couldn't parse GnuPG's version info.\"\"\"",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "_copy_data",
          "code": "def _copy_data(instream, outstream):  # type: ignore[no-untyped-def]\n    \"\"\"Copy data from one stream to another.\n\n    :type instream: :class:`io.BytesIO` or :class:`io.StringIO` or file\n    :param instream: A byte stream or open file to read from.\n    :param file outstream: The file descriptor of a tmpfile to write to.\n    \"\"\"\n    sent = 0\n\n    while True:\n        if isinstance(instream, str):\n            data = instream[:1024]\n            instream = instream[1024:]\n        else:\n            data = instream.read(1024)\n        if len(data) == 0:\n            break\n\n        sent += len(data)\n        if isinstance(data, str):\n            encoded = data.encode()\n        else:\n            encoded = data\n        log.debug(\"Sending %d bytes of data...\" % sent)\n        log.debug(f\"Encoded data (type {type(encoded)}):\\n{encoded}\")\n\n        try:\n            outstream.write(bytes(encoded))\n        except TypeError as te:\n            # XXX FIXME This appears to happen because\n            # _threaded_copy_data() sometimes passes the `outstream` as an\n            # object with type <_io.BufferredWriter> and at other times\n            # with type <encodings.utf_8.StreamWriter>.  We hit the\n            # following error when the `outstream` has type\n            # <encodings.utf_8.StreamWriter>.\n            if \"convert 'bytes' object to str implicitly\" not in str(te):\n                log.error(str(te))\n            try:\n                outstream.write(encoded.decode())\n            except TypeError as yate:\n                # We hit the \"'str' does not support the buffer interface\"\n                # error in Python3 when the `outstream` is an io.BytesIO and\n                # we try to write a str to it.  We don't care about that\n                # error, we'll just try again with bytes.\n                if \"does not support the buffer interface\" not in str(yate):\n                    log.error(str(yate))\n            except OSError as ioe:\n                # Can get 'broken pipe' errors even when all data was sent\n                if \"Broken pipe\" in str(ioe):\n                    log.error(\"Error sending data: Broken pipe\")\n                else:\n                    log.exception(ioe)\n                break\n            else:\n                log.debug(\"Wrote data type <class 'str'> outstream.\")\n        except OSError as ioe:\n            # Can get 'broken pipe' errors even when all data was sent\n            if \"Broken pipe\" in str(ioe):\n                log.error(\"Error sending data: Broken pipe\")\n            else:\n                log.exception(ioe)\n            break\n        else:\n            log.debug(\"Wrote data type <class 'bytes'> to outstream.\")\n\n    try:\n        outstream.close()\n    except OSError as ioe:\n        log.error(f\"Unable to close outstream {outstream}:\\r\\t{ioe}\")\n    else:\n        log.debug(\"Closed outstream: %d bytes sent.\" % sent)",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "_create_if_necessary",
          "code": "def _create_if_necessary(directory):  # type: ignore[no-untyped-def]\n    \"\"\"Create the specified directory, if necessary.\n\n    :param str directory: The directory to use.\n    :rtype: bool\n    :returns: True if no errors occurred and the directory was created or\n              existed beforehand, False otherwise.\n    \"\"\"\n\n    if not os.path.isabs(directory):\n        log.debug(\"Got non-absolute path: %s\" % directory)\n        directory = os.path.abspath(directory)\n\n    if not os.path.isdir(directory):\n        log.info(\"Creating directory: %s\" % directory)\n        try:\n            os.makedirs(directory, 0x1C0)\n        except OSError as ose:\n            log.error(ose, exc_info=1)\n            return False\n        else:\n            log.debug(\"Created directory.\")\n    return True",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "create_uid_email",
          "code": "def create_uid_email(username=None, hostname=None):  # type: ignore[no-untyped-def]\n    \"\"\"Create an email address suitable for a UID on a GnuPG key.\n\n    :param str username: The username portion of an email address.  If None,\n                         defaults to the username of the running Python\n                         process.\n\n    :param str hostname: The FQDN portion of an email address. If None, the\n                         hostname is obtained from gethostname(2).\n\n    :rtype: str\n    :returns: A string formatted as <username>@<hostname>.\n    \"\"\"\n    if hostname:\n        hostname = hostname.replace(\" \", \"_\")\n    if not username:\n        try:\n            username = os.environ[\"LOGNAME\"]\n        except KeyError:\n            username = os.environ[\"USERNAME\"]\n\n        if not hostname:\n            hostname = gethostname()\n\n        uid = \"{}@{}\".format(username.replace(\" \", \"_\"), hostname)\n    else:\n        username = username.replace(\" \", \"_\")\n        if (not hostname) and (username.find(\"@\") == 0):\n            uid = f\"{username}@{gethostname()}\"\n        elif hostname:\n            uid = f\"{username}@{hostname}\"\n        else:\n            uid = username\n\n    return uid",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "_deprefix",
          "code": "def _deprefix(line, prefix, callback=None):  # type: ignore[no-untyped-def]\n    \"\"\"Remove the prefix string from the beginning of line, if it exists.\n\n    :param string line: A line, such as one output by GnuPG's status-fd.\n    :param string prefix: A substring to remove from the beginning of\n        ``line``. Case insensitive.\n    :type callback: callable\n    :param callback: Function to call if the prefix is found. The signature to\n        callback will be only one argument, the ``line`` without the ``prefix``, i.e.\n        ``callback(line)``.\n    :rtype: string\n    :returns: If the prefix was found, the ``line`` without the prefix is\n        returned. Otherwise, the original ``line`` is returned.\n    \"\"\"\n    try:\n        assert line.upper().startswith(\"\".join(prefix).upper())\n    except AssertionError:\n        log.debug(f\"Line doesn't start with prefix '{prefix}':\\n{line}\")\n        return line\n    else:\n        newline = line[len(prefix) :]\n        if callback is not None:\n            try:\n                callback(newline)\n            except Exception as exc:\n                log.exception(exc)\n        return newline",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "_find_binary",
          "code": "def _find_binary(binary=None):  # type: ignore[no-untyped-def]\n    \"\"\"Find the absolute path to the GnuPG binary.\n\n    Also run checks that the binary is not a symlink, and check that\n    our process real uid has exec permissions.\n\n    :param str binary: The path to the GnuPG binary.\n    :raises: :exc:`~exceptions.RuntimeError` if it appears that GnuPG is not\n             installed.\n    :rtype: str\n    :returns: The absolute path to the GnuPG binary to use, if no exceptions\n              occur.\n    \"\"\"\n    found = None\n    if binary is not None:\n        if os.path.isabs(binary) and os.path.isfile(binary):\n            return binary\n        if not os.path.isabs(binary):\n            try:\n                found = _which(binary)\n                log.debug(\"Found potential binary paths: %s\" % \"\\n\".join([path for path in found]))\n                found = found[0]\n            except IndexError:\n                log.info(\"Could not determine absolute path of binary: '%s'\" % binary)\n        elif os.access(binary, os.X_OK):\n            found = binary\n    if found is None:\n        try:\n            found = _which(\"gpg\", abspath_only=True, disallow_symlinks=True)[0]\n        except IndexError:\n            log.error(\"Could not find binary for 'gpg'.\")\n            try:\n                found = _which(\"gpg2\")[0]\n            except IndexError:\n                log.error(\"Could not find binary for 'gpg2'.\")\n    if found is None:\n        raise RuntimeError(\"GnuPG is not installed!\")\n\n    return found",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "_has_readwrite",
          "code": "def _has_readwrite(path):  # type: ignore[no-untyped-def]\n    \"\"\"\n    Determine if the real uid/gid of the executing user has read and write\n    permissions for a directory or a file.\n\n    :param str path: The path to the directory or file to check permissions\n                     for.\n    :rtype: bool\n    :returns: True if real uid/gid has read+write permissions, False otherwise.\n    \"\"\"\n    return os.access(path, os.R_OK | os.W_OK)",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "_is_file",
          "code": "def _is_file(filename):  # type: ignore[no-untyped-def]\n    \"\"\"Check that the size of the thing which is supposed to be a filename has\n    size greater than zero, without following symbolic links or using\n    :func:os.path.isfile.\n\n    :param filename: An object to check.\n    :rtype: bool\n    :returns: True if **filename** is file-like, False otherwise.\n    \"\"\"\n    try:\n        statinfo = os.lstat(filename)\n        log.debug(\n            f\"lstat({repr(filename)!r}) with type={type(filename)} gave us {repr(statinfo)!r}\"\n        )\n        if not (statinfo.st_size > 0):\n            raise ValueError(\"'%s' appears to be an empty file!\" % filename)\n    except OSError as oserr:\n        log.error(oserr)\n        if filename == \"-\":\n            log.debug(\"Got '-' for filename, assuming sys.stdin...\")\n            return True\n    except (ValueError, TypeError) as err:\n        log.error(err)\n    else:\n        return True\n    return False",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "_is_stream",
          "code": "def _is_stream(input):  # type: ignore[no-untyped-def]\n    \"\"\"Check that the input is a byte stream.\n\n    :param input: An object provided for reading from or writing to.\n    :rtype: bool\n    :returns: True if :param:input is a stream, False if otherwise.\n    \"\"\"\n    return isinstance(input, tuple(_STREAMLIKE_TYPES))",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "_is_list_or_tuple",
          "code": "def _is_list_or_tuple(instance):  # type: ignore[no-untyped-def]\n    \"\"\"Check that ``instance`` is a list or tuple.\n\n    :param instance: The object to type check.\n    :rtype: bool\n    :returns: True if ``instance`` is a list or tuple, False otherwise.\n    \"\"\"\n    return isinstance(\n        instance,\n        (\n            list,\n            tuple,\n        ),\n    )",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "_make_binary_stream",
          "code": "def _make_binary_stream(thing, encoding=None, armor=True):  # type: ignore[no-untyped-def]\n    \"\"\"Encode **thing**, then make it stream/file-like.\n\n    :param thing: The thing to turn into a encoded stream.\n    :rtype: ``io.BytesIO`` or ``io.StringIO``.\n    :returns: The encoded **thing**, wrapped in an ``io.BytesIO`` (if\n        available), otherwise wrapped in a ``io.StringIO``.\n    \"\"\"\n    if isinstance(thing, str):\n        thing = thing.encode(encoding)\n\n    return BytesIO(thing)",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "_next_year",
          "code": "def _next_year():  # type: ignore[no-untyped-def]\n    \"\"\"Get the date of today plus one year.\n\n    :rtype: str\n    :returns: The date of this day next year, in the format '%Y-%m-%d'.\n    \"\"\"\n    now = datetime.now().__str__()\n    date = now.split(\" \", 1)[0]\n    year, month, day = date.split(\"-\", 2)\n    next_year = str(int(year) + 1)\n    return \"-\".join((next_year, month, day))",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "_now",
          "code": "def _now():  # type: ignore[no-untyped-def]\n    \"\"\"Get a timestamp for right now, formatted according to ISO 8601.\"\"\"\n    return datetime.isoformat(datetime.now())",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "_separate_keyword",
          "code": "def _separate_keyword(line):  # type: ignore[no-untyped-def]\n    \"\"\"Split the line, and return (first_word, the_rest).\"\"\"\n    try:\n        first, rest = line.split(None, 1)\n    except ValueError:\n        first = line.strip()\n        rest = \"\"\n    return first, rest",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "_threaded_copy_data",
          "code": "def _threaded_copy_data(instream, outstream):  # type: ignore[no-untyped-def]\n    \"\"\"Copy data from one stream to another in a separate thread.\n\n    Wraps ``_copy_data()`` in a :class:`threading.Thread`.\n\n    :type instream: :class:`io.BytesIO` or :class:`io.StringIO`\n    :param instream: A byte stream to read from.\n    :param file outstream: The file descriptor of a tmpfile to write to.\n    \"\"\"\n    copy_thread = threading.Thread(target=_copy_data, args=(instream, outstream))\n    copy_thread.setDaemon(True)\n    log.debug(\"%r, %r, %r\", copy_thread, instream, outstream)\n    copy_thread.start()\n    return copy_thread",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "_which",
          "code": "def _which(executable, flags=os.X_OK, abspath_only=False, disallow_symlinks=False):  # type: ignore[no-untyped-def]\n    \"\"\"Borrowed from Twisted's :mod:twisted.python.proutils .\n\n    Search PATH for executable files with the given name.\n\n    On newer versions of MS-Windows, the PATHEXT environment variable will be\n    set to the list of file extensions for files considered executable. This\n    will normally include things like \".EXE\". This fuction will also find files\n    with the given name ending with any of these extensions.\n\n    On MS-Windows the only flag that has any meaning is os.F_OK. Any other\n    flags will be ignored.\n\n    Note: This function does not help us prevent an attacker who can already\n    manipulate the environment's PATH settings from placing malicious code\n    higher in the PATH. It also does happily follows links.\n\n    :param str name: The name for which to search.\n    :param int flags: Arguments to L{os.access}.\n    :rtype: list\n    :returns: A list of the full paths to files found, in the order in which\n              they were found.\n    \"\"\"\n\n    def _can_allow(p):  # type: ignore[no-untyped-def]\n        if not os.access(p, flags):\n            return False\n        if abspath_only and not os.path.abspath(p):\n            log.warn(\"Ignoring %r (path is not absolute)\", p)\n            return False\n        if disallow_symlinks and os.path.islink(p):\n            log.warn(\"Ignoring %r (path is a symlink)\", p)\n            return False\n        return True\n\n    result = []\n    exts = filter(None, os.environ.get(\"PATHEXT\", \"\").split(os.pathsep))\n    path = os.environ.get(\"PATH\", None)\n    if path is None:\n        return []\n    for p in os.environ.get(\"PATH\", \"\").split(os.pathsep):\n        p = os.path.join(p, executable)\n        if _can_allow(p):\n            result.append(p)\n        for e in exts:\n            pext = p + e\n            if _can_allow(pext):\n                result.append(pext)\n    return result",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "_can_allow",
          "code": "def _can_allow(p):  # type: ignore[no-untyped-def]\n        if not os.access(p, flags):\n            return False\n        if abspath_only and not os.path.abspath(p):\n            log.warn(\"Ignoring %r (path is not absolute)\", p)\n            return False\n        if disallow_symlinks and os.path.islink(p):\n            log.warn(\"Ignoring %r (path is a symlink)\", p)\n            return False\n        return True",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "_write_passphrase",
          "code": "def _write_passphrase(stream, passphrase, encoding):  # type: ignore[no-untyped-def]\n    \"\"\"Write the passphrase from memory to the GnuPG process' stdin.\n\n    :type stream: file, :class:`~io.BytesIO`, or :class:`~io.StringIO`\n    :param stream: The input file descriptor to write the password to.\n    :param str passphrase: The passphrase for the secret key material.\n    :param str encoding: The data encoding expected by GnuPG. Usually, this\n                         is ``sys.getfilesystemencoding()``.\n    \"\"\"\n    passphrase = \"%s\\n\" % passphrase\n    passphrase = passphrase.encode(encoding)\n    stream.write(passphrase)\n    log.debug(\"Wrote passphrase on stdin.\")",
          "file": "_util.py"
        },
        {
          "type": "class",
          "name": "InheritableProperty",
          "code": "class InheritableProperty:\n    \"\"\"Based on the emulation of PyProperty_Type() in Objects/descrobject.c\"\"\"\n\n    def __init__(self, fget=None, fset=None, fdel=None, doc=None):  # type: ignore[no-untyped-def]\n        self.fget = fget\n        self.fset = fset\n        self.fdel = fdel\n        self.__doc__ = doc\n\n    def __get__(self, obj, objtype=None):  # type: ignore[no-untyped-def]\n        if obj is None:\n            return self\n        if self.fget is None:\n            raise AttributeError(\"unreadable attribute\")\n        if self.fget.__name__ == \"<lambda>\" or not self.fget.__name__:\n            return self.fget(obj)\n        else:\n            return getattr(obj, self.fget.__name__)()\n\n    def __set__(self, obj, value):  # type: ignore[no-untyped-def]\n        if self.fset is None:\n            raise AttributeError(\"can't set attribute\")\n        if self.fset.__name__ == \"<lambda>\" or not self.fset.__name__:\n            self.fset(obj, value)\n        else:\n            getattr(obj, self.fset.__name__)(value)\n\n    def __delete__(self, obj):  # type: ignore[no-untyped-def]\n        if self.fdel is None:\n            raise AttributeError(\"can't delete attribute\")\n        if self.fdel.__name__ == \"<lambda>\" or not self.fdel.__name__:\n            self.fdel(obj)\n        else:\n            getattr(obj, self.fdel.__name__)()",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, fget=None, fset=None, fdel=None, doc=None):  # type: ignore[no-untyped-def]\n        self.fget = fget\n        self.fset = fset\n        self.fdel = fdel\n        self.__doc__ = doc",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "__get__",
          "code": "def __get__(self, obj, objtype=None):  # type: ignore[no-untyped-def]\n        if obj is None:\n            return self\n        if self.fget is None:\n            raise AttributeError(\"unreadable attribute\")\n        if self.fget.__name__ == \"<lambda>\" or not self.fget.__name__:\n            return self.fget(obj)\n        else:\n            return getattr(obj, self.fget.__name__)()",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "__set__",
          "code": "def __set__(self, obj, value):  # type: ignore[no-untyped-def]\n        if self.fset is None:\n            raise AttributeError(\"can't set attribute\")\n        if self.fset.__name__ == \"<lambda>\" or not self.fset.__name__:\n            self.fset(obj, value)\n        else:\n            getattr(obj, self.fset.__name__)(value)",
          "file": "_util.py"
        },
        {
          "type": "function",
          "name": "__delete__",
          "code": "def __delete__(self, obj):  # type: ignore[no-untyped-def]\n        if self.fdel is None:\n            raise AttributeError(\"can't delete attribute\")\n        if self.fdel.__name__ == \"<lambda>\" or not self.fdel.__name__:\n            self.fdel(obj)\n        else:\n            getattr(obj, self.fdel.__name__)()",
          "file": "_util.py"
        }
      ],
      "_meta.py": [
        {
          "type": "class",
          "name": "GPGMeta",
          "code": "class GPGMeta(type):\n    \"\"\"Metaclass for changing the :meth:GPG.__init__ initialiser.\n\n    Detects running gpg-agent processes and the presence of a pinentry\n    program, and disables pinentry so that python-gnupg can write the\n    passphrase to the controlled GnuPG process without killing the agent.\n\n    :attr _agent_proc: If a :program:`gpg-agent` process is currently running\n                       for the effective userid, then **_agent_proc** will be\n                       set to a ``psutil.Process`` for that process.\n    \"\"\"\n\n    def __new__(cls, name, bases, attrs):  # type: ignore[no-untyped-def]\n        \"\"\"Construct the initialiser for GPG\"\"\"\n        log.debug(\"Metaclass __new__ constructor called for %r\" % cls)\n        if cls._find_agent():\n            # call the normal GPG.__init__() initialiser:\n            attrs[\"init\"] = cls.__init__\n            attrs[\"_remove_agent\"] = True\n        return super().__new__(cls, name, bases, attrs)\n\n    @classmethod\n    def _find_agent(cls):  # type: ignore[no-untyped-def]\n        \"\"\"Discover if a gpg-agent process for the current euid is running.\n\n        If there is a matching gpg-agent process, set a :class:`psutil.Process`\n        instance containing the gpg-agent process' information to\n        ``cls._agent_proc``.\n\n        For Unix systems, we check that the effective UID of this\n        ``python-gnupg`` process is also the owner of the gpg-agent\n        process. For Windows, we check that the usernames of the owners are\n        the same. (Sorry Windows users; maybe you should switch to anything\n        else.)\n\n        :returns: True if there exists a gpg-agent process running under the\n                  same effective user ID as that of this program. Otherwise,\n                  returns False.\n        \"\"\"\n        this_process = psutil.Process(os.getpid())\n        ownership_match = False\n\n        identity = this_process.uids\n\n        for proc in psutil.process_iter():\n            try:\n                # In my system proc.name & proc.is_running are methods\n                if (proc.name() == \"gpg-agent\") and proc.is_running():\n                    log.debug(\"Found gpg-agent process with pid %d\" % proc.pid)\n                    # proc.uids & identity are methods to\n                    if proc.uids() == identity():\n                        ownership_match = True\n            except psutil.Error as err:\n                # Exception when getting proc info, possibly because the\n                # process is zombie / process no longer exist. Just ignore it.\n                log.warn(\"Error while attempting to find gpg-agent process: %s\" % err)\n            # Next code must be inside for operator.\n            # Otherwise to _agent_proc will be saved not \"gpg-agent\" process buth an other.\n            if ownership_match:\n                log.debug(\"Effective UIDs of this process and gpg-agent match\")\n                cls._agent_proc = proc\n                return True\n\n        return False",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "__new__",
          "code": "def __new__(cls, name, bases, attrs):  # type: ignore[no-untyped-def]\n        \"\"\"Construct the initialiser for GPG\"\"\"\n        log.debug(\"Metaclass __new__ constructor called for %r\" % cls)\n        if cls._find_agent():\n            # call the normal GPG.__init__() initialiser:\n            attrs[\"init\"] = cls.__init__\n            attrs[\"_remove_agent\"] = True\n        return super().__new__(cls, name, bases, attrs)",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "_find_agent",
          "code": "def _find_agent(cls):  # type: ignore[no-untyped-def]\n        \"\"\"Discover if a gpg-agent process for the current euid is running.\n\n        If there is a matching gpg-agent process, set a :class:`psutil.Process`\n        instance containing the gpg-agent process' information to\n        ``cls._agent_proc``.\n\n        For Unix systems, we check that the effective UID of this\n        ``python-gnupg`` process is also the owner of the gpg-agent\n        process. For Windows, we check that the usernames of the owners are\n        the same. (Sorry Windows users; maybe you should switch to anything\n        else.)\n\n        :returns: True if there exists a gpg-agent process running under the\n                  same effective user ID as that of this program. Otherwise,\n                  returns False.\n        \"\"\"\n        this_process = psutil.Process(os.getpid())\n        ownership_match = False\n\n        identity = this_process.uids\n\n        for proc in psutil.process_iter():\n            try:\n                # In my system proc.name & proc.is_running are methods\n                if (proc.name() == \"gpg-agent\") and proc.is_running():\n                    log.debug(\"Found gpg-agent process with pid %d\" % proc.pid)\n                    # proc.uids & identity are methods to\n                    if proc.uids() == identity():\n                        ownership_match = True\n            except psutil.Error as err:\n                # Exception when getting proc info, possibly because the\n                # process is zombie / process no longer exist. Just ignore it.\n                log.warn(\"Error while attempting to find gpg-agent process: %s\" % err)\n            # Next code must be inside for operator.\n            # Otherwise to _agent_proc will be saved not \"gpg-agent\" process buth an other.\n            if ownership_match:\n                log.debug(\"Effective UIDs of this process and gpg-agent match\")\n                cls._agent_proc = proc\n                return True\n\n        return False",
          "file": "_meta.py"
        },
        {
          "type": "class",
          "name": "GPGBase",
          "code": "class GPGBase:\n    \"\"\"Base class for storing properties and controlling process initialisation.\n\n    :const _result_map: A *dict* containing classes from\n                        :mod:`~gnupg._parsers`, used for parsing results\n                        obtained from GnuPG commands.\n    :const _decode_errors: How to handle encoding errors.\n    \"\"\"\n\n    __metaclass__ = GPGMeta\n    _decode_errors = \"strict\"\n    _result_map = {\n        \"crypt\": _parsers.Crypt,\n        \"delete\": _parsers.DeleteResult,\n        \"generate\": _parsers.GenKey,\n        \"import\": _parsers.ImportResult,\n        \"export\": _parsers.ExportResult,\n        \"list\": _parsers.ListKeys,\n        \"sign\": _parsers.Sign,\n        \"verify\": _parsers.Verify,\n        \"expire\": _parsers.KeyExpirationResult,\n        \"signing\": _parsers.KeySigningResult,\n        \"packets\": _parsers.ListPackets,\n    }\n\n    def __init__(  # type: ignore[no-untyped-def]\n        self,\n        binary=None,\n        home=None,\n        keyring=None,\n        secring=None,\n        use_agent=False,\n        default_preference_list=None,\n        ignore_homedir_permissions=False,\n        verbose=False,\n        options=None,\n    ):\n        \"\"\"Create a ``GPGBase``.\n\n        This class is used to set up properties for controlling the behaviour\n        of configuring various options for GnuPG, such as setting GnuPG's\n        **homedir** , and the paths to its **binary** and **keyring** .\n\n        :const binary: (:obj:`str`) The full path to the GnuPG binary.\n\n        :ivar homedir: (:class:`~gnupg._util.InheritableProperty`) The full\n                       path to the current setting for the GnuPG\n                       ``--homedir``.\n\n        :ivar _generated_keys: (:class:`~gnupg._util.InheritableProperty`)\n                               Controls setting the directory for storing any\n                               keys which are generated with\n                               :meth:`~gnupg.GPG.gen_key`.\n\n        :ivar str keyring: The filename in **homedir** to use as the keyring\n                           file for public keys.\n        :ivar str secring: The filename in **homedir** to use as the keyring\n                           file for secret keys.\n        \"\"\"\n        self.ignore_homedir_permissions = ignore_homedir_permissions\n        self.binary = _util._find_binary(binary)\n        self.homedir = os.path.expanduser(home) if home else _util._conf\n        pub = _parsers._fix_unsafe(keyring) if keyring else \"pubring.gpg\"\n        sec = _parsers._fix_unsafe(secring) if secring else \"secring.gpg\"\n        self.keyring = os.path.join(self._homedir, pub)\n        self.secring = os.path.join(self._homedir, sec)\n        self.options = list(_parsers._sanitise_list(options)) if options else None\n\n        #: The version string of our GnuPG binary\n        self.binary_version = \"0.0.0\"\n        self.verbose = False\n\n        if default_preference_list:\n            self._prefs = _check_preferences(default_preference_list, \"all\")\n        else:\n            self._prefs = \"SHA512 SHA384 SHA256 AES256 CAMELLIA256 TWOFISH\"\n            self._prefs += \" AES192 ZLIB ZIP Uncompressed\"\n\n        encoding = locale.getpreferredencoding()\n        if encoding is None:  # This happens on Jython!\n            encoding = sys.stdin.encoding\n        self._encoding = encoding.lower().replace(\"-\", \"_\")\n        self._filesystemencoding = encodings.normalize_encoding(sys.getfilesystemencoding().lower())\n\n        # Issue #49: https://github.com/isislovecruft/python-gnupg/issues/49\n        #\n        # During `line = stream.readline()` in `_read_response()`, the Python\n        # codecs module will choke on Unicode data, so we globally monkeypatch\n        # the \"strict\" error handler to use the builtin `replace_errors`\n        # handler:\n        codecs.register_error(\"strict\", codecs.replace_errors)\n\n        self._keyserver = \"hkp://wwwkeys.pgp.net\"\n        self.__generated_keys = os.path.join(self.homedir, \"generated-keys\")\n\n        try:\n            assert self.binary, \"Could not find binary %s\" % binary\n            assert isinstance(\n                verbose, (bool, str, int)\n            ), \"'verbose' must be boolean, string, or 0 <= n <= 9\"\n            assert isinstance(use_agent, bool), \"'use_agent' must be boolean\"\n            if self.options is not None:\n                assert isinstance(self.options, list), \"options not list\"\n        except (AssertionError, AttributeError) as ae:\n            log.error(\"GPGBase.__init__(): %s\" % str(ae))\n            raise RuntimeError(str(ae))\n        else:\n            self._set_verbose(verbose)\n            self.use_agent = use_agent\n\n        if hasattr(self, \"_agent_proc\") and getattr(self, \"_remove_agent\", None) is True:\n            if hasattr(self, \"__remove_path__\"):\n                self.__remove_path__(\"pinentry\")\n\n        # Assign our self.binary_version attribute:\n        self._check_sane_and_get_gpg_version()\n\n    def __remove_path__(self, prog=None, at_exit=True):  # type: ignore[no-untyped-def]\n        \"\"\"Remove the directories containing a program from the system's\n        ``$PATH``. If ``GPGBase.binary`` is in a directory being removed, it\n        is linked to :file:'./gpg' in the current directory.\n\n        :param str prog: The program to remove from ``$PATH``.\n        :param bool at_exit: Add the program back into the ``$PATH`` when the\n                             Python interpreter exits, and delete any symlinks\n                             to ``GPGBase.binary`` which were created.\n        \"\"\"\n        #: A list of ``$PATH`` entries which were removed to disable pinentry.\n        self._removed_path_entries = []\n\n        log.debug(\"Attempting to remove %s from system PATH\" % str(prog))\n        if (prog is None) or (not isinstance(prog, str)):\n            return\n\n        try:\n            _util._which(prog)[0]\n        except (OSError, IndexError) as err:\n            log.err(str(err))\n            log.err(\"Cannot find program '%s', not changing PATH.\" % prog)\n            return\n\n        # __remove_path__ cannot be an @classmethod in GPGMeta, because\n        # the use_agent attribute must be set by the instance.\n        if not self.use_agent:\n            program_base = os.path.dirname(prog)\n            gnupg_base = os.path.dirname(self.binary)\n\n            # symlink our gpg binary into $PWD if the path we are removing is\n            # the one which contains our gpg executable:\n            new_gpg_location = os.path.join(os.getcwd(), \"gpg\")\n            if gnupg_base == program_base:\n                os.symlink(self.binary, new_gpg_location)\n                self.binary = new_gpg_location\n\n            # copy the original environment so that we can put it back later:\n            env_copy = os.environ  # this one should not be touched\n            path_copy = os.environ.pop(\"PATH\")\n            log.debug(\"Created a copy of system PATH: %r\" % path_copy)\n            assert \"PATH\" not in os.environ, \"OS env kept $PATH anyway!\"\n\n            @staticmethod\n            def remove_program_from_path(path, prog_base):  # type: ignore[no-untyped-def]\n                \"\"\"Remove all directories which contain a program from PATH.\n\n                :param str path: The contents of the system environment's\n                                 ``$PATH``.\n\n                :param str prog_base: The directory portion of a program's\n                                      location, without the trailing slash,\n                                      and without the program name. For\n                                      example, ``prog_base='/usr/bin'``.\n                \"\"\"\n                paths = path.split(\":\")\n                for directory in paths:\n                    if directory == prog_base:\n                        log.debug(\"Found directory with target program: %s\" % directory)\n                        path.remove(directory)\n                        self._removed_path_entries.append(directory)\n                log.debug(\"Deleted all found instance of %s.\" % directory)\n                log.debug(f\"PATH is now:{os.linesep}{path}\")\n                return \":\".join([p for p in path])\n\n            @staticmethod\n            def update_path(environment, path):  # type: ignore[no-untyped-def]\n                \"\"\"Add paths to the string at ``os.environ['PATH']``.\n\n                :param str environment: The environment mapping to update.\n                :param list path: A list of strings to update the PATH with.\n                \"\"\"\n                log.debug(\"Updating system path...\")\n                # This assignment doesn't reset the environment, but it does reset the monkey-patch\n                # as intended. Leaving as-is from upstream.\n                os.environ = environment  # noqa: B003\n                new_path = \":\".join([p for p in path])\n                if \"PATH\" in os.environ:\n                    new_path = \":\".join([os.environ[\"PATH\"], new_path])\n                os.environ.update({\"PATH\": new_path})\n                log.debug(\"System $PATH: %s\" % os.environ[\"PATH\"])\n\n            modified_path = remove_program_from_path(path_copy, program_base)\n            update_path(env_copy, modified_path)\n\n            # register an _exithandler with the python interpreter:\n            atexit.register(update_path, env_copy, path_copy)\n\n            def remove_symlinked_binary(symlink):  # type: ignore[no-untyped-def]\n                if os.path.islink(symlink):\n                    os.unlink(symlink)\n                    log.debug(\"Removed binary symlink '%s'\" % symlink)\n\n            atexit.register(remove_symlinked_binary, new_gpg_location)\n\n    @property\n    def default_preference_list(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the default preference list.\"\"\"\n        return self._prefs\n\n    @default_preference_list.setter\n    def default_preference_list(self, prefs):  # type: ignore[no-untyped-def]\n        \"\"\"Set the default preference list.\n\n        :param str prefs: A string containing the default preferences for\n                          ciphers, digests, and compression algorithms.\n        \"\"\"\n        prefs = _check_preferences(prefs)\n        if prefs is not None:\n            self._prefs = prefs\n\n    @default_preference_list.deleter\n    def default_preference_list(self):  # type: ignore[no-untyped-def]\n        \"\"\"Reset the default preference list to its original state.\n\n        Note that \"original state\" does not mean the default preference\n        list for whichever version of GnuPG is being used. It means the\n        default preference list defined by :attr:`GPGBase._prefs`.\n\n        Using BZIP2 is avoided due to not interacting well with some versions\n        of GnuPG>=2.0.0.\n        \"\"\"\n        self._prefs = \"SHA512 SHA384 SHA256 AES256 CAMELLIA256 TWOFISH ZLIB ZIP\"\n\n    @property\n    def keyserver(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the current keyserver setting.\"\"\"\n        return self._keyserver\n\n    @keyserver.setter\n    def keyserver(self, location):  # type: ignore[no-untyped-def]\n        \"\"\"Set the default keyserver to use for sending and receiving keys.\n\n        The ``location`` is sent to :func:`_parsers._check_keyserver` when\n        option are parsed in :meth:`gnupg.GPG._make_options`.\n\n        :param str location: A string containing the default keyserver. This\n                             should contain the desired keyserver protocol\n                             which is supported by the keyserver, for example,\n                             ``'hkps://keys.mayfirst.org'``. The default\n                             keyserver is ``'hkp://wwwkeys.pgp.net'``.\n        \"\"\"\n        self._keyserver = location\n\n    @keyserver.deleter\n    def keyserver(self):  # type: ignore[no-untyped-def]\n        \"\"\"Reset the keyserver to the default setting.\"\"\"\n        self._keyserver = \"hkp://wwwkeys.pgp.net\"\n\n    def _homedir_getter(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the directory currently being used as GnuPG's homedir.\n\n        If unspecified, use :file:`~/.config/python-gnupg/`\n\n        :rtype: str\n        :returns: The absolute path to the current GnuPG homedir.\n        \"\"\"\n        return self._homedir\n\n    def _homedir_setter(self, directory):  # type: ignore[no-untyped-def]\n        \"\"\"Set the directory to use as GnuPG's homedir.\n\n        If unspecified, use $HOME/.config/python-gnupg. If specified, ensure\n        that the ``directory`` does not contain various shell escape\n        characters. If ``directory`` is not found, it will be automatically\n        created. Lastly, the ``direcory`` will be checked that the EUID has\n        read and write permissions for it.\n\n        :param str directory: A relative or absolute path to the directory to\n                            use for storing/accessing GnuPG's files, including\n                            keyrings and the trustdb.\n        :raises: :exc:`~exceptions.RuntimeError` if unable to find a suitable\n                 directory to use.\n        \"\"\"\n        if not directory:\n            log.debug(\"GPGBase._homedir_setter(): Using default homedir: '%s'\" % _util._conf)\n            directory = _util._conf\n\n        hd = _parsers._fix_unsafe(directory)\n        log.debug(\"GPGBase._homedir_setter(): got directory '%s'\" % hd)\n\n        if hd:\n            log.debug(\"GPGBase._homedir_setter(): Check existence of '%s'\" % hd)\n            _util._create_if_necessary(hd)\n\n        if self.ignore_homedir_permissions:\n            self._homedir = hd\n        else:\n            try:\n                log.debug(\"GPGBase._homedir_setter(): checking permissions\")\n                assert _util._has_readwrite(hd), \"Homedir '%s' needs read/write permissions\" % hd\n            except AssertionError as ae:\n                msg = \"Unable to set '%s' as GnuPG homedir\" % directory\n                log.debug(\"GPGBase.homedir.setter(): %s\" % msg)\n                log.debug(str(ae))\n                raise RuntimeError(str(ae))\n            else:\n                log.info(\"Setting homedir to '%s'\" % hd)\n                self._homedir = hd\n\n    homedir = _util.InheritableProperty(_homedir_getter, _homedir_setter)\n\n    def _generated_keys_getter(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the ``homedir`` subdirectory for storing generated keys.\n\n        :rtype: str\n        :returns: The absolute path to the current GnuPG homedir.\n        \"\"\"\n        return self.__generated_keys\n\n    def _generated_keys_setter(self, directory):  # type: ignore[no-untyped-def]\n        \"\"\"Set the directory for storing generated keys.\n\n        If unspecified, use\n        :meth:`~gnupg._meta.GPGBase.homedir`/generated-keys. If specified,\n        ensure that the ``directory`` does not contain various shell escape\n        characters. If ``directory`` isn't found, it will be automatically\n        created. Lastly, the ``directory`` will be checked to ensure that the\n        current EUID has read and write permissions for it.\n\n        :param str directory: A relative or absolute path to the directory to\n             use for storing/accessing GnuPG's files, including keyrings and\n             the trustdb.\n        :raises: :exc:`~exceptions.RuntimeError` if unable to find a suitable\n             directory to use.\n        \"\"\"\n        if not directory:\n            directory = os.path.join(self.homedir, \"generated-keys\")\n            log.debug(\"GPGBase._generated_keys_setter(): Using '%s'\" % directory)\n\n        hd = _parsers._fix_unsafe(directory)\n        log.debug(\"GPGBase._generated_keys_setter(): got directory '%s'\" % hd)\n\n        if hd:\n            log.debug(\"GPGBase._generated_keys_setter(): Check exists '%s'\" % hd)\n            _util._create_if_necessary(hd)\n\n        try:\n            log.debug(\"GPGBase._generated_keys_setter(): check permissions\")\n            assert _util._has_readwrite(hd), \"Keys dir '%s' needs read/write permissions\" % hd\n        except AssertionError as ae:\n            msg = \"Unable to set '%s' as generated keys dir\" % directory\n            log.debug(\"GPGBase._generated_keys_setter(): %s\" % msg)\n            log.debug(str(ae))\n            raise RuntimeError(str(ae))\n        else:\n            log.info(\"Setting homedir to '%s'\" % hd)\n            self.__generated_keys = hd\n\n    _generated_keys = _util.InheritableProperty(_generated_keys_getter, _generated_keys_setter)\n\n    def _check_sane_and_get_gpg_version(self):  # type: ignore[no-untyped-def]\n        \"\"\"Check that everything runs alright, and grab the gpg binary's\n        version number while we're at it, storing it as :data:`binary_version`.\n\n        :raises RuntimeError: if we cannot invoke the gpg binary.\n        \"\"\"\n        proc = self._open_subprocess([\"--list-config\", \"--with-colons\"])\n        result = self._result_map[\"list\"](self)\n        self._read_data(proc.stdout, result)\n        if proc.returncode:\n            raise RuntimeError(\"Error invoking gpg: %s\" % result.data)\n        else:\n            try:\n                proc.terminate()\n            except OSError:\n                log.error(\n                    \"Could neither invoke nor terminate a gpg process... \"\n                    \"Are you sure you specified the corrent (and full) \"\n                    \"path to the gpg binary?\"\n                )\n\n        version_line = result.data.partition(b\":version:\")[2].decode()\n        if not version_line:\n            raise RuntimeError(\"Got invalid version line from gpg: %s\\n\" % result.data)\n        self.binary_version = version_line.split(\"\\n\")[0]\n        if not _VERSION_RE.match(self.binary_version):\n            raise RuntimeError(\"Got invalid version line from gpg: %s\\n\" % self.binary_version)\n        log.debug(\"Using GnuPG version %s\" % self.binary_version)\n\n    def _make_args(self, args, passphrase=False):  # type: ignore[no-untyped-def]\n        \"\"\"Make a list of command line elements for GPG.\n\n        The value of ``args`` will be appended only if it passes the checks in\n        :func:`gnupg._parsers._sanitise`. The ``passphrase`` argument needs to\n        be True if a passphrase will be sent to GnuPG, else False.\n\n        :param list args: A list of strings of options and flags to pass to\n                          ``GPG.binary``. This is input safe, meaning that\n                          these values go through strict checks (see\n                          ``parsers._sanitise_list``) before being passed to to\n                          the input file descriptor for the GnuPG process.\n                          Each string should be given exactly as it would be on\n                          the commandline interface to GnuPG,\n                          e.g. [\"--cipher-algo AES256\", \"--default-key\n                          A3ADB67A2CDB8B35\"].\n\n        :param bool passphrase: If True, the passphrase will be sent to the\n                                stdin file descriptor for the attached GnuPG\n                                process.\n        \"\"\"\n        # see TODO file, tag :io:makeargs:\n        cmd = [self.binary, \"--no-options --no-emit-version --no-tty --status-fd 2\"]\n\n        if self.homedir:\n            cmd.append('--homedir \"%s\"' % self.homedir)\n\n        if self.keyring:\n            cmd.append(\"--no-default-keyring --keyring %s\" % self.keyring)\n        if self.secring and self.binary_version != \"2.4.4\":\n            # In GnuPG 2.4.4, --secret-keyring has no effect\n            cmd.append(\"--secret-keyring %s\" % self.secring)\n\n        if passphrase:\n            cmd.append(\"--batch --passphrase-fd 0\")\n\n        if self.use_agent is True:\n            cmd.append(\"--use-agent\")\n        elif self.use_agent is False:\n            cmd.append(\"--no-use-agent\")\n\n        # The arguments for debugging and verbosity should be placed into the\n        # cmd list before the options/args in order to resolve Issue #76:\n        # https://github.com/isislovecruft/python-gnupg/issues/76\n        if self.verbose:\n            cmd.append(\"--debug-all\")\n\n            if isinstance(self.verbose, str) or (\n                isinstance(self.verbose, int) and (self.verbose >= 1)\n            ):\n                # GnuPG<=1.4.18 parses the `--debug-level` command in a way\n                # that is incompatible with all other GnuPG versions. :'(\n                if self.binary_version and (self.binary_version <= \"1.4.18\"):\n                    cmd.append(\"--debug-level=%s\" % self.verbose)\n                else:\n                    cmd.append(\"--debug-level %s\" % self.verbose)\n\n        if self.options:\n            [cmd.append(opt) for opt in iter(_sanitise_list(self.options))]\n        if args:\n            [cmd.append(arg) for arg in iter(_sanitise_list(args))]\n\n        return cmd\n\n    def _open_subprocess(self, args=None, passphrase=False):  # type: ignore[no-untyped-def]\n        \"\"\"Open a pipe to a GPG subprocess and return the file objects for\n        communicating with it.\n\n        :param list args: A list of strings of options and flags to pass to\n                          ``GPG.binary``. This is input safe, meaning that\n                          these values go through strict checks (see\n                          ``parsers._sanitise_list``) before being passed to to\n                          the input file descriptor for the GnuPG process.\n                          Each string should be given exactly as it would be on\n                          the commandline interface to GnuPG,\n                          e.g. [\"--cipher-algo AES256\", \"--default-key\n                          A3ADB67A2CDB8B35\"].\n\n        :param bool passphrase: If True, the passphrase will be sent to the\n                                stdin file descriptor for the attached GnuPG\n                                process.\n        \"\"\"\n        # see http://docs.python.org/2/library/subprocess.html#converting-an\\\n        #    -argument-sequence-to-a-string-on-windows\n        cmd = shlex.split(\" \".join(self._make_args(args, passphrase)))\n        log.debug(f\"Sending command to GnuPG process:{os.linesep}{cmd}\")\n\n        environment = {\n            \"LANGUAGE\": os.environ.get(\"LANGUAGE\") or \"en\",\n            \"GPG_TTY\": os.environ.get(\"GPG_TTY\") or \"\",\n            \"DISPLAY\": os.environ.get(\"DISPLAY\") or \"\",\n            \"GPG_AGENT_INFO\": os.environ.get(\"GPG_AGENT_INFO\") or \"\",\n            \"GPG_PINENTRY_PATH\": os.environ.get(\"GPG_PINENTRY_PATH\") or \"\",\n        }\n\n        return subprocess.Popen(\n            cmd,\n            shell=False,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            env=environment,\n        )\n\n    def _read_response(self, stream, result):  # type: ignore[no-untyped-def]\n        \"\"\"Reads all the stderr output from GPG, taking notice only of lines\n        that begin with the magic [GNUPG:] prefix.\n\n        Calls methods on the response object for each valid token found, with\n        the arg being the remainder of the status line.\n\n        :param stream: A byte-stream, file handle, or a\n                       :data:`subprocess.PIPE` for parsing the status codes\n                       from the GnuPG process.\n\n        :param result: The result parser class from :mod:`~gnupg._parsers` ―\n                       the ``handle_status()`` method of that class will be\n                       called in order to parse the output of ``stream``.\n        \"\"\"\n        # All of the userland messages (i.e. not status-fd lines) we're not\n        # interested in passing to our logger\n        userland_messages_to_ignore = []\n\n        if self.ignore_homedir_permissions:\n            userland_messages_to_ignore.append(\"unsafe ownership on homedir\")\n\n        lines = []\n\n        while True:\n            line = stream.readline()\n            if len(line) == 0:\n                break\n            lines.append(line)\n            line = line.rstrip()\n\n            if line.startswith(\"[GNUPG:]\"):\n                line = _util._deprefix(line, \"[GNUPG:] \", log.status)\n                keyword, value = _util._separate_keyword(line)\n                result._handle_status(keyword, value)\n            elif line.startswith(\"gpg:\"):\n                line = _util._deprefix(line, \"gpg: \")\n                keyword, value = _util._separate_keyword(line)\n\n                # Silence warnings from gpg we're supposed to ignore\n                ignore = any(msg in value for msg in userland_messages_to_ignore)\n\n                if not ignore:\n                    # Log gpg's userland messages at our own levels:\n                    if keyword.upper().startswith(\"WARNING\"):\n                        log.warn(\"%s\" % value)\n                    elif keyword.upper().startswith(\"FATAL\"):\n                        log.critical(\"%s\" % value)\n                        # Handle the gpg2 error where a missing trustdb.gpg is,\n                        # for some stupid reason, considered fatal:\n                        if value.find(\"trustdb.gpg\") and value.find(\"No such file\"):\n                            result._handle_status(\"NEED_TRUSTDB\", \"\")\n            elif self.verbose:\n                log.info(\"%s\" % line)\n            else:\n                log.debug(\"%s\" % line)\n        result.stderr = \"\".join(lines)\n\n    def _read_data(self, stream, result):  # type: ignore[no-untyped-def]\n        \"\"\"Incrementally read from ``stream`` and store read data.\n\n        All data gathered from calling ``stream.read()`` will be concatenated\n        and stored as ``result.data``.\n\n        :param stream: An open file-like object to read() from.\n        :param result: An instance of one of the :ref:`result parsing classes\n            <parsers>` from :const:`~gnupg._meta.GPGBase._result_map`.\n        \"\"\"\n        chunks = []\n        log.debug(\"Reading data from stream %r...\" % stream.__repr__())\n\n        while True:\n            data = stream.read(1024)\n            if len(data) == 0:\n                break\n            chunks.append(data)\n            log.debug(\"Read %4d bytes\" % len(data))\n\n        # Join using b'' or '', as appropriate\n        result.data = type(data)().join(chunks)\n        log.debug(\"Finishing reading from stream %r...\" % stream.__repr__())\n        log.debug(\"Read %4d bytes total\" % len(result.data))\n\n    def _set_verbose(self, verbose):  # type: ignore[no-untyped-def]\n        \"\"\"Check and set our :data:`verbose` attribute.\n        The debug-level must be a string or an integer. If it is one of\n        the allowed strings, GnuPG will translate it internally to it's\n        corresponding integer level:\n\n        basic     = 1-2\n        advanced  = 3-5\n        expert    = 6-8\n        guru      = 9+\n\n        If it's not one of the recognised string levels, then then\n        entire argument is ignored by GnuPG. :(\n\n        To fix that stupid behaviour, if they wanted debugging but typo'd\n        the string level (or specified ``verbose=True``), we'll default to\n        'basic' logging.\n        \"\"\"\n        string_levels = (\"basic\", \"advanced\", \"expert\", \"guru\")\n\n        if verbose is True:\n            # The caller wants logging, but we need a valid --debug-level\n            # for gpg. Default to \"basic\", and warn about the ambiguity.\n            verbose = \"basic\"\n\n        if isinstance(verbose, str) and verbose not in string_levels:\n            verbose = \"basic\"\n\n        self.verbose = verbose\n\n    def _collect_output(self, process, result, writer=None, stdin=None):  # type: ignore[no-untyped-def]\n        \"\"\"Drain the subprocesses output streams, writing the collected output\n        to the result. If a writer thread (writing to the subprocess) is given,\n        make sure it's joined before returning. If a stdin stream is given,\n        close it before returning.\n        \"\"\"\n        stderr = codecs.getreader(self._encoding)(process.stderr)\n        rr = threading.Thread(target=self._read_response, args=(stderr, result))\n        rr.setDaemon(True)\n        log.debug(\"stderr reader: %r\", rr)\n        rr.start()\n\n        stdout = process.stdout\n        dr = threading.Thread(target=self._read_data, args=(stdout, result))\n        dr.setDaemon(True)\n        log.debug(\"stdout reader: %r\", dr)\n        dr.start()\n\n        dr.join()\n        rr.join()\n        if writer is not None:\n            writer.join()\n        process.wait()\n        if stdin is not None:\n            try:\n                stdin.close()\n            except OSError:\n                pass\n        stderr.close()\n        stdout.close()\n\n    def _handle_io(self, args, file, result, passphrase=False, binary=False):  # type: ignore[no-untyped-def]\n        \"\"\"Handle a call to GPG - pass input data, collect output data.\"\"\"\n        p = self._open_subprocess(args, passphrase)\n        if not binary:\n            stdin = codecs.getwriter(self._encoding)(p.stdin)\n        else:\n            stdin = p.stdin\n        if passphrase:\n            _util._write_passphrase(stdin, passphrase, self._encoding)\n        writer = _util._threaded_copy_data(file, stdin)\n        self._collect_output(p, result, writer, stdin)\n        return result\n\n    def _recv_keys(self, keyids, keyserver=None):  # type: ignore[no-untyped-def]\n        \"\"\"Import keys from a keyserver.\n\n        :param str keyids: A space-delimited string containing the keyids to\n                           request.\n        :param str keyserver: The keyserver to request the ``keyids`` from;\n                              defaults to `gnupg.GPG.keyserver`.\n        \"\"\"\n        if not keyserver:\n            keyserver = self.keyserver\n\n        args = [f\"--keyserver {keyserver}\", f\"--recv-keys {keyids}\"]\n        log.info(f\"Requesting keys from {keyserver}: {keyids}\")\n\n        result = self._result_map[\"import\"](self)\n        proc = self._open_subprocess(args)\n        self._collect_output(proc, result)\n        log.debug(\"recv_keys result: %r\", result.__dict__)\n        return result\n\n    def _sign_file(  # type: ignore[no-untyped-def]\n        self,\n        file,\n        default_key=None,\n        passphrase=None,\n        clearsign=True,\n        detach=False,\n        binary=False,\n        digest_algo=\"SHA512\",\n    ):\n        \"\"\"Create a signature for a file.\n\n        :param file: The file stream (i.e. it's already been open()'d) to sign.\n        :param str default_key: The key to sign with.\n        :param str passphrase: The passphrase to pipe to stdin.\n        :param bool clearsign: If True, create a cleartext signature.\n        :param bool detach: If True, create a detached signature.\n        :param bool binary: If True, do not ascii armour the output.\n        :param str digest_algo: The hash digest to use. Again, to see which\n                                hashes your GnuPG is capable of using, do:\n                                ``$ gpg --with-colons --list-config\n                                digestname``. The default, if unspecified, is\n                                ``'SHA512'``.\n        \"\"\"\n        log.debug(\"_sign_file():\")\n        if binary:\n            log.info(\"Creating binary signature for file %s\" % file)\n            args = [\"--sign\"]\n        else:\n            log.info(\"Creating ascii-armoured signature for file %s\" % file)\n            args = [\"--sign --armor\"]\n\n        if clearsign:\n            args.append(\"--clearsign\")\n            if detach:\n                log.warn(\"Cannot use both --clearsign and --detach-sign.\")\n                log.warn(\"Using default GPG behaviour: --clearsign only.\")\n        elif detach and not clearsign:\n            args.append(\"--detach-sign\")\n\n        if default_key:\n            args.append(str(\"--default-key %s\" % default_key))\n\n        args.append(str(\"--digest-algo %s\" % digest_algo))\n\n        # We could use _handle_io here except for the fact that if the\n        # passphrase is bad, gpg bails and you can't write the message.\n        result = self._result_map[\"sign\"](self)\n\n        # If the passphrase is an empty string, the message up to and\n        # including its first newline will be cut off before making it to the\n        # GnuPG process. Therefore, if the passphrase='' or passphrase=b'',\n        # we set passphrase=None.  See Issue #82:\n        # https://github.com/isislovecruft/python-gnupg/issues/82\n        if isinstance(passphrase, str):\n            passphrase = passphrase if len(passphrase) > 0 else None\n        elif isinstance(passphrase, (bytes, bytearray)):\n            passphrase = passphrase.decode() if len(passphrase) > 0 else None\n        else:\n            passphrase = None\n\n        proc = self._open_subprocess(args, passphrase is not None)\n        try:\n            if passphrase:\n                _util._write_passphrase(proc.stdin, passphrase, self._encoding)\n            writer = _util._threaded_copy_data(file, proc.stdin)\n        except OSError as ioe:\n            log.exception(\"Error writing message: %s\" % str(ioe))\n            writer = None\n        self._collect_output(proc, result, writer, proc.stdin)\n        return result\n\n    def _encrypt(  # type: ignore[no-untyped-def]\n        self,\n        data,\n        recipients,\n        default_key=None,\n        passphrase=None,\n        armor=True,\n        encrypt=True,\n        symmetric=False,\n        always_trust=True,\n        output=None,\n        throw_keyids=False,\n        hidden_recipients=None,\n        cipher_algo=\"AES256\",\n        digest_algo=\"SHA512\",\n        compress_algo=\"ZLIB\",\n    ):\n        \"\"\"Encrypt the message read from the file-like object **data**.\n\n        :param str data: The file or bytestream to encrypt.\n\n        :param str recipients: The recipients to encrypt to. Recipients must\n                               be specified keyID/fingerprint.\n\n        .. warning:: Care should be taken in Python2 to make sure that the\n                     given fingerprints for **recipients** are in fact strings\n                     and not unicode objects.\n\n        :param str default_key: The keyID/fingerprint of the key to use for\n                                signing. If given, **data** will be encrypted\n                                *and* signed.\n\n        :param str passphrase: If given, and **default_key** is also given,\n                               use this passphrase to unlock the secret\n                               portion of the **default_key** to sign the\n                               encrypted **data**.  Otherwise, if\n                               **default_key** is not given, but **symmetric**\n                               is ``True``, then use this passphrase as the\n                               passphrase for symmetric encryption. Signing\n                               and symmetric encryption should *not* be\n                               combined when sending the **data** to other\n                               recipients, else the passphrase to the secret\n                               key would be shared with them.\n\n        :param bool armor: If True, ascii armor the output; otherwise, the\n                           output will be in binary format. (Default: True)\n\n        :param bool encrypt: If True, encrypt the **data** using the\n                             **recipients** public keys. (Default: True)\n\n        :param bool symmetric: If True, encrypt the **data** to **recipients**\n                               using a symmetric key. See the **passphrase**\n                               parameter. Symmetric encryption and public key\n                               encryption can be used simultaneously, and will\n                               result in a ciphertext which is decryptable\n                               with either the symmetric **passphrase** or one\n                               of the corresponding private keys.\n\n        :param bool always_trust: If True, ignore trust warnings on\n                                  **recipients** keys. If False, display trust\n                                  warnings. (default: True)\n\n        :type output: str or file-like object\n        :param output: The output file to write to. If not specified, the\n                       encrypted output is returned, and thus should be stored\n                       as an object in Python. For example:\n\n        >>> import shutil\n        >>> import gnupg\n        >>> if os.path.exists(\"doctests\"):\n        ...     shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_settings = gpg.gen_key_input(key_type='RSA',\n        ...                                  key_length=1024,\n        ...                                  key_usage='ESCA',\n        ...                                  passphrase='foo')\n        >>> key = gpg.gen_key(key_settings)\n        >>> message = \"The crow flies at midnight.\"\n        >>> encrypted = str(gpg.encrypt(message, key.fingerprint))\n        >>> assert encrypted != message\n        >>> assert not encrypted.isspace()\n        >>> decrypted = str(gpg.decrypt(encrypted, passphrase='foo'))\n        >>> assert not decrypted.isspace()\n        >>> decrypted\n        'The crow flies at midnight.'\n\n\n        :param bool throw_keyids: If True, make all **recipients** keyids be\n            zero'd out in packet information. This is the same as using\n            **hidden_recipients** for all **recipients**. (Default: False).\n\n        :param list hidden_recipients: A list of recipients that should have\n            their keyids zero'd out in packet information.\n\n        :param str cipher_algo: The cipher algorithm to use. To see available\n                                algorithms with your version of GnuPG, do:\n                                :command:`$ gpg --with-colons --list-config\n                                ciphername`. The default **cipher_algo**, if\n                                unspecified, is ``'AES256'``.\n\n        :param str digest_algo: The hash digest to use. Again, to see which\n                                hashes your GnuPG is capable of using, do:\n                                :command:`$ gpg --with-colons --list-config\n                                digestname`.  The default, if unspecified, is\n                                ``'SHA512'``.\n\n        :param str compress_algo: The compression algorithm to use. Can be one\n                                  of ``'ZLIB'``, ``'BZIP2'``, ``'ZIP'``, or\n                                  ``'Uncompressed'``.\n        \"\"\"\n        args = []\n\n        # FIXME: GnuPG appears to ignore the --output directive when being\n        # programmatically driven. We'll handle the IO ourselves to fix this\n        # for now.\n        output_filename = None\n        if output:\n            if getattr(output, \"fileno\", None) is not None:\n                # avoid overwrite confirmation message\n                if getattr(output, \"name\", None) is not None:\n                    output_filename = output.name\n                    if os.path.exists(output.name):\n                        os.remove(output.name)\n                    # args.append('--output %s' % output.name)\n            else:\n                output_filename = output\n                if os.path.exists(output):\n                    os.remove(output)\n                # args.append('--output %s' % output)\n\n        if armor:\n            args.append(\"--armor\")\n        if always_trust:\n            args.append(\"--always-trust\")\n        if cipher_algo:\n            args.append(\"--cipher-algo %s\" % cipher_algo)\n        if compress_algo:\n            args.append(\"--compress-algo %s\" % compress_algo)\n\n        if default_key:\n            args.append(\"--sign\")\n            args.append(\"--default-key %s\" % default_key)\n            if digest_algo:\n                args.append(\"--digest-algo %s\" % digest_algo)\n\n        # both can be used at the same time for an encrypted file which\n        # is decryptable with a passphrase or secretkey.\n        if symmetric:\n            args.append(\"--symmetric\")\n        if encrypt:\n            args.append(\"--encrypt\")\n        if throw_keyids:\n            args.append(\"--throw-keyids\")\n\n        if len(recipients) >= 1:\n            log.debug(\n                f\"GPG.encrypt() called for recipients '{recipients}' with type '{type(recipients)}'\"\n            )\n\n            if isinstance(recipients, (list, tuple)):\n                for recp in recipients:\n                    if isinstance(recp, str):\n                        self._add_recipient_string(args, hidden_recipients, recp)\n\n            elif isinstance(recp, str):\n                for recp in recipients.split(\" \"):\n                    self._add_recipient_string(args, hidden_recipients, recp)\n                    # ...and now that we've proven py3k is better...\n            else:\n                log.debug(\"Don't know what to do with recipients: %r\" % recipients)\n\n        result = self._result_map[\"crypt\"](self)\n        log.debug(f\"Got data '{data}' with type '{type(data)}'.\")\n        self._handle_io(args, data, result, passphrase=passphrase, binary=True)\n        # Avoid writing raw encrypted bytes to terminal loggers and breaking\n        # them in that adorable way where they spew hieroglyphics until reset:\n        if armor:\n            log.debug(\"\\n%s\" % result.data)\n\n        if output_filename:\n            log.info(\"Writing encrypted output to file: %s\" % output_filename)\n            with open(output_filename, \"wb\") as fh:\n                fh.write(result.data)\n                fh.flush()\n                log.info(\"Encrypted output written successfully.\")\n\n        return result\n\n    def _add_recipient_string(self, args, hidden_recipients, recipient):  # type: ignore[no-untyped-def]\n        if isinstance(hidden_recipients, (list, tuple)):\n            if [s for s in hidden_recipients if recipient in str(s)]:\n                args.append(\"--hidden-recipient %s\" % recipient)\n            else:\n                args.append(\"--recipient %s\" % recipient)\n        else:\n            args.append(\"--recipient %s\" % recipient)",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(  # type: ignore[no-untyped-def]\n        self,\n        binary=None,\n        home=None,\n        keyring=None,\n        secring=None,\n        use_agent=False,\n        default_preference_list=None,\n        ignore_homedir_permissions=False,\n        verbose=False,\n        options=None,\n    ):\n        \"\"\"Create a ``GPGBase``.\n\n        This class is used to set up properties for controlling the behaviour\n        of configuring various options for GnuPG, such as setting GnuPG's\n        **homedir** , and the paths to its **binary** and **keyring** .\n\n        :const binary: (:obj:`str`) The full path to the GnuPG binary.\n\n        :ivar homedir: (:class:`~gnupg._util.InheritableProperty`) The full\n                       path to the current setting for the GnuPG\n                       ``--homedir``.\n\n        :ivar _generated_keys: (:class:`~gnupg._util.InheritableProperty`)\n                               Controls setting the directory for storing any\n                               keys which are generated with\n                               :meth:`~gnupg.GPG.gen_key`.\n\n        :ivar str keyring: The filename in **homedir** to use as the keyring\n                           file for public keys.\n        :ivar str secring: The filename in **homedir** to use as the keyring\n                           file for secret keys.\n        \"\"\"\n        self.ignore_homedir_permissions = ignore_homedir_permissions\n        self.binary = _util._find_binary(binary)\n        self.homedir = os.path.expanduser(home) if home else _util._conf\n        pub = _parsers._fix_unsafe(keyring) if keyring else \"pubring.gpg\"\n        sec = _parsers._fix_unsafe(secring) if secring else \"secring.gpg\"\n        self.keyring = os.path.join(self._homedir, pub)\n        self.secring = os.path.join(self._homedir, sec)\n        self.options = list(_parsers._sanitise_list(options)) if options else None\n\n        #: The version string of our GnuPG binary\n        self.binary_version = \"0.0.0\"\n        self.verbose = False\n\n        if default_preference_list:\n            self._prefs = _check_preferences(default_preference_list, \"all\")\n        else:\n            self._prefs = \"SHA512 SHA384 SHA256 AES256 CAMELLIA256 TWOFISH\"\n            self._prefs += \" AES192 ZLIB ZIP Uncompressed\"\n\n        encoding = locale.getpreferredencoding()\n        if encoding is None:  # This happens on Jython!\n            encoding = sys.stdin.encoding\n        self._encoding = encoding.lower().replace(\"-\", \"_\")\n        self._filesystemencoding = encodings.normalize_encoding(sys.getfilesystemencoding().lower())\n\n        # Issue #49: https://github.com/isislovecruft/python-gnupg/issues/49\n        #\n        # During `line = stream.readline()` in `_read_response()`, the Python\n        # codecs module will choke on Unicode data, so we globally monkeypatch\n        # the \"strict\" error handler to use the builtin `replace_errors`\n        # handler:\n        codecs.register_error(\"strict\", codecs.replace_errors)\n\n        self._keyserver = \"hkp://wwwkeys.pgp.net\"\n        self.__generated_keys = os.path.join(self.homedir, \"generated-keys\")\n\n        try:\n            assert self.binary, \"Could not find binary %s\" % binary\n            assert isinstance(\n                verbose, (bool, str, int)\n            ), \"'verbose' must be boolean, string, or 0 <= n <= 9\"\n            assert isinstance(use_agent, bool), \"'use_agent' must be boolean\"\n            if self.options is not None:\n                assert isinstance(self.options, list), \"options not list\"\n        except (AssertionError, AttributeError) as ae:\n            log.error(\"GPGBase.__init__(): %s\" % str(ae))\n            raise RuntimeError(str(ae))\n        else:\n            self._set_verbose(verbose)\n            self.use_agent = use_agent\n\n        if hasattr(self, \"_agent_proc\") and getattr(self, \"_remove_agent\", None) is True:\n            if hasattr(self, \"__remove_path__\"):\n                self.__remove_path__(\"pinentry\")\n\n        # Assign our self.binary_version attribute:\n        self._check_sane_and_get_gpg_version()",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "__remove_path__",
          "code": "def __remove_path__(self, prog=None, at_exit=True):  # type: ignore[no-untyped-def]\n        \"\"\"Remove the directories containing a program from the system's\n        ``$PATH``. If ``GPGBase.binary`` is in a directory being removed, it\n        is linked to :file:'./gpg' in the current directory.\n\n        :param str prog: The program to remove from ``$PATH``.\n        :param bool at_exit: Add the program back into the ``$PATH`` when the\n                             Python interpreter exits, and delete any symlinks\n                             to ``GPGBase.binary`` which were created.\n        \"\"\"\n        #: A list of ``$PATH`` entries which were removed to disable pinentry.\n        self._removed_path_entries = []\n\n        log.debug(\"Attempting to remove %s from system PATH\" % str(prog))\n        if (prog is None) or (not isinstance(prog, str)):\n            return\n\n        try:\n            _util._which(prog)[0]\n        except (OSError, IndexError) as err:\n            log.err(str(err))\n            log.err(\"Cannot find program '%s', not changing PATH.\" % prog)\n            return\n\n        # __remove_path__ cannot be an @classmethod in GPGMeta, because\n        # the use_agent attribute must be set by the instance.\n        if not self.use_agent:\n            program_base = os.path.dirname(prog)\n            gnupg_base = os.path.dirname(self.binary)\n\n            # symlink our gpg binary into $PWD if the path we are removing is\n            # the one which contains our gpg executable:\n            new_gpg_location = os.path.join(os.getcwd(), \"gpg\")\n            if gnupg_base == program_base:\n                os.symlink(self.binary, new_gpg_location)\n                self.binary = new_gpg_location\n\n            # copy the original environment so that we can put it back later:\n            env_copy = os.environ  # this one should not be touched\n            path_copy = os.environ.pop(\"PATH\")\n            log.debug(\"Created a copy of system PATH: %r\" % path_copy)\n            assert \"PATH\" not in os.environ, \"OS env kept $PATH anyway!\"\n\n            @staticmethod\n            def remove_program_from_path(path, prog_base):  # type: ignore[no-untyped-def]\n                \"\"\"Remove all directories which contain a program from PATH.\n\n                :param str path: The contents of the system environment's\n                                 ``$PATH``.\n\n                :param str prog_base: The directory portion of a program's\n                                      location, without the trailing slash,\n                                      and without the program name. For\n                                      example, ``prog_base='/usr/bin'``.\n                \"\"\"\n                paths = path.split(\":\")\n                for directory in paths:\n                    if directory == prog_base:\n                        log.debug(\"Found directory with target program: %s\" % directory)\n                        path.remove(directory)\n                        self._removed_path_entries.append(directory)\n                log.debug(\"Deleted all found instance of %s.\" % directory)\n                log.debug(f\"PATH is now:{os.linesep}{path}\")\n                return \":\".join([p for p in path])\n\n            @staticmethod\n            def update_path(environment, path):  # type: ignore[no-untyped-def]\n                \"\"\"Add paths to the string at ``os.environ['PATH']``.\n\n                :param str environment: The environment mapping to update.\n                :param list path: A list of strings to update the PATH with.\n                \"\"\"\n                log.debug(\"Updating system path...\")\n                # This assignment doesn't reset the environment, but it does reset the monkey-patch\n                # as intended. Leaving as-is from upstream.\n                os.environ = environment  # noqa: B003\n                new_path = \":\".join([p for p in path])\n                if \"PATH\" in os.environ:\n                    new_path = \":\".join([os.environ[\"PATH\"], new_path])\n                os.environ.update({\"PATH\": new_path})\n                log.debug(\"System $PATH: %s\" % os.environ[\"PATH\"])\n\n            modified_path = remove_program_from_path(path_copy, program_base)\n            update_path(env_copy, modified_path)\n\n            # register an _exithandler with the python interpreter:\n            atexit.register(update_path, env_copy, path_copy)\n\n            def remove_symlinked_binary(symlink):  # type: ignore[no-untyped-def]\n                if os.path.islink(symlink):\n                    os.unlink(symlink)\n                    log.debug(\"Removed binary symlink '%s'\" % symlink)\n\n            atexit.register(remove_symlinked_binary, new_gpg_location)",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "remove_program_from_path",
          "code": "def remove_program_from_path(path, prog_base):  # type: ignore[no-untyped-def]\n                \"\"\"Remove all directories which contain a program from PATH.\n\n                :param str path: The contents of the system environment's\n                                 ``$PATH``.\n\n                :param str prog_base: The directory portion of a program's\n                                      location, without the trailing slash,\n                                      and without the program name. For\n                                      example, ``prog_base='/usr/bin'``.\n                \"\"\"\n                paths = path.split(\":\")\n                for directory in paths:\n                    if directory == prog_base:\n                        log.debug(\"Found directory with target program: %s\" % directory)\n                        path.remove(directory)\n                        self._removed_path_entries.append(directory)\n                log.debug(\"Deleted all found instance of %s.\" % directory)\n                log.debug(f\"PATH is now:{os.linesep}{path}\")\n                return \":\".join([p for p in path])",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "update_path",
          "code": "def update_path(environment, path):  # type: ignore[no-untyped-def]\n                \"\"\"Add paths to the string at ``os.environ['PATH']``.\n\n                :param str environment: The environment mapping to update.\n                :param list path: A list of strings to update the PATH with.\n                \"\"\"\n                log.debug(\"Updating system path...\")\n                # This assignment doesn't reset the environment, but it does reset the monkey-patch\n                # as intended. Leaving as-is from upstream.\n                os.environ = environment  # noqa: B003\n                new_path = \":\".join([p for p in path])\n                if \"PATH\" in os.environ:\n                    new_path = \":\".join([os.environ[\"PATH\"], new_path])\n                os.environ.update({\"PATH\": new_path})\n                log.debug(\"System $PATH: %s\" % os.environ[\"PATH\"])",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "remove_symlinked_binary",
          "code": "def remove_symlinked_binary(symlink):  # type: ignore[no-untyped-def]\n                if os.path.islink(symlink):\n                    os.unlink(symlink)\n                    log.debug(\"Removed binary symlink '%s'\" % symlink)",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "default_preference_list",
          "code": "def default_preference_list(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the default preference list.\"\"\"\n        return self._prefs",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "default_preference_list",
          "code": "def default_preference_list(self, prefs):  # type: ignore[no-untyped-def]\n        \"\"\"Set the default preference list.\n\n        :param str prefs: A string containing the default preferences for\n                          ciphers, digests, and compression algorithms.\n        \"\"\"\n        prefs = _check_preferences(prefs)\n        if prefs is not None:\n            self._prefs = prefs",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "default_preference_list",
          "code": "def default_preference_list(self):  # type: ignore[no-untyped-def]\n        \"\"\"Reset the default preference list to its original state.\n\n        Note that \"original state\" does not mean the default preference\n        list for whichever version of GnuPG is being used. It means the\n        default preference list defined by :attr:`GPGBase._prefs`.\n\n        Using BZIP2 is avoided due to not interacting well with some versions\n        of GnuPG>=2.0.0.\n        \"\"\"\n        self._prefs = \"SHA512 SHA384 SHA256 AES256 CAMELLIA256 TWOFISH ZLIB ZIP\"",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "keyserver",
          "code": "def keyserver(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the current keyserver setting.\"\"\"\n        return self._keyserver",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "keyserver",
          "code": "def keyserver(self, location):  # type: ignore[no-untyped-def]\n        \"\"\"Set the default keyserver to use for sending and receiving keys.\n\n        The ``location`` is sent to :func:`_parsers._check_keyserver` when\n        option are parsed in :meth:`gnupg.GPG._make_options`.\n\n        :param str location: A string containing the default keyserver. This\n                             should contain the desired keyserver protocol\n                             which is supported by the keyserver, for example,\n                             ``'hkps://keys.mayfirst.org'``. The default\n                             keyserver is ``'hkp://wwwkeys.pgp.net'``.\n        \"\"\"\n        self._keyserver = location",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "keyserver",
          "code": "def keyserver(self):  # type: ignore[no-untyped-def]\n        \"\"\"Reset the keyserver to the default setting.\"\"\"\n        self._keyserver = \"hkp://wwwkeys.pgp.net\"",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "_homedir_getter",
          "code": "def _homedir_getter(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the directory currently being used as GnuPG's homedir.\n\n        If unspecified, use :file:`~/.config/python-gnupg/`\n\n        :rtype: str\n        :returns: The absolute path to the current GnuPG homedir.\n        \"\"\"\n        return self._homedir",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "_homedir_setter",
          "code": "def _homedir_setter(self, directory):  # type: ignore[no-untyped-def]\n        \"\"\"Set the directory to use as GnuPG's homedir.\n\n        If unspecified, use $HOME/.config/python-gnupg. If specified, ensure\n        that the ``directory`` does not contain various shell escape\n        characters. If ``directory`` is not found, it will be automatically\n        created. Lastly, the ``direcory`` will be checked that the EUID has\n        read and write permissions for it.\n\n        :param str directory: A relative or absolute path to the directory to\n                            use for storing/accessing GnuPG's files, including\n                            keyrings and the trustdb.\n        :raises: :exc:`~exceptions.RuntimeError` if unable to find a suitable\n                 directory to use.\n        \"\"\"\n        if not directory:\n            log.debug(\"GPGBase._homedir_setter(): Using default homedir: '%s'\" % _util._conf)\n            directory = _util._conf\n\n        hd = _parsers._fix_unsafe(directory)\n        log.debug(\"GPGBase._homedir_setter(): got directory '%s'\" % hd)\n\n        if hd:\n            log.debug(\"GPGBase._homedir_setter(): Check existence of '%s'\" % hd)\n            _util._create_if_necessary(hd)\n\n        if self.ignore_homedir_permissions:\n            self._homedir = hd\n        else:\n            try:\n                log.debug(\"GPGBase._homedir_setter(): checking permissions\")\n                assert _util._has_readwrite(hd), \"Homedir '%s' needs read/write permissions\" % hd\n            except AssertionError as ae:\n                msg = \"Unable to set '%s' as GnuPG homedir\" % directory\n                log.debug(\"GPGBase.homedir.setter(): %s\" % msg)\n                log.debug(str(ae))\n                raise RuntimeError(str(ae))\n            else:\n                log.info(\"Setting homedir to '%s'\" % hd)\n                self._homedir = hd",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "_generated_keys_getter",
          "code": "def _generated_keys_getter(self):  # type: ignore[no-untyped-def]\n        \"\"\"Get the ``homedir`` subdirectory for storing generated keys.\n\n        :rtype: str\n        :returns: The absolute path to the current GnuPG homedir.\n        \"\"\"\n        return self.__generated_keys",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "_generated_keys_setter",
          "code": "def _generated_keys_setter(self, directory):  # type: ignore[no-untyped-def]\n        \"\"\"Set the directory for storing generated keys.\n\n        If unspecified, use\n        :meth:`~gnupg._meta.GPGBase.homedir`/generated-keys. If specified,\n        ensure that the ``directory`` does not contain various shell escape\n        characters. If ``directory`` isn't found, it will be automatically\n        created. Lastly, the ``directory`` will be checked to ensure that the\n        current EUID has read and write permissions for it.\n\n        :param str directory: A relative or absolute path to the directory to\n             use for storing/accessing GnuPG's files, including keyrings and\n             the trustdb.\n        :raises: :exc:`~exceptions.RuntimeError` if unable to find a suitable\n             directory to use.\n        \"\"\"\n        if not directory:\n            directory = os.path.join(self.homedir, \"generated-keys\")\n            log.debug(\"GPGBase._generated_keys_setter(): Using '%s'\" % directory)\n\n        hd = _parsers._fix_unsafe(directory)\n        log.debug(\"GPGBase._generated_keys_setter(): got directory '%s'\" % hd)\n\n        if hd:\n            log.debug(\"GPGBase._generated_keys_setter(): Check exists '%s'\" % hd)\n            _util._create_if_necessary(hd)\n\n        try:\n            log.debug(\"GPGBase._generated_keys_setter(): check permissions\")\n            assert _util._has_readwrite(hd), \"Keys dir '%s' needs read/write permissions\" % hd\n        except AssertionError as ae:\n            msg = \"Unable to set '%s' as generated keys dir\" % directory\n            log.debug(\"GPGBase._generated_keys_setter(): %s\" % msg)\n            log.debug(str(ae))\n            raise RuntimeError(str(ae))\n        else:\n            log.info(\"Setting homedir to '%s'\" % hd)\n            self.__generated_keys = hd",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "_check_sane_and_get_gpg_version",
          "code": "def _check_sane_and_get_gpg_version(self):  # type: ignore[no-untyped-def]\n        \"\"\"Check that everything runs alright, and grab the gpg binary's\n        version number while we're at it, storing it as :data:`binary_version`.\n\n        :raises RuntimeError: if we cannot invoke the gpg binary.\n        \"\"\"\n        proc = self._open_subprocess([\"--list-config\", \"--with-colons\"])\n        result = self._result_map[\"list\"](self)\n        self._read_data(proc.stdout, result)\n        if proc.returncode:\n            raise RuntimeError(\"Error invoking gpg: %s\" % result.data)\n        else:\n            try:\n                proc.terminate()\n            except OSError:\n                log.error(\n                    \"Could neither invoke nor terminate a gpg process... \"\n                    \"Are you sure you specified the corrent (and full) \"\n                    \"path to the gpg binary?\"\n                )\n\n        version_line = result.data.partition(b\":version:\")[2].decode()\n        if not version_line:\n            raise RuntimeError(\"Got invalid version line from gpg: %s\\n\" % result.data)\n        self.binary_version = version_line.split(\"\\n\")[0]\n        if not _VERSION_RE.match(self.binary_version):\n            raise RuntimeError(\"Got invalid version line from gpg: %s\\n\" % self.binary_version)\n        log.debug(\"Using GnuPG version %s\" % self.binary_version)",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "_make_args",
          "code": "def _make_args(self, args, passphrase=False):  # type: ignore[no-untyped-def]\n        \"\"\"Make a list of command line elements for GPG.\n\n        The value of ``args`` will be appended only if it passes the checks in\n        :func:`gnupg._parsers._sanitise`. The ``passphrase`` argument needs to\n        be True if a passphrase will be sent to GnuPG, else False.\n\n        :param list args: A list of strings of options and flags to pass to\n                          ``GPG.binary``. This is input safe, meaning that\n                          these values go through strict checks (see\n                          ``parsers._sanitise_list``) before being passed to to\n                          the input file descriptor for the GnuPG process.\n                          Each string should be given exactly as it would be on\n                          the commandline interface to GnuPG,\n                          e.g. [\"--cipher-algo AES256\", \"--default-key\n                          A3ADB67A2CDB8B35\"].\n\n        :param bool passphrase: If True, the passphrase will be sent to the\n                                stdin file descriptor for the attached GnuPG\n                                process.\n        \"\"\"\n        # see TODO file, tag :io:makeargs:\n        cmd = [self.binary, \"--no-options --no-emit-version --no-tty --status-fd 2\"]\n\n        if self.homedir:\n            cmd.append('--homedir \"%s\"' % self.homedir)\n\n        if self.keyring:\n            cmd.append(\"--no-default-keyring --keyring %s\" % self.keyring)\n        if self.secring and self.binary_version != \"2.4.4\":\n            # In GnuPG 2.4.4, --secret-keyring has no effect\n            cmd.append(\"--secret-keyring %s\" % self.secring)\n\n        if passphrase:\n            cmd.append(\"--batch --passphrase-fd 0\")\n\n        if self.use_agent is True:\n            cmd.append(\"--use-agent\")\n        elif self.use_agent is False:\n            cmd.append(\"--no-use-agent\")\n\n        # The arguments for debugging and verbosity should be placed into the\n        # cmd list before the options/args in order to resolve Issue #76:\n        # https://github.com/isislovecruft/python-gnupg/issues/76\n        if self.verbose:\n            cmd.append(\"--debug-all\")\n\n            if isinstance(self.verbose, str) or (\n                isinstance(self.verbose, int) and (self.verbose >= 1)\n            ):\n                # GnuPG<=1.4.18 parses the `--debug-level` command in a way\n                # that is incompatible with all other GnuPG versions. :'(\n                if self.binary_version and (self.binary_version <= \"1.4.18\"):\n                    cmd.append(\"--debug-level=%s\" % self.verbose)\n                else:\n                    cmd.append(\"--debug-level %s\" % self.verbose)\n\n        if self.options:\n            [cmd.append(opt) for opt in iter(_sanitise_list(self.options))]\n        if args:\n            [cmd.append(arg) for arg in iter(_sanitise_list(args))]\n\n        return cmd",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "_open_subprocess",
          "code": "def _open_subprocess(self, args=None, passphrase=False):  # type: ignore[no-untyped-def]\n        \"\"\"Open a pipe to a GPG subprocess and return the file objects for\n        communicating with it.\n\n        :param list args: A list of strings of options and flags to pass to\n                          ``GPG.binary``. This is input safe, meaning that\n                          these values go through strict checks (see\n                          ``parsers._sanitise_list``) before being passed to to\n                          the input file descriptor for the GnuPG process.\n                          Each string should be given exactly as it would be on\n                          the commandline interface to GnuPG,\n                          e.g. [\"--cipher-algo AES256\", \"--default-key\n                          A3ADB67A2CDB8B35\"].\n\n        :param bool passphrase: If True, the passphrase will be sent to the\n                                stdin file descriptor for the attached GnuPG\n                                process.\n        \"\"\"\n        # see http://docs.python.org/2/library/subprocess.html#converting-an\\\n        #    -argument-sequence-to-a-string-on-windows\n        cmd = shlex.split(\" \".join(self._make_args(args, passphrase)))\n        log.debug(f\"Sending command to GnuPG process:{os.linesep}{cmd}\")\n\n        environment = {\n            \"LANGUAGE\": os.environ.get(\"LANGUAGE\") or \"en\",\n            \"GPG_TTY\": os.environ.get(\"GPG_TTY\") or \"\",\n            \"DISPLAY\": os.environ.get(\"DISPLAY\") or \"\",\n            \"GPG_AGENT_INFO\": os.environ.get(\"GPG_AGENT_INFO\") or \"\",\n            \"GPG_PINENTRY_PATH\": os.environ.get(\"GPG_PINENTRY_PATH\") or \"\",\n        }\n\n        return subprocess.Popen(\n            cmd,\n            shell=False,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            env=environment,\n        )",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "_read_response",
          "code": "def _read_response(self, stream, result):  # type: ignore[no-untyped-def]\n        \"\"\"Reads all the stderr output from GPG, taking notice only of lines\n        that begin with the magic [GNUPG:] prefix.\n\n        Calls methods on the response object for each valid token found, with\n        the arg being the remainder of the status line.\n\n        :param stream: A byte-stream, file handle, or a\n                       :data:`subprocess.PIPE` for parsing the status codes\n                       from the GnuPG process.\n\n        :param result: The result parser class from :mod:`~gnupg._parsers` ―\n                       the ``handle_status()`` method of that class will be\n                       called in order to parse the output of ``stream``.\n        \"\"\"\n        # All of the userland messages (i.e. not status-fd lines) we're not\n        # interested in passing to our logger\n        userland_messages_to_ignore = []\n\n        if self.ignore_homedir_permissions:\n            userland_messages_to_ignore.append(\"unsafe ownership on homedir\")\n\n        lines = []\n\n        while True:\n            line = stream.readline()\n            if len(line) == 0:\n                break\n            lines.append(line)\n            line = line.rstrip()\n\n            if line.startswith(\"[GNUPG:]\"):\n                line = _util._deprefix(line, \"[GNUPG:] \", log.status)\n                keyword, value = _util._separate_keyword(line)\n                result._handle_status(keyword, value)\n            elif line.startswith(\"gpg:\"):\n                line = _util._deprefix(line, \"gpg: \")\n                keyword, value = _util._separate_keyword(line)\n\n                # Silence warnings from gpg we're supposed to ignore\n                ignore = any(msg in value for msg in userland_messages_to_ignore)\n\n                if not ignore:\n                    # Log gpg's userland messages at our own levels:\n                    if keyword.upper().startswith(\"WARNING\"):\n                        log.warn(\"%s\" % value)\n                    elif keyword.upper().startswith(\"FATAL\"):\n                        log.critical(\"%s\" % value)\n                        # Handle the gpg2 error where a missing trustdb.gpg is,\n                        # for some stupid reason, considered fatal:\n                        if value.find(\"trustdb.gpg\") and value.find(\"No such file\"):\n                            result._handle_status(\"NEED_TRUSTDB\", \"\")\n            elif self.verbose:\n                log.info(\"%s\" % line)\n            else:\n                log.debug(\"%s\" % line)\n        result.stderr = \"\".join(lines)",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "_read_data",
          "code": "def _read_data(self, stream, result):  # type: ignore[no-untyped-def]\n        \"\"\"Incrementally read from ``stream`` and store read data.\n\n        All data gathered from calling ``stream.read()`` will be concatenated\n        and stored as ``result.data``.\n\n        :param stream: An open file-like object to read() from.\n        :param result: An instance of one of the :ref:`result parsing classes\n            <parsers>` from :const:`~gnupg._meta.GPGBase._result_map`.\n        \"\"\"\n        chunks = []\n        log.debug(\"Reading data from stream %r...\" % stream.__repr__())\n\n        while True:\n            data = stream.read(1024)\n            if len(data) == 0:\n                break\n            chunks.append(data)\n            log.debug(\"Read %4d bytes\" % len(data))\n\n        # Join using b'' or '', as appropriate\n        result.data = type(data)().join(chunks)\n        log.debug(\"Finishing reading from stream %r...\" % stream.__repr__())\n        log.debug(\"Read %4d bytes total\" % len(result.data))",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "_set_verbose",
          "code": "def _set_verbose(self, verbose):  # type: ignore[no-untyped-def]\n        \"\"\"Check and set our :data:`verbose` attribute.\n        The debug-level must be a string or an integer. If it is one of\n        the allowed strings, GnuPG will translate it internally to it's\n        corresponding integer level:\n\n        basic     = 1-2\n        advanced  = 3-5\n        expert    = 6-8\n        guru      = 9+\n\n        If it's not one of the recognised string levels, then then\n        entire argument is ignored by GnuPG. :(\n\n        To fix that stupid behaviour, if they wanted debugging but typo'd\n        the string level (or specified ``verbose=True``), we'll default to\n        'basic' logging.\n        \"\"\"\n        string_levels = (\"basic\", \"advanced\", \"expert\", \"guru\")\n\n        if verbose is True:\n            # The caller wants logging, but we need a valid --debug-level\n            # for gpg. Default to \"basic\", and warn about the ambiguity.\n            verbose = \"basic\"\n\n        if isinstance(verbose, str) and verbose not in string_levels:\n            verbose = \"basic\"\n\n        self.verbose = verbose",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "_collect_output",
          "code": "def _collect_output(self, process, result, writer=None, stdin=None):  # type: ignore[no-untyped-def]\n        \"\"\"Drain the subprocesses output streams, writing the collected output\n        to the result. If a writer thread (writing to the subprocess) is given,\n        make sure it's joined before returning. If a stdin stream is given,\n        close it before returning.\n        \"\"\"\n        stderr = codecs.getreader(self._encoding)(process.stderr)\n        rr = threading.Thread(target=self._read_response, args=(stderr, result))\n        rr.setDaemon(True)\n        log.debug(\"stderr reader: %r\", rr)\n        rr.start()\n\n        stdout = process.stdout\n        dr = threading.Thread(target=self._read_data, args=(stdout, result))\n        dr.setDaemon(True)\n        log.debug(\"stdout reader: %r\", dr)\n        dr.start()\n\n        dr.join()\n        rr.join()\n        if writer is not None:\n            writer.join()\n        process.wait()\n        if stdin is not None:\n            try:\n                stdin.close()\n            except OSError:\n                pass\n        stderr.close()\n        stdout.close()",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "_handle_io",
          "code": "def _handle_io(self, args, file, result, passphrase=False, binary=False):  # type: ignore[no-untyped-def]\n        \"\"\"Handle a call to GPG - pass input data, collect output data.\"\"\"\n        p = self._open_subprocess(args, passphrase)\n        if not binary:\n            stdin = codecs.getwriter(self._encoding)(p.stdin)\n        else:\n            stdin = p.stdin\n        if passphrase:\n            _util._write_passphrase(stdin, passphrase, self._encoding)\n        writer = _util._threaded_copy_data(file, stdin)\n        self._collect_output(p, result, writer, stdin)\n        return result",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "_recv_keys",
          "code": "def _recv_keys(self, keyids, keyserver=None):  # type: ignore[no-untyped-def]\n        \"\"\"Import keys from a keyserver.\n\n        :param str keyids: A space-delimited string containing the keyids to\n                           request.\n        :param str keyserver: The keyserver to request the ``keyids`` from;\n                              defaults to `gnupg.GPG.keyserver`.\n        \"\"\"\n        if not keyserver:\n            keyserver = self.keyserver\n\n        args = [f\"--keyserver {keyserver}\", f\"--recv-keys {keyids}\"]\n        log.info(f\"Requesting keys from {keyserver}: {keyids}\")\n\n        result = self._result_map[\"import\"](self)\n        proc = self._open_subprocess(args)\n        self._collect_output(proc, result)\n        log.debug(\"recv_keys result: %r\", result.__dict__)\n        return result",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "_sign_file",
          "code": "def _sign_file(  # type: ignore[no-untyped-def]\n        self,\n        file,\n        default_key=None,\n        passphrase=None,\n        clearsign=True,\n        detach=False,\n        binary=False,\n        digest_algo=\"SHA512\",\n    ):\n        \"\"\"Create a signature for a file.\n\n        :param file: The file stream (i.e. it's already been open()'d) to sign.\n        :param str default_key: The key to sign with.\n        :param str passphrase: The passphrase to pipe to stdin.\n        :param bool clearsign: If True, create a cleartext signature.\n        :param bool detach: If True, create a detached signature.\n        :param bool binary: If True, do not ascii armour the output.\n        :param str digest_algo: The hash digest to use. Again, to see which\n                                hashes your GnuPG is capable of using, do:\n                                ``$ gpg --with-colons --list-config\n                                digestname``. The default, if unspecified, is\n                                ``'SHA512'``.\n        \"\"\"\n        log.debug(\"_sign_file():\")\n        if binary:\n            log.info(\"Creating binary signature for file %s\" % file)\n            args = [\"--sign\"]\n        else:\n            log.info(\"Creating ascii-armoured signature for file %s\" % file)\n            args = [\"--sign --armor\"]\n\n        if clearsign:\n            args.append(\"--clearsign\")\n            if detach:\n                log.warn(\"Cannot use both --clearsign and --detach-sign.\")\n                log.warn(\"Using default GPG behaviour: --clearsign only.\")\n        elif detach and not clearsign:\n            args.append(\"--detach-sign\")\n\n        if default_key:\n            args.append(str(\"--default-key %s\" % default_key))\n\n        args.append(str(\"--digest-algo %s\" % digest_algo))\n\n        # We could use _handle_io here except for the fact that if the\n        # passphrase is bad, gpg bails and you can't write the message.\n        result = self._result_map[\"sign\"](self)\n\n        # If the passphrase is an empty string, the message up to and\n        # including its first newline will be cut off before making it to the\n        # GnuPG process. Therefore, if the passphrase='' or passphrase=b'',\n        # we set passphrase=None.  See Issue #82:\n        # https://github.com/isislovecruft/python-gnupg/issues/82\n        if isinstance(passphrase, str):\n            passphrase = passphrase if len(passphrase) > 0 else None\n        elif isinstance(passphrase, (bytes, bytearray)):\n            passphrase = passphrase.decode() if len(passphrase) > 0 else None\n        else:\n            passphrase = None\n\n        proc = self._open_subprocess(args, passphrase is not None)\n        try:\n            if passphrase:\n                _util._write_passphrase(proc.stdin, passphrase, self._encoding)\n            writer = _util._threaded_copy_data(file, proc.stdin)\n        except OSError as ioe:\n            log.exception(\"Error writing message: %s\" % str(ioe))\n            writer = None\n        self._collect_output(proc, result, writer, proc.stdin)\n        return result",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "_encrypt",
          "code": "def _encrypt(  # type: ignore[no-untyped-def]\n        self,\n        data,\n        recipients,\n        default_key=None,\n        passphrase=None,\n        armor=True,\n        encrypt=True,\n        symmetric=False,\n        always_trust=True,\n        output=None,\n        throw_keyids=False,\n        hidden_recipients=None,\n        cipher_algo=\"AES256\",\n        digest_algo=\"SHA512\",\n        compress_algo=\"ZLIB\",\n    ):\n        \"\"\"Encrypt the message read from the file-like object **data**.\n\n        :param str data: The file or bytestream to encrypt.\n\n        :param str recipients: The recipients to encrypt to. Recipients must\n                               be specified keyID/fingerprint.\n\n        .. warning:: Care should be taken in Python2 to make sure that the\n                     given fingerprints for **recipients** are in fact strings\n                     and not unicode objects.\n\n        :param str default_key: The keyID/fingerprint of the key to use for\n                                signing. If given, **data** will be encrypted\n                                *and* signed.\n\n        :param str passphrase: If given, and **default_key** is also given,\n                               use this passphrase to unlock the secret\n                               portion of the **default_key** to sign the\n                               encrypted **data**.  Otherwise, if\n                               **default_key** is not given, but **symmetric**\n                               is ``True``, then use this passphrase as the\n                               passphrase for symmetric encryption. Signing\n                               and symmetric encryption should *not* be\n                               combined when sending the **data** to other\n                               recipients, else the passphrase to the secret\n                               key would be shared with them.\n\n        :param bool armor: If True, ascii armor the output; otherwise, the\n                           output will be in binary format. (Default: True)\n\n        :param bool encrypt: If True, encrypt the **data** using the\n                             **recipients** public keys. (Default: True)\n\n        :param bool symmetric: If True, encrypt the **data** to **recipients**\n                               using a symmetric key. See the **passphrase**\n                               parameter. Symmetric encryption and public key\n                               encryption can be used simultaneously, and will\n                               result in a ciphertext which is decryptable\n                               with either the symmetric **passphrase** or one\n                               of the corresponding private keys.\n\n        :param bool always_trust: If True, ignore trust warnings on\n                                  **recipients** keys. If False, display trust\n                                  warnings. (default: True)\n\n        :type output: str or file-like object\n        :param output: The output file to write to. If not specified, the\n                       encrypted output is returned, and thus should be stored\n                       as an object in Python. For example:\n\n        >>> import shutil\n        >>> import gnupg\n        >>> if os.path.exists(\"doctests\"):\n        ...     shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_settings = gpg.gen_key_input(key_type='RSA',\n        ...                                  key_length=1024,\n        ...                                  key_usage='ESCA',\n        ...                                  passphrase='foo')\n        >>> key = gpg.gen_key(key_settings)\n        >>> message = \"The crow flies at midnight.\"\n        >>> encrypted = str(gpg.encrypt(message, key.fingerprint))\n        >>> assert encrypted != message\n        >>> assert not encrypted.isspace()\n        >>> decrypted = str(gpg.decrypt(encrypted, passphrase='foo'))\n        >>> assert not decrypted.isspace()\n        >>> decrypted\n        'The crow flies at midnight.'\n\n\n        :param bool throw_keyids: If True, make all **recipients** keyids be\n            zero'd out in packet information. This is the same as using\n            **hidden_recipients** for all **recipients**. (Default: False).\n\n        :param list hidden_recipients: A list of recipients that should have\n            their keyids zero'd out in packet information.\n\n        :param str cipher_algo: The cipher algorithm to use. To see available\n                                algorithms with your version of GnuPG, do:\n                                :command:`$ gpg --with-colons --list-config\n                                ciphername`. The default **cipher_algo**, if\n                                unspecified, is ``'AES256'``.\n\n        :param str digest_algo: The hash digest to use. Again, to see which\n                                hashes your GnuPG is capable of using, do:\n                                :command:`$ gpg --with-colons --list-config\n                                digestname`.  The default, if unspecified, is\n                                ``'SHA512'``.\n\n        :param str compress_algo: The compression algorithm to use. Can be one\n                                  of ``'ZLIB'``, ``'BZIP2'``, ``'ZIP'``, or\n                                  ``'Uncompressed'``.\n        \"\"\"\n        args = []\n\n        # FIXME: GnuPG appears to ignore the --output directive when being\n        # programmatically driven. We'll handle the IO ourselves to fix this\n        # for now.\n        output_filename = None\n        if output:\n            if getattr(output, \"fileno\", None) is not None:\n                # avoid overwrite confirmation message\n                if getattr(output, \"name\", None) is not None:\n                    output_filename = output.name\n                    if os.path.exists(output.name):\n                        os.remove(output.name)\n                    # args.append('--output %s' % output.name)\n            else:\n                output_filename = output\n                if os.path.exists(output):\n                    os.remove(output)\n                # args.append('--output %s' % output)\n\n        if armor:\n            args.append(\"--armor\")\n        if always_trust:\n            args.append(\"--always-trust\")\n        if cipher_algo:\n            args.append(\"--cipher-algo %s\" % cipher_algo)\n        if compress_algo:\n            args.append(\"--compress-algo %s\" % compress_algo)\n\n        if default_key:\n            args.append(\"--sign\")\n            args.append(\"--default-key %s\" % default_key)\n            if digest_algo:\n                args.append(\"--digest-algo %s\" % digest_algo)\n\n        # both can be used at the same time for an encrypted file which\n        # is decryptable with a passphrase or secretkey.\n        if symmetric:\n            args.append(\"--symmetric\")\n        if encrypt:\n            args.append(\"--encrypt\")\n        if throw_keyids:\n            args.append(\"--throw-keyids\")\n\n        if len(recipients) >= 1:\n            log.debug(\n                f\"GPG.encrypt() called for recipients '{recipients}' with type '{type(recipients)}'\"\n            )\n\n            if isinstance(recipients, (list, tuple)):\n                for recp in recipients:\n                    if isinstance(recp, str):\n                        self._add_recipient_string(args, hidden_recipients, recp)\n\n            elif isinstance(recp, str):\n                for recp in recipients.split(\" \"):\n                    self._add_recipient_string(args, hidden_recipients, recp)\n                    # ...and now that we've proven py3k is better...\n            else:\n                log.debug(\"Don't know what to do with recipients: %r\" % recipients)\n\n        result = self._result_map[\"crypt\"](self)\n        log.debug(f\"Got data '{data}' with type '{type(data)}'.\")\n        self._handle_io(args, data, result, passphrase=passphrase, binary=True)\n        # Avoid writing raw encrypted bytes to terminal loggers and breaking\n        # them in that adorable way where they spew hieroglyphics until reset:\n        if armor:\n            log.debug(\"\\n%s\" % result.data)\n\n        if output_filename:\n            log.info(\"Writing encrypted output to file: %s\" % output_filename)\n            with open(output_filename, \"wb\") as fh:\n                fh.write(result.data)\n                fh.flush()\n                log.info(\"Encrypted output written successfully.\")\n\n        return result",
          "file": "_meta.py"
        },
        {
          "type": "function",
          "name": "_add_recipient_string",
          "code": "def _add_recipient_string(self, args, hidden_recipients, recipient):  # type: ignore[no-untyped-def]\n        if isinstance(hidden_recipients, (list, tuple)):\n            if [s for s in hidden_recipients if recipient in str(s)]:\n                args.append(\"--hidden-recipient %s\" % recipient)\n            else:\n                args.append(\"--recipient %s\" % recipient)\n        else:\n            args.append(\"--recipient %s\" % recipient)",
          "file": "_meta.py"
        }
      ]
    },
    "alembic": {
      "env.py": [
        {
          "type": "function",
          "name": "run_migrations_offline",
          "code": "def run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url, target_metadata=target_metadata, compare_type=True, literal_binds=True\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()",
          "file": "env.py"
        },
        {
          "type": "function",
          "name": "run_migrations_online",
          "code": "def run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n    connectable = engine_from_config(\n        config.get_section(config.config_ini_section), prefix=\"sqlalchemy.\", poolclass=pool.NullPool\n    )\n\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection,\n            target_metadata=target_metadata,\n            compare_type=True,\n            render_as_batch=True,\n        )\n\n        with context.begin_transaction():\n            context.run_migrations()",
          "file": "env.py"
        }
      ],
      "versions": {
        "17c559a7a685_pgp_public_keys.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    \"\"\"\n    Migrate public keys from the GPG keyring into the SQLite database\n\n    We iterate over all the secret keys in the keyring and see if we\n    can identify the corresponding Source record. If we can, and it\n    doesn't already have key material migrated, export the key and\n    save it in the database.\n    \"\"\"\n    try:\n        config = SecureDropConfig.get_current()\n    except ModuleNotFoundError:\n        # Fresh install, nothing to migrate\n        return\n\n    gpg = gnupg.GPG(\n        binary=\"gpg2\",\n        homedir=str(config.GPG_KEY_DIR),\n        options=[\"--pinentry-mode loopback\", \"--trust-model direct\"],\n    )\n    # Source keys all have a secret key, so we can filter on that\n    for keyinfo in gpg.list_keys(secret=True):\n        if len(keyinfo[\"uids\"]) > 1:\n            # Source keys should only have one UID\n            continue\n        uid = keyinfo[\"uids\"][0]\n        search = EncryptionManager.SOURCE_KEY_UID_RE.search(uid)\n        if not search:\n            # Didn't match at all\n            continue\n        filesystem_id = search.group(2)\n        # Check that it's a valid ID\n        conn = op.get_bind()\n        result = conn.execute(\n            sa.text(\n                \"\"\"\n                SELECT pgp_public_key, pgp_fingerprint\n                FROM sources\n                WHERE filesystem_id=:filesystem_id\n                \"\"\"\n            ).bindparams(filesystem_id=filesystem_id)\n        ).first()\n        if result != (None, None):\n            # Either not in the database or there's already some data in the DB.\n            # In any case, skip.\n            continue\n        fingerprint = keyinfo[\"fingerprint\"]\n        try:\n            public_key = gpg.export_keys(fingerprint)\n            redwood.is_valid_public_key(public_key)\n        except:  # noqa: E722\n            # Exporting the key failed in some manner\n            traceback.print_exc()\n            continue\n\n        # Save to database\n        op.execute(\n            sa.text(\n                \"\"\"\n                UPDATE sources\n                SET pgp_public_key=:pgp_public_key, pgp_fingerprint=:pgp_fingerprint\n                WHERE filesystem_id=:filesystem_id\n                \"\"\"\n            ).bindparams(\n                pgp_public_key=public_key,\n                pgp_fingerprint=fingerprint,\n                filesystem_id=filesystem_id,\n            )\n        )",
            "file": "17c559a7a685_pgp_public_keys.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    \"\"\"\n    This is a non-destructive operation, so it's not worth implementing a\n    migration from database storage to GPG.\n    \"\"\"",
            "file": "17c559a7a685_pgp_public_keys.py"
          }
        ],
        "523fff3f969c_add_versioned_instance_config.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"instance_config\",\n        sa.Column(\"version\", sa.Integer(), nullable=False),\n        sa.Column(\"valid_until\", sa.DateTime(), nullable=True),\n        sa.Column(\"allow_document_uploads\", sa.Boolean(), nullable=True),\n        sa.PrimaryKeyConstraint(\"version\"),\n        sa.UniqueConstraint(\"valid_until\"),\n    )\n    # ### end Alembic commands ###\n\n    # Data migration:  Since allow_document_uploads is the first\n    # instance_config setting (column), all we have to do is insert a\n    # row with its default value.\n    conn = op.get_bind()\n    conn.execute(\"\"\"INSERT INTO instance_config (allow_document_uploads) VALUES (1)\"\"\")",
            "file": "523fff3f969c_add_versioned_instance_config.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"instance_config\")\n    # ### end Alembic commands ###",
            "file": "523fff3f969c_add_versioned_instance_config.py"
          }
        ],
        "2e24fc7536e8_make_journalist_id_non_nullable.py": [
          {
            "type": "function",
            "name": "generate_passphrase_hash",
            "code": "def generate_passphrase_hash() -> str:\n    passphrase = PassphraseGenerator.get_default().generate_passphrase()\n    return argon2.PasswordHasher(**ARGON2_PARAMS).hash(passphrase)",
            "file": "2e24fc7536e8_make_journalist_id_non_nullable.py"
          },
          {
            "type": "function",
            "name": "create_deleted",
            "code": "def create_deleted() -> int:\n    \"\"\"manually insert a \"deleted\" journalist user.\n\n    We need to do it this way since the model will reflect the current state of\n    the schema, not what it is at the current migration step\n\n    It should be basically identical to what Journalist.get_deleted() does\n    \"\"\"\n    op.execute(\n        sa.text(\n            \"\"\"\\\n        INSERT INTO journalists (uuid, username, session_nonce, passphrase_hash, otp_secret)\n        VALUES (:uuid, \"deleted\", 0, :passphrase_hash, :otp_secret);\n        \"\"\"\n        ).bindparams(\n            uuid=str(uuid.uuid4()),\n            passphrase_hash=generate_passphrase_hash(),\n            otp_secret=two_factor.random_base32(),\n        )\n    )\n    # Get the autoincrement ID back\n    conn = op.get_bind()\n    result = conn.execute('SELECT id FROM journalists WHERE username=\"deleted\";').fetchall()\n    return result[0][0]",
            "file": "2e24fc7536e8_make_journalist_id_non_nullable.py"
          },
          {
            "type": "function",
            "name": "migrate_nulls",
            "code": "def migrate_nulls() -> None:\n    \"\"\"migrate existing journalist_id=NULL over to deleted or delete them\"\"\"\n    op.execute(\"DELETE FROM journalist_login_attempt WHERE journalist_id IS NULL;\")\n    op.execute(\"DELETE FROM revoked_tokens WHERE journalist_id IS NULL;\")\n    # Look to see if we have data to migrate\n    tables = (\"replies\", \"seen_files\", \"seen_messages\", \"seen_replies\")\n    needs_migration = []\n    conn = op.get_bind()\n    for table in tables:\n        result = conn.execute(\n            f\"SELECT 1 FROM {table} WHERE journalist_id IS NULL;\"  # noqa: S608\n        ).first()\n        if result is not None:\n            needs_migration.append(table)\n\n    if not needs_migration:\n        return\n\n    deleted_id = create_deleted()\n    for table in needs_migration:\n        # The seen_ tables have UNIQUE(fk_id, journalist_id), so the deleted journalist can only\n        # have seen each item once. It is possible multiple NULL journalist have seen the same thing\n        # so we do this update in two passes.\n        # First we update as many rows to point to the deleted journalist as possible, ignoring any\n        # unique key violations.\n        op.execute(\n            sa.text(\n                f\"UPDATE OR IGNORE {table} SET journalist_id=:journalist_id \"  # noqa: S608\n                \"WHERE journalist_id IS NULL;\"\n            ).bindparams(journalist_id=deleted_id)\n        )\n        # Then we delete any leftovers which had been ignored earlier.\n        op.execute(f\"DELETE FROM {table} WHERE journalist_id IS NULL\")  # noqa: S608",
            "file": "2e24fc7536e8_make_journalist_id_non_nullable.py"
          },
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    try:\n        # While this migration doesn't use SecureDrop config directly,\n        # it's transitively used via PassphraseGenerator\n        SecureDropConfig.get_current()\n    except ModuleNotFoundError:\n        # Fresh install, nothing to migrate\n        return\n\n    migrate_nulls()\n\n    with op.batch_alter_table(\"journalist_login_attempt\", schema=None) as batch_op:\n        batch_op.alter_column(\"journalist_id\", existing_type=sa.INTEGER(), nullable=False)\n\n    with op.batch_alter_table(\"replies\", schema=None) as batch_op:\n        batch_op.alter_column(\"journalist_id\", existing_type=sa.INTEGER(), nullable=False)\n\n    with op.batch_alter_table(\"revoked_tokens\", schema=None) as batch_op:\n        batch_op.alter_column(\"journalist_id\", existing_type=sa.INTEGER(), nullable=False)\n\n    with op.batch_alter_table(\"seen_files\", schema=None) as batch_op:\n        batch_op.alter_column(\"journalist_id\", existing_type=sa.INTEGER(), nullable=False)\n\n    with op.batch_alter_table(\"seen_messages\", schema=None) as batch_op:\n        batch_op.alter_column(\"journalist_id\", existing_type=sa.INTEGER(), nullable=False)\n\n    with op.batch_alter_table(\"seen_replies\", schema=None) as batch_op:\n        batch_op.alter_column(\"journalist_id\", existing_type=sa.INTEGER(), nullable=False)",
            "file": "2e24fc7536e8_make_journalist_id_non_nullable.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    # We do not un-migrate the data back to journalist_id=NULL\n\n    with op.batch_alter_table(\"seen_replies\", schema=None) as batch_op:\n        batch_op.alter_column(\"journalist_id\", existing_type=sa.INTEGER(), nullable=True)\n\n    with op.batch_alter_table(\"seen_messages\", schema=None) as batch_op:\n        batch_op.alter_column(\"journalist_id\", existing_type=sa.INTEGER(), nullable=True)\n\n    with op.batch_alter_table(\"seen_files\", schema=None) as batch_op:\n        batch_op.alter_column(\"journalist_id\", existing_type=sa.INTEGER(), nullable=True)\n\n    with op.batch_alter_table(\"revoked_tokens\", schema=None) as batch_op:\n        batch_op.alter_column(\"journalist_id\", existing_type=sa.INTEGER(), nullable=True)\n\n    with op.batch_alter_table(\"replies\", schema=None) as batch_op:\n        batch_op.alter_column(\"journalist_id\", existing_type=sa.INTEGER(), nullable=True)\n\n    with op.batch_alter_table(\"journalist_login_attempt\", schema=None) as batch_op:\n        batch_op.alter_column(\"journalist_id\", existing_type=sa.INTEGER(), nullable=True)",
            "file": "2e24fc7536e8_make_journalist_id_non_nullable.py"
          }
        ],
        "48a75abc0121_add_seen_tables.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    op.create_table(\n        \"seen_files\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"file_id\", sa.Integer(), nullable=False),\n        sa.Column(\"journalist_id\", sa.Integer(), nullable=True),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"file_id\", \"journalist_id\"),\n        sa.ForeignKeyConstraint([\"file_id\"], [\"submissions.id\"]),\n        sa.ForeignKeyConstraint([\"journalist_id\"], [\"journalists.id\"]),\n    )\n\n    op.create_table(\n        \"seen_messages\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"message_id\", sa.Integer(), nullable=False),\n        sa.Column(\"journalist_id\", sa.Integer(), nullable=True),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"message_id\", \"journalist_id\"),\n        sa.ForeignKeyConstraint([\"message_id\"], [\"submissions.id\"]),\n        sa.ForeignKeyConstraint([\"journalist_id\"], [\"journalists.id\"]),\n    )\n\n    op.create_table(\n        \"seen_replies\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"reply_id\", sa.Integer(), nullable=False),\n        sa.Column(\"journalist_id\", sa.Integer(), nullable=True),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"reply_id\", \"journalist_id\"),\n        sa.ForeignKeyConstraint([\"reply_id\"], [\"replies.id\"]),\n        sa.ForeignKeyConstraint([\"journalist_id\"], [\"journalists.id\"]),\n    )",
            "file": "48a75abc0121_add_seen_tables.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    op.drop_table(\"seen_files\")\n    op.drop_table(\"seen_messages\")\n    op.drop_table(\"seen_replies\")",
            "file": "48a75abc0121_add_seen_tables.py"
          }
        ],
        "b7f98cfd6a70_make_filesystem_id_non_nullable.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    # Not having a filesystem_id makes the source useless, so if any of those do exist, we'll\n    # delete them first, as part of this migration.\n    # Because we can't rely on SQLAlchemy's cascade deletion, we have to do it manually.\n    # First we delete out of replies/seen_files/seen_messages (things that refer to things\n    # (source_stars/submissions/replies) that refer to sources)\n    op.execute(\n        \"DELETE FROM seen_replies WHERE reply_id IN (\"\n        \"SELECT replies.id FROM replies \"\n        \"JOIN sources ON sources.id=replies.source_id \"\n        \"WHERE filesystem_id IS NULL)\"\n    )\n    op.execute(\n        \"DELETE FROM seen_files WHERE file_id IN (\"\n        \"SELECT submissions.id FROM submissions \"\n        \"JOIN sources ON sources.id=submissions.source_id \"\n        \"WHERE filesystem_id IS NULL)\"\n    )\n    op.execute(\n        \"DELETE FROM seen_messages WHERE message_id IN (\"\n        \"SELECT submissions.id FROM submissions \"\n        \"JOIN sources ON sources.id=submissions.source_id \"\n        \"WHERE filesystem_id IS NULL)\"\n    )\n    # Now things that directly refer to sources\n    for table in (\"source_stars\", \"submissions\", \"replies\"):\n        op.execute(\n            f\"DELETE FROM {table} WHERE source_id IN \"  # noqa: S608\n            f\"(SELECT id FROM sources WHERE filesystem_id IS NULL)\"\n        )\n    # And now the sources\n    op.execute(\"DELETE FROM sources WHERE filesystem_id IS NULL\")\n    with op.batch_alter_table(\"sources\", schema=None) as batch_op:\n        batch_op.alter_column(\"filesystem_id\", existing_type=sa.VARCHAR(length=96), nullable=False)",
            "file": "b7f98cfd6a70_make_filesystem_id_non_nullable.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    with op.batch_alter_table(\"sources\", schema=None) as batch_op:\n        batch_op.alter_column(\"filesystem_id\", existing_type=sa.VARCHAR(length=96), nullable=True)",
            "file": "b7f98cfd6a70_make_filesystem_id_non_nullable.py"
          }
        ],
        "b58139cfdc8c_add_checksum_columns_revoke_table.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    with op.batch_alter_table(\"replies\", schema=None) as batch_op:\n        batch_op.add_column(sa.Column(\"checksum\", sa.String(length=255), nullable=True))\n\n    with op.batch_alter_table(\"submissions\", schema=None) as batch_op:\n        batch_op.add_column(sa.Column(\"checksum\", sa.String(length=255), nullable=True))\n\n    op.create_table(\n        \"revoked_tokens\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"journalist_id\", sa.Integer(), nullable=True),\n        sa.Column(\"token\", sa.Text(), nullable=False),\n        sa.ForeignKeyConstraint([\"journalist_id\"], [\"journalists.id\"]),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"token\"),\n    )\n\n    try:\n        config = SecureDropConfig.get_current()\n    except ModuleNotFoundError:\n        # Fresh install, nothing to migrate\n        return\n\n    app = create_app(config)\n\n    # we need an app context for the rq worker extension to work properly\n    with app.app_context():\n        conn = op.get_bind()\n        query = sa.text(\n            \"\"\"SELECT submissions.id, sources.filesystem_id, submissions.filename\n                           FROM submissions\n                           INNER JOIN sources\n                           ON submissions.source_id = sources.id\n                        \"\"\"\n        )\n        for sub_id, filesystem_id, filename in conn.execute(query):\n            full_path = Storage.get_default().path(filesystem_id, filename)\n            create_queue(config.RQ_WORKER_NAME).enqueue(\n                queued_add_checksum_for_file,\n                Submission,\n                int(sub_id),\n                full_path,\n                app.config[\"SQLALCHEMY_DATABASE_URI\"],\n            )\n\n        query = sa.text(\n            \"\"\"SELECT replies.id, sources.filesystem_id, replies.filename\n                           FROM replies\n                           INNER JOIN sources\n                           ON replies.source_id = sources.id\n                        \"\"\"\n        )\n        for rep_id, filesystem_id, filename in conn.execute(query):\n            full_path = Storage.get_default().path(filesystem_id, filename)\n            create_queue(config.RQ_WORKER_NAME).enqueue(\n                queued_add_checksum_for_file,\n                Reply,\n                int(rep_id),\n                full_path,\n                app.config[\"SQLALCHEMY_DATABASE_URI\"],\n            )",
            "file": "b58139cfdc8c_add_checksum_columns_revoke_table.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    op.drop_table(\"revoked_tokens\")\n\n    with op.batch_alter_table(\"submissions\", schema=None) as batch_op:\n        batch_op.drop_column(\"checksum\")\n\n    with op.batch_alter_table(\"replies\", schema=None) as batch_op:\n        batch_op.drop_column(\"checksum\")",
            "file": "b58139cfdc8c_add_checksum_columns_revoke_table.py"
          }
        ],
        "1ddb81fb88c2_unique_index_for_instanceconfig_valid_.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    with op.batch_alter_table(\"instance_config\", schema=None) as batch_op:\n        batch_op.create_index(\n            \"ix_one_active_instance_config\",\n            [sa.text(\"valid_until IS NULL\")],\n            unique=True,\n            sqlite_where=sa.text(\"valid_until IS NULL\"),\n        )\n\n    # ### end Alembic commands ###",
            "file": "1ddb81fb88c2_unique_index_for_instanceconfig_valid_.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    with op.batch_alter_table(\"instance_config\", schema=None) as batch_op:\n        batch_op.drop_index(\"ix_one_active_instance_config\")\n\n    # ### end Alembic commands ###",
            "file": "1ddb81fb88c2_unique_index_for_instanceconfig_valid_.py"
          }
        ],
        "92fba0be98e9_added_organization_name_field_in_.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    with op.batch_alter_table(\"instance_config\", schema=None) as batch_op:\n        batch_op.add_column(sa.Column(\"organization_name\", sa.String(length=255), nullable=True))\n\n    # ### end Alembic commands ###",
            "file": "92fba0be98e9_added_organization_name_field_in_.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    with op.batch_alter_table(\"instance_config\", schema=None) as batch_op:\n        batch_op.drop_column(\"organization_name\")\n\n    # ### end Alembic commands ###",
            "file": "92fba0be98e9_added_organization_name_field_in_.py"
          }
        ],
        "a9fe328b053a_migrations_for_0_14_0.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    with op.batch_alter_table(\"journalists\", schema=None) as batch_op:\n        batch_op.add_column(sa.Column(\"first_name\", sa.String(length=255), nullable=True))\n        batch_op.add_column(sa.Column(\"last_name\", sa.String(length=255), nullable=True))",
            "file": "a9fe328b053a_migrations_for_0_14_0.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    with op.batch_alter_table(\"journalists\", schema=None) as batch_op:\n        batch_op.drop_column(\"last_name\")\n        batch_op.drop_column(\"first_name\")",
            "file": "a9fe328b053a_migrations_for_0_14_0.py"
          }
        ],
        "f2833ac34bb6_add_uuid_column_for_users_table.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    conn = op.get_bind()\n    conn.execute(\"PRAGMA legacy_alter_table=ON\")\n    # Save existing journalist table.\n    op.rename_table(\"journalists\", \"journalists_tmp\")\n\n    # Add UUID column.\n    op.add_column(\"journalists_tmp\", sa.Column(\"uuid\", sa.String(length=36)))\n\n    # Add UUIDs to journalists_tmp table.\n    journalists = conn.execute(sa.text(\"SELECT * FROM journalists_tmp\")).fetchall()\n\n    for journalist in journalists:\n        conn.execute(\n            sa.text(\n                \"\"\"UPDATE journalists_tmp SET uuid=:journalist_uuid WHERE\n                       id=:id\"\"\"\n            ).bindparams(journalist_uuid=str(uuid.uuid4()), id=journalist.id)\n        )\n\n    # Now create new table with unique constraint applied.\n    op.create_table(\n        \"journalists\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"uuid\", sa.String(length=36), nullable=False),\n        sa.Column(\"username\", sa.String(length=255), nullable=False),\n        sa.Column(\"pw_salt\", sa.LargeBinary(), nullable=True),\n        sa.Column(\"pw_hash\", sa.LargeBinary(), nullable=True),\n        sa.Column(\"passphrase_hash\", sa.String(length=256), nullable=True),\n        sa.Column(\"is_admin\", sa.Boolean(), nullable=True),\n        sa.Column(\"otp_secret\", sa.String(length=16), nullable=True),\n        sa.Column(\"is_totp\", sa.Boolean(), nullable=True),\n        sa.Column(\"hotp_counter\", sa.Integer(), nullable=True),\n        sa.Column(\"last_token\", sa.String(length=6), nullable=True),\n        sa.Column(\"created_on\", sa.DateTime(), nullable=True),\n        sa.Column(\"last_access\", sa.DateTime(), nullable=True),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"username\"),\n        sa.UniqueConstraint(\"uuid\"),\n    )\n\n    conn.execute(\n        \"\"\"\n        INSERT INTO journalists\n        SELECT id, uuid, username, pw_salt, pw_hash, passphrase_hash,\n               is_admin, otp_secret, is_totp, hotp_counter, last_token,\n               created_on, last_access\n        FROM journalists_tmp\n    \"\"\"\n    )\n\n    # Now delete the old table.\n    op.drop_table(\"journalists_tmp\")",
            "file": "f2833ac34bb6_add_uuid_column_for_users_table.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    with op.batch_alter_table(\"journalists\", schema=None) as batch_op:\n        batch_op.drop_column(\"uuid\")",
            "file": "f2833ac34bb6_add_uuid_column_for_users_table.py"
          }
        ],
        "15ac9509fc68_init.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    op.create_table(\n        \"journalists\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"username\", sa.String(length=255), nullable=False),\n        sa.Column(\"pw_salt\", sa.LargeBinary(), nullable=True),\n        sa.Column(\"pw_hash\", sa.LargeBinary(), nullable=True),\n        sa.Column(\"is_admin\", sa.Boolean(), nullable=True),\n        sa.Column(\"otp_secret\", sa.String(length=16), nullable=True),\n        sa.Column(\"is_totp\", sa.Boolean(), nullable=True),\n        sa.Column(\"hotp_counter\", sa.Integer(), nullable=True),\n        sa.Column(\"last_token\", sa.String(length=6), nullable=True),\n        sa.Column(\"created_on\", sa.DateTime(), nullable=True),\n        sa.Column(\"last_access\", sa.DateTime(), nullable=True),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"username\"),\n    )\n    op.create_table(\n        \"sources\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"filesystem_id\", sa.String(length=96), nullable=True),\n        sa.Column(\"journalist_designation\", sa.String(length=255), nullable=False),\n        sa.Column(\"flagged\", sa.Boolean(), nullable=True),\n        sa.Column(\"last_updated\", sa.DateTime(), nullable=True),\n        sa.Column(\"pending\", sa.Boolean(), nullable=True),\n        sa.Column(\"interaction_count\", sa.Integer(), nullable=False),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"filesystem_id\"),\n    )\n    op.create_table(\n        \"journalist_login_attempt\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"timestamp\", sa.DateTime(), nullable=True),\n        sa.Column(\"journalist_id\", sa.Integer(), nullable=True),\n        sa.ForeignKeyConstraint([\"journalist_id\"], [\"journalists.id\"]),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_table(\n        \"replies\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"journalist_id\", sa.Integer(), nullable=True),\n        sa.Column(\"source_id\", sa.Integer(), nullable=True),\n        sa.Column(\"filename\", sa.String(length=255), nullable=False),\n        sa.Column(\"size\", sa.Integer(), nullable=False),\n        sa.ForeignKeyConstraint([\"journalist_id\"], [\"journalists.id\"]),\n        sa.ForeignKeyConstraint([\"source_id\"], [\"sources.id\"]),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_table(\n        \"source_stars\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"source_id\", sa.Integer(), nullable=True),\n        sa.Column(\"starred\", sa.Boolean(), nullable=True),\n        sa.ForeignKeyConstraint([\"source_id\"], [\"sources.id\"]),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_table(\n        \"submissions\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"source_id\", sa.Integer(), nullable=True),\n        sa.Column(\"filename\", sa.String(length=255), nullable=False),\n        sa.Column(\"size\", sa.Integer(), nullable=False),\n        sa.Column(\"downloaded\", sa.Boolean(), nullable=True),\n        sa.ForeignKeyConstraint([\"source_id\"], [\"sources.id\"]),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )",
            "file": "15ac9509fc68_init.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    op.drop_table(\"submissions\")\n    op.drop_table(\"source_stars\")\n    op.drop_table(\"replies\")\n    op.drop_table(\"journalist_login_attempt\")\n    op.drop_table(\"sources\")\n    op.drop_table(\"journalists\")",
            "file": "15ac9509fc68_init.py"
          }
        ],
        "c5a02eb52f2d_dropped_session_nonce_from_journalist_.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"revoked_tokens\")\n    with op.batch_alter_table(\"journalists\", schema=None) as batch_op:\n        batch_op.drop_column(\"session_nonce\")\n\n    # ### end Alembic commands ###",
            "file": "c5a02eb52f2d_dropped_session_nonce_from_journalist_.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    \"\"\"This would have been the easy way, however previous does not have\n    default value and thus up/down assertion fails\"\"\"\n    # op.add_column('journalists', sa.Column('session_nonce', sa.Integer(),\n    #               nullable=False, server_default='0'))\n\n    conn = op.get_bind()\n    conn.execute(\"PRAGMA legacy_alter_table=ON\")\n    # Save existing journalist table.\n    op.rename_table(\"journalists\", \"journalists_tmp\")\n\n    # Add nonce column.\n    op.add_column(\"journalists_tmp\", sa.Column(\"session_nonce\", sa.Integer()))\n\n    # Populate nonce column.\n    journalists = conn.execute(sa.text(\"SELECT * FROM journalists_tmp\")).fetchall()\n\n    for journalist in journalists:\n        conn.execute(\n            sa.text(\n                \"\"\"UPDATE journalists_tmp SET session_nonce=0 WHERE\n                       id=:id\"\"\"\n            ).bindparams(id=journalist.id)\n        )\n\n    # Now create new table with null constraint applied.\n    op.create_table(\n        \"journalists\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"uuid\", sa.String(length=36), nullable=False),\n        sa.Column(\"username\", sa.String(length=255), nullable=False),\n        sa.Column(\"first_name\", sa.String(length=255), nullable=True),\n        sa.Column(\"last_name\", sa.String(length=255), nullable=True),\n        sa.Column(\"pw_salt\", sa.LargeBinary(), nullable=True),\n        sa.Column(\"pw_hash\", sa.LargeBinary(), nullable=True),\n        sa.Column(\"passphrase_hash\", sa.String(length=256), nullable=True),\n        sa.Column(\"is_admin\", sa.Boolean(), nullable=True),\n        sa.Column(\"session_nonce\", sa.Integer(), nullable=False),\n        sa.Column(\"otp_secret\", sa.String(length=32), nullable=True),\n        sa.Column(\"is_totp\", sa.Boolean(), nullable=True),\n        sa.Column(\"hotp_counter\", sa.Integer(), nullable=True),\n        sa.Column(\"last_token\", sa.String(length=6), nullable=True),\n        sa.Column(\"created_on\", sa.DateTime(), nullable=True),\n        sa.Column(\"last_access\", sa.DateTime(), nullable=True),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"username\"),\n        sa.UniqueConstraint(\"uuid\"),\n    )\n\n    conn.execute(\n        \"\"\"\n        INSERT INTO journalists\n        SELECT id, uuid, username, first_name, last_name, pw_salt, pw_hash,\n               passphrase_hash, is_admin, session_nonce, otp_secret, is_totp,\n               hotp_counter, last_token, created_on, last_access\n        FROM journalists_tmp\n    \"\"\"\n    )\n\n    # Now delete the old table.\n    op.drop_table(\"journalists_tmp\")\n\n    op.create_table(\n        \"revoked_tokens\",\n        sa.Column(\"id\", sa.INTEGER(), nullable=False),\n        sa.Column(\"journalist_id\", sa.INTEGER(), nullable=False),\n        sa.Column(\"token\", sa.TEXT(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"journalist_id\"],\n            [\"journalists.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"token\"),\n    )",
            "file": "c5a02eb52f2d_dropped_session_nonce_from_journalist_.py"
          }
        ],
        "811334d7105f_sequoia_pgp.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    with op.batch_alter_table(\"sources\", schema=None) as batch_op:\n        batch_op.add_column(sa.Column(\"pgp_fingerprint\", sa.String(length=40), nullable=True))\n        batch_op.add_column(sa.Column(\"pgp_public_key\", sa.Text(), nullable=True))\n        batch_op.add_column(sa.Column(\"pgp_secret_key\", sa.Text(), nullable=True))",
            "file": "811334d7105f_sequoia_pgp.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    # We do NOT drop the columns here, because doing so would break any\n    # source that had its key pair stored here. If a downgrade is needed for\n    # whatever reason, the extra columns will just be ignored, and the sources\n    # will still be temporarily broken, but there will be no data loss.\n    pass",
            "file": "811334d7105f_sequoia_pgp.py"
          }
        ],
        "fccf57ceef02_create_submission_uuid_column.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    conn = op.get_bind()\n    conn.execute(\"PRAGMA legacy_alter_table=ON\")\n    # Schema migration\n    op.rename_table(\"submissions\", \"submissions_tmp\")\n\n    # Add UUID column.\n    op.add_column(\"submissions_tmp\", sa.Column(\"uuid\", sa.String(length=36)))\n\n    # Add UUIDs to submissions_tmp table.\n    submissions = conn.execute(sa.text(\"SELECT * FROM submissions_tmp\")).fetchall()\n\n    for submission in submissions:\n        conn.execute(\n            sa.text(\n                \"\"\"UPDATE submissions_tmp SET uuid=:submission_uuid WHERE\n                       id=:id\"\"\"\n            ).bindparams(submission_uuid=str(uuid.uuid4()), id=submission.id)\n        )\n\n    # Now create new table with unique constraint applied.\n    op.create_table(\n        \"submissions\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"uuid\", sa.String(length=36), nullable=False),\n        sa.Column(\"source_id\", sa.Integer(), nullable=True),\n        sa.Column(\"filename\", sa.String(length=255), nullable=False),\n        sa.Column(\"size\", sa.Integer(), nullable=False),\n        sa.Column(\"downloaded\", sa.Boolean(), nullable=True),\n        sa.ForeignKeyConstraint([\"source_id\"], [\"sources.id\"]),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"uuid\"),\n    )\n\n    # Data Migration: move all submissions into the new table.\n    conn.execute(\n        \"\"\"\n        INSERT INTO submissions\n        SELECT id, uuid, source_id, filename, size, downloaded\n        FROM submissions_tmp\n    \"\"\"\n    )\n\n    # Now delete the old table.\n    op.drop_table(\"submissions_tmp\")",
            "file": "fccf57ceef02_create_submission_uuid_column.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    with op.batch_alter_table(\"submissions\", schema=None) as batch_op:\n        batch_op.drop_column(\"uuid\")",
            "file": "fccf57ceef02_create_submission_uuid_column.py"
          }
        ],
        "60f41bb14d98_add_session_nonce_to_journalist.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    conn = op.get_bind()\n    conn.execute(\"PRAGMA legacy_alter_table=ON\")\n    # Save existing journalist table.\n    op.rename_table(\"journalists\", \"journalists_tmp\")\n\n    # Add nonce column.\n    op.add_column(\"journalists_tmp\", sa.Column(\"session_nonce\", sa.Integer()))\n\n    # Populate nonce column.\n    journalists = conn.execute(sa.text(\"SELECT * FROM journalists_tmp\")).fetchall()\n\n    for journalist in journalists:\n        conn.execute(\n            sa.text(\n                \"\"\"UPDATE journalists_tmp SET session_nonce=0 WHERE\n                       id=:id\"\"\"\n            ).bindparams(id=journalist.id)\n        )\n\n    # Now create new table with null constraint applied.\n    op.create_table(\n        \"journalists\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"uuid\", sa.String(length=36), nullable=False),\n        sa.Column(\"username\", sa.String(length=255), nullable=False),\n        sa.Column(\"first_name\", sa.String(length=255), nullable=True),\n        sa.Column(\"last_name\", sa.String(length=255), nullable=True),\n        sa.Column(\"pw_salt\", sa.LargeBinary(), nullable=True),\n        sa.Column(\"pw_hash\", sa.LargeBinary(), nullable=True),\n        sa.Column(\"passphrase_hash\", sa.String(length=256), nullable=True),\n        sa.Column(\"is_admin\", sa.Boolean(), nullable=True),\n        sa.Column(\"session_nonce\", sa.Integer(), nullable=False),\n        sa.Column(\"otp_secret\", sa.String(length=16), nullable=True),\n        sa.Column(\"is_totp\", sa.Boolean(), nullable=True),\n        sa.Column(\"hotp_counter\", sa.Integer(), nullable=True),\n        sa.Column(\"last_token\", sa.String(length=6), nullable=True),\n        sa.Column(\"created_on\", sa.DateTime(), nullable=True),\n        sa.Column(\"last_access\", sa.DateTime(), nullable=True),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"username\"),\n        sa.UniqueConstraint(\"uuid\"),\n    )\n\n    conn.execute(\n        \"\"\"\n        INSERT INTO journalists\n        SELECT id, uuid, username, first_name, last_name, pw_salt, pw_hash,\n               passphrase_hash, is_admin, session_nonce, otp_secret, is_totp,\n               hotp_counter, last_token, created_on, last_access\n        FROM journalists_tmp\n    \"\"\"\n    )\n\n    # Now delete the old table.\n    op.drop_table(\"journalists_tmp\")",
            "file": "60f41bb14d98_add_session_nonce_to_journalist.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    with op.batch_alter_table(\"journalists\", schema=None) as batch_op:\n        batch_op.drop_column(\"session_nonce\")\n\n    # ### end Alembic commands ###",
            "file": "60f41bb14d98_add_session_nonce_to_journalist.py"
          }
        ],
        "d9d36b6f4d1e_remove_partial_index_on_valid_until_set_.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    # remove the old partial index on valid_until, as alembic can't handle it.\n    op.execute(sa.text(\"DROP INDEX IF EXISTS ix_one_active_instance_config\"))\n\n    # valid_until will be non-nullable after batch, so set existing nulls to\n    # the new default unix epoch datetime\n    op.execute(\n        sa.text(\n            \"UPDATE OR IGNORE instance_config SET \" \"valid_until=:epoch WHERE valid_until IS NULL;\"\n        ).bindparams(epoch=datetime.fromtimestamp(0))\n    )\n\n    # add new columns with default values\n    with op.batch_alter_table(\"instance_config\", schema=None) as batch_op:\n        batch_op.add_column(\n            sa.Column(\"initial_message_min_len\", sa.Integer(), nullable=False, server_default=\"0\")\n        )\n        batch_op.add_column(\n            sa.Column(\n                \"reject_message_with_codename\", sa.Boolean(), nullable=False, server_default=\"0\"\n            )\n        )\n        batch_op.alter_column(\"valid_until\", existing_type=sa.DATETIME(), nullable=False)\n\n    # remove the old partial index *again* in case the batch op recreated it.\n    op.execute(sa.text(\"DROP INDEX IF EXISTS ix_one_active_instance_config\"))\n\n    # ### end Alembic commands ###",
            "file": "d9d36b6f4d1e_remove_partial_index_on_valid_until_set_.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    with op.batch_alter_table(\"instance_config\", schema=None) as batch_op:\n        batch_op.alter_column(\"valid_until\", existing_type=sa.DATETIME(), nullable=True)\n        batch_op.drop_column(\"reject_message_with_codename\")\n        batch_op.drop_column(\"initial_message_min_len\")\n\n    # valid_until is nullable again, set entries with unix epoch datetime value to NULL\n    op.execute(\n        sa.text(\n            \"UPDATE OR IGNORE instance_config SET \" \"valid_until = NULL WHERE valid_until=:epoch;\"\n        ).bindparams(epoch=datetime.fromtimestamp(0))\n    )\n\n    # manually restore the partial index\n    op.execute(\n        sa.text(\n            \"CREATE UNIQUE INDEX IF NOT EXISTS ix_one_active_instance_config ON \"\n            \"instance_config (valid_until IS NULL) WHERE valid_until IS NULL\"\n        )\n    )\n\n    # ### end Alembic commands ###",
            "file": "d9d36b6f4d1e_remove_partial_index_on_valid_until_set_.py"
          }
        ],
        "2d0ce3ee5bdc_added_passphrase_hash_column_to_.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    op.add_column(\"journalists\", sa.Column(\"passphrase_hash\", sa.String(length=256), nullable=True))",
            "file": "2d0ce3ee5bdc_added_passphrase_hash_column_to_.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    # sqlite has no `drop column` command, so we recreate the original table\n    # then load it from a temp table\n    conn = op.get_bind()\n    conn.execute(\"PRAGMA legacy_alter_table=ON\")\n    op.rename_table(\"journalists\", \"journalists_tmp\")\n\n    op.create_table(\n        \"journalists\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"username\", sa.String(length=255), nullable=False),\n        sa.Column(\"pw_salt\", sa.LargeBinary(), nullable=True),\n        sa.Column(\"pw_hash\", sa.LargeBinary(), nullable=True),\n        sa.Column(\"is_admin\", sa.Boolean(), nullable=True),\n        sa.Column(\"otp_secret\", sa.String(length=16), nullable=True),\n        sa.Column(\"is_totp\", sa.Boolean(), nullable=True),\n        sa.Column(\"hotp_counter\", sa.Integer(), nullable=True),\n        sa.Column(\"last_token\", sa.String(length=6), nullable=True),\n        sa.Column(\"created_on\", sa.DateTime(), nullable=True),\n        sa.Column(\"last_access\", sa.DateTime(), nullable=True),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"username\"),\n    )\n\n    conn.execute(\n        \"\"\"\n        INSERT INTO journalists\n        SELECT id, username, pw_salt, pw_hash, is_admin, otp_secret, is_totp,\n               hotp_counter, last_token, created_on, last_access\n        FROM journalists_tmp\n    \"\"\"\n    )\n\n    op.drop_table(\"journalists_tmp\")",
            "file": "2d0ce3ee5bdc_added_passphrase_hash_column_to_.py"
          }
        ],
        "3d91d6948753_create_source_uuid_column.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    conn = op.get_bind()\n    conn.execute(\"PRAGMA legacy_alter_table=ON\")\n    # Schema migration\n    op.rename_table(\"sources\", \"sources_tmp\")\n\n    # Add UUID column.\n    op.add_column(\"sources_tmp\", sa.Column(\"uuid\", sa.String(length=36)))\n\n    # Add UUIDs to sources_tmp table.\n    sources = conn.execute(sa.text(\"SELECT * FROM sources_tmp\")).fetchall()\n\n    for source in sources:\n        conn.execute(\n            sa.text(\n                \"\"\"UPDATE sources_tmp SET uuid=:source_uuid WHERE\n                       id=:id\"\"\"\n            ).bindparams(source_uuid=str(uuid.uuid4()), id=source.id)\n        )\n\n    # Now create new table with unique constraint applied.\n    op.create_table(\n        \"sources\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"uuid\", sa.String(length=36), nullable=False),\n        sa.Column(\"filesystem_id\", sa.String(length=96), nullable=True),\n        sa.Column(\"journalist_designation\", sa.String(length=255), nullable=False),\n        sa.Column(\"flagged\", sa.Boolean(), nullable=True),\n        sa.Column(\"last_updated\", sa.DateTime(), nullable=True),\n        sa.Column(\"pending\", sa.Boolean(), nullable=True),\n        sa.Column(\"interaction_count\", sa.Integer(), nullable=False),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"uuid\"),\n        sa.UniqueConstraint(\"filesystem_id\"),\n    )\n\n    # Data Migration: move all sources into the new table.\n    conn.execute(\n        \"\"\"\n        INSERT INTO sources\n        SELECT id, uuid, filesystem_id, journalist_designation, flagged,\n               last_updated, pending, interaction_count\n        FROM sources_tmp\n    \"\"\"\n    )\n\n    # Now delete the old table.\n    op.drop_table(\"sources_tmp\")",
            "file": "3d91d6948753_create_source_uuid_column.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    with op.batch_alter_table(\"sources\", schema=None) as batch_op:\n        batch_op.drop_column(\"uuid\")",
            "file": "3d91d6948753_create_source_uuid_column.py"
          }
        ],
        "de00920916bf_updates_journalists_otp_secret_length_.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    with op.batch_alter_table(\"journalists\", schema=None) as batch_op:\n        batch_op.alter_column(\n            \"otp_secret\",\n            existing_type=sa.VARCHAR(length=16),\n            type_=sa.String(length=32),\n            existing_nullable=True,\n        )\n\n    # ### end Alembic commands ###",
            "file": "de00920916bf_updates_journalists_otp_secret_length_.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    with op.batch_alter_table(\"journalists\", schema=None) as batch_op:\n        batch_op.alter_column(\n            \"otp_secret\",\n            existing_type=sa.String(length=32),\n            type_=sa.VARCHAR(length=16),\n            existing_nullable=True,\n        )\n\n    # ### end Alembic commands ###",
            "file": "de00920916bf_updates_journalists_otp_secret_length_.py"
          }
        ],
        "3da3fcab826a_delete_orphaned_submissions.py": [
          {
            "type": "function",
            "name": "raw_sql_grab_orphaned_objects",
            "code": "def raw_sql_grab_orphaned_objects(table_name: str) -> str:\n    \"\"\"Objects that have a source ID that doesn't exist in the\n    sources table OR a NULL source ID should be deleted.\"\"\"\n    return (\n        f\"SELECT id, filename, source_id FROM {table_name} \"  # noqa: S608\n        \"WHERE source_id NOT IN (SELECT id FROM sources) \"\n        f\"UNION SELECT id, filename, source_id FROM {table_name} \"\n        \"WHERE source_id IS NULL\"\n    )",
            "file": "3da3fcab826a_delete_orphaned_submissions.py"
          },
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    try:\n        config = SecureDropConfig.get_current()\n    except ModuleNotFoundError:\n        # Fresh install, nothing to migrate\n        return\n\n    conn = op.get_bind()\n    submissions = conn.execute(sa.text(raw_sql_grab_orphaned_objects(\"submissions\"))).fetchall()\n\n    replies = conn.execute(sa.text(raw_sql_grab_orphaned_objects(\"replies\"))).fetchall()\n\n    app = create_app(config)\n    with app.app_context():\n        for submission in submissions:\n            try:\n                conn.execute(\n                    sa.text(\n                        \"\"\"\n                    DELETE FROM submissions\n                    WHERE id=:id\n                \"\"\"\n                    ).bindparams(id=submission.id)\n                )\n\n                path = Storage.get_default().path_without_filesystem_id(submission.filename)\n                Storage.get_default().move_to_shredder(path)\n            except NoFileFoundException:\n                # The file must have been deleted by the admin, remove the row\n                conn.execute(\n                    sa.text(\n                        \"\"\"\n                    DELETE FROM submissions\n                    WHERE id=:id\n                \"\"\"\n                    ).bindparams(id=submission.id)\n                )\n            except TooManyFilesException:\n                pass\n\n        for reply in replies:\n            try:\n                conn.execute(\n                    sa.text(\n                        \"\"\"\n                        DELETE FROM replies\n                        WHERE id=:id\n                    \"\"\"\n                    ).bindparams(id=reply.id)\n                )\n\n                path = Storage.get_default().path_without_filesystem_id(reply.filename)\n                Storage.get_default().move_to_shredder(path)\n            except NoFileFoundException:\n                # The file must have been deleted by the admin, remove the row\n                conn.execute(\n                    sa.text(\n                        \"\"\"\n                        DELETE FROM replies\n                        WHERE id=:id\n                    \"\"\"\n                    ).bindparams(id=reply.id)\n                )\n            except TooManyFilesException:\n                pass",
            "file": "3da3fcab826a_delete_orphaned_submissions.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    # This is a destructive alembic migration, it cannot be downgraded\n    pass",
            "file": "3da3fcab826a_delete_orphaned_submissions.py"
          }
        ],
        "e0a525cbab83_add_column_to_track_source_deletion_of_.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    conn = op.get_bind()\n    conn.execute(\"PRAGMA legacy_alter_table=ON\")\n    # Schema migration\n    op.rename_table(\"replies\", \"replies_tmp\")\n\n    # Add new column.\n    op.add_column(\"replies_tmp\", sa.Column(\"deleted_by_source\", sa.Boolean()))\n\n    # Populate deleted_by_source column in replies_tmp table.\n    replies = conn.execute(sa.text(\"SELECT * FROM replies_tmp\")).fetchall()\n\n    for reply in replies:\n        conn.execute(\n            sa.text(\n                \"\"\"UPDATE replies_tmp SET deleted_by_source=0 WHERE\n                       id=:id\"\"\"\n            ).bindparams(id=reply.id)\n        )\n\n    # Now create new table with not null constraint applied to\n    # deleted_by_source.\n    op.create_table(\n        \"replies\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"journalist_id\", sa.Integer(), nullable=True),\n        sa.Column(\"source_id\", sa.Integer(), nullable=True),\n        sa.Column(\"filename\", sa.String(length=255), nullable=False),\n        sa.Column(\"size\", sa.Integer(), nullable=False),\n        sa.Column(\"deleted_by_source\", sa.Boolean(), nullable=False),\n        sa.ForeignKeyConstraint([\"journalist_id\"], [\"journalists.id\"]),\n        sa.ForeignKeyConstraint([\"source_id\"], [\"sources.id\"]),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n\n    # Data Migration: move all replies into the new table.\n    conn.execute(\n        \"\"\"\n        INSERT INTO replies\n        SELECT id, journalist_id, source_id, filename, size, deleted_by_source\n        FROM replies_tmp\n    \"\"\"\n    )\n\n    # Now delete the old table.\n    op.drop_table(\"replies_tmp\")",
            "file": "e0a525cbab83_add_column_to_track_source_deletion_of_.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    with op.batch_alter_table(\"replies\", schema=None) as batch_op:\n        batch_op.drop_column(\"deleted_by_source\")",
            "file": "e0a525cbab83_add_column_to_track_source_deletion_of_.py"
          }
        ],
        "faac8092c123_enable_security_pragmas.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    conn = op.get_bind()\n    conn.execute(sa.text(\"PRAGMA secure_delete = ON\"))\n    conn.execute(sa.text(\"PRAGMA auto_vacuum = FULL\"))",
            "file": "faac8092c123_enable_security_pragmas.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    pass",
            "file": "faac8092c123_enable_security_pragmas.py"
          }
        ],
        "6db892e17271_add_reply_uuid.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    conn = op.get_bind()\n    conn.execute(\"PRAGMA legacy_alter_table=ON\")\n    # Schema migration\n    op.rename_table(\"replies\", \"replies_tmp\")\n\n    # Add new column.\n    op.add_column(\"replies_tmp\", sa.Column(\"uuid\", sa.String(length=36)))\n\n    # Populate new column in replies_tmp table.\n    replies = conn.execute(sa.text(\"SELECT * FROM replies_tmp\")).fetchall()\n\n    for reply in replies:\n        conn.execute(\n            sa.text(\n                \"\"\"UPDATE replies_tmp SET uuid=:reply_uuid WHERE\n                       id=:id\"\"\"\n            ).bindparams(reply_uuid=str(uuid.uuid4()), id=reply.id)\n        )\n\n    # Now create new table with constraints applied to UUID column.\n    op.create_table(\n        \"replies\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"uuid\", sa.String(length=36), nullable=False),\n        sa.Column(\"journalist_id\", sa.Integer(), nullable=True),\n        sa.Column(\"source_id\", sa.Integer(), nullable=True),\n        sa.Column(\"filename\", sa.String(length=255), nullable=False),\n        sa.Column(\"size\", sa.Integer(), nullable=False),\n        sa.Column(\"deleted_by_source\", sa.Boolean(), nullable=False),\n        sa.ForeignKeyConstraint([\"journalist_id\"], [\"journalists.id\"]),\n        sa.ForeignKeyConstraint([\"source_id\"], [\"sources.id\"]),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"uuid\"),\n    )\n\n    # Data Migration: move all replies into the new table.\n    conn.execute(\n        \"\"\"\n        INSERT INTO replies\n        SELECT id, uuid, journalist_id, source_id, filename, size,\n            deleted_by_source\n        FROM replies_tmp\n    \"\"\"\n    )\n\n    # Now delete the old table.\n    op.drop_table(\"replies_tmp\")",
            "file": "6db892e17271_add_reply_uuid.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    with op.batch_alter_table(\"replies\", schema=None) as batch_op:\n        batch_op.drop_column(\"uuid\")\n\n    # ### end Alembic commands ###",
            "file": "6db892e17271_add_reply_uuid.py"
          }
        ],
        "b060f38c0c31_drop_source_flagged.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    with op.batch_alter_table(\"sources\", schema=None) as batch_op:\n        batch_op.drop_column(\"flagged\")",
            "file": "b060f38c0c31_drop_source_flagged.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    # You might be tempted to try Alembic's batch_ops for the\n    # downgrade too. Don't. SQLite's unnamed check constraints require\n    # kludges.\n\n    conn = op.get_bind()\n    conn.execute(\"PRAGMA legacy_alter_table=ON\")\n\n    op.rename_table(\"sources\", \"sources_tmp\")\n\n    conn.execute(\n        sa.text(\n            \"\"\"\n            CREATE TABLE \"sources\" (\n                id INTEGER NOT NULL,\n                uuid VARCHAR(36) NOT NULL,\n                filesystem_id VARCHAR(96),\n                journalist_designation VARCHAR(255) NOT NULL,\n                last_updated DATETIME,\n                pending BOOLEAN,\n                interaction_count INTEGER NOT NULL,\n                deleted_at DATETIME,\n                flagged BOOLEAN,\n                PRIMARY KEY (id),\n                CHECK (pending IN (0, 1)),\n                CHECK (flagged IN (0, 1)),\n                UNIQUE (filesystem_id),\n                UNIQUE (uuid)\n            )\n            \"\"\"\n        )\n    )\n\n    conn.execute(\n        \"\"\"\n        INSERT INTO sources (\n            id, uuid, filesystem_id, journalist_designation,\n            last_updated, pending, interaction_count, deleted_at\n        ) SELECT\n            id, uuid, filesystem_id, journalist_designation,\n            last_updated, pending, interaction_count, deleted_at\n        FROM sources_tmp;\n        \"\"\"\n    )\n\n    # Now delete the old table.\n    op.drop_table(\"sources_tmp\")",
            "file": "b060f38c0c31_drop_source_flagged.py"
          }
        ],
        "35513370ba0d_add_source_deleted_at.py": [
          {
            "type": "function",
            "name": "upgrade",
            "code": "def upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    with op.batch_alter_table(\"sources\", schema=None) as batch_op:\n        batch_op.add_column(sa.Column(\"deleted_at\", sa.DateTime(), nullable=True))\n\n    # ### end Alembic commands ###",
            "file": "35513370ba0d_add_source_deleted_at.py"
          },
          {
            "type": "function",
            "name": "downgrade",
            "code": "def downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    with op.batch_alter_table(\"sources\", schema=None) as batch_op:\n        batch_op.drop_column(\"deleted_at\")\n\n    # ### end Alembic commands ###",
            "file": "35513370ba0d_add_source_deleted_at.py"
          }
        ]
      }
    },
    "tests": {
      "test_store.py": [
        {
          "type": "function",
          "name": "test_storage",
          "code": "def test_storage() -> Generator[Storage, None, None]:\n    # Setup the filesystem for the storage object\n    with TemporaryDirectory() as data_dir_name:\n        data_dir = Path(data_dir_name)\n        store_dir = data_dir / \"store\"\n        store_dir.mkdir()\n        tmp_dir = data_dir / \"tmp\"\n        tmp_dir.mkdir()\n\n        storage = Storage(str(store_dir), str(tmp_dir))\n\n        yield storage",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "create_file_in_source_dir",
          "code": "def create_file_in_source_dir(base_dir, filesystem_id, filename):\n    \"\"\"Helper function for simulating files\"\"\"\n    source_directory = os.path.join(base_dir, filesystem_id)\n    os.makedirs(source_directory)\n\n    file_path = os.path.join(source_directory, filename)\n    with open(file_path, \"a\"):\n        os.utime(file_path, None)\n\n    return source_directory, file_path",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_path_returns_filename_of_folder",
          "code": "def test_path_returns_filename_of_folder(test_storage):\n    \"\"\"`Storage.path` is called in this way in\n    journalist.delete_collection\n    \"\"\"\n    filesystem_id = \"example\"\n    generated_absolute_path = test_storage.path(filesystem_id)\n\n    expected_absolute_path = os.path.join(test_storage.storage_path, filesystem_id)\n    assert generated_absolute_path == expected_absolute_path",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_path_returns_filename_of_items_within_folder",
          "code": "def test_path_returns_filename_of_items_within_folder(test_storage):\n    \"\"\"`Storage.path` is called in this  way in journalist.bulk_delete\"\"\"\n    filesystem_id = \"example\"\n    item_filename = \"1-quintuple_cant-msg.gpg\"\n    generated_absolute_path = test_storage.path(filesystem_id, item_filename)\n\n    expected_absolute_path = os.path.join(test_storage.storage_path, filesystem_id, item_filename)\n    assert generated_absolute_path == expected_absolute_path",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_path_without_filesystem_id",
          "code": "def test_path_without_filesystem_id(test_storage):\n    filesystem_id = \"example\"\n    item_filename = \"1-quintuple_cant-msg.gpg\"\n\n    basedir = os.path.join(test_storage.storage_path, filesystem_id)\n    os.makedirs(basedir)\n\n    path_to_file = os.path.join(basedir, item_filename)\n    with open(path_to_file, \"a\"):\n        os.utime(path_to_file, None)\n\n    generated_absolute_path = test_storage.path_without_filesystem_id(item_filename)\n\n    expected_absolute_path = os.path.join(test_storage.storage_path, filesystem_id, item_filename)\n    assert generated_absolute_path == expected_absolute_path",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_path_without_filesystem_id_duplicate_files",
          "code": "def test_path_without_filesystem_id_duplicate_files(test_storage):\n    filesystem_id = \"example\"\n    filesystem_id_duplicate = \"example2\"\n    item_filename = \"1-quintuple_cant-msg.gpg\"\n\n    basedir = os.path.join(test_storage.storage_path, filesystem_id)\n    duplicate_basedir = os.path.join(test_storage.storage_path, filesystem_id_duplicate)\n\n    for directory in [basedir, duplicate_basedir]:\n        os.makedirs(directory)\n        path_to_file = os.path.join(directory, item_filename)\n        with open(path_to_file, \"a\"):\n            os.utime(path_to_file, None)\n\n    with pytest.raises(store.TooManyFilesException):\n        test_storage.path_without_filesystem_id(item_filename)",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_path_without_filesystem_id_no_file",
          "code": "def test_path_without_filesystem_id_no_file(test_storage):\n    item_filename = \"not there\"\n    with pytest.raises(store.NoFileFoundException):\n        test_storage.path_without_filesystem_id(item_filename)",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_verify_path_not_absolute",
          "code": "def test_verify_path_not_absolute(test_storage):\n    with pytest.raises(store.PathException):\n        test_storage.verify(os.path.join(test_storage.storage_path, \"..\", \"etc\", \"passwd\"))",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_verify_in_store_dir",
          "code": "def test_verify_in_store_dir(test_storage):\n    path = test_storage.storage_path + \"_backup\"\n    with pytest.raises(store.PathException, match=\"Path not valid in store: \"):\n        test_storage.verify(path)",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_verify_store_path_not_absolute",
          "code": "def test_verify_store_path_not_absolute(test_storage):\n    with pytest.raises(store.PathException, match=\"Path not valid in store: \"):\n        test_storage.verify(\"..\")",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_verify_rejects_symlinks",
          "code": "def test_verify_rejects_symlinks(test_storage):\n    \"\"\"\n    Test that verify rejects paths involving links outside the store.\n    \"\"\"\n    link = os.path.join(test_storage.storage_path, \"foo\")\n    try:\n        os.symlink(\"/foo\", link)\n        with pytest.raises(store.PathException, match=\"Path not valid in store: \"):\n            test_storage.verify(link)\n    finally:\n        os.unlink(link)",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_verify_store_dir_not_absolute",
          "code": "def test_verify_store_dir_not_absolute():\n    with pytest.raises(store.PathException) as exc_info:\n        Storage(\"..\", \"/\")\n\n    msg = str(exc_info.value)\n    assert re.compile(\"storage_path.*is not absolute\").match(msg)",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_verify_store_temp_dir_not_absolute",
          "code": "def test_verify_store_temp_dir_not_absolute():\n    with pytest.raises(store.PathException) as exc_info:\n        Storage(\"/\", \"..\")\n\n    msg = str(exc_info.value)\n    assert re.compile(\"temp_dir.*is not absolute\").match(msg)",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_verify_regular_submission_in_sourcedir_returns_true",
          "code": "def test_verify_regular_submission_in_sourcedir_returns_true(test_storage):\n    \"\"\"\n    Tests that verify is happy with a regular submission file.\n\n    Verify should return True for a regular file that matches the\n    naming scheme of submissions.\n    \"\"\"\n    source_directory, file_path = create_file_in_source_dir(\n        test_storage.storage_path, \"example-filesystem-id\", \"1-regular-doc.gz.gpg\"\n    )\n\n    assert test_storage.verify(file_path)",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_verify_invalid_file_extension_in_sourcedir_raises_exception",
          "code": "def test_verify_invalid_file_extension_in_sourcedir_raises_exception(test_storage):\n    source_directory, file_path = create_file_in_source_dir(\n        test_storage.storage_path, \"example-filesystem-id\", \"not_valid.txt\"\n    )\n\n    with pytest.raises(store.PathException) as e:\n        test_storage.verify(file_path)\n\n    assert f\"Path not valid in store: {file_path}\" in str(e)",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_verify_invalid_filename_in_sourcedir_raises_exception",
          "code": "def test_verify_invalid_filename_in_sourcedir_raises_exception(test_storage):\n    source_directory, file_path = create_file_in_source_dir(\n        test_storage.storage_path, \"example-filesystem-id\", \"NOTVALID.gpg\"\n    )\n\n    with pytest.raises(store.PathException, match=\"Path not valid in store: \"):\n        test_storage.verify(file_path)",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_get_zip",
          "code": "def test_get_zip(journalist_app, test_source, app_storage, config):\n    with journalist_app.app_context():\n        submissions = utils.db_helper.submit(app_storage, test_source[\"source\"], 2)\n        filenames = [\n            os.path.join(config.STORE_DIR, test_source[\"filesystem_id\"], submission.filename)\n            for submission in submissions\n        ]\n\n        archive = zipfile.ZipFile(app_storage.get_bulk_archive(submissions))\n        archivefile_contents = archive.namelist()\n\n    for archived_file, actual_file in zip(archivefile_contents, filenames):\n        with open(actual_file, \"rb\") as f:\n            actual_file_content = f.read()\n        zipped_file_content = archive.read(archived_file)\n        assert zipped_file_content == actual_file_content",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_add_checksum_for_file",
          "code": "def test_add_checksum_for_file(config, app_storage, db_model):\n    \"\"\"\n    Check that when we execute the `add_checksum_for_file` function, the database object is\n    correctly updated with the actual hash of the file.\n\n    We have to create our own app in order to have more control over the SQLAlchemy sessions. The\n    fixture pushes a single app context that forces us to work within a single transaction.\n    \"\"\"\n    app = create_app(config)\n\n    test_storage = app_storage\n\n    with app.app_context():\n        db.create_all()\n        source_user = create_source_user(\n            db_session=db.session,\n            source_passphrase=PassphraseGenerator.get_default().generate_passphrase(),\n            source_app_storage=test_storage,\n        )\n        source = source_user.get_db_record()\n        target_file_path = test_storage.path(source.filesystem_id, \"1-foo-msg.gpg\")\n        test_message = b\"hash me!\"\n        expected_hash = \"f1df4a6d8659471333f7f6470d593e0911b4d487856d88c83d2d187afa195927\"\n\n        with open(target_file_path, \"wb\") as f:\n            f.write(test_message)\n\n        if db_model == Submission:\n            db_obj = Submission(source, target_file_path, app_storage)\n        else:\n            journalist, _ = utils.db_helper.init_journalist()\n            db_obj = Reply(journalist, source, target_file_path, app_storage)\n\n        db.session.add(db_obj)\n        db.session.commit()\n        db_obj_id = db_obj.id\n\n    queued_add_checksum_for_file(\n        db_model, db_obj_id, target_file_path, app.config[\"SQLALCHEMY_DATABASE_URI\"]\n    )\n\n    with app.app_context():\n        # requery to get a new object\n        db_obj = db_model.query.filter_by(id=db_obj_id).one()\n        assert db_obj.checksum == \"sha256:\" + expected_hash",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "_wait_for_redis_worker",
          "code": "def _wait_for_redis_worker(job: Job, timeout: int = 60) -> None:\n    \"\"\"Raise an error if the Redis job doesn't complete successfully\n    before a timeout.\n\n    :param rq.job.Job job: A Redis job to wait for.\n\n    :param int timeout: Seconds to wait for the job to finish.\n\n    :raises: An :exc:`AssertionError`.\n    \"\"\"\n    # This is an arbitrarily defined value in the SD codebase and not something from rqworker\n    redis_success_return_value = \"success\"\n\n    start_time = time.time()\n    while time.time() - start_time < timeout:\n        if job.result == redis_success_return_value:\n            return\n        elif job.result not in (None, redis_success_return_value):\n            pytest.fail(\"Redis worker failed!\")\n        time.sleep(0.1)\n    pytest.fail(\"Redis worker timed out!\")",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_async_add_checksum_for_file",
          "code": "def test_async_add_checksum_for_file(config, app_storage, db_model):\n    \"\"\"\n    Check that when we execute the `add_checksum_for_file` function, the database object is\n    correctly updated with the actual hash of the file.\n\n    We have to create our own app in order to have more control over the SQLAlchemy sessions. The\n    fixture pushes a single app context that forces us to work within a single transaction.\n    \"\"\"\n    app = create_app(config)\n\n    with app.app_context():\n        db.create_all()\n        source_user = create_source_user(\n            db_session=db.session,\n            source_passphrase=PassphraseGenerator.get_default().generate_passphrase(),\n            source_app_storage=app_storage,\n        )\n        source = source_user.get_db_record()\n        target_file_path = app_storage.path(source.filesystem_id, \"1-foo-msg.gpg\")\n        test_message = b\"hash me!\"\n        expected_hash = \"f1df4a6d8659471333f7f6470d593e0911b4d487856d88c83d2d187afa195927\"\n\n        with open(target_file_path, \"wb\") as f:\n            f.write(test_message)\n\n        if db_model == Submission:\n            db_obj = Submission(source, target_file_path, app_storage)\n        else:\n            journalist, _ = utils.db_helper.init_journalist()\n            db_obj = Reply(journalist, source, target_file_path, app_storage)\n\n        db.session.add(db_obj)\n        db.session.commit()\n        db_obj_id = db_obj.id\n\n        job = async_add_checksum_for_file(db_obj, app_storage)\n\n    _wait_for_redis_worker(job, timeout=5)\n\n    with app.app_context():\n        # requery to get a new object\n        db_obj = db_model.query.filter_by(id=db_obj_id).one()\n        assert db_obj.checksum == \"sha256:\" + expected_hash",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_path_configuration_is_immutable",
          "code": "def test_path_configuration_is_immutable(test_storage):\n    \"\"\"\n    Check that the store's paths cannot be changed.\n\n    They're exposed via properties that are supposed to be\n    read-only. It is of course possible to change them via the mangled\n    attribute names, but we want to confirm that accidental changes\n    are prevented.\n    \"\"\"\n    with pytest.raises(AttributeError):\n        test_storage.storage_path = \"/foo\"\n\n    original_storage_path = test_storage.storage_path[:]\n    test_storage.__storage_path = \"/foo\"\n    assert test_storage.storage_path == original_storage_path\n\n    with pytest.raises(AttributeError):\n        test_storage.shredder_path = \"/foo\"\n\n    original_shredder_path = test_storage.shredder_path[:]\n    test_storage.__shredder_path = \"/foo\"\n    assert test_storage.shredder_path == original_shredder_path",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_shredder_configuration",
          "code": "def test_shredder_configuration(journalist_app, app_storage):\n    \"\"\"\n    Ensure we're creating the shredder directory correctly.\n\n    We want to ensure that it's a sibling of the store directory, with\n    mode 0700.\n    \"\"\"\n    store_path = app_storage.storage_path\n    shredder_path = app_storage.shredder_path\n    assert os.path.dirname(shredder_path) == os.path.dirname(store_path)\n    s = os.stat(shredder_path)\n    assert stat.S_ISDIR(s.st_mode) is True\n    assert stat.S_IMODE(s.st_mode) == 0o700",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_shredder_deletes_symlinks",
          "code": "def test_shredder_deletes_symlinks(journalist_app, app_storage, caplog):\n    \"\"\"\n    Confirm that `store.clear_shredder` removes any symlinks in the shredder.\n    \"\"\"\n    caplog.set_level(logging.DEBUG)\n\n    link_target = \"/foo\"\n    link = os.path.abspath(os.path.join(app_storage.shredder_path, \"foo\"))\n    os.symlink(link_target, link)\n    app_storage.clear_shredder()\n    assert f\"Deleting link {link} to {link_target}\" in caplog.text\n    assert not os.path.exists(link)",
          "file": "test_store.py"
        },
        {
          "type": "function",
          "name": "test_shredder_shreds",
          "code": "def test_shredder_shreds(journalist_app, app_storage, caplog):\n    \"\"\"\n    Confirm that `store.clear_shredder` removes files.\n    \"\"\"\n    caplog.set_level(logging.DEBUG)\n\n    testdir = os.path.abspath(os.path.join(app_storage.shredder_path, \"testdir\"))\n    os.makedirs(testdir)\n    testfile = os.path.join(testdir, \"testfile\")\n    with open(testfile, \"w\") as f:\n        f.write(\"testdata\\n\")\n\n    app_storage.clear_shredder()\n    assert f\"Securely deleted file 1/1: {testfile}\" in caplog.text\n    assert not os.path.isfile(testfile)\n    assert not os.path.isdir(testdir)",
          "file": "test_store.py"
        }
      ],
      "test_startup.py": [
        {
          "type": "function",
          "name": "test_validate_journalist_key",
          "code": "def test_validate_journalist_key(config, journalist_app, capsys):\n    # The test key passes validation\n    assert validate_journalist_key(journalist_app) is True\n    # Reading the key file fails\n    with patch(\n        \"encryption.EncryptionManager.get_journalist_public_key\", side_effect=RuntimeError(\"err\")\n    ):\n        assert validate_journalist_key(journalist_app) is False\n    assert capsys.readouterr().err == \"ERROR: Unable to read journalist public key: err\\n\"\n    # Key fails validation\n    with patch(\"redwood.is_valid_public_key\", side_effect=RedwoodError(\"err\")):\n        assert validate_journalist_key(journalist_app) is False\n    assert capsys.readouterr().err == \"ERROR: Journalist public key is not valid: err\\n\"",
          "file": "test_startup.py"
        }
      ],
      "conftest.py": [
        {
          "type": "function",
          "name": "pytest_addoption",
          "code": "def pytest_addoption(parser):\n    parser.addoption(\n        \"--page-layout\", action=\"store_true\", default=False, help=\"run page layout tests\"\n    )",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "pytest_configure",
          "code": "def pytest_configure(config):\n    config.addinivalue_line(\"markers\", \"pagelayout: Tests which verify page layout\")",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "pytest_collection_modifyitems",
          "code": "def pytest_collection_modifyitems(config, items):\n    if config.getoption(\"--page-layout\"):\n        return\n    skip_page_layout = pytest.mark.skip(reason=\"need --page-layout option to run page layout tests\")\n    for item in items:\n        if \"pagelayout\" in item.keywords:\n            item.add_marker(skip_page_layout)",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "insecure_scrypt",
          "code": "def insecure_scrypt() -> Generator[None, None, None]:\n    \"\"\"Make scrypt insecure but fast for the test suite.\"\"\"\n    insecure_scrypt_mgr = _SourceScryptManager(\n        salt_for_gpg_secret=b\"abcd\",\n        salt_for_filesystem_id=b\"1234\",\n        # Insecure but fast\n        scrypt_n=2**1,\n        scrypt_r=1,\n        scrypt_p=1,\n    )\n\n    with mock.patch.object(_SourceScryptManager, \"get_default\", return_value=insecure_scrypt_mgr):\n        yield",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "setup_journalist_key_and_gpg_folder",
          "code": "def setup_journalist_key_and_gpg_folder() -> Generator[Tuple[str, Path], None, None]:\n    \"\"\"Set up the journalist test key and the key folder, and reduce source key length for speed.\n\n    This fixture takes about 2s to complete hence we use the \"session\" scope to only run it once.\n    \"\"\"\n    # This path matches the GPG_KEY_DIR defined in the config.py used for the tests\n    # If they don't match, it can make the tests flaky and very hard to debug\n    tmp_gpg_dir = Path(\"/tmp\") / \"securedrop\" / \"keys\"\n    tmp_gpg_dir.mkdir(parents=True, exist_ok=True, mode=0o0700)\n\n    try:\n        # GPG 2.1+ requires gpg-agent, see #4013\n        gpg_agent_config = tmp_gpg_dir / \"gpg-agent.conf\"\n        gpg_agent_config.write_text(\"allow-loopback-pinentry\\ndefault-cache-ttl 0\")\n\n        # Import the journalist public key in GPG\n        # WARNING: don't import the journalist secret key; it will make the decryption tests\n        # unreliable\n        gpg = gnupg.GPG(\"gpg2\", homedir=str(tmp_gpg_dir))\n        journalist_public_key_path = Path(__file__).parent / \"files\" / \"test_journalist_key.pub\"\n        journalist_public_key = journalist_public_key_path.read_text()\n        # TODO: we don't need the journalist pub key in the keyring anymore, but include\n        # it anyways to match a legacy prod keyring.\n        journalist_key_fingerprint = gpg.import_keys(journalist_public_key).fingerprints[0]\n\n        yield journalist_key_fingerprint, tmp_gpg_dir\n\n    finally:\n        shutil.rmtree(tmp_gpg_dir, ignore_errors=True)",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "config",
          "code": "def config(\n    setup_journalist_key_and_gpg_folder: Tuple[str, Path],\n    setup_rqworker: Tuple[str, str],\n) -> Generator[SecureDropConfig, None, None]:\n    journalist_key_fingerprint, gpg_key_dir = setup_journalist_key_and_gpg_folder\n    worker_name, _ = setup_rqworker\n    config = SecureDropConfigFactory.create(\n        SECUREDROP_DATA_ROOT=Path(f\"/tmp/sd-tests/conftest-{uuid4()}\"),\n        GPG_KEY_DIR=gpg_key_dir,\n        JOURNALIST_KEY=journalist_key_fingerprint,\n        SUPPORTED_LOCALES=i18n.get_test_locales(),\n        RQ_WORKER_NAME=worker_name,\n    )\n\n    # Set this newly-created config as the current config\n    with mock.patch.object(sdconfig.SecureDropConfig, \"get_current\", return_value=config):\n        yield config",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "alembic_config",
          "code": "def alembic_config(config: SecureDropConfig) -> Generator[Path, None, None]:\n    base_dir = path.join(path.dirname(__file__), \"..\")\n    migrations_dir = path.join(base_dir, \"alembic\")\n    ini = configparser.ConfigParser()\n    ini.read(path.join(base_dir, \"alembic.ini\"))\n\n    ini.set(\"alembic\", \"script_location\", path.join(migrations_dir))\n    ini.set(\"alembic\", \"sqlalchemy.url\", config.DATABASE_URI)\n\n    alembic_path = config.SECUREDROP_DATA_ROOT / \"alembic.ini\"\n    with open(alembic_path, \"w\") as f:\n        ini.write(f)\n\n    # The alembic tests require the SECUREDROP_ENV env variable to be set to \"test\"\n    # because alembic migrations are run in a separate process that reads\n    # the config.py separately; hence the config fixture doesn't get applied\n    # and without this, the alembic process will use the \"prod\" value of\n    # SECUREDROP_DATA_ROOT that is defined in the config.py.example\n    previous_env_value = os.getenv(\"SECUREDROP_ENV\", default=\"\")\n    os.environ[\"SECUREDROP_ENV\"] = \"test\"\n\n    try:\n        yield alembic_path\n\n    finally:\n        os.environ[\"SECUREDROP_ENV\"] = previous_env_value",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "app_storage",
          "code": "def app_storage(config: SecureDropConfig) -> \"Storage\":\n    return Storage(str(config.STORE_DIR), str(config.TEMP_DIR))",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "source_app",
          "code": "def source_app(config: SecureDropConfig, app_storage: Storage) -> Generator[Flask, None, None]:\n    with mock.patch(\"store.Storage.get_default\") as mock_storage_global:\n        mock_storage_global.return_value = app_storage\n        app = create_source_app(config)\n        app.config[\"SERVER_NAME\"] = \"localhost.localdomain\"\n        with app.app_context():\n            db.create_all()\n            try:\n                yield app\n            finally:\n                db.session.rollback()\n                db.drop_all()",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "journalist_app",
          "code": "def journalist_app(config: SecureDropConfig, app_storage: Storage) -> Generator[Flask, None, None]:\n    with mock.patch(\"store.Storage.get_default\") as mock_storage_global:\n        mock_storage_global.return_value = app_storage\n        app = create_journalist_app(config)\n        app.config[\"SERVER_NAME\"] = \"localhost.localdomain\"\n        with app.app_context():\n            db.create_all()\n            try:\n                yield app\n            finally:\n                db.session.rollback()\n                db.drop_all()",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "test_journo",
          "code": "def test_journo(journalist_app: Flask) -> Dict[str, Any]:\n    with journalist_app.app_context():\n        user, password = utils.db_helper.init_journalist(is_admin=False)\n        username = user.username\n        otp_secret = user.otp_secret\n        return {\n            \"journalist\": user,\n            \"username\": username,\n            \"password\": password,\n            \"otp_secret\": otp_secret,\n            \"id\": user.id,\n            \"uuid\": user.uuid,\n            \"first_name\": user.first_name,\n            \"last_name\": user.last_name,\n        }",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "test_admin",
          "code": "def test_admin(journalist_app: Flask) -> Dict[str, Any]:\n    with journalist_app.app_context():\n        user, password = utils.db_helper.init_journalist(is_admin=True)\n        username = user.username\n        otp_secret = user.otp_secret\n        return {\n            \"admin\": user,\n            \"username\": username,\n            \"password\": password,\n            \"otp_secret\": otp_secret,\n            \"id\": user.id,\n        }",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "test_source",
          "code": "def test_source(journalist_app: Flask, app_storage: Storage) -> Dict[str, Any]:\n    with journalist_app.app_context():\n        passphrase = PassphraseGenerator.get_default().generate_passphrase()\n        source_user = create_source_user(\n            db_session=db.session,\n            source_passphrase=passphrase,\n            source_app_storage=app_storage,\n        )\n        source = source_user.get_db_record()\n        return {\n            \"source_user\": source_user,\n            # TODO(AD): Eventually the next keys could be removed as they are in source_user\n            \"source\": source,\n            \"codename\": passphrase,\n            \"filesystem_id\": source_user.filesystem_id,\n            \"uuid\": source.uuid,\n            \"id\": source.id,\n        }",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "test_submissions",
          "code": "def test_submissions(journalist_app: Flask, app_storage: Storage) -> Dict[str, Any]:\n    with journalist_app.app_context():\n        source, codename = utils.db_helper.init_source(app_storage)\n        utils.db_helper.submit(app_storage, source, 2)\n        return {\n            \"source\": source,\n            \"codename\": codename,\n            \"filesystem_id\": source.filesystem_id,\n            \"uuid\": source.uuid,\n            \"submissions\": source.submissions,\n        }",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "test_files",
          "code": "def test_files(journalist_app, test_journo, app_storage):\n    with journalist_app.app_context():\n        source, codename = utils.db_helper.init_source(app_storage)\n        utils.db_helper.submit(app_storage, source, 2, submission_type=\"file\")\n        utils.db_helper.reply(app_storage, test_journo[\"journalist\"], source, 1)\n        return {\n            \"source\": source,\n            \"codename\": codename,\n            \"filesystem_id\": source.filesystem_id,\n            \"uuid\": source.uuid,\n            \"submissions\": source.submissions,\n            \"replies\": source.replies,\n        }",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "test_files_deleted_journalist",
          "code": "def test_files_deleted_journalist(journalist_app, test_journo, app_storage):\n    with journalist_app.app_context():\n        source, codename = utils.db_helper.init_source(app_storage)\n        utils.db_helper.submit(app_storage, source, 2)\n        test_journo[\"journalist\"]\n        juser, _ = utils.db_helper.init_journalist(\"f\", \"l\", is_admin=False)\n        utils.db_helper.reply(app_storage, juser, source, 1)\n        utils.db_helper.delete_journalist(juser)\n        return {\n            \"source\": source,\n            \"codename\": codename,\n            \"filesystem_id\": source.filesystem_id,\n            \"uuid\": source.uuid,\n            \"submissions\": source.submissions,\n            \"replies\": source.replies,\n        }",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "journalist_api_token",
          "code": "def journalist_api_token(journalist_app, test_journo):\n    with journalist_app.test_client() as app:\n        valid_token = TOTP(test_journo[\"otp_secret\"]).now()\n        response = app.post(\n            url_for(\"api.get_token\"),\n            data=json.dumps(\n                {\n                    \"username\": test_journo[\"username\"],\n                    \"passphrase\": test_journo[\"password\"],\n                    \"one_time_code\": valid_token,\n                }\n            ),\n            headers=utils.api_helper.get_api_headers(),\n        )\n        return response.json[\"token\"]",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "setup_rqworker",
          "code": "def setup_rqworker() -> Generator[Tuple[str, Path], None, None]:\n    # The PID file and name for the redis worker are hard-coded below\n    test_worker_pid_file = Path(\"/tmp/securedrop_test_worker.pid\")\n    test_worker_name = \"test\"\n\n    try:\n        _start_test_rqworker(\n            worker_name=test_worker_name,\n            worker_pid_file=test_worker_pid_file,\n            securedrop_root=DEFAULT_SECUREDROP_ROOT,\n        )\n\n        yield test_worker_name, test_worker_pid_file\n\n    finally:\n        _stop_test_rqworker(test_worker_pid_file)",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "_start_test_rqworker",
          "code": "def _start_test_rqworker(worker_name: str, worker_pid_file: Path, securedrop_root: Path) -> None:\n    if not psutil.pid_exists(_get_pid_from_file(worker_pid_file)):\n        tmp_logfile = open(\"/tmp/test_rqworker.log\", \"w\")\n        subprocess.Popen(\n            [\n                \"rqworker\",\n                worker_name,\n                \"-P\",\n                securedrop_root,\n                \"-c\",\n                \"rq_config\",\n                \"--pid\",\n                worker_pid_file,\n                \"--logging_level\",\n                \"DEBUG\",\n                \"-v\",\n            ],\n            stdout=tmp_logfile,\n            stderr=subprocess.STDOUT,\n        )",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "_stop_test_rqworker",
          "code": "def _stop_test_rqworker(worker_pid_file: Path) -> None:\n    rqworker_pid = _get_pid_from_file(worker_pid_file)\n    if rqworker_pid:\n        os.kill(rqworker_pid, signal.SIGTERM)\n        try:\n            os.remove(worker_pid_file)\n        except OSError:\n            pass",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "_get_pid_from_file",
          "code": "def _get_pid_from_file(pid_file_name: Path) -> int:\n    try:\n        return int(open(pid_file_name).read())\n    except OSError:\n        return -1",
          "file": "conftest.py"
        }
      ],
      "test_two_factor.py": [
        {
          "type": "class",
          "name": "TestHOTP",
          "code": "class TestHOTP:\n    _SECRET = \"YQTEGUTJCMBETH3KUUZZMRWZAVBKGT5O\"\n    _VALID_TOKEN = \"464263\"\n    _VALID_TOKEN_COUNTER = 12\n\n    def test_invalid_secret_wrong_length(self) -> None:\n        with pytest.raises(OtpSecretInvalid, match=\"length\"):\n            TOTP(secret_as_base32=\"JHCOGO7VCER3EJ\")\n\n    def test_invalid_secret_not_base32(self) -> None:\n        with pytest.raises(OtpSecretInvalid, match=\"base32\"):\n            TOTP(secret_as_base32=\"not actually &&&!!! base 32\")\n\n    def test_generate(self) -> None:\n        # Given a HOTP validator\n        hotp = HOTP(secret_as_base32=self._SECRET)\n\n        # When generating a token with the corresponding counter value\n        # Then it succeeds\n        token = hotp.generate(counter=self._VALID_TOKEN_COUNTER)\n\n        # And the expected token is returned\n        assert token == self._VALID_TOKEN\n\n    def test_verify(self) -> None:\n        # Given a HOTP validator and a token it generated\n        hotp = HOTP(secret_as_base32=self._SECRET)\n\n        # When verifying a token with the corresponding counter value\n        counter = self._VALID_TOKEN_COUNTER\n\n        # Then it succeeds\n        hotp.verify(self._VALID_TOKEN, counter)\n\n    def test_verify_within_look_ahead_window(self) -> None:\n        # Given a HOTP validator and a token it generated\n        hotp = HOTP(secret_as_base32=self._SECRET)\n\n        # When verifying a token with the a counter value that is slightly off but within\n        # the look-ahead window\n        counter = self._VALID_TOKEN_COUNTER - 10\n\n        # Then it succeeds\n        hotp.verify(self._VALID_TOKEN, counter)\n\n    @pytest.mark.parametrize(\n        (\"token\", \"counter\"),\n        [\n            pytest.param(_VALID_TOKEN, _VALID_TOKEN_COUNTER + 10),\n            pytest.param(_VALID_TOKEN, 12345),\n        ],\n    )\n    def test_verify_but_token_invalid(self, token: str, counter: int) -> None:\n        # Given a HOTP validator and a token\n        hotp = HOTP(secret_as_base32=self._SECRET)\n\n        # When verifying the token with an invalid value or counter, then it fails\n        with pytest.raises(OtpTokenInvalid):\n            hotp.verify(token, counter)",
          "file": "test_two_factor.py"
        },
        {
          "type": "function",
          "name": "test_invalid_secret_wrong_length",
          "code": "def test_invalid_secret_wrong_length(self) -> None:\n        with pytest.raises(OtpSecretInvalid, match=\"length\"):\n            TOTP(secret_as_base32=\"JHCOGO7VCER3EJ\")",
          "file": "test_two_factor.py"
        },
        {
          "type": "function",
          "name": "test_invalid_secret_not_base32",
          "code": "def test_invalid_secret_not_base32(self) -> None:\n        with pytest.raises(OtpSecretInvalid, match=\"base32\"):\n            TOTP(secret_as_base32=\"not actually &&&!!! base 32\")",
          "file": "test_two_factor.py"
        },
        {
          "type": "function",
          "name": "test_generate",
          "code": "def test_generate(self) -> None:\n        # Given a HOTP validator\n        hotp = HOTP(secret_as_base32=self._SECRET)\n\n        # When generating a token with the corresponding counter value\n        # Then it succeeds\n        token = hotp.generate(counter=self._VALID_TOKEN_COUNTER)\n\n        # And the expected token is returned\n        assert token == self._VALID_TOKEN",
          "file": "test_two_factor.py"
        },
        {
          "type": "function",
          "name": "test_verify",
          "code": "def test_verify(self) -> None:\n        # Given a HOTP validator and a token it generated\n        hotp = HOTP(secret_as_base32=self._SECRET)\n\n        # When verifying a token with the corresponding counter value\n        counter = self._VALID_TOKEN_COUNTER\n\n        # Then it succeeds\n        hotp.verify(self._VALID_TOKEN, counter)",
          "file": "test_two_factor.py"
        },
        {
          "type": "function",
          "name": "test_verify_within_look_ahead_window",
          "code": "def test_verify_within_look_ahead_window(self) -> None:\n        # Given a HOTP validator and a token it generated\n        hotp = HOTP(secret_as_base32=self._SECRET)\n\n        # When verifying a token with the a counter value that is slightly off but within\n        # the look-ahead window\n        counter = self._VALID_TOKEN_COUNTER - 10\n\n        # Then it succeeds\n        hotp.verify(self._VALID_TOKEN, counter)",
          "file": "test_two_factor.py"
        },
        {
          "type": "function",
          "name": "test_verify_but_token_invalid",
          "code": "def test_verify_but_token_invalid(self, token: str, counter: int) -> None:\n        # Given a HOTP validator and a token\n        hotp = HOTP(secret_as_base32=self._SECRET)\n\n        # When verifying the token with an invalid value or counter, then it fails\n        with pytest.raises(OtpTokenInvalid):\n            hotp.verify(token, counter)",
          "file": "test_two_factor.py"
        },
        {
          "type": "class",
          "name": "TestTOTP",
          "code": "class TestTOTP:\n    _SECRET = \"JHCOGO7VCER3EJ4L\"\n    _VALID_TOKEN = \"705334\"\n    _VALID_TOKEN_GENERATED_AT = 1666515039\n\n    def test_invalid_secret_wrong_length(self) -> None:\n        with pytest.raises(OtpSecretInvalid, match=\"length\"):\n            TOTP(secret_as_base32=\"JHCOGO7\")\n\n    def test_invalid_secret_not_base32(self) -> None:\n        with pytest.raises(OtpSecretInvalid, match=\"base32\"):\n            TOTP(secret_as_base32=\"not actually &&&!!! base 32\")\n\n    def test_invalid_secret(self) -> None:\n        with pytest.raises(OtpSecretInvalid):\n            TOTP(secret_as_base32=\"invalid_secret\")\n\n    def test_get_provisioning_uri(self) -> None:\n        totp = TOTP(secret_as_base32=self._SECRET)\n        assert totp.get_provisioning_uri(\"account\")\n\n    def test_generate(self) -> None:\n        # Given a TOTP validator\n        totp = TOTP(secret_as_base32=self._SECRET)\n\n        # When generating a token at a specific time\n        generation_time = datetime.fromtimestamp(self._VALID_TOKEN_GENERATED_AT)\n\n        # Then it succeeds\n        token = totp.generate(generation_time)\n\n        # And the expected token is returned\n        assert token == self._VALID_TOKEN\n\n    @pytest.mark.parametrize(\n        (\"token\", \"verification_timestamp\"),\n        [\n            # Ensure the token is valid at the exact time\n            pytest.param(_VALID_TOKEN, _VALID_TOKEN_GENERATED_AT),\n            # And at the previous & following time windows\n            pytest.param(_VALID_TOKEN, _VALID_TOKEN_GENERATED_AT - 30),\n            pytest.param(_VALID_TOKEN, _VALID_TOKEN_GENERATED_AT + 30),\n        ],\n    )\n    def test_verify(self, token: str, verification_timestamp: int) -> None:\n        # Given a TOTP validator and a token it generated\n        totp = TOTP(secret_as_base32=self._SECRET)\n\n        # And the verification time is within the time window when the token is considered valid\n        verification_time = datetime.fromtimestamp(verification_timestamp)\n\n        # When verifying the token, then it succeeds\n        totp.verify(token, verification_time)\n\n    @pytest.mark.parametrize(\n        (\"token\", \"verification_timestamp\"),\n        [\n            # Ensure the token is invalid at a totally wrong time\n            pytest.param(_VALID_TOKEN, 123456),\n            # And at times that are very close to the valid time windows\n            pytest.param(_VALID_TOKEN, _VALID_TOKEN_GENERATED_AT - 60),\n            pytest.param(_VALID_TOKEN, _VALID_TOKEN_GENERATED_AT + 60),\n        ],\n    )\n    def test_verify_but_token_invalid(self, token: str, verification_timestamp: int) -> None:\n        # Given a TOTP validator and a token it generated\n        totp = TOTP(secret_as_base32=self._SECRET)\n\n        # And the verification time is within the time window when the token is considered NOT valid\n        verification_time = datetime.fromtimestamp(verification_timestamp)\n\n        # When verifying the token, then it fails\n        with pytest.raises(OtpTokenInvalid):\n            totp.verify(token, verification_time)",
          "file": "test_two_factor.py"
        },
        {
          "type": "function",
          "name": "test_invalid_secret_wrong_length",
          "code": "def test_invalid_secret_wrong_length(self) -> None:\n        with pytest.raises(OtpSecretInvalid, match=\"length\"):\n            TOTP(secret_as_base32=\"JHCOGO7\")",
          "file": "test_two_factor.py"
        },
        {
          "type": "function",
          "name": "test_invalid_secret_not_base32",
          "code": "def test_invalid_secret_not_base32(self) -> None:\n        with pytest.raises(OtpSecretInvalid, match=\"base32\"):\n            TOTP(secret_as_base32=\"not actually &&&!!! base 32\")",
          "file": "test_two_factor.py"
        },
        {
          "type": "function",
          "name": "test_invalid_secret",
          "code": "def test_invalid_secret(self) -> None:\n        with pytest.raises(OtpSecretInvalid):\n            TOTP(secret_as_base32=\"invalid_secret\")",
          "file": "test_two_factor.py"
        },
        {
          "type": "function",
          "name": "test_get_provisioning_uri",
          "code": "def test_get_provisioning_uri(self) -> None:\n        totp = TOTP(secret_as_base32=self._SECRET)\n        assert totp.get_provisioning_uri(\"account\")",
          "file": "test_two_factor.py"
        },
        {
          "type": "function",
          "name": "test_generate",
          "code": "def test_generate(self) -> None:\n        # Given a TOTP validator\n        totp = TOTP(secret_as_base32=self._SECRET)\n\n        # When generating a token at a specific time\n        generation_time = datetime.fromtimestamp(self._VALID_TOKEN_GENERATED_AT)\n\n        # Then it succeeds\n        token = totp.generate(generation_time)\n\n        # And the expected token is returned\n        assert token == self._VALID_TOKEN",
          "file": "test_two_factor.py"
        },
        {
          "type": "function",
          "name": "test_verify",
          "code": "def test_verify(self, token: str, verification_timestamp: int) -> None:\n        # Given a TOTP validator and a token it generated\n        totp = TOTP(secret_as_base32=self._SECRET)\n\n        # And the verification time is within the time window when the token is considered valid\n        verification_time = datetime.fromtimestamp(verification_timestamp)\n\n        # When verifying the token, then it succeeds\n        totp.verify(token, verification_time)",
          "file": "test_two_factor.py"
        },
        {
          "type": "function",
          "name": "test_verify_but_token_invalid",
          "code": "def test_verify_but_token_invalid(self, token: str, verification_timestamp: int) -> None:\n        # Given a TOTP validator and a token it generated\n        totp = TOTP(secret_as_base32=self._SECRET)\n\n        # And the verification time is within the time window when the token is considered NOT valid\n        verification_time = datetime.fromtimestamp(verification_timestamp)\n\n        # When verifying the token, then it fails\n        with pytest.raises(OtpTokenInvalid):\n            totp.verify(token, verification_time)",
          "file": "test_two_factor.py"
        }
      ],
      "test_remove_pending_sources.py": [
        {
          "type": "function",
          "name": "test_remove_pending_sources_none_pending",
          "code": "def test_remove_pending_sources_none_pending(n, m, source_app, config, app_storage):\n    \"\"\"remove_pending_sources() is a no-op on active sources.\"\"\"\n\n    # Override the configuration to point at the per-test database.\n    data_root = config.SECUREDROP_DATA_ROOT\n\n    with source_app.app_context():\n        sources = []\n        for _i in range(n):\n            source_user = create_source_user(\n                db_session=db.session,\n                source_passphrase=PassphraseGenerator.get_default().generate_passphrase(),\n                source_app_storage=app_storage,\n            )\n            source = source_user.get_db_record()\n            source.pending = False\n            sources.append(source.id)\n        db.session.commit()\n\n        # Make sure we have n sources.\n        assert db.session.query(Source).count() == n\n\n        # If we're keeping the n most-recent sources, then remove_pending_sources()\n        # shouldn't remove any.\n        args = argparse.Namespace(data_root=data_root, verbose=True, keep_most_recent=n)\n        manage.setup_verbosity(args)\n        manage.remove_pending_sources(args)\n        assert db.session.query(Source).count() == n\n\n        # If we're keeping the m most-recent sources, then remove_pending_sources()\n        # still shouldn't do anything, because none are pending.\n        args = argparse.Namespace(data_root=data_root, verbose=True, keep_most_recent=m)\n        manage.setup_verbosity(args)\n        manage.remove_pending_sources(args)\n        assert db.session.query(Source).count() == n",
          "file": "test_remove_pending_sources.py"
        },
        {
          "type": "function",
          "name": "test_remove_pending_sources_all_pending",
          "code": "def test_remove_pending_sources_all_pending(n, m, source_app, config, app_storage):\n    \"\"\"remove_pending_sources() removes all but the most-recent m of n pending sources.\"\"\"\n    # Override the configuration to point at the per-test database.\n    data_root = config.SECUREDROP_DATA_ROOT\n\n    with source_app.app_context():\n        sources = []\n        for _i in range(n):\n            source_user = create_source_user(\n                db_session=db.session,\n                source_passphrase=PassphraseGenerator.get_default().generate_passphrase(),\n                source_app_storage=app_storage,\n            )\n            source = source_user.get_db_record()\n            sources.append(source.id)\n        db.session.commit()\n\n        # Make sure we have n sources.\n        assert db.session.query(Source).count() == n\n\n        # If we're keeping the n most-recent sources, then remove_pending_sources()\n        # shouldn't remove any.\n        args = argparse.Namespace(data_root=data_root, verbose=True, keep_most_recent=n)\n        manage.setup_verbosity(args)\n        manage.remove_pending_sources(args)\n        assert db.session.query(Source).count() == n\n\n        # If we're keeping the m most-recent sources, then remove_pending_sources()\n        # should remove n - m.\n        args = argparse.Namespace(data_root=data_root, verbose=True, keep_most_recent=m)\n        manage.setup_verbosity(args)\n        manage.remove_pending_sources(args)\n        assert db.session.query(Source).count() == m\n\n        # Specifically, the first n - m sources should be gone...\n        for source in sources[0 : n - m]:\n            assert db.session.query(Source).get(source) is None\n\n        # ...and only the last m should remain.\n        for source in sources[n - m : n]:\n            assert db.session.query(Source).get(source) is not None",
          "file": "test_remove_pending_sources.py"
        }
      ],
      "test_two_factor_in_apps.py": [
        {
          "type": "class",
          "name": "TestTwoFactorInJournalistApp",
          "code": "class TestTwoFactorInJournalistApp:\n    def test_rejects_already_used_totp_token(self, journalist_app, test_journo):\n        # Given a journalist that previously logged in using a specific 2fa token\n        token = TOTP(test_journo[\"otp_secret\"]).now()\n        with journalist_app.test_client() as app:\n            resp1 = app.post(\n                url_for(\"main.login\"),\n                data=dict(\n                    username=test_journo[\"username\"],\n                    password=test_journo[\"password\"],\n                    token=token,\n                ),\n                follow_redirects=True,\n            )\n            assert resp1.status_code == 200\n\n            resp2 = app.post(url_for(\"main.logout\"), follow_redirects=True)\n            assert resp2.status_code == 200\n\n        with journalist_app.app_context():\n            journo = Journalist.query.get(test_journo[\"id\"])\n            assert journo.last_token == token\n\n        # When they try to login again using the same 2fa token\n        with journalist_app.test_client() as app:\n            resp = app.post(\n                url_for(\"main.login\"),\n                data=dict(\n                    username=test_journo[\"username\"],\n                    password=test_journo[\"password\"],\n                    token=token,\n                ),\n            )\n\n            # Then a login error is displayed\n            assert resp.status_code == 200\n            text = resp.data.decode(\"utf-8\")\n            assert \"Login failed\" in text\n\n    def test_rejects_already_used_totp_token_with_padding(self, journalist_app, test_journo):\n        # Given a journalist that previously logged in using a specific 2fa token\n        token = TOTP(test_journo[\"otp_secret\"]).now()\n        with journalist_app.app_context():\n            Journalist.login(test_journo[\"username\"], test_journo[\"password\"], token)\n\n            # When they try to login again using the same token but with spaces added to the token\n            token_for_second_login = token + \"   \"\n\n            # Then it fails\n            with pytest.raises(OtpTokenInvalid, match=\"already used\"):\n                Journalist.login(\n                    test_journo[\"username\"], test_journo[\"password\"], token_for_second_login\n                )\n\n    def test_rejects_already_used_totp_token_after_failed_login(self, journalist_app, test_journo):\n        # Given a journalist that previously logged in using a specific 2fa token\n        token = TOTP(test_journo[\"otp_secret\"]).now()\n        with journalist_app.app_context():\n            Journalist.login(test_journo[\"username\"], test_journo[\"password\"], token)\n\n            # And they later tried to login using an invalid 2fa token\n            invalid_token = \"000000\"\n            with pytest.raises(OtpTokenInvalid):\n                Journalist.login(test_journo[\"username\"], test_journo[\"password\"], invalid_token)\n\n            # When they try to login again by reusing the initial/valid 2fa token\n            # Then it fails with the right exception: the last_used_token in the DB was not\n            #  overwritten by the token from the invalid login attempt\n            with pytest.raises(OtpTokenInvalid, match=\"already used\"):\n                Journalist.login(test_journo[\"username\"], test_journo[\"password\"], token)\n\n    @pytest.mark.parametrize(\"otp_secret\", [\"\", \"GARBAGE\", \"notbase32:&&&&aaa\" \"JHCOGO7VCER3EJ4\"])\n    def test_rejects_user_with_invalid_otp_secret(self, journalist_app, otp_secret):\n        # Given a journalist that has an invalid OTP secret\n        with journalist_app.app_context():\n            new_username = \"badotp\" + otp_secret\n            user, password = db_helper.init_journalist(is_admin=False)\n            user.otp_secret = otp_secret\n            user.username = new_username\n            db.session.add(user)\n            db.session.commit()\n\n        # When they try to login\n        with journalist_app.test_client() as app, InstrumentedApp(app) as ins:\n            resp = app.post(\n                url_for(\"main.login\"),\n                data={\n                    \"username\": new_username,\n                    \"password\": password,\n                    \"token\": \"705334\",\n                },\n                follow_redirects=True,\n            )\n\n            # It fails and they didn't get a session\n            assert resp.status_code == 200\n            assert session.get_user() is None\n\n            # And the corresponding error messages was displayed\n            assert len(ins.flashed_messages) == 1\n            assert \"2FA details are invalid\" in ins.flashed_messages[0][0]\n\n    def test_can_login_after_regenerating_hotp(self, journalist_app, test_journo):\n        # Given a journalist logged into the journalist app\n        with journalist_app.test_client() as app:\n            resp = app.post(\n                \"/login\",\n                data=dict(\n                    username=test_journo[\"username\"],\n                    password=test_journo[\"password\"],\n                    token=TOTP(test_journo[\"otp_secret\"]).now(),\n                ),\n            )\n            assert resp.status_code == 302\n\n            # Who goes to the page to reset their 2fa as HOTP\n            otp_secret = \"0123456789abcdef0123456789abcdef01234567\"\n            b32_otp_secret = b32encode(unhexlify(otp_secret)).decode(\"ascii\")\n            resp = app.post(\"/account/reset-2fa-hotp\", data=dict(otp_secret=otp_secret))\n            assert resp.status_code == 200\n            assert (\n                '<form id=\"check-token\" method=\"post\" action=\"/account/verify-2fa-hotp\">'\n                in resp.data.decode()\n            )\n\n            with InstrumentedApp(journalist_app) as ins:\n                # And they successfully confirm the reset\n                app.post(\n                    \"/account/verify-2fa-hotp\", data=dict(token=HOTP(b32_otp_secret).generate(0))\n                )\n                ins.assert_message_flashed(\n                    \"Your two-factor credentials have been reset successfully.\", \"notification\"\n                )\n\n            # And they then log out\n            resp = app.post(\"/logout\")\n            assert resp.status_code == 302\n\n        # When they later try to login using a 2fa token based on their new HOTP secret\n        with journalist_app.test_client() as app, InstrumentedApp(journalist_app) as ins:\n            resp = app.post(\n                \"/login\",\n                data=dict(\n                    username=test_journo[\"username\"],\n                    password=test_journo[\"password\"],\n                    token=HOTP(b32_otp_secret).generate(1),\n                ),\n            )\n\n            # Then it succeeds\n            ins.assert_redirects(resp, \"/\")",
          "file": "test_two_factor_in_apps.py"
        },
        {
          "type": "function",
          "name": "test_rejects_already_used_totp_token",
          "code": "def test_rejects_already_used_totp_token(self, journalist_app, test_journo):\n        # Given a journalist that previously logged in using a specific 2fa token\n        token = TOTP(test_journo[\"otp_secret\"]).now()\n        with journalist_app.test_client() as app:\n            resp1 = app.post(\n                url_for(\"main.login\"),\n                data=dict(\n                    username=test_journo[\"username\"],\n                    password=test_journo[\"password\"],\n                    token=token,\n                ),\n                follow_redirects=True,\n            )\n            assert resp1.status_code == 200\n\n            resp2 = app.post(url_for(\"main.logout\"), follow_redirects=True)\n            assert resp2.status_code == 200\n\n        with journalist_app.app_context():\n            journo = Journalist.query.get(test_journo[\"id\"])\n            assert journo.last_token == token\n\n        # When they try to login again using the same 2fa token\n        with journalist_app.test_client() as app:\n            resp = app.post(\n                url_for(\"main.login\"),\n                data=dict(\n                    username=test_journo[\"username\"],\n                    password=test_journo[\"password\"],\n                    token=token,\n                ),\n            )\n\n            # Then a login error is displayed\n            assert resp.status_code == 200\n            text = resp.data.decode(\"utf-8\")\n            assert \"Login failed\" in text",
          "file": "test_two_factor_in_apps.py"
        },
        {
          "type": "function",
          "name": "test_rejects_already_used_totp_token_with_padding",
          "code": "def test_rejects_already_used_totp_token_with_padding(self, journalist_app, test_journo):\n        # Given a journalist that previously logged in using a specific 2fa token\n        token = TOTP(test_journo[\"otp_secret\"]).now()\n        with journalist_app.app_context():\n            Journalist.login(test_journo[\"username\"], test_journo[\"password\"], token)\n\n            # When they try to login again using the same token but with spaces added to the token\n            token_for_second_login = token + \"   \"\n\n            # Then it fails\n            with pytest.raises(OtpTokenInvalid, match=\"already used\"):\n                Journalist.login(\n                    test_journo[\"username\"], test_journo[\"password\"], token_for_second_login\n                )",
          "file": "test_two_factor_in_apps.py"
        },
        {
          "type": "function",
          "name": "test_rejects_already_used_totp_token_after_failed_login",
          "code": "def test_rejects_already_used_totp_token_after_failed_login(self, journalist_app, test_journo):\n        # Given a journalist that previously logged in using a specific 2fa token\n        token = TOTP(test_journo[\"otp_secret\"]).now()\n        with journalist_app.app_context():\n            Journalist.login(test_journo[\"username\"], test_journo[\"password\"], token)\n\n            # And they later tried to login using an invalid 2fa token\n            invalid_token = \"000000\"\n            with pytest.raises(OtpTokenInvalid):\n                Journalist.login(test_journo[\"username\"], test_journo[\"password\"], invalid_token)\n\n            # When they try to login again by reusing the initial/valid 2fa token\n            # Then it fails with the right exception: the last_used_token in the DB was not\n            #  overwritten by the token from the invalid login attempt\n            with pytest.raises(OtpTokenInvalid, match=\"already used\"):\n                Journalist.login(test_journo[\"username\"], test_journo[\"password\"], token)",
          "file": "test_two_factor_in_apps.py"
        },
        {
          "type": "function",
          "name": "test_rejects_user_with_invalid_otp_secret",
          "code": "def test_rejects_user_with_invalid_otp_secret(self, journalist_app, otp_secret):\n        # Given a journalist that has an invalid OTP secret\n        with journalist_app.app_context():\n            new_username = \"badotp\" + otp_secret\n            user, password = db_helper.init_journalist(is_admin=False)\n            user.otp_secret = otp_secret\n            user.username = new_username\n            db.session.add(user)\n            db.session.commit()\n\n        # When they try to login\n        with journalist_app.test_client() as app, InstrumentedApp(app) as ins:\n            resp = app.post(\n                url_for(\"main.login\"),\n                data={\n                    \"username\": new_username,\n                    \"password\": password,\n                    \"token\": \"705334\",\n                },\n                follow_redirects=True,\n            )\n\n            # It fails and they didn't get a session\n            assert resp.status_code == 200\n            assert session.get_user() is None\n\n            # And the corresponding error messages was displayed\n            assert len(ins.flashed_messages) == 1\n            assert \"2FA details are invalid\" in ins.flashed_messages[0][0]",
          "file": "test_two_factor_in_apps.py"
        },
        {
          "type": "function",
          "name": "test_can_login_after_regenerating_hotp",
          "code": "def test_can_login_after_regenerating_hotp(self, journalist_app, test_journo):\n        # Given a journalist logged into the journalist app\n        with journalist_app.test_client() as app:\n            resp = app.post(\n                \"/login\",\n                data=dict(\n                    username=test_journo[\"username\"],\n                    password=test_journo[\"password\"],\n                    token=TOTP(test_journo[\"otp_secret\"]).now(),\n                ),\n            )\n            assert resp.status_code == 302\n\n            # Who goes to the page to reset their 2fa as HOTP\n            otp_secret = \"0123456789abcdef0123456789abcdef01234567\"\n            b32_otp_secret = b32encode(unhexlify(otp_secret)).decode(\"ascii\")\n            resp = app.post(\"/account/reset-2fa-hotp\", data=dict(otp_secret=otp_secret))\n            assert resp.status_code == 200\n            assert (\n                '<form id=\"check-token\" method=\"post\" action=\"/account/verify-2fa-hotp\">'\n                in resp.data.decode()\n            )\n\n            with InstrumentedApp(journalist_app) as ins:\n                # And they successfully confirm the reset\n                app.post(\n                    \"/account/verify-2fa-hotp\", data=dict(token=HOTP(b32_otp_secret).generate(0))\n                )\n                ins.assert_message_flashed(\n                    \"Your two-factor credentials have been reset successfully.\", \"notification\"\n                )\n\n            # And they then log out\n            resp = app.post(\"/logout\")\n            assert resp.status_code == 302\n\n        # When they later try to login using a 2fa token based on their new HOTP secret\n        with journalist_app.test_client() as app, InstrumentedApp(journalist_app) as ins:\n            resp = app.post(\n                \"/login\",\n                data=dict(\n                    username=test_journo[\"username\"],\n                    password=test_journo[\"password\"],\n                    token=HOTP(b32_otp_secret).generate(1),\n                ),\n            )\n\n            # Then it succeeds\n            ins.assert_redirects(resp, \"/\")",
          "file": "test_two_factor_in_apps.py"
        },
        {
          "type": "class",
          "name": "TestTwoFactorInAdminApp",
          "code": "class TestTwoFactorInAdminApp:\n    def test_rejects_invalid_token_in_new_user_2fa_page(self, journalist_app, test_admin):\n        \"\"\"Regression test for https://github.com/freedomofpress/securedrop/pull/1692\"\"\"\n        # Given an admin logged into the admin app\n        with journalist_app.test_client() as app:\n            login_journalist(\n                app,\n                test_admin[\"username\"],\n                test_admin[\"password\"],\n                test_admin[\"otp_secret\"],\n            )\n\n            # When they try to submit an invalid token for setting up 2fa for a new user\n            invalid_token = \"000000\"\n            with InstrumentedApp(journalist_app) as ins:\n                resp = app.post(\n                    url_for(\"admin.new_user_two_factor_totp\"),\n                    data=dict(\n                        token=invalid_token,\n                        otp_secret=test_admin[\"otp_secret\"],\n                        userid=test_admin[\"id\"],\n                    ),\n                )\n\n                # Then the corresponding error message is displayed\n                assert resp.status_code == 200\n                ins.assert_message_flashed(\n                    \"There was a problem verifying the two-factor code. Please try again.\",\n                    \"error\",\n                )\n\n    def test_rejects_invalid_token_in_account_2fa_page(self, journalist_app, test_journo):\n        \"\"\"Regression test for https://github.com/freedomofpress/securedrop/pull/1692\"\"\"\n        # Given an admin logged into the admin app\n        with journalist_app.test_client() as app:\n            login_journalist(\n                app,\n                test_journo[\"username\"],\n                test_journo[\"password\"],\n                test_journo[\"otp_secret\"],\n            )\n\n            # When they try to submit an invalid token for setting up 2fa for themselves\n            invalid_token = \"000000\"\n            with InstrumentedApp(journalist_app) as ins:\n                resp = app.post(\n                    url_for(\"account.new_two_factor_totp\"),\n                    data=dict(token=invalid_token, otp_secret=test_journo[\"otp_secret\"]),\n                )\n\n                # Then the corresponding error message is displayed\n                assert resp.status_code == 200\n                ins.assert_message_flashed(\n                    \"There was a problem verifying the two-factor code. Please try again.\",\n                    \"error\",\n                )",
          "file": "test_two_factor_in_apps.py"
        },
        {
          "type": "function",
          "name": "test_rejects_invalid_token_in_new_user_2fa_page",
          "code": "def test_rejects_invalid_token_in_new_user_2fa_page(self, journalist_app, test_admin):\n        \"\"\"Regression test for https://github.com/freedomofpress/securedrop/pull/1692\"\"\"\n        # Given an admin logged into the admin app\n        with journalist_app.test_client() as app:\n            login_journalist(\n                app,\n                test_admin[\"username\"],\n                test_admin[\"password\"],\n                test_admin[\"otp_secret\"],\n            )\n\n            # When they try to submit an invalid token for setting up 2fa for a new user\n            invalid_token = \"000000\"\n            with InstrumentedApp(journalist_app) as ins:\n                resp = app.post(\n                    url_for(\"admin.new_user_two_factor_totp\"),\n                    data=dict(\n                        token=invalid_token,\n                        otp_secret=test_admin[\"otp_secret\"],\n                        userid=test_admin[\"id\"],\n                    ),\n                )\n\n                # Then the corresponding error message is displayed\n                assert resp.status_code == 200\n                ins.assert_message_flashed(\n                    \"There was a problem verifying the two-factor code. Please try again.\",\n                    \"error\",\n                )",
          "file": "test_two_factor_in_apps.py"
        },
        {
          "type": "function",
          "name": "test_rejects_invalid_token_in_account_2fa_page",
          "code": "def test_rejects_invalid_token_in_account_2fa_page(self, journalist_app, test_journo):\n        \"\"\"Regression test for https://github.com/freedomofpress/securedrop/pull/1692\"\"\"\n        # Given an admin logged into the admin app\n        with journalist_app.test_client() as app:\n            login_journalist(\n                app,\n                test_journo[\"username\"],\n                test_journo[\"password\"],\n                test_journo[\"otp_secret\"],\n            )\n\n            # When they try to submit an invalid token for setting up 2fa for themselves\n            invalid_token = \"000000\"\n            with InstrumentedApp(journalist_app) as ins:\n                resp = app.post(\n                    url_for(\"account.new_two_factor_totp\"),\n                    data=dict(token=invalid_token, otp_secret=test_journo[\"otp_secret\"]),\n                )\n\n                # Then the corresponding error message is displayed\n                assert resp.status_code == 200\n                ins.assert_message_flashed(\n                    \"There was a problem verifying the two-factor code. Please try again.\",\n                    \"error\",\n                )",
          "file": "test_two_factor_in_apps.py"
        }
      ],
      "factories.py": [
        {
          "type": "function",
          "name": "_generate_random_token",
          "code": "def _generate_random_token() -> str:\n    return secrets.token_hex(32)",
          "file": "factories.py"
        },
        {
          "type": "class",
          "name": "JournalistInterfaceConfigFactory",
          "code": "class JournalistInterfaceConfigFactory:\n    @staticmethod\n    def create() -> JournalistInterfaceConfig:\n        return JournalistInterfaceConfig(\n            SESSION_COOKIE_NAME=\"js\",\n            SECRET_KEY=_generate_random_token(),\n            TESTING=True,\n            DEBUG=False,\n            MAX_CONTENT_LENGTH=524288000,\n            USE_X_SENDFILE=False,\n            # Disable CSRF checks to make writing tests easier\n            WTF_CSRF_ENABLED=False,\n        )",
          "file": "factories.py"
        },
        {
          "type": "function",
          "name": "create",
          "code": "def create() -> JournalistInterfaceConfig:\n        return JournalistInterfaceConfig(\n            SESSION_COOKIE_NAME=\"js\",\n            SECRET_KEY=_generate_random_token(),\n            TESTING=True,\n            DEBUG=False,\n            MAX_CONTENT_LENGTH=524288000,\n            USE_X_SENDFILE=False,\n            # Disable CSRF checks to make writing tests easier\n            WTF_CSRF_ENABLED=False,\n        )",
          "file": "factories.py"
        },
        {
          "type": "class",
          "name": "SourceInterfaceConfigFactory",
          "code": "class SourceInterfaceConfigFactory:\n    @staticmethod\n    def create() -> SourceInterfaceConfig:\n        return SourceInterfaceConfig(\n            SESSION_COOKIE_NAME=\"ss\",\n            SECRET_KEY=_generate_random_token(),\n            TESTING=True,\n            DEBUG=False,\n            MAX_CONTENT_LENGTH=524288000,\n            USE_X_SENDFILE=False,\n            # Disable CSRF checks to make writing tests easier\n            WTF_CSRF_ENABLED=False,\n        )",
          "file": "factories.py"
        },
        {
          "type": "function",
          "name": "create",
          "code": "def create() -> SourceInterfaceConfig:\n        return SourceInterfaceConfig(\n            SESSION_COOKIE_NAME=\"ss\",\n            SECRET_KEY=_generate_random_token(),\n            TESTING=True,\n            DEBUG=False,\n            MAX_CONTENT_LENGTH=524288000,\n            USE_X_SENDFILE=False,\n            # Disable CSRF checks to make writing tests easier\n            WTF_CSRF_ENABLED=False,\n        )",
          "file": "factories.py"
        },
        {
          "type": "class",
          "name": "SecureDropConfigFactory",
          "code": "class SecureDropConfigFactory:\n    @staticmethod\n    def create(\n        SECUREDROP_DATA_ROOT: Path,\n        GPG_KEY_DIR: Path,\n        JOURNALIST_KEY: str,\n        RQ_WORKER_NAME: str,\n        SESSION_EXPIRATION_MINUTES: float = 120,\n        NOUNS: Optional[Path] = None,\n        ADJECTIVES: Optional[Path] = None,\n        SUPPORTED_LOCALES: Optional[List[str]] = None,\n        DEFAULT_LOCALE: str = \"en_US\",\n        TRANSLATION_DIRS: Path = DEFAULT_SECUREDROP_ROOT / \"translations\",\n    ) -> SecureDropConfig:\n        \"\"\"Create a securedrop config suitable for the unit tests.\n\n        It will erase any existing file within SECUREDROP_DATA_ROOT and then create an initialized\n        DB at SECUREDROP_DATA_ROOT/db.sqlite which will be set as the DATABASE_FILE.\n        \"\"\"\n        # Clear the data root directory\n        if SECUREDROP_DATA_ROOT.exists():\n            shutil.rmtree(SECUREDROP_DATA_ROOT)\n        SECUREDROP_DATA_ROOT.mkdir(parents=True)\n        database_file = SECUREDROP_DATA_ROOT / \"db.sqlite\"\n\n        dictionaries_path = DEFAULT_SECUREDROP_ROOT / \"dictionaries\"\n\n        config = SecureDropConfig(\n            SESSION_EXPIRATION_MINUTES=SESSION_EXPIRATION_MINUTES,\n            SECUREDROP_DATA_ROOT=SECUREDROP_DATA_ROOT,\n            SECUREDROP_ROOT=DEFAULT_SECUREDROP_ROOT,\n            DATABASE_FILE=database_file,\n            JOURNALIST_APP_FLASK_CONFIG_CLS=JournalistInterfaceConfigFactory.create(),\n            SOURCE_APP_FLASK_CONFIG_CLS=SourceInterfaceConfigFactory.create(),\n            SCRYPT_GPG_PEPPER=_generate_random_token(),\n            SCRYPT_ID_PEPPER=_generate_random_token(),\n            SCRYPT_PARAMS=dict(N=2**14, r=8, p=1),\n            RQ_WORKER_NAME=RQ_WORKER_NAME,\n            NOUNS=NOUNS if NOUNS else dictionaries_path / \"nouns.txt\",\n            ADJECTIVES=ADJECTIVES if ADJECTIVES else dictionaries_path / \"adjectives.txt\",\n            GPG_KEY_DIR=GPG_KEY_DIR,\n            JOURNALIST_KEY=JOURNALIST_KEY,\n            SUPPORTED_LOCALES=SUPPORTED_LOCALES if SUPPORTED_LOCALES is not None else [\"en_US\"],\n            STATIC_DIR=DEFAULT_SECUREDROP_ROOT / \"static\",\n            TRANSLATION_DIRS=TRANSLATION_DIRS,\n            SOURCE_TEMPLATES_DIR=DEFAULT_SECUREDROP_ROOT / \"source_templates\",\n            JOURNALIST_TEMPLATES_DIR=DEFAULT_SECUREDROP_ROOT / \"journalist_templates\",\n            DEFAULT_LOCALE=DEFAULT_LOCALE,\n            REDIS_PASSWORD=REDIS_PASSWORD,\n        )\n\n        # Delete any previous/existing DB and initialize a new one\n        reset_database(database_file)\n        with _get_fake_db_module(database_uri=config.DATABASE_URI) as initialized_db_module:\n            initialized_db_module.create_all()\n\n        # Create the other directories\n        config.TEMP_DIR.mkdir(parents=True)\n        config.STORE_DIR.mkdir(parents=True)\n\n        # Copy the journalist public key into DATA_ROOT\n        shutil.copy2(\n            Path(__file__).parent / \"files/test_journalist_key.pub\",\n            config.SECUREDROP_DATA_ROOT / \"journalist.pub\",\n        )\n        # All done\n        return config",
          "file": "factories.py"
        },
        {
          "type": "function",
          "name": "create",
          "code": "def create(\n        SECUREDROP_DATA_ROOT: Path,\n        GPG_KEY_DIR: Path,\n        JOURNALIST_KEY: str,\n        RQ_WORKER_NAME: str,\n        SESSION_EXPIRATION_MINUTES: float = 120,\n        NOUNS: Optional[Path] = None,\n        ADJECTIVES: Optional[Path] = None,\n        SUPPORTED_LOCALES: Optional[List[str]] = None,\n        DEFAULT_LOCALE: str = \"en_US\",\n        TRANSLATION_DIRS: Path = DEFAULT_SECUREDROP_ROOT / \"translations\",\n    ) -> SecureDropConfig:\n        \"\"\"Create a securedrop config suitable for the unit tests.\n\n        It will erase any existing file within SECUREDROP_DATA_ROOT and then create an initialized\n        DB at SECUREDROP_DATA_ROOT/db.sqlite which will be set as the DATABASE_FILE.\n        \"\"\"\n        # Clear the data root directory\n        if SECUREDROP_DATA_ROOT.exists():\n            shutil.rmtree(SECUREDROP_DATA_ROOT)\n        SECUREDROP_DATA_ROOT.mkdir(parents=True)\n        database_file = SECUREDROP_DATA_ROOT / \"db.sqlite\"\n\n        dictionaries_path = DEFAULT_SECUREDROP_ROOT / \"dictionaries\"\n\n        config = SecureDropConfig(\n            SESSION_EXPIRATION_MINUTES=SESSION_EXPIRATION_MINUTES,\n            SECUREDROP_DATA_ROOT=SECUREDROP_DATA_ROOT,\n            SECUREDROP_ROOT=DEFAULT_SECUREDROP_ROOT,\n            DATABASE_FILE=database_file,\n            JOURNALIST_APP_FLASK_CONFIG_CLS=JournalistInterfaceConfigFactory.create(),\n            SOURCE_APP_FLASK_CONFIG_CLS=SourceInterfaceConfigFactory.create(),\n            SCRYPT_GPG_PEPPER=_generate_random_token(),\n            SCRYPT_ID_PEPPER=_generate_random_token(),\n            SCRYPT_PARAMS=dict(N=2**14, r=8, p=1),\n            RQ_WORKER_NAME=RQ_WORKER_NAME,\n            NOUNS=NOUNS if NOUNS else dictionaries_path / \"nouns.txt\",\n            ADJECTIVES=ADJECTIVES if ADJECTIVES else dictionaries_path / \"adjectives.txt\",\n            GPG_KEY_DIR=GPG_KEY_DIR,\n            JOURNALIST_KEY=JOURNALIST_KEY,\n            SUPPORTED_LOCALES=SUPPORTED_LOCALES if SUPPORTED_LOCALES is not None else [\"en_US\"],\n            STATIC_DIR=DEFAULT_SECUREDROP_ROOT / \"static\",\n            TRANSLATION_DIRS=TRANSLATION_DIRS,\n            SOURCE_TEMPLATES_DIR=DEFAULT_SECUREDROP_ROOT / \"source_templates\",\n            JOURNALIST_TEMPLATES_DIR=DEFAULT_SECUREDROP_ROOT / \"journalist_templates\",\n            DEFAULT_LOCALE=DEFAULT_LOCALE,\n            REDIS_PASSWORD=REDIS_PASSWORD,\n        )\n\n        # Delete any previous/existing DB and initialize a new one\n        reset_database(database_file)\n        with _get_fake_db_module(database_uri=config.DATABASE_URI) as initialized_db_module:\n            initialized_db_module.create_all()\n\n        # Create the other directories\n        config.TEMP_DIR.mkdir(parents=True)\n        config.STORE_DIR.mkdir(parents=True)\n\n        # Copy the journalist public key into DATA_ROOT\n        shutil.copy2(\n            Path(__file__).parent / \"files/test_journalist_key.pub\",\n            config.SECUREDROP_DATA_ROOT / \"journalist.pub\",\n        )\n        # All done\n        return config",
          "file": "factories.py"
        }
      ],
      "test_db.py": [
        {
          "type": "function",
          "name": "test_get_one_or_else_returns_one",
          "code": "def test_get_one_or_else_returns_one(journalist_app, test_journo):\n    with journalist_app.app_context():\n        # precondition: there must be one journalist\n        assert Journalist.query.count() == 1\n\n        query = Journalist.query.filter_by(username=test_journo[\"username\"])\n        selected_journo = get_one_or_else(query, MagicMock(), MagicMock())\n\n        assert selected_journo.id == test_journo[\"id\"]",
          "file": "test_db.py"
        },
        {
          "type": "function",
          "name": "test_get_one_or_else_multiple_results",
          "code": "def test_get_one_or_else_multiple_results(journalist_app, test_admin, test_journo):\n    with journalist_app.app_context():\n        # precondition: there must be multiple journalists\n        assert Journalist.query.count() == 2\n\n        mock_logger = MagicMock()\n        mock_abort = MagicMock()\n\n        # this is equivalent to \"SELECT *\" which we know returns 2\n        query = Journalist.query\n        get_one_or_else(query, mock_logger, mock_abort)\n        # Not specifying the very long log line in `logger.error`\n        mock_logger.error.assert_called()\n        mock_abort.assert_called_with(500)",
          "file": "test_db.py"
        },
        {
          "type": "function",
          "name": "test_get_one_or_else_no_result_found",
          "code": "def test_get_one_or_else_no_result_found(journalist_app, test_journo):\n    with journalist_app.app_context():\n        # precondition: there must be one journalist\n        assert Journalist.query.count() == 1\n\n        bad_name = test_journo[\"username\"] + \"aaaaaa\"\n        query = Journalist.query.filter_by(username=bad_name)\n\n        mock_logger = MagicMock()\n        mock_abort = MagicMock()\n        get_one_or_else(query, mock_logger, mock_abort)\n\n        log_line = \"Found none when one was expected: \" \"No row was found for one()\"\n        mock_logger.error.assert_called_with(log_line)\n        mock_abort.assert_called_with(404)",
          "file": "test_db.py"
        },
        {
          "type": "function",
          "name": "test_throttle_login",
          "code": "def test_throttle_login(journalist_app, test_journo):\n    with journalist_app.app_context():\n        journalist = test_journo[\"journalist\"]\n        for _ in range(Journalist._MAX_LOGIN_ATTEMPTS_PER_PERIOD):\n            Journalist.throttle_login(journalist)\n        with pytest.raises(LoginThrottledException):\n            Journalist.throttle_login(journalist)",
          "file": "test_db.py"
        },
        {
          "type": "function",
          "name": "test_submission_string_representation",
          "code": "def test_submission_string_representation(journalist_app, test_source, app_storage):\n    with journalist_app.app_context():\n        db_helper.submit(app_storage, test_source[\"source\"], 2)\n        test_submission = Submission.query.first()\n        test_submission.__repr__()",
          "file": "test_db.py"
        },
        {
          "type": "function",
          "name": "test_reply_string_representation",
          "code": "def test_reply_string_representation(journalist_app, test_journo, test_source, app_storage):\n    with journalist_app.app_context():\n        db_helper.reply(app_storage, test_journo[\"journalist\"], test_source[\"source\"], 2)\n        test_reply = Reply.query.first()\n        test_reply.__repr__()",
          "file": "test_db.py"
        },
        {
          "type": "function",
          "name": "test_journalist_string_representation",
          "code": "def test_journalist_string_representation(journalist_app, test_journo):\n    with journalist_app.app_context():\n        test_journo[\"journalist\"].__repr__()",
          "file": "test_db.py"
        },
        {
          "type": "function",
          "name": "test_source_string_representation",
          "code": "def test_source_string_representation(journalist_app, test_source):\n    with journalist_app.app_context():\n        test_source[\"source\"].__repr__()",
          "file": "test_db.py"
        },
        {
          "type": "function",
          "name": "test_only_one_active_instance_config_can_exist",
          "code": "def test_only_one_active_instance_config_can_exist(config, source_app):\n    \"\"\"\n    Checks that attempts to add multiple active InstanceConfig records fail.\n\n    InstanceConfig is supposed to be invalidated by setting\n    valid_until to the time the config was no longer in effect. Until\n    we added the partial index preventing multiple rows with a null\n    valid_until, it was possible for the system to create multiple\n    active records, which would cause MultipleResultsFound exceptions\n    in InstanceConfig.get_current.\n    \"\"\"\n    # create a separate session\n    engine = create_engine(source_app.config[\"SQLALCHEMY_DATABASE_URI\"])\n    session = sessionmaker(bind=engine)()\n\n    # in the separate session, create an InstanceConfig with default\n    # values, but don't commit it\n    conflicting_config = InstanceConfig()\n    session.add(conflicting_config)\n\n    with source_app.app_context():\n        # get_current will create another InstanceConfig with default values\n        InstanceConfig.get_current()\n\n    # now the commit of the first instance should fail\n    with pytest.raises(IntegrityError):\n        session.commit()",
          "file": "test_db.py"
        }
      ],
      "test_journalist_session.py": [
        {
          "type": "function",
          "name": "redis",
          "code": "def redis(config):\n    return Redis(**config.REDIS_KWARGS)",
          "file": "test_journalist_session.py"
        },
        {
          "type": "function",
          "name": "_check_sig",
          "code": "def _check_sig(session_cookie, journalist_app, api=False):\n    if api:\n        salt = \"api_\" + journalist_app.config[\"SESSION_SIGNER_SALT\"]\n    else:\n        salt = journalist_app.config[\"SESSION_SIGNER_SALT\"]\n\n    signer = URLSafeTimedSerializer(journalist_app.secret_key, salt)\n    return signer.loads(session_cookie)",
          "file": "test_journalist_session.py"
        },
        {
          "type": "function",
          "name": "_get_session",
          "code": "def _get_session(sid, journalist_app, redis, api=False):\n    if api:\n        key = \"api_\" + journalist_app.config[\"SESSION_KEY_PREFIX\"] + sid\n    else:\n        key = journalist_app.config[\"SESSION_KEY_PREFIX\"] + sid\n\n    return session_json_serializer.loads(redis.get(key))",
          "file": "test_journalist_session.py"
        },
        {
          "type": "function",
          "name": "test_session_login",
          "code": "def test_session_login(journalist_app, test_journo, redis):\n    # Given a test client and a valid journalist user\n    with journalist_app.test_client() as app:\n        # When sending a correct login request\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n\n        # When checking the local session cookie jar\n        session_cookie = _session_from_cookiejar(app.cookie_jar, journalist_app)\n        # Then there is a session cookie in it\n        assert session_cookie is not None\n        # Verify correct `SameSite` value was set on the cookie\n        assert session_cookie.get_nonstandard_attr(\"SameSite\") == \"Strict\"\n\n        # Then such cookie is properly signed\n        sid = _check_sig(session_cookie.value, journalist_app)\n        # Then such session cookie has a corresponding payload in redis\n        redis_session = _get_session(sid, journalist_app, redis)\n        ttl = redis.ttl(journalist_app.config[\"SESSION_KEY_PREFIX\"] + sid)\n\n        # Then the TTL of such key in redis conforms to the lifetime configuration\n        assert (\n            (journalist_app.config[\"SESSION_LIFETIME\"] - 10)\n            < ttl\n            <= journalist_app.config[\"SESSION_LIFETIME\"]\n        )\n\n        # Then the user id of the user who logged in is the same as the user id in session\n        assert redis_session[\"uid\"] == test_journo[\"id\"]\n\n        # Finally load the main page\n        resp = app.get(url_for(\"main.index\"))\n        # And expect a successful status code\n        assert resp.status_code == 200",
          "file": "test_journalist_session.py"
        },
        {
          "type": "function",
          "name": "test_session_renew",
          "code": "def test_session_renew(journalist_app, test_journo, redis):\n    # Given a test client and a valid journalist user\n    with journalist_app.test_client() as app:\n        # When sending a correct login request\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n        # Then check session existence, signature, and redis payload\n        session_cookie = _session_from_cookiejar(app.cookie_jar, journalist_app)\n        assert session_cookie is not None\n\n        sid = _check_sig(session_cookie.value, journalist_app)\n        redis_session = _get_session(sid, journalist_app, redis)\n        # The `renew_count` must exists in the session payload and must be equal to the app config\n        assert redis_session[\"renew_count\"] == journalist_app.config[\"SESSION_RENEW_COUNT\"]\n\n        # When forcing the session TTL in redis to be below the threshold\n        # Threshold for auto renew is less than 60*30\n        redis.setex(\n            name=journalist_app.config[\"SESSION_KEY_PREFIX\"] + sid,\n            value=session_json_serializer.dumps(redis_session),\n            time=15 * 60,\n        )\n        # When doing a generic request to trigger the auto-renew\n        resp = app.get(url_for(\"main.index\"))\n        # Then the session must still be valid\n        assert resp.status_code == 200\n\n        # Then the corresponding renew_count in redis must have been decreased\n        redis_session = _get_session(sid, journalist_app, redis)\n        assert redis_session[\"renew_count\"] == (journalist_app.config[\"SESSION_RENEW_COUNT\"] - 1)\n\n        # Then the ttl must have been updated and the new lifetime must be > of app confing lifetime\n        # (Bigger because there is also a variable amount of time in the threshold that is kept)\n        ttl = redis.ttl(journalist_app.config[\"SESSION_KEY_PREFIX\"] + sid)\n        assert ttl > journalist_app.config[\"SESSION_LIFETIME\"]",
          "file": "test_journalist_session.py"
        },
        {
          "type": "function",
          "name": "test_session_logout",
          "code": "def test_session_logout(journalist_app, test_journo, redis):\n    # Given a test client and a valid journalist user\n    with journalist_app.test_client() as app:\n        # When sending a correct login request\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n        # Then check session as in the previous tests\n        session_cookie = _session_from_cookiejar(app.cookie_jar, journalist_app)\n        assert session_cookie is not None\n\n        sid = _check_sig(session_cookie.value, journalist_app)\n        assert (redis.get(journalist_app.config[\"SESSION_KEY_PREFIX\"] + sid)) is not None\n\n        # When sending a logout request from a logged in journalist\n        resp = app.post(url_for(\"main.logout\"), follow_redirects=False)\n        # Then it redirects to login\n        assert resp.status_code == 302\n        # Then the session no longer exists in redis\n        assert (redis.get(journalist_app.config[\"SESSION_KEY_PREFIX\"] + sid)) is None\n\n        # Then a request to the index redirects back to login\n        resp = app.get(url_for(\"main.index\"), follow_redirects=False)\n        assert resp.status_code == 302",
          "file": "test_journalist_session.py"
        },
        {
          "type": "function",
          "name": "test_session_admin_change_password_logout",
          "code": "def test_session_admin_change_password_logout(journalist_app, test_journo, test_admin, redis):\n    # Given a test client and a valid journalist user\n    with journalist_app.test_client() as app:\n        # When sending a correct login request\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n        session_cookie = _session_from_cookiejar(app.cookie_jar, journalist_app)\n        assert session_cookie is not None\n        # Then save the cookie for later\n        cookie_val = session_cookie.value\n        # Then also save the session id for later\n        sid = _check_sig(session_cookie.value, journalist_app)\n        assert (redis.get(journalist_app.config[\"SESSION_KEY_PREFIX\"] + sid)) is not None\n\n    # Given a another test client and a valid admin user\n    with journalist_app.test_client() as admin_app:\n        # When sending a valid login request as the admin user\n        login_journalist(\n            admin_app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        # When changing password of the journalist (non-admin) user\n        prepare_password_change(admin_app, test_journo[\"id\"], NEW_PASSWORD)\n        resp = admin_app.post(\n            url_for(\"admin.new_password\", user_id=test_journo[\"id\"]),\n            data=dict(password=NEW_PASSWORD),\n            follow_redirects=False,\n        )\n        # Then the password change has been successful\n        assert resp.status_code == 302\n        # Then the journalist (non-admin) user session does no longer exist in redis\n        assert (redis.get(journalist_app.config[\"SESSION_KEY_PREFIX\"] + sid)) is None\n\n    with journalist_app.test_client() as app:\n        # Add our original cookie back into the session, and try to re-use it\n        app.set_cookie(\n            \"localhost.localdomain\",\n            \"js\",\n            cookie_val,\n            domain=\".localhost.localdomain\",\n            httponly=True,\n            path=\"/\",\n        )\n        resp = app.get(url_for(\"main.index\"), follow_redirects=False)\n        # Then trying to reuse the same journalist user cookie fails and redirects\n        assert resp.status_code == 302",
          "file": "test_journalist_session.py"
        },
        {
          "type": "function",
          "name": "test_session_change_password_logout",
          "code": "def test_session_change_password_logout(journalist_app, test_journo, redis):\n    # Given a test client and a valid journalist user\n    with journalist_app.test_client() as app:\n        # When sending a correct login request\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n        # Then check session as the previous tests\n        session_cookie = _session_from_cookiejar(app.cookie_jar, journalist_app)\n        assert session_cookie is not None\n\n        sid = _check_sig(session_cookie.value, journalist_app)\n        assert (redis.get(journalist_app.config[\"SESSION_KEY_PREFIX\"] + sid)) is not None\n\n        # When sending a self change password request\n        prepare_password_change(app, test_journo[\"id\"], NEW_PASSWORD)\n        resp = app.post(\n            url_for(\"account.new_password\"),\n            data=dict(\n                current_password=test_journo[\"password\"],\n                token=TOTP(test_journo[\"otp_secret\"]).now(),\n                password=NEW_PASSWORD,\n            ),\n        )\n        # Then the session is no longer valid\n        assert resp.status_code == 302\n        # Then the session no longer exists in redis\n        assert (redis.get(journalist_app.config[\"SESSION_KEY_PREFIX\"] + sid)) is None\n\n        # Then a request for the index redirects back to login\n        resp = app.get(url_for(\"main.index\"), follow_redirects=False)\n        assert resp.status_code == 302",
          "file": "test_journalist_session.py"
        },
        {
          "type": "function",
          "name": "test_session_login_regenerate_sid",
          "code": "def test_session_login_regenerate_sid(journalist_app, test_journo):\n    # Given a test client and a valid journalist user\n    with journalist_app.test_client() as app:\n        # When sending an anonymous get request\n        resp = app.get(url_for(\"main.login\"))\n        # Then check the response code is correct\n        assert resp.status_code == 200\n\n        # Given a valid unauthenticated session id from the previous request\n        session_cookie_pre_login = _session_from_cookiejar(app.cookie_jar, journalist_app)\n        assert session_cookie_pre_login is not None\n\n        # When sending a valid login request using the same client (same cookiejar)\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n        session_cookie_post_login = _session_from_cookiejar(app.cookie_jar, journalist_app)\n        # Then the two session ids are different as the session id gets regenerated post login\n        assert session_cookie_post_login != session_cookie_pre_login",
          "file": "test_journalist_session.py"
        },
        {
          "type": "function",
          "name": "test_session_api_login",
          "code": "def test_session_api_login(journalist_app, test_journo, redis):\n    # Given a test client and a valid journalist user\n    with journalist_app.test_client() as app:\n        # When sending a `get_token` request to the API with valid creds\n        resp = app.post(\n            url_for(\"api.get_token\"),\n            data=json.dumps(\n                {\n                    \"username\": test_journo[\"username\"],\n                    \"passphrase\": test_journo[\"password\"],\n                    \"one_time_code\": TOTP(test_journo[\"otp_secret\"]).now(),\n                }\n            ),\n            headers=get_api_headers(),\n        )\n\n        # Then the API token is issued and returned with the correct journalist id\n        assert resp.json[\"journalist_uuid\"] == test_journo[\"uuid\"]\n        assert resp.status_code == 200\n\n        # Then such token is properly signed\n        sid = _check_sig(resp.json[\"token\"], journalist_app, api=True)\n        redis_session = _get_session(sid, journalist_app, redis, api=True)\n        # Then the session id in redis match that of the credentials\n        assert redis_session[\"uid\"] == test_journo[\"id\"]\n\n        # Then the ttl of the session in redis is lower than the lifetime configured in the app\n        ttl = redis.ttl(\"api_\" + journalist_app.config[\"SESSION_KEY_PREFIX\"] + sid)\n        assert (\n            (journalist_app.config[\"SESSION_LIFETIME\"] - 10)\n            < ttl\n            <= journalist_app.config[\"SESSION_LIFETIME\"]\n        )\n\n        # Then the expiration date returned in `get_token` response also conforms to the same rules\n        assert (\n            datetime.now(timezone.utc)\n            < datetime.fromisoformat(resp.json[\"expiration\"])\n            < (\n                datetime.now(timezone.utc)\n                + timedelta(seconds=journalist_app.config[\"SESSION_LIFETIME\"])\n            )\n        )\n\n        # When querying the endpoint that return the corrent user with the token\n        response = app.get(\n            url_for(\"api.get_current_user\"), headers=get_api_headers(resp.json[\"token\"])\n        )\n        # Then the request is successful and the correct journalist id is returned\n        assert response.status_code == 200\n        assert response.json[\"uuid\"] == test_journo[\"uuid\"]",
          "file": "test_journalist_session.py"
        },
        {
          "type": "function",
          "name": "test_session_api_logout",
          "code": "def test_session_api_logout(journalist_app, test_journo, redis):\n    # Given a test client and a valid journalist user\n    with journalist_app.test_client() as app:\n        # When sending a valid login request and asking an API token\n        resp = app.post(\n            url_for(\"api.get_token\"),\n            data=json.dumps(\n                {\n                    \"username\": test_journo[\"username\"],\n                    \"passphrase\": test_journo[\"password\"],\n                    \"one_time_code\": TOTP(test_journo[\"otp_secret\"]).now(),\n                }\n            ),\n            headers=get_api_headers(),\n        )\n\n        # Then the token is issued successfully with the correct attributed\n        assert resp.json[\"journalist_uuid\"] == test_journo[\"uuid\"]\n        assert resp.status_code == 200\n        token = resp.json[\"token\"]\n        sid = _check_sig(token, journalist_app, api=True)\n\n        # When querying the endpoint for the current user information\n        resp = app.get(url_for(\"api.get_current_user\"), headers=get_api_headers(token))\n        # Then the request is successful and the returned id matches the creds journalist id\n        assert resp.status_code == 200\n        assert resp.json[\"uuid\"] == test_journo[\"uuid\"]\n\n        # When sending a logout request using the API token\n        resp = app.post(url_for(\"api.logout\"), headers=get_api_headers(token))\n        # Then it is successful\n        assert resp.status_code == 200\n        # Then the token and the corresponding payload no longer exist in redis\n        assert (redis.get(\"api_\" + journalist_app.config[\"SESSION_KEY_PREFIX\"] + sid)) is None\n\n        # When sending an authenticated request with the deleted token\n        resp = app.get(url_for(\"api.get_current_user\"), headers=get_api_headers(token))\n        # Then the request is unsuccessful\n        assert resp.status_code == 403",
          "file": "test_journalist_session.py"
        },
        {
          "type": "function",
          "name": "test_session_bad_signature",
          "code": "def test_session_bad_signature(journalist_app, test_journo):\n    # Given a test client and a valid journalist user\n    with journalist_app.test_client() as app:\n        # When sending a valid login request and asking an API token\n        resp = app.post(\n            url_for(\"api.get_token\"),\n            data=json.dumps(\n                {\n                    \"username\": test_journo[\"username\"],\n                    \"passphrase\": test_journo[\"password\"],\n                    \"one_time_code\": TOTP(test_journo[\"otp_secret\"]).now(),\n                }\n            ),\n            headers=get_api_headers(),\n        )\n\n        # Then the request is successful and the uid matched the creds one\n        assert resp.json[\"journalist_uuid\"] == test_journo[\"uuid\"]\n        assert resp.status_code == 200\n\n        # Given the valid token in the response\n        token = resp.json[\"token\"]\n        # When checking the signature and sreipping it\n        sid = _check_sig(token, journalist_app, api=True)\n\n        # When requesting an authenticated endpoint with a valid unsigned token\n        resp = app.get(url_for(\"api.get_current_user\"), headers=get_api_headers(sid))\n        # Then the request is refused\n        assert resp.status_code == 403\n\n        # When requesting an authenticated endpoint with a valid unsigned token with a trailing dot\n        resp = app.get(url_for(\"api.get_current_user\"), headers=get_api_headers(sid + \".\"))\n        # Then the request is refused\n        assert resp.status_code == 403\n\n        # Given the correct app secret key and a wrong salt\n        signer = URLSafeTimedSerializer(journalist_app.secret_key, \"wrong_salt\")\n        # Given a valid token signed with the correct secret key and the wrong salt\n        token_wrong_salt = signer.dumps(sid)\n\n        # When requesting an authenticated endpoint with a valid token signed with the wrong salt\n        resp = app.get(url_for(\"api.get_current_user\"), headers=get_api_headers(token_wrong_salt))\n        # Then the request is refused\n        assert resp.status_code == 403\n\n        # Given the correct app secret and the Journalist Interface salt\n        signer = URLSafeTimedSerializer(\n            journalist_app.secret_key, journalist_app.config[\"SESSION_SIGNER_SALT\"]\n        )\n        # Given a valid token signed with the corrects secret key and the Journalist Interface salt\n        token_not_api_salt = signer.dumps(sid)\n\n        # When requesting an authenticated endpoint with such token\n        resp = app.get(url_for(\"api.get_current_user\"), headers=get_api_headers(token_not_api_salt))\n        # Then the request is refused since the JI salt is not valid for the API\n        assert resp.status_code == 403\n\n        # When sending again an authenticated request with the original, valid, signed token\n        resp = app.get(url_for(\"api.get_current_user\"), headers=get_api_headers(token))\n        # Then the request is successful\n        assert resp.status_code == 200\n        assert resp.json[\"uuid\"] == test_journo[\"uuid\"]",
          "file": "test_journalist_session.py"
        },
        {
          "type": "function",
          "name": "test_session_race_condition",
          "code": "def test_session_race_condition(mocker, journalist_app, test_journo, redis):\n    # Given a test client and a valid journalist user\n    with journalist_app.test_request_context() as app:\n        # When manually creating a session in the context\n        session = journalist_app.session_interface.open_session(journalist_app, app.request)\n        assert session.sid is not None\n        # When manually setting the journalist uid in session\n        session[\"uid\"] = test_journo[\"id\"]\n\n        # When manually building a Flask response object\n        app.response = Response()\n        # When manually calling save_session() to write the session in redis\n        journalist_app.session_interface.save_session(journalist_app, session, app.response)\n        # Then the session gets written in redis\n        assert redis.get(journalist_app.config[\"SESSION_KEY_PREFIX\"] + session.sid) is not None\n\n        # When manually adding the created session token in the request cookies\n        app.request.cookies = {journalist_app.config[\"SESSION_COOKIE_NAME\"]: session.token}\n        # When getting the session object by supplying a request context to open_session()\n        session2 = journalist_app.session_interface.open_session(journalist_app, app.request)\n        # Then the properties of the two sessions are the same\n        # (They are indeed the same session)\n        assert session2.sid == session.sid\n        assert session2[\"uid\"] == test_journo[\"id\"]\n        # When setting the modified properties to issue a write in redis\n        # (To force entering the redis set xx case)\n        session.modified = True\n        session.new = False\n        session.to_regenerate = False\n        # When deleting the original session token and object from redis\n        redis.delete(journalist_app.config[\"SESSION_KEY_PREFIX\"] + session.sid)\n        # Then the session_save() fails since the original object no longer exists\n        journalist_app.session_interface.save_session(journalist_app, session, app.response)\n        assert redis.get(journalist_app.config[\"SESSION_KEY_PREFIX\"] + session.sid) is None",
          "file": "test_journalist_session.py"
        }
      ],
      "test_i18n.py": [
        {
          "type": "function",
          "name": "create_config_for_i18n_test",
          "code": "def create_config_for_i18n_test(\n    supported_locales: List[str],\n    default_locale: str = \"en_US\",\n    translation_dirs: Path = DEFAULT_SECUREDROP_ROOT / \"translations\",\n) -> SecureDropConfig:\n    tmp_root_for_test = Path(\"/tmp/sd-tests/test_i18n\")\n    tmp_root_for_test.mkdir(exist_ok=True, parents=True)\n\n    i18n_config = SecureDropConfigFactory.create(\n        SECUREDROP_DATA_ROOT=tmp_root_for_test,\n        DEFAULT_LOCALE=default_locale,\n        SUPPORTED_LOCALES=supported_locales,\n        TRANSLATION_DIRS=translation_dirs,\n        # For the tests in these files, the following argument / config fields are not used so\n        # we set them to invalid values\n        RQ_WORKER_NAME=\"\",\n        GPG_KEY_DIR=tmp_root_for_test,\n        JOURNALIST_KEY=\"\",\n    )\n\n    # Create an empty key file just to pass the sanity checks when starting the source or\n    # journalist app; the encryption code is not exercised as part of these tests\n    gpg_key_path = tmp_root_for_test / \"private-keys-v1.d\"\n    gpg_key_path.mkdir(exist_ok=True)\n\n    return i18n_config",
          "file": "test_i18n.py"
        },
        {
          "type": "function",
          "name": "set_msg_translation_in_po_file",
          "code": "def set_msg_translation_in_po_file(po_file: Path, msgid_to_translate: str, msgstr: str) -> None:\n    po_content = po_file.read_text()\n    content_to_update = f\"\"\"\nmsgid \"{msgid_to_translate}\"\nmsgstr \"\"\n\"\"\"\n    assert content_to_update in po_content\n\n    content_with_translation = f\"\"\"\nmsgid \"{msgid_to_translate}\"\nmsgstr \"{msgstr}\"\n\"\"\"\n    po_content_with_translation = po_content.replace(content_to_update, content_with_translation)\n    po_file.write_text(po_content_with_translation)",
          "file": "test_i18n.py"
        },
        {
          "type": "function",
          "name": "verify_i18n",
          "code": "def verify_i18n(app):\n    not_translated = \"code hello i18n\"\n    translated_fr = \"code bonjour\"\n\n    for accepted in (\"unknown\", \"en_US\"):\n        headers = Headers([(\"Accept-Language\", accepted)])\n        with app.test_request_context(headers=headers):\n            assert not hasattr(request, \"babel_locale\")\n            assert not_translated == gettext(not_translated)\n            assert hasattr(request, \"babel_locale\")\n            assert (\n                render_template_string(\n                    \"\"\"\n            {{ gettext('code hello i18n') }}\n            \"\"\"\n                ).strip()\n                == not_translated\n            )\n\n    for lang in (\"fr\", \"fr-FR\"):\n        headers = Headers([(\"Accept-Language\", lang)])\n        with app.test_request_context(headers=headers):\n            assert not hasattr(request, \"babel_locale\")\n            assert translated_fr == gettext(not_translated)\n            assert hasattr(request, \"babel_locale\")\n            assert (\n                render_template_string(\n                    \"\"\"\n            {{ gettext('code hello i18n') }}\n            \"\"\"\n                ).strip()\n                == translated_fr\n            )\n\n    # https://github.com/freedomofpress/securedrop/issues/2379\n    headers = Headers([(\"Accept-Language\", \"en-US;q=0.6,fr_FR;q=0.4,nb_NO;q=0.2\")])\n    with app.test_request_context(headers=headers):\n        assert not hasattr(request, \"babel_locale\")\n        assert not_translated == gettext(not_translated)\n\n    translated_cn = \"code chinese\"\n\n    for lang in (\"zh-CN\", \"zh-Hans-CN\"):\n        headers = Headers([(\"Accept-Language\", lang)])\n        with app.test_request_context(headers=headers):\n            assert not hasattr(request, \"babel_locale\")\n            assert translated_cn == gettext(not_translated)\n            assert hasattr(request, \"babel_locale\")\n            assert (\n                render_template_string(\n                    \"\"\"\n            {{ gettext('code hello i18n') }}\n            \"\"\"\n                ).strip()\n                == translated_cn\n            )\n\n    translated_ar = \"code arabic\"\n\n    for lang in (\"ar\", \"ar-kw\"):\n        headers = Headers([(\"Accept-Language\", lang)])\n        with app.test_request_context(headers=headers):\n            assert not hasattr(request, \"babel_locale\")\n            assert translated_ar == gettext(not_translated)\n            assert hasattr(request, \"babel_locale\")\n            assert (\n                render_template_string(\n                    \"\"\"\n            {{ gettext('code hello i18n') }}\n            \"\"\"\n                ).strip()\n                == translated_ar\n            )\n\n    with app.test_client() as c:\n        # a request without Accept-Language or \"l\" argument gets the\n        # default locale\n        page = c.get(\"/login\")\n        assert session.get(\"locale\") == \"en_US\"\n        assert not_translated == gettext(not_translated)\n        assert b\"?l=fr_FR\" in page.data\n        assert b\"?l=en_US\" not in page.data\n\n        # the session locale should change when the \"l\" request\n        # argument is present and valid\n        page = c.get(\"/login?l=fr_FR\", headers=Headers([(\"Accept-Language\", \"en_US\")]))\n        assert session.get(\"locale\") == \"fr_FR\"\n        assert translated_fr == gettext(not_translated)\n        assert b\"?l=fr_FR\" not in page.data\n        assert b\"?l=en_US\" in page.data\n\n        # confirm that the chosen locale, now in the session, is used\n        # despite not matching the client's Accept-Language header\n        c.get(\"/\", headers=Headers([(\"Accept-Language\", \"en_US\")]))\n        assert session.get(\"locale\") == \"fr_FR\"\n        assert translated_fr == gettext(not_translated)\n\n        # the session locale should not change if an empty \"l\" request\n        # argument is sent\n        c.get(\"/?l=\")\n        assert session.get(\"locale\") == \"fr_FR\"\n        assert translated_fr == gettext(not_translated)\n\n        # the session locale should not change if no \"l\" request\n        # argument is sent\n        c.get(\"/\")\n        assert session.get(\"locale\") == \"fr_FR\"\n        assert translated_fr == gettext(not_translated)\n\n        # sending an invalid locale identifier should not change the\n        # session locale\n        c.get(\"/?l=YY_ZZ\")\n        assert session.get(\"locale\") == \"fr_FR\"\n        assert translated_fr == gettext(not_translated)\n\n        # requesting a valid locale via the request argument \"l\"\n        # should change the session locale\n        c.get(\"/?l=en_US\", headers=Headers([(\"Accept-Language\", \"fr_FR\")]))\n        assert session.get(\"locale\") == \"en_US\"\n        assert not_translated == gettext(not_translated)\n\n        # again, the session locale should stick even if not included\n        # in the client's Accept-Language header\n        c.get(\"/\", headers=Headers([(\"Accept-Language\", \"fr_FR\")]))\n        assert session.get(\"locale\") == \"en_US\"\n        assert not_translated == gettext(not_translated)\n\n    with app.test_request_context():\n        assert render_template(\"locales.html\") == \"\"\n\n    with app.test_client() as c:\n        c.get(\"/\")\n        locales = render_template(\"locales.html\")\n        assert \"?l=fr_FR\" in locales\n        assert \"?l=en_US\" not in locales\n\n        # Test that A[lang,hreflang] attributes (if present) will validate as\n        # BCP47/RFC5646 language tags from `i18n.RequestLocaleInfo.language_tag`.\n        if 'lang=\"' in locales:\n            assert 'lang=\"en-US\"' in locales\n            assert 'lang=\"fr-FR\"' in locales\n        if 'hreflang=\"' in locales:\n            assert 'hreflang=\"en-US\"' in locales\n            assert 'hreflang=\"fr-FR\"' in locales\n\n        c.get(\"/?l=ar\")\n        # We have to render a template that inherits from \"base.html\" so that \"tab_title\" will be\n        # set.  But we're just checking that when a page is rendered in an RTL language the\n        # directionality is correct, so it doesn't matter which template we render.\n        base = render_template(\"error.html\", error={})\n        assert 'dir=\"rtl\"' in base",
          "file": "test_i18n.py"
        },
        {
          "type": "function",
          "name": "test_i18n",
          "code": "def test_i18n():\n    translation_dirs = Path(\"/tmp/sd-tests/test_i18n/translations\")\n    translation_dirs.mkdir(exist_ok=True, parents=True)\n    test_config = create_config_for_i18n_test(\n        supported_locales=[\"ar\", \"en_US\", \"fr_FR\", \"nb_NO\", \"zh_Hans\"],\n        translation_dirs=translation_dirs,\n    )\n\n    i18n_dir = Path(__file__).absolute().parent / \"i18n\"\n    sources = [str(i18n_dir / \"code.py\"), str(i18n_dir / \"template.html\")]\n    pot = i18n_dir / \"messages.pot\"\n    subprocess.check_call(\n        [\n            \"pybabel\",\n            \"extract\",\n            \"--mapping\",\n            str(i18n_dir / \"babel.cfg\"),\n            \"--output\",\n            pot,\n            *sources,\n        ]\n    )\n\n    subprocess.check_call(\n        [\n            \"pybabel\",\n            \"init\",\n            \"--input-file\",\n            pot,\n            \"--output-dir\",\n            translation_dirs,\n            \"--locale\",\n            \"en_US\",\n        ]\n    )\n\n    for locale, translated_msg in (\n        (\"fr_FR\", \"code bonjour\"),\n        (\"zh_Hans\", \"code chinese\"),\n        (\"ar\", \"code arabic\"),\n        (\"nb_NO\", \"code norwegian\"),\n        (\"es_ES\", \"code spanish\"),\n    ):\n        subprocess.check_call(\n            [\n                \"pybabel\",\n                \"init\",\n                \"--input-file\",\n                pot,\n                \"--output-dir\",\n                translation_dirs,\n                \"--locale\",\n                locale,\n            ]\n        )\n\n        # Populate the po file with a translation\n        po_file = translation_dirs / locale / \"LC_MESSAGES\" / \"messages.po\"\n        set_msg_translation_in_po_file(\n            po_file,\n            msgid_to_translate=\"code hello i18n\",\n            msgstr=translated_msg,\n        )\n\n        subprocess.check_call(\n            [\n                \"pybabel\",\n                \"compile\",\n                \"--directory\",\n                translation_dirs,\n                \"--locale\",\n                locale,\n                \"--input-file\",\n                po_file,\n            ]\n        )\n\n    # Use our config (and not an app fixture) because the i18n module\n    # grabs values at init time and we can't inject them later.\n    for app in (\n        journalist_app_module.create_app(test_config),\n        source_app.create_app(test_config),\n    ):\n        with app.app_context():\n            db.create_all()\n        assert list(app.config[\"LOCALES\"].keys()) == test_config.SUPPORTED_LOCALES\n        verify_i18n(app)",
          "file": "test_i18n.py"
        },
        {
          "type": "function",
          "name": "test_parse_locale_set",
          "code": "def test_parse_locale_set():\n    assert parse_locale_set([FALLBACK_LOCALE]) == {Locale.parse(FALLBACK_LOCALE)}",
          "file": "test_i18n.py"
        },
        {
          "type": "function",
          "name": "test_no_usable_fallback_locale",
          "code": "def test_no_usable_fallback_locale():\n    \"\"\"\n    The apps fail if neither the default nor the fallback locale is usable.\n    \"\"\"\n    test_config = create_config_for_i18n_test(\n        default_locale=NEVER_LOCALE, supported_locales=[NEVER_LOCALE]\n    )\n\n    with pytest.raises(ValueError, match=\"in the set of usable locales\"):\n        journalist_app_module.create_app(test_config)\n\n    with pytest.raises(ValueError, match=\"in the set of usable locales\"):\n        source_app.create_app(test_config)",
          "file": "test_i18n.py"
        },
        {
          "type": "function",
          "name": "test_unusable_default_but_usable_fallback_locale",
          "code": "def test_unusable_default_but_usable_fallback_locale(caplog):\n    \"\"\"\n    The apps start even if the default locale is unusable, as along as the fallback locale is\n    usable, but log an error for OSSEC to pick up.\n    \"\"\"\n    test_config = create_config_for_i18n_test(\n        default_locale=NEVER_LOCALE, supported_locales=[NEVER_LOCALE, FALLBACK_LOCALE]\n    )\n\n    for app in (journalist_app_module.create_app(test_config), source_app.create_app(test_config)):\n        with app.app_context():\n            assert NEVER_LOCALE in caplog.text\n            assert \"not in the set of usable locales\" in caplog.text",
          "file": "test_i18n.py"
        },
        {
          "type": "function",
          "name": "test_invalid_locales",
          "code": "def test_invalid_locales():\n    \"\"\"\n    An invalid locale raises an error during app configuration.\n    \"\"\"\n    test_config = create_config_for_i18n_test(supported_locales=[FALLBACK_LOCALE, \"yy_ZZ\"])\n\n    with pytest.raises(UnknownLocaleError):\n        journalist_app_module.create_app(test_config)\n\n    with pytest.raises(UnknownLocaleError):\n        source_app.create_app(test_config)",
          "file": "test_i18n.py"
        },
        {
          "type": "function",
          "name": "test_valid_but_unusable_locales",
          "code": "def test_valid_but_unusable_locales(caplog):\n    \"\"\"\n    The apps start with one or more unusable, but still valid, locales, but log an error for\n    OSSEC to pick up.\n    \"\"\"\n    test_config = create_config_for_i18n_test(\n        supported_locales=[FALLBACK_LOCALE, \"wae_CH\"],\n    )\n\n    for app in (journalist_app_module.create_app(test_config), source_app.create_app(test_config)):\n        with app.app_context():\n            assert \"wae\" in caplog.text\n            assert \"not in the set of usable locales\" in caplog.text",
          "file": "test_i18n.py"
        },
        {
          "type": "function",
          "name": "test_language_tags",
          "code": "def test_language_tags():\n    assert i18n.RequestLocaleInfo(Locale.parse(\"en\")).language_tag == \"en\"\n    assert i18n.RequestLocaleInfo(Locale.parse(\"en-US\", sep=\"-\")).language_tag == \"en-US\"\n    assert i18n.RequestLocaleInfo(Locale.parse(\"en-us\", sep=\"-\")).language_tag == \"en-US\"\n    assert i18n.RequestLocaleInfo(Locale.parse(\"en_US\")).language_tag == \"en-US\"\n    assert i18n.RequestLocaleInfo(Locale.parse(\"zh_Hant\")).language_tag == \"zh-Hant\"",
          "file": "test_i18n.py"
        },
        {
          "type": "function",
          "name": "test_html_en_lang_correct",
          "code": "def test_html_en_lang_correct():\n    test_config = create_config_for_i18n_test(supported_locales=[\"en_US\"])\n    app = journalist_app_module.create_app(test_config).test_client()\n    resp = app.get(\"/\", follow_redirects=True)\n    html = resp.data.decode(\"utf-8\")\n    assert re.compile('<html lang=\"en-US\".*>').search(html), html\n\n    app = source_app.create_app(test_config).test_client()\n    resp = app.get(\"/\", follow_redirects=True)\n    html = resp.data.decode(\"utf-8\")\n    assert re.compile('<html lang=\"en-US\".*>').search(html), html\n\n    # check '/generate' too because '/' uses a different template\n    resp = app.post(\"/generate\", data={\"tor2web_check\": 'href=\"fake.onion\"'}, follow_redirects=True)\n    html = resp.data.decode(\"utf-8\")\n    assert re.compile('<html lang=\"en-US\".*>').search(html), html",
          "file": "test_i18n.py"
        },
        {
          "type": "function",
          "name": "test_html_fr_lang_correct",
          "code": "def test_html_fr_lang_correct():\n    \"\"\"Check that when the locale is fr_FR the lang property is correct\"\"\"\n    test_config = create_config_for_i18n_test(supported_locales=[\"fr_FR\", \"en_US\"])\n\n    app = journalist_app_module.create_app(test_config).test_client()\n    resp = app.get(\"/?l=fr_FR\", follow_redirects=True)\n    html = resp.data.decode(\"utf-8\")\n    assert re.compile('<html lang=\"fr-FR\".*>').search(html), html\n\n    app = source_app.create_app(test_config).test_client()\n    resp = app.get(\"/?l=fr_FR\", follow_redirects=True)\n    html = resp.data.decode(\"utf-8\")\n    assert re.compile('<html lang=\"fr-FR\".*>').search(html), html\n\n    # check '/generate' too because '/' uses a different template\n    resp = app.post(\n        \"/generate?l=fr_FR\", data={\"tor2web_check\": 'href=\"fake.onion\"'}, follow_redirects=True\n    )\n    html = resp.data.decode(\"utf-8\")\n    assert re.compile('<html lang=\"fr-FR\".*>').search(html), html",
          "file": "test_i18n.py"
        },
        {
          "type": "function",
          "name": "test_html_attributes",
          "code": "def test_html_attributes():\n    \"\"\"Check that HTML lang and dir attributes respect locale.\"\"\"\n    test_config = create_config_for_i18n_test(supported_locales=[\"ar\", \"en_US\"])\n\n    app = journalist_app_module.create_app(test_config).test_client()\n    resp = app.get(\"/?l=ar\", follow_redirects=True)\n    html = resp.data.decode(\"utf-8\")\n    assert '<html lang=\"ar\" dir=\"rtl\">' in html\n    resp = app.get(\"/?l=en_US\", follow_redirects=True)\n    html = resp.data.decode(\"utf-8\")\n    assert '<html lang=\"en-US\" dir=\"ltr\">' in html\n\n    app = source_app.create_app(test_config).test_client()\n    resp = app.get(\"/?l=ar\", follow_redirects=True)\n    html = resp.data.decode(\"utf-8\")\n    assert '<html lang=\"ar\" dir=\"rtl\">' in html\n    resp = app.get(\"/?l=en_US\", follow_redirects=True)\n    html = resp.data.decode(\"utf-8\")\n    assert '<html lang=\"en-US\" dir=\"ltr\">' in html\n\n    # check '/generate' too because '/' uses a different template\n    resp = app.post(\n        \"/generate?l=ar\", data={\"tor2web_check\": 'href=\"fake.onion\"'}, follow_redirects=True\n    )\n    html = resp.data.decode(\"utf-8\")\n    assert '<html lang=\"ar\" dir=\"rtl\">' in html\n    resp = app.post(\n        \"/generate?l=en_US\", data={\"tor2web_check\": 'href=\"fake.onion\"'}, follow_redirects=True\n    )\n    html = resp.data.decode(\"utf-8\")\n    assert '<html lang=\"en-US\" dir=\"ltr\">' in html",
          "file": "test_i18n.py"
        },
        {
          "type": "function",
          "name": "test_same_lang_diff_locale",
          "code": "def test_same_lang_diff_locale():\n    \"\"\"\n    Verify that when two locales with the same lang are specified, the full locale\n    name is used for both.\n    \"\"\"\n    test_config = create_config_for_i18n_test(supported_locales=[\"en_US\", \"pt_BR\", \"pt_PT\"])\n\n    app = journalist_app_module.create_app(test_config).test_client()\n    resp = app.get(\"/\", follow_redirects=True)\n    html = resp.data.decode(\"utf-8\")\n    assert \"português (Brasil)\" in html\n    assert \"português (Portugal)\" in html",
          "file": "test_i18n.py"
        },
        {
          "type": "function",
          "name": "test_duplicate_locales",
          "code": "def test_duplicate_locales():\n    \"\"\"\n    Verify that we don't display the full locale name for duplicate locales,\n    whether from user input or securedrop.sdconfig's enforcement of the\n    fallback locale.\n    \"\"\"\n\n    # [\"en_US\", \"en_US\"] alone will not display the locale switcher, which\n    # *does* pass through set deduplication.\n    test_config = create_config_for_i18n_test(supported_locales=[\"en_US\", \"en_US\", \"ar\"])\n\n    app = journalist_app_module.create_app(test_config).test_client()\n    resp = app.get(\"/\", follow_redirects=True)\n    html = resp.data.decode(\"utf-8\")\n    assert \"English (United States)\" not in html",
          "file": "test_i18n.py"
        }
      ],
      "test_source.py": [
        {
          "type": "function",
          "name": "test_logo_default_available",
          "code": "def test_logo_default_available(config, source_app):\n    # if the custom image is available, this test will fail\n    custom_image_location = os.path.join(config.SECUREDROP_ROOT, \"static/i/custom_logo.png\")\n    if os.path.exists(custom_image_location):\n        os.remove(custom_image_location)\n\n    with source_app.test_client() as app:\n        logo_url = get_logo_url(source_app)\n        assert logo_url.endswith(\"i/logo.png\")\n        response = app.get(logo_url, follow_redirects=False)\n        assert response.status_code == 200",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_logo_custom_available",
          "code": "def test_logo_custom_available(config, source_app):\n    # if the custom image is available, this test will fail\n    custom_image = os.path.join(config.SECUREDROP_ROOT, \"static/i/custom_logo.png\")\n    default_image = os.path.join(config.SECUREDROP_ROOT, \"static/i/logo.png\")\n    if os.path.exists(default_image) and not os.path.exists(custom_image):\n        shutil.copyfile(default_image, custom_image)\n\n    with source_app.test_client() as app:\n        logo_url = get_logo_url(source_app)\n        assert logo_url.endswith(\"i/custom_logo.png\")\n        response = app.get(logo_url, follow_redirects=False)\n        assert response.status_code == 200",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_page_not_found",
          "code": "def test_page_not_found(source_app):\n    \"\"\"Verify the page not found condition returns the intended template\"\"\"\n    with InstrumentedApp(source_app) as ins:\n        with source_app.test_client() as app:\n            resp = app.get(\"UNKNOWN\")\n            assert resp.status_code == 404\n            ins.assert_template_used(\"notfound.html\")",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_orgname_default_set",
          "code": "def test_orgname_default_set(source_app):\n    class dummy_current:\n        organization_name = None\n\n    with patch.object(InstanceConfig, \"get_current\") as iMock:\n        with source_app.test_client() as app:\n            iMock.return_value = dummy_current()\n            resp = app.get(url_for(\"main.index\"))\n            assert resp.status_code == 200\n            assert g.organization_name == \"SecureDrop\"",
          "file": "test_source.py"
        },
        {
          "type": "class",
          "name": "dummy_current",
          "code": "class dummy_current:\n        organization_name = None",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_index",
          "code": "def test_index(source_app):\n    \"\"\"Test that the landing page loads and looks how we expect\"\"\"\n    with source_app.test_client() as app:\n        resp = app.get(url_for(\"main.index\"))\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"First submission\" in text\n        assert \"Return visit\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "_find_codename",
          "code": "def _find_codename(html):\n    \"\"\"Find a source codename (diceware passphrase) in HTML\"\"\"\n    # Codenames may contain HTML escape characters, and the wordlist\n    # contains various symbols.\n    codename_re = (\n        r'<mark [^>]*id=\"codename\"[^>]*>[^<]*<span>'\n        r'(?P<codename>[a-z0-9 &#;?:=@_.*+()\\'\"$%!-]+)</span>[^<]*</mark>'\n    )\n    codename_match = re.search(codename_re, html)\n    assert codename_match is not None\n    return codename_match.group(\"codename\")",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_generate_already_logged_in",
          "code": "def test_generate_already_logged_in(source_app):\n    with source_app.test_client() as app:\n        new_codename(app, session)\n        # Make sure it redirects to /lookup when logged in\n        resp = app.post(url_for(\"main.generate\"), data=GENERATE_DATA)\n        assert resp.status_code == 302\n        # Make sure it flashes the message on the lookup page\n        resp = app.post(url_for(\"main.generate\"), data=GENERATE_DATA, follow_redirects=True)\n        # Should redirect to /lookup\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"because you are already logged in.\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_create_new_source",
          "code": "def test_create_new_source(source_app):\n    with source_app.test_client() as app:\n        resp = app.post(url_for(\"main.generate\"), data=GENERATE_DATA)\n        assert resp.status_code == 200\n        tab_id = next(iter(session[\"codenames\"].keys()))\n        resp = app.post(url_for(\"main.create\"), data={\"tab_id\": tab_id}, follow_redirects=True)\n        assert SessionManager.is_user_logged_in(db_session=db.session)\n        session_cookie = utils._session_from_cookiejar(app.cookie_jar, source_app)\n        # Verify correct `SameSite` value was set on the cookie\n        assert session_cookie.get_nonstandard_attr(\"SameSite\") == \"Strict\"\n\n        # should be redirected to /lookup\n        text = resp.data.decode(\"utf-8\")\n        assert \"Submit Files\" in text\n        assert \"codenames\" not in session",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_generate_as_post",
          "code": "def test_generate_as_post(source_app):\n    with source_app.test_client() as app:\n        resp = app.post(url_for(\"main.generate\"), data=GENERATE_DATA)\n        assert resp.status_code == 200\n        session_codename = next(iter(session[\"codenames\"].values()))\n\n    text = resp.data.decode(\"utf-8\")\n    assert \"functions as both your username and your password\" in text\n\n    codename = _find_codename(resp.data.decode(\"utf-8\"))\n    # codename is also stored in the session - make sure it matches the\n    # codename displayed to the source\n    assert codename == escape(session_codename)",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_generate_as_get",
          "code": "def test_generate_as_get(source_app):\n    with source_app.test_client() as app:\n        resp = app.get(url_for(\"main.generate\"))\n        assert resp.status_code == 200\n        session_codename = next(iter(session[\"codenames\"].values()))\n\n    text = resp.data.decode(\"utf-8\")\n    assert \"functions as both your username and your password\" in text\n\n    codename = _find_codename(resp.data.decode(\"utf-8\"))\n    # codename is also stored in the session - make sure it matches the\n    # codename displayed to the source\n    assert codename == escape(session_codename)",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_create_duplicate_codename_logged_in_not_in_session",
          "code": "def test_create_duplicate_codename_logged_in_not_in_session(source_app):\n    with patch.object(source_app.logger, \"error\") as logger:\n        with source_app.test_client() as app:\n            resp = app.post(url_for(\"main.generate\"), data=GENERATE_DATA)\n            assert resp.status_code == 200\n            tab_id, codename = next(iter(session[\"codenames\"].items()))\n\n            # Create a source the first time\n            resp = app.post(url_for(\"main.create\"), data={\"tab_id\": tab_id}, follow_redirects=True)\n            assert resp.status_code == 200\n\n        with source_app.test_client() as app:\n            # Attempt to add the same source\n            with app.session_transaction() as sess:\n                sess[\"codenames\"] = {tab_id: codename}\n                sess[\"codenames_expire\"] = datetime.utcnow() + timedelta(hours=1)\n            resp = app.post(url_for(\"main.create\"), data={\"tab_id\": tab_id}, follow_redirects=True)\n            logger.assert_called_once()\n            assert \"Could not create a source\" in logger.call_args[0][0]\n            assert resp.status_code == 200\n            assert not SessionManager.is_user_logged_in(db_session=db.session)",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_create_duplicate_codename_logged_in_in_session",
          "code": "def test_create_duplicate_codename_logged_in_in_session(source_app):\n    with source_app.test_client() as app:\n        # Given a user who generated a codename in a browser tab\n        resp = app.post(url_for(\"main.generate\"), data=GENERATE_DATA)\n        assert resp.status_code == 200\n        first_tab_id, first_codename = list(session[\"codenames\"].items())[0]\n\n        # And then they opened a new browser tab to generate a second codename\n        resp = app.post(url_for(\"main.generate\"), data=GENERATE_DATA)\n        assert resp.status_code == 200\n        second_tab_id, second_codename = list(session[\"codenames\"].items())[1]\n        assert first_codename != second_codename\n\n        # And the user then completed the account creation flow in the first tab\n        resp = app.post(\n            url_for(\"main.create\"), data={\"tab_id\": first_tab_id}, follow_redirects=True\n        )\n        assert resp.status_code == 200\n        first_tab_account = SessionManager.get_logged_in_user(db_session=db.session)\n\n        # When the user tries to complete the account creation flow again, in the second tab\n        resp = app.post(\n            url_for(\"main.create\"), data={\"tab_id\": second_tab_id}, follow_redirects=True\n        )\n\n        # Then the user is shown the \"already logged in\" message\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"You are already logged in.\" in text\n\n        # And no new account was created\n        second_tab_account = SessionManager.get_logged_in_user(db_session=db.session)\n        assert second_tab_account.filesystem_id == first_tab_account.filesystem_id",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_lookup",
          "code": "def test_lookup(source_app):\n    \"\"\"Test various elements on the /lookup page.\"\"\"\n    with source_app.test_client() as app:\n        codename = new_codename(app, session)\n        resp = app.post(url_for(\"main.login\"), data=dict(codename=codename), follow_redirects=True)\n        # redirects to /lookup\n        text = resp.data.decode(\"utf-8\")\n        assert \"public key\" in text\n        # download the public key\n        resp = app.get(url_for(\"info.download_public_key\"))\n        text = resp.data.decode(\"utf-8\")\n        assert redwood.is_valid_public_key(text)",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_journalist_key_redirects_to_public_key",
          "code": "def test_journalist_key_redirects_to_public_key(source_app):\n    \"\"\"Test that the /journalist-key route redirects to /public-key.\"\"\"\n    with source_app.test_client() as app:\n        resp = app.get(url_for(\"info.download_journalist_key\"))\n        assert resp.status_code == 301\n        resp = app.get(url_for(\"info.download_journalist_key\"), follow_redirects=True)\n        assert request.path == url_for(\"info.download_public_key\")\n        assert redwood.is_valid_public_key(resp.data.decode(\"utf-8\"))",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_login_and_logout",
          "code": "def test_login_and_logout(source_app):\n    with source_app.test_client() as app:\n        resp = app.get(url_for(\"main.login\"))\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Enter Codename\" in text\n\n        codename = new_codename(app, session)\n        resp = app.post(url_for(\"main.login\"), data=dict(codename=codename), follow_redirects=True)\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Submit Files\" in text\n        assert SessionManager.is_user_logged_in(db_session=db.session)\n\n        session_cookie = utils._session_from_cookiejar(app.cookie_jar, source_app)\n        # Verify correct `SameSite` value was set on the cookie\n        assert session_cookie.get_nonstandard_attr(\"SameSite\") == \"Strict\"\n\n    with source_app.test_client() as app:\n        resp = app.post(url_for(\"main.login\"), data=dict(codename=\"invalid\"), follow_redirects=True)\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Sorry, that is not a recognized codename.\" in text\n        assert not SessionManager.is_user_logged_in(db_session=db.session)\n\n    with source_app.test_client() as app:\n        resp = app.post(url_for(\"main.login\"), data=dict(codename=codename), follow_redirects=True)\n        assert resp.status_code == 200\n        assert SessionManager.is_user_logged_in(db_session=db.session)\n\n        resp = app.post(url_for(\"main.login\"), data=dict(codename=codename), follow_redirects=True)\n        assert resp.status_code == 200\n        assert SessionManager.is_user_logged_in(db_session=db.session)\n\n        resp = app.post(url_for(\"main.logout\"), follow_redirects=True)\n        assert resp.status_code == 200\n        assert not SessionManager.is_user_logged_in(db_session=db.session)\n        text = resp.data.decode(\"utf-8\")\n\n        # This is part of the logout page message instructing users\n        # to click the 'New Identity' icon\n        assert \"This will clear your Tor Browser activity data\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_user_must_log_in_for_protected_views",
          "code": "def test_user_must_log_in_for_protected_views(source_app):\n    with source_app.test_client() as app:\n        resp = app.get(url_for(\"main.lookup\"), follow_redirects=True)\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Enter Codename\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_login_with_whitespace",
          "code": "def test_login_with_whitespace(source_app):\n    \"\"\"\n    Test that codenames with leading or trailing whitespace still work\n    \"\"\"\n\n    def login_test(app, codename):\n        resp = app.get(url_for(\"main.login\"))\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Enter Codename\" in text\n\n        resp = app.post(url_for(\"main.login\"), data=dict(codename=codename), follow_redirects=True)\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Submit Files\" in text\n        assert SessionManager.is_user_logged_in(db_session=db.session)\n\n    with source_app.test_client() as app:\n        codename = new_codename(app, session)\n\n    codenames = [\n        codename + \" \",\n        \" \" + codename + \" \",\n        \" \" + codename,\n    ]\n\n    for codename_ in codenames:\n        with source_app.test_client() as app:\n            login_test(app, codename_)",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "login_test",
          "code": "def login_test(app, codename):\n        resp = app.get(url_for(\"main.login\"))\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Enter Codename\" in text\n\n        resp = app.post(url_for(\"main.login\"), data=dict(codename=codename), follow_redirects=True)\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Submit Files\" in text\n        assert SessionManager.is_user_logged_in(db_session=db.session)",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_login_with_missing_reply_files",
          "code": "def test_login_with_missing_reply_files(source_app, app_storage):\n    \"\"\"\n    Test that source can log in when replies are present in database but missing\n    from storage.\n    \"\"\"\n    source, codename = utils.db_helper.init_source(app_storage)\n    journalist, _ = utils.db_helper.init_journalist()\n    replies = utils.db_helper.reply(app_storage, journalist, source, 1)\n    assert len(replies) > 0\n    # Delete the reply file\n    reply_file_path = Path(app_storage.path(source.filesystem_id, replies[0].filename))\n    reply_file_path.unlink()\n    assert not reply_file_path.exists()\n\n    with source_app.test_client() as app:\n        resp = app.get(url_for(\"main.login\"))\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Enter Codename\" in text\n\n        resp = app.post(url_for(\"main.login\"), data=dict(codename=codename), follow_redirects=True)\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Submit Files\" in text\n        assert SessionManager.is_user_logged_in(db_session=db.session)",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_login_with_undecryptable_reply_files",
          "code": "def test_login_with_undecryptable_reply_files(source_app, app_storage):\n    \"\"\"\n    Test that source can log in when replies are present but cannot be decrypted\n    \"\"\"\n    source, codename = utils.db_helper.init_source(app_storage)\n    journalist, _ = utils.db_helper.init_journalist()\n    replies = utils.db_helper.reply(app_storage, journalist, source, 1)\n    assert len(replies) > 0\n    reply_file_path = Path(app_storage.path(source.filesystem_id, replies[0].filename))\n    assert reply_file_path.exists()\n\n    with mock.patch(\"encryption.EncryptionManager.decrypt_journalist_reply\") as repMock:\n        repMock.side_effect = GpgDecryptError()\n        with source_app.test_client() as app:\n            resp = app.get(url_for(\"main.login\"))\n            assert resp.status_code == 200\n            text = resp.data.decode(\"utf-8\")\n            assert \"Enter Codename\" in text\n\n            resp = app.post(\n                url_for(\"main.login\"), data=dict(codename=codename), follow_redirects=True\n            )\n            assert resp.status_code == 200\n            text = resp.data.decode(\"utf-8\")\n            assert \"Submit Files\" in text\n            assert SessionManager.is_user_logged_in(db_session=db.session)",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_login_no_pgp_keypair",
          "code": "def test_login_no_pgp_keypair(source_app, app_storage):\n    \"\"\"\n    Test the on-demand creation of a keypair when none exists\n    \"\"\"\n    source, codename = utils.db_helper.init_source(app_storage)\n    source.pgp_fingerprint = None\n    source.pgp_public_key = None\n    source.pgp_secret_key = None\n    db.session.add(source)\n    db.session.commit()\n    # And there's no legacy GPG key hiding in the background\n    assert source.fingerprint is None\n    with source_app.test_client() as app:\n        resp = app.post(url_for(\"main.login\"), data=dict(codename=codename), follow_redirects=True)\n        assert resp.status_code == 200\n        assert SessionManager.is_user_logged_in(db_session=db.session)\n    # Now check to see PGP fields are populated\n    assert len(source.pgp_fingerprint) == 40\n    assert redwood.is_valid_public_key(source.pgp_public_key)\n    assert source.pgp_secret_key.startswith(\"-----BEGIN PGP PRIVATE KEY BLOCK-----\")",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_login_gpg_secret_key_migration",
          "code": "def test_login_gpg_secret_key_migration(source_app, app_storage):\n    \"\"\"\n    Test the on-demand migration of GPG secret keys to database-backed storage\n    \"\"\"\n    # First create a source that is backed by a GPG key\n    passphrase = PassphraseGenerator.get_default().generate_passphrase()\n    source_user = create_source_user(\n        db_session=db.session,\n        source_passphrase=passphrase,\n        source_app_storage=app_storage,\n    )\n    source = source_user.get_db_record()\n    encryption_mgr = EncryptionManager.get_default()\n    utils.create_legacy_gpg_key(encryption_mgr, source_user, source)\n    # Copy the fingerprint and public key to DB storage, like the alembic migration does\n    source.pgp_fingerprint = source.fingerprint\n    source.pgp_public_key = source.public_key\n    source.pgp_secret_key = None\n    db.session.add(source)\n    db.session.commit()\n    # Now log in\n    with source_app.test_client() as app:\n        resp = app.post(\n            url_for(\"main.login\"), data=dict(codename=passphrase), follow_redirects=True\n        )\n        assert resp.status_code == 200\n        assert SessionManager.is_user_logged_in(db_session=db.session)\n    # Now check to see PGP secret key is populated\n    assert source.pgp_secret_key.startswith(\"-----BEGIN PGP PRIVATE KEY BLOCK-----\")\n    # And the GPG key has been deleted\n    with pytest.raises(GpgKeyNotFoundError):\n        encryption_mgr.get_source_key_fingerprint(source.filesystem_id)",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "_dummy_submission",
          "code": "def _dummy_submission(app):\n    \"\"\"\n    Helper to make a submission (content unimportant), mostly useful in\n    testing notification behavior for a source's first vs. their\n    subsequent submissions\n    \"\"\"\n    return app.post(\n        url_for(\"main.submit\"),\n        data=dict(\n            msg=\"Hallo! ö, ü, ä, or ß...Pay no attention to the man behind the curtain.\",\n            fh=(BytesIO(b\"\"), \"\"),\n        ),\n        follow_redirects=True,\n    )",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_initial_submission_notification",
          "code": "def test_initial_submission_notification(source_app):\n    \"\"\"\n    Regardless of the type of submission (message, file, or both), the\n    first submission is always greeted with a notification\n    reminding sources to check back later for replies.\n    \"\"\"\n    with source_app.test_client() as app:\n        new_codename(app, session)\n        resp = _dummy_submission(app)\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Thank you for sending this information to us.\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_submit_message",
          "code": "def test_submit_message(source_app):\n    with source_app.test_client() as app:\n        new_codename(app, session)\n        _dummy_submission(app)\n        resp = app.post(\n            url_for(\"main.submit\"),\n            data=dict(msg=\"This is a test.\", fh=(StringIO(\"\"), \"\")),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Thanks! We received your message\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_submit_empty_message",
          "code": "def test_submit_empty_message(source_app):\n    with source_app.test_client() as app:\n        new_codename(app, session)\n        resp = app.post(\n            url_for(\"main.submit\"), data=dict(msg=\"\", fh=(StringIO(\"\"), \"\")), follow_redirects=True\n        )\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"You must enter a message or choose a file to submit.\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_submit_big_message",
          "code": "def test_submit_big_message(source_app):\n    \"\"\"\n    Test the message size limit.\n    \"\"\"\n    with source_app.test_client() as app:\n        new_codename(app, session)\n        _dummy_submission(app)\n        resp = app.post(\n            url_for(\"main.submit\"),\n            data=dict(msg=\"AA\" * (1024 * 512), fh=(StringIO(\"\"), \"\")),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Message text too long.\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_submit_initial_short_message",
          "code": "def test_submit_initial_short_message(source_app):\n    \"\"\"\n    Test the message size limit.\n    \"\"\"\n    with source_app.test_client() as app:\n        InstanceConfig.get_default().update_submission_prefs(\n            allow_uploads=True, min_length=10, reject_codenames=False\n        )\n        new_codename(app, session)\n        resp = app.post(\n            url_for(\"main.submit\"),\n            data=dict(msg=\"A\" * 5, fh=(StringIO(\"\"), \"\")),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Your first message must be at least 10 characters long.\" in text\n        # Now retry with a longer message\n        resp = app.post(\n            url_for(\"main.submit\"),\n            data=dict(msg=\"A\" * 25, fh=(StringIO(\"\"), \"\")),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Thank you for sending this information to us.\" in text\n        # Now send another short message, that should still be accepted since\n        # it's no longer the initial one\n        resp = app.post(\n            url_for(\"main.submit\"), data=dict(msg=\"A\", fh=(StringIO(\"\"), \"\")), follow_redirects=True\n        )\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Thanks! We received your message.\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_submit_file",
          "code": "def test_submit_file(source_app):\n    with source_app.test_client() as app:\n        new_codename(app, session)\n        _dummy_submission(app)\n        resp = app.post(\n            url_for(\"main.submit\"),\n            data=dict(msg=\"\", fh=(BytesIO(b\"This is a test\"), \"test.txt\")),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Thanks! We received your file\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_submit_both",
          "code": "def test_submit_both(source_app):\n    with source_app.test_client() as app:\n        new_codename(app, session)\n        _dummy_submission(app)\n        resp = app.post(\n            url_for(\"main.submit\"),\n            data=dict(msg=\"This is a test\", fh=(BytesIO(b\"This is a test\"), \"test.txt\")),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Thanks! We received your file and message\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_submit_antispam",
          "code": "def test_submit_antispam(source_app):\n    \"\"\"\n    Test the antispam check.\n    \"\"\"\n    with source_app.test_client() as app:\n        new_codename(app, session)\n        _dummy_submission(app)\n        resp = app.post(\n            url_for(\"main.submit\"),\n            data=dict(msg=\"Test\", fh=(StringIO(\"\"), \"\"), text=\"blah\"),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 403",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_submit_codename_second_login",
          "code": "def test_submit_codename_second_login(source_app):\n    \"\"\"\n    Test codename submissions *not* prevented on second session\n    \"\"\"\n    with source_app.test_client() as app:\n        InstanceConfig.get_default().update_submission_prefs(\n            allow_uploads=True, min_length=0, reject_codenames=True\n        )\n        codename = new_codename(app, session)\n        resp = app.post(\n            url_for(\"main.submit\"),\n            data=dict(msg=codename, fh=(StringIO(\"\"), \"\")),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Please do not submit your codename!\" in text\n\n        resp = app.post(url_for(\"main.logout\"), follow_redirects=True)\n        assert resp.status_code == 200\n        assert not SessionManager.is_user_logged_in(db_session=db.session)\n        text = resp.data.decode(\"utf-8\")\n        assert \"This will clear your Tor Browser activity data\" in text\n\n        resp = app.post(url_for(\"main.login\"), data=dict(codename=codename), follow_redirects=True)\n        assert resp.status_code == 200\n        assert SessionManager.is_user_logged_in(db_session=db.session)\n\n        resp = app.post(\n            url_for(\"main.submit\"),\n            data=dict(msg=codename, fh=(StringIO(\"\"), \"\")),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Thank you for sending this information\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_submit_codename",
          "code": "def test_submit_codename(source_app):\n    \"\"\"\n    Test preventions against people submitting their codename.\n    \"\"\"\n    with source_app.test_client() as app:\n        InstanceConfig.get_default().update_submission_prefs(\n            allow_uploads=True, min_length=0, reject_codenames=True\n        )\n        codename = new_codename(app, session)\n        resp = app.post(\n            url_for(\"main.submit\"),\n            data=dict(msg=codename, fh=(StringIO(\"\"), \"\")),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Please do not submit your codename!\" in text\n        # Do a dummy submission\n        _dummy_submission(app)\n        # Now resubmit the codename, should be accepted.\n        resp = app.post(\n            url_for(\"main.submit\"),\n            data=dict(msg=codename, fh=(StringIO(\"\"), \"\")),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Thanks! We received your message\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_delete_all_successfully_deletes_replies",
          "code": "def test_delete_all_successfully_deletes_replies(source_app, app_storage):\n    with source_app.app_context():\n        journalist, _ = utils.db_helper.init_journalist()\n        source, codename = utils.db_helper.init_source(app_storage)\n        source_id = source.id\n        utils.db_helper.reply(app_storage, journalist, source, 1)\n\n    with source_app.test_client() as app:\n        resp = app.post(url_for(\"main.login\"), data=dict(codename=codename), follow_redirects=True)\n        assert resp.status_code == 200\n        resp = app.post(url_for(\"main.batch_delete\"), follow_redirects=True)\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"All replies have been deleted\" in text\n\n    with source_app.app_context():\n        source = Source.query.get(source_id)\n        replies = Reply.query.filter(Reply.source_id == source_id).all()\n        for reply in replies:\n            assert reply.deleted_by_source is True",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_delete_all_replies_deleted_by_source_but_not_journalist",
          "code": "def test_delete_all_replies_deleted_by_source_but_not_journalist(source_app, app_storage):\n    \"\"\"Replies can be deleted by a source, but not by journalists. As such,\n    replies may still exist in the replies table, but no longer be visible.\"\"\"\n    with source_app.app_context():\n        journalist, _ = utils.db_helper.init_journalist()\n        source, codename = utils.db_helper.init_source(app_storage)\n        utils.db_helper.reply(app_storage, journalist, source, 1)\n        replies = Reply.query.filter(Reply.source_id == source.id).all()\n        for reply in replies:\n            reply.deleted_by_source = True\n            db.session.add(reply)\n            db.session.commit()\n\n    with source_app.test_client() as app:\n        with patch.object(source_app.logger, \"error\") as logger:\n            resp = app.post(\n                url_for(\"main.login\"), data=dict(codename=codename), follow_redirects=True\n            )\n            assert resp.status_code == 200\n            resp = app.post(url_for(\"main.batch_delete\"), follow_redirects=True)\n            assert resp.status_code == 200\n            logger.assert_called_once_with(\"Found no replies when at least one was expected\")",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_delete_all_replies_already_deleted_by_journalists",
          "code": "def test_delete_all_replies_already_deleted_by_journalists(source_app, app_storage):\n    with source_app.app_context():\n        journalist, _ = utils.db_helper.init_journalist()\n        source, codename = utils.db_helper.init_source(app_storage)\n        # Note that we are creating the source and no replies\n\n    with source_app.test_client() as app:\n        with patch.object(source_app.logger, \"error\") as logger:\n            resp = app.post(\n                url_for(\"main.login\"), data=dict(codename=codename), follow_redirects=True\n            )\n            assert resp.status_code == 200\n            resp = app.post(url_for(\"main.batch_delete\"), follow_redirects=True)\n            assert resp.status_code == 200\n            logger.assert_called_once_with(\"Found no replies when at least one was expected\")",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_submit_sanitizes_filename",
          "code": "def test_submit_sanitizes_filename(source_app):\n    \"\"\"Test that upload file name is sanitized\"\"\"\n    insecure_filename = \"../../bin/gpg\"\n    sanitized_filename = \"bin_gpg\"\n\n    with patch.object(gzip, \"GzipFile\", wraps=gzip.GzipFile) as gzipfile:\n        with source_app.test_client() as app:\n            new_codename(app, session)\n            resp = app.post(\n                url_for(\"main.submit\"),\n                data=dict(msg=\"\", fh=(BytesIO(b\"This is a test\"), insecure_filename)),\n                follow_redirects=True,\n            )\n            assert resp.status_code == 200\n            gzipfile.assert_called_with(filename=sanitized_filename, mode=ANY, fileobj=ANY, mtime=0)",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_redirect_when_tor2web",
          "code": "def test_redirect_when_tor2web(config, source_app, test_url):\n    with source_app.test_client() as app:\n        resp = app.get(\n            url_for(test_url), headers=[(\"X-tor2web\", \"encrypted\")], follow_redirects=True\n        )\n        text = resp.data.decode(\"utf-8\")\n        assert resp.status_code == 403\n        assert \"Proxy Service Detected\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_tor2web_warning",
          "code": "def test_tor2web_warning(source_app):\n    with source_app.test_client() as app:\n        resp = app.get(url_for(\"info.tor2web_warning\"))\n        assert resp.status_code == 403\n        text = resp.data.decode(\"utf-8\")\n        assert \"Proxy Service Detected\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_why_use_tor_browser",
          "code": "def test_why_use_tor_browser(source_app):\n    with source_app.test_client() as app:\n        resp = app.get(url_for(\"info.recommend_tor_browser\"))\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"You Should Use Tor Browser\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_why_journalist_key",
          "code": "def test_why_journalist_key(source_app):\n    with source_app.test_client() as app:\n        resp = app.get(url_for(\"info.why_download_public_key\"))\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Why download the team's public key?\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_metadata_route",
          "code": "def test_metadata_route(config, source_app):\n    with patch(\"server_os.get_os_release\", return_value=\"20.04\"):\n        with source_app.test_client() as app:\n            resp = app.get(url_for(\"api.metadata\"))\n            assert resp.status_code == 200\n            assert resp.headers.get(\"Content-Type\") == \"application/json\"\n            assert (\n                resp.json.get(\"allow_document_uploads\")\n                == InstanceConfig.get_current().allow_document_uploads\n            )\n            assert resp.json.get(\"sd_version\") == version.__version__\n            assert resp.json.get(\"server_os\") == \"20.04\"\n            assert resp.json.get(\"supported_languages\") == config.SUPPORTED_LOCALES\n            assert resp.json.get(\"v3_source_url\") is None",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_metadata_v3_url",
          "code": "def test_metadata_v3_url(source_app):\n    onion_test_url = \"abcdefghabcdefghabcdefghabcdefghabcdefghabcdefghabcdefgh.onion\"\n    with patch.object(source_app_api, \"get_sourcev3_url\") as mocked_v3_url:\n        mocked_v3_url.return_value = onion_test_url\n        with source_app.test_client() as app:\n            resp = app.get(url_for(\"api.metadata\"))\n            assert resp.status_code == 200\n            assert resp.headers.get(\"Content-Type\") == \"application/json\"\n            assert resp.json.get(\"v3_source_url\") == onion_test_url",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_login_with_overly_long_codename",
          "code": "def test_login_with_overly_long_codename(source_app):\n    \"\"\"Attempting to login with an overly long codename should result in\n    an error to avoid DoS.\"\"\"\n    overly_long_codename = \"a\" * (PassphraseGenerator.MAX_PASSPHRASE_LENGTH + 1)\n    with source_app.test_client() as app:\n        resp = app.post(\n            url_for(\"main.login\"), data=dict(codename=overly_long_codename), follow_redirects=True\n        )\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert (\n            f\"Field must be between 1 and {PassphraseGenerator.MAX_PASSPHRASE_LENGTH} characters \"\n            \"long.\"\n        ) in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_normalize_timestamps",
          "code": "def test_normalize_timestamps(source_app, app_storage):\n    \"\"\"\n    Check function of source_app.utils.normalize_timestamps.\n\n    All submissions for a source should have the same timestamp. Any\n    existing submissions' files that did not exist at the time of a\n    new submission should not be created by normalize_timestamps.\n    \"\"\"\n    with source_app.test_client() as app:\n        # create a source\n        source, codename = utils.db_helper.init_source(app_storage)\n\n        # create one submission\n        first_submission = submit(app_storage, source, 1)[0]\n\n        # delete the submission's file from the store\n        first_submission_path = Path(\n            app_storage.path(source.filesystem_id, first_submission.filename)\n        )\n        first_submission_path.unlink()\n        assert not first_submission_path.exists()\n\n        # log in as the source\n        resp = app.post(url_for(\"main.login\"), data=dict(codename=codename), follow_redirects=True)\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Submit Files\" in text\n        assert SessionManager.is_user_logged_in(db_session=db.session)\n\n        # submit another message\n        resp = _dummy_submission(app)\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Thanks! We received your message\" in text\n\n        # sleep to ensure timestamps would differ\n        time.sleep(1)\n\n        # submit another message\n        resp = _dummy_submission(app)\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Thanks! We received your message\" in text\n\n        # only two of the source's three submissions should have files in the store\n        assert len(source.submissions) == 3\n        submission_paths = [\n            Path(app_storage.path(source.filesystem_id, s.filename)) for s in source.submissions\n        ]\n        extant_paths = [p for p in submission_paths if p.exists()]\n        assert len(extant_paths) == 2\n\n        # verify that the deleted file has not been recreated\n        assert not first_submission_path.exists()\n        assert first_submission_path not in extant_paths\n\n        # and the timestamps of all existing files should match exactly\n        assert extant_paths[0].stat().st_atime_ns == extant_paths[1].stat().st_atime_ns\n        assert extant_paths[0].stat().st_ctime_ns == extant_paths[1].stat().st_ctime_ns\n        assert extant_paths[0].stat().st_mtime_ns == extant_paths[1].stat().st_mtime_ns",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_failed_normalize_timestamps_logs_warning",
          "code": "def test_failed_normalize_timestamps_logs_warning(source_app):\n    \"\"\"If a normalize timestamps event fails, the subprocess that calls\n    touch will fail and exit 1. When this happens, the submission should\n    still occur, but a warning should be logged (this will trigger an\n    OSSEC alert).\"\"\"\n\n    with patch.object(source_app.logger, \"warning\") as logger:\n        with patch.object(subprocess, \"call\", return_value=1):\n            with source_app.test_client() as app:\n                new_codename(app, session)\n                _dummy_submission(app)\n                resp = app.post(\n                    url_for(\"main.submit\"),\n                    data=dict(msg=\"This is a test.\", fh=(StringIO(\"\"), \"\")),\n                    follow_redirects=True,\n                )\n                assert resp.status_code == 200\n                text = resp.data.decode(\"utf-8\")\n                assert \"Thanks! We received your message\" in text\n\n                logger.assert_called_once_with(\n                    \"Couldn't normalize submission \" \"timestamps (touch exited with 1)\"\n                )",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_source_is_deleted_while_logged_in",
          "code": "def test_source_is_deleted_while_logged_in(source_app):\n    \"\"\"If a source is deleted by a journalist when they are logged in,\n    a NoResultFound will occur. The source should be redirected to the\n    index when this happens, and a warning logged.\"\"\"\n    with source_app.test_client() as app:\n        codename = new_codename(app, session)\n        app.post(\"login\", data=dict(codename=codename), follow_redirects=True)\n\n        # Now that the source is logged in, the journalist deletes the source\n        source_user = SessionManager.get_logged_in_user(db_session=db.session)\n        delete_collection(source_user.filesystem_id)\n\n        # Source attempts to continue to navigate\n        resp = app.get(url_for(\"main.lookup\"), follow_redirects=True)\n        assert resp.status_code == 200\n        assert not SessionManager.is_user_logged_in(db_session=db.session)\n        text = resp.data.decode(\"utf-8\")\n        assert \"First submission\" in text\n        assert not SessionManager.is_user_logged_in(db_session=db.session)",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_login_with_invalid_codename",
          "code": "def test_login_with_invalid_codename(source_app):\n    \"\"\"Logging in with a codename with invalid characters should return\n    an informative message to the user.\"\"\"\n\n    invalid_codename = \"[]\"\n\n    with source_app.test_client() as app:\n        resp = app.post(\n            url_for(\"main.login\"), data=dict(codename=invalid_codename), follow_redirects=True\n        )\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Invalid input.\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_source_session_expiration",
          "code": "def test_source_session_expiration(source_app):\n    with source_app.test_client() as app:\n        # Given a source user who logs in\n        codename = new_codename(app, session)\n        resp = app.post(url_for(\"main.login\"), data=dict(codename=codename), follow_redirects=True)\n        assert resp.status_code == 200\n\n        # But we're now 6 hours later hence their session expired\n        with mock.patch(\"source_app.session_manager.datetime\") as mock_datetime:\n            six_hours_later = datetime.now(timezone.utc) + timedelta(hours=6)\n            mock_datetime.now.return_value = six_hours_later\n\n            # When they browse to an authenticated page\n            resp = app.get(url_for(\"main.lookup\"), follow_redirects=True)\n\n        # They get redirected to the index page with the \"logged out\" message\n        text = resp.data.decode(\"utf-8\")\n        assert \"You have been logged out due to inactivity or a problem with your session.\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_source_session_expiration_create",
          "code": "def test_source_session_expiration_create(source_app):\n    with source_app.test_client() as app:\n        # Given a source user who is in the middle of the account creation flow\n        resp = app.post(url_for(\"main.generate\"), data=GENERATE_DATA)\n        assert resp.status_code == 200\n\n        # But we're now 6 hours later hence they did not finish the account creation flow in time\n        with mock.patch(\"source_app.main.datetime\") as mock_datetime:\n            six_hours_later = datetime.now(timezone.utc) + timedelta(hours=6)\n            mock_datetime.now.return_value = six_hours_later\n\n            # When the user tries to complete the create flow\n            resp = app.post(url_for(\"main.create\"), follow_redirects=True)\n\n        # They get redirected to the index page with the \"logged out\" message\n        text = resp.data.decode(\"utf-8\")\n        assert \"You have been logged out due to inactivity or a problem with your session.\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_source_no_session_expiration_message_when_not_logged_in",
          "code": "def test_source_no_session_expiration_message_when_not_logged_in(source_app):\n    with source_app.test_client() as app:\n        # Given an unauthenticated source user\n        resp = app.get(url_for(\"main.index\"))\n        assert resp.status_code == 200\n\n        # And their session expired\n        with mock.patch(\"source_app.session_manager.datetime\") as mock_datetime:\n            six_hours_later = datetime.utcnow() + timedelta(hours=6)\n            mock_datetime.now.return_value = six_hours_later\n\n        # When they browse again the index page\n        refreshed_resp = app.get(url_for(\"main.index\"), follow_redirects=True)\n\n        # The session expiration message is NOT displayed\n        text = refreshed_resp.data.decode(\"utf-8\")\n        assert (\n            \"You have been logged out due to inactivity or a problem with your session.\" not in text\n        )",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_csrf_error_page",
          "code": "def test_csrf_error_page(source_app):\n    source_app.config[\"WTF_CSRF_ENABLED\"] = True\n    with source_app.test_client() as app:\n        # /create without a CSRF token should redirect...\n        with InstrumentedApp(source_app) as ins:\n            resp = app.post(url_for(\"main.create\"))\n            ins.assert_redirects(resp, url_for(\"main.index\"))\n\n        # ...and show an error message.\n        resp = app.post(url_for(\"main.create\"), follow_redirects=True)\n        text = resp.data.decode(\"utf-8\")\n        assert \"You have been logged out due to inactivity or a problem with your session.\" in text\n\n        # /logout without a CSRF token should also error.\n        resp = app.post(url_for(\"main.logout\"), follow_redirects=True)\n        text = resp.data.decode(\"utf-8\")\n        assert \"You have been logged out due to inactivity or a problem with your session.\" in text",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_source_can_only_delete_own_replies",
          "code": "def test_source_can_only_delete_own_replies(source_app, app_storage):\n    \"\"\"This test checks for a bug an authenticated source A could delete\n    replies send to source B by \"guessing\" the filename.\n    \"\"\"\n    source0, codename0 = utils.db_helper.init_source(app_storage)\n    source1, codename1 = utils.db_helper.init_source(app_storage)\n    journalist, _ = utils.db_helper.init_journalist()\n    replies = utils.db_helper.reply(app_storage, journalist, source0, 1)\n    filename = replies[0].filename\n    confirmation_msg = \"Reply deleted\"\n\n    with source_app.test_client() as app:\n        resp = app.post(url_for(\"main.login\"), data={\"codename\": codename1}, follow_redirects=True)\n        assert resp.status_code == 200\n        assert SessionManager.get_logged_in_user(db_session=db.session).db_record_id == source1.id\n\n        resp = app.post(\n            url_for(\"main.delete\"), data={\"reply_filename\": filename}, follow_redirects=True\n        )\n        assert resp.status_code == 404\n        assert confirmation_msg not in resp.data.decode(\"utf-8\")\n\n    reply = Reply.query.filter_by(filename=filename).one()\n    assert not reply.deleted_by_source\n\n    with source_app.test_client() as app:\n        resp = app.post(url_for(\"main.login\"), data={\"codename\": codename0}, follow_redirects=True)\n        assert resp.status_code == 200\n        assert SessionManager.get_logged_in_user(db_session=db.session).db_record_id == source0.id\n\n        resp = app.post(\n            url_for(\"main.delete\"), data={\"reply_filename\": filename}, follow_redirects=True\n        )\n        assert resp.status_code == 200\n        assert confirmation_msg in resp.data.decode(\"utf-8\")\n\n    reply = Reply.query.filter_by(filename=filename).one()\n    assert reply.deleted_by_source",
          "file": "test_source.py"
        },
        {
          "type": "function",
          "name": "test_robots_txt",
          "code": "def test_robots_txt(source_app):\n    \"\"\"Test that robots.txt works\"\"\"\n    with source_app.test_client() as app:\n        # Not using url_for here because we care about the actual URL path\n        resp = app.get(\"/robots.txt\")\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Disallow: /\" in text",
          "file": "test_source.py"
        }
      ],
      "test_journalist_api.py": [
        {
          "type": "function",
          "name": "assert_valid_timestamp",
          "code": "def assert_valid_timestamp(timestamp: str) -> None:\n    \"\"\"verify the timestamp is encoded in the format we want\"\"\"\n    assert timestamp == datetime.fromisoformat(timestamp).isoformat()",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_unauthenticated_user_gets_all_endpoints",
          "code": "def test_unauthenticated_user_gets_all_endpoints(journalist_app):\n    with journalist_app.test_client() as app:\n        response = app.get(url_for(\"api.get_endpoints\"))\n\n        expected_endpoints = [\n            \"current_user_url\",\n            \"all_users_url\",\n            \"submissions_url\",\n            \"sources_url\",\n            \"auth_token_url\",\n            \"replies_url\",\n            \"seen_url\",\n        ]\n        expected_endpoints.sort()\n        sorted_observed_endpoints = list(response.json.keys())\n        sorted_observed_endpoints.sort()\n        assert expected_endpoints == sorted_observed_endpoints",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_valid_user_can_get_an_api_token",
          "code": "def test_valid_user_can_get_an_api_token(journalist_app, test_journo):\n    with journalist_app.test_client() as app:\n        valid_token = TOTP(test_journo[\"otp_secret\"]).now()\n        response = app.post(\n            url_for(\"api.get_token\"),\n            data=json.dumps(\n                {\n                    \"username\": test_journo[\"username\"],\n                    \"passphrase\": test_journo[\"password\"],\n                    \"one_time_code\": valid_token,\n                }\n            ),\n            headers=get_api_headers(),\n        )\n\n        assert response.json[\"journalist_uuid\"] == test_journo[\"uuid\"]\n        assert response.status_code == 200\n        assert response.json[\"journalist_first_name\"] == test_journo[\"first_name\"]\n        assert response.json[\"journalist_last_name\"] == test_journo[\"last_name\"]\n\n        assert_valid_timestamp(response.json[\"expiration\"])\n\n        response = app.get(\n            url_for(\"api.get_current_user\"),\n            headers=get_api_headers(response.json[\"token\"]),\n        )\n        assert response.status_code == 200\n        assert response.json[\"uuid\"] == test_journo[\"uuid\"]",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_user_cannot_get_an_api_token_with_wrong_password",
          "code": "def test_user_cannot_get_an_api_token_with_wrong_password(journalist_app, test_journo):\n    with journalist_app.test_client() as app:\n        valid_token = TOTP(test_journo[\"otp_secret\"]).now()\n        response = app.post(\n            url_for(\"api.get_token\"),\n            data=json.dumps(\n                {\n                    \"username\": test_journo[\"username\"],\n                    \"passphrase\": \"wrong password\",\n                    \"one_time_code\": valid_token,\n                }\n            ),\n            headers=get_api_headers(),\n        )\n\n        assert response.status_code == 403\n        assert response.json[\"error\"] == \"Forbidden\"",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_user_cannot_get_an_api_token_with_wrong_2fa_token",
          "code": "def test_user_cannot_get_an_api_token_with_wrong_2fa_token(journalist_app, test_journo):\n    with journalist_app.test_client() as app:\n        response = app.post(\n            url_for(\"api.get_token\"),\n            data=json.dumps(\n                {\n                    \"username\": test_journo[\"username\"],\n                    \"passphrase\": test_journo[\"password\"],\n                    \"one_time_code\": \"123456\",\n                }\n            ),\n            headers=get_api_headers(),\n        )\n\n        assert response.status_code == 403\n        assert response.json[\"error\"] == \"Forbidden\"",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_user_cannot_get_an_api_token_with_no_passphrase_field",
          "code": "def test_user_cannot_get_an_api_token_with_no_passphrase_field(journalist_app, test_journo):\n    with journalist_app.test_client() as app:\n        valid_token = TOTP(test_journo[\"otp_secret\"]).now()\n        response = app.post(\n            url_for(\"api.get_token\"),\n            data=json.dumps({\"username\": test_journo[\"username\"], \"one_time_code\": valid_token}),\n            headers=get_api_headers(),\n        )\n\n        assert response.status_code == 400\n        assert response.json[\"error\"] == \"Bad Request\"\n        assert response.json[\"message\"] == \"passphrase field is missing\"",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_user_cannot_get_an_api_token_with_no_username_field",
          "code": "def test_user_cannot_get_an_api_token_with_no_username_field(journalist_app, test_journo):\n    with journalist_app.test_client() as app:\n        valid_token = TOTP(test_journo[\"otp_secret\"]).now()\n        response = app.post(\n            url_for(\"api.get_token\"),\n            data=json.dumps({\"passphrase\": test_journo[\"password\"], \"one_time_code\": valid_token}),\n            headers=get_api_headers(),\n        )\n\n        assert response.status_code == 400\n        assert response.json[\"error\"] == \"Bad Request\"\n        assert response.json[\"message\"] == \"username field is missing\"",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_user_cannot_get_an_api_token_with_no_otp_field",
          "code": "def test_user_cannot_get_an_api_token_with_no_otp_field(journalist_app, test_journo):\n    with journalist_app.test_client() as app:\n        response = app.post(\n            url_for(\"api.get_token\"),\n            data=json.dumps(\n                {\n                    \"username\": test_journo[\"username\"],\n                    \"passphrase\": test_journo[\"password\"],\n                }\n            ),\n            headers=get_api_headers(),\n        )\n\n        assert response.status_code == 400\n        assert response.json[\"error\"] == \"Bad Request\"\n        assert response.json[\"message\"] == \"one_time_code field is missing\"",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_gets_all_sources",
          "code": "def test_authorized_user_gets_all_sources(journalist_app, test_submissions, journalist_api_token):\n    with journalist_app.test_client() as app:\n        response = app.get(\n            url_for(\"api.get_all_sources\"),\n            headers=get_api_headers(journalist_api_token),\n        )\n\n        assert response.status_code == 200\n\n        # We expect to see our test source in the response\n        assert (\n            test_submissions[\"source\"].journalist_designation\n            == response.json[\"sources\"][0][\"journalist_designation\"]\n        )\n        for source in response.json[\"sources\"]:\n            assert_valid_timestamp(source[\"last_updated\"])",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_user_without_token_cannot_get_protected_endpoints",
          "code": "def test_user_without_token_cannot_get_protected_endpoints(journalist_app, test_files):\n    with journalist_app.app_context():\n        uuid = test_files[\"source\"].uuid\n        protected_routes = [\n            url_for(\"api.get_all_sources\"),\n            url_for(\"api.single_source\", source_uuid=uuid),\n            url_for(\"api.all_source_submissions\", source_uuid=uuid),\n            url_for(\n                \"api.single_submission\",\n                source_uuid=uuid,\n                submission_uuid=test_files[\"submissions\"][0].uuid,\n            ),\n            url_for(\n                \"api.download_submission\",\n                source_uuid=uuid,\n                submission_uuid=test_files[\"submissions\"][0].uuid,\n            ),\n            url_for(\"api.get_all_submissions\"),\n            url_for(\"api.get_all_replies\"),\n            url_for(\n                \"api.single_reply\",\n                source_uuid=uuid,\n                reply_uuid=test_files[\"replies\"][0].uuid,\n            ),\n            url_for(\"api.all_source_replies\", source_uuid=uuid),\n            url_for(\"api.get_current_user\"),\n            url_for(\"api.get_all_users\"),\n        ]\n\n    with journalist_app.test_client() as app:\n        for protected_route in protected_routes:\n            response = app.get(protected_route, headers=get_api_headers(\"\"))\n\n            assert response.status_code == 403",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_user_without_token_cannot_del_protected_endpoints",
          "code": "def test_user_without_token_cannot_del_protected_endpoints(journalist_app, test_submissions):\n    with journalist_app.app_context():\n        uuid = test_submissions[\"source\"].uuid\n        protected_routes = [\n            url_for(\"api.single_source\", source_uuid=uuid),\n            url_for(\n                \"api.single_submission\",\n                source_uuid=uuid,\n                submission_uuid=test_submissions[\"submissions\"][0].uuid,\n            ),\n            url_for(\"api.remove_star\", source_uuid=uuid),\n            url_for(\"api.source_conversation\", source_uuid=uuid),\n        ]\n\n    with journalist_app.test_client() as app:\n        for protected_route in protected_routes:\n            response = app.delete(protected_route, headers=get_api_headers(\"\"))\n\n            assert response.status_code == 403",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_attacker_cannot_use_token_after_admin_deletes",
          "code": "def test_attacker_cannot_use_token_after_admin_deletes(\n    journalist_app, test_source, journalist_api_token\n):\n    with journalist_app.test_client() as app:\n        uuid = test_source[\"source\"].uuid\n\n        # In a scenario where an attacker compromises a journalist workstation\n        # the admin should be able to delete the user and their token should\n        # no longer be valid.\n        attacker = app.get(\n            url_for(\"api.get_current_user\"),\n            headers=get_api_headers(journalist_api_token),\n        ).json\n\n        attacker = Journalist.query.filter_by(uuid=attacker[\"uuid\"]).first()\n\n        db.session.delete(attacker)\n        db.session.commit()\n\n        # Now this token should not be valid.\n        response = app.delete(\n            url_for(\"api.single_source\", source_uuid=uuid),\n            headers=get_api_headers(journalist_api_token),\n        )\n\n        assert response.status_code == 403",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_user_without_token_cannot_post_protected_endpoints",
          "code": "def test_user_without_token_cannot_post_protected_endpoints(journalist_app, test_source):\n    with journalist_app.app_context():\n        uuid = test_source[\"source\"].uuid\n        protected_routes = [\n            url_for(\"api.all_source_replies\", source_uuid=uuid),\n            url_for(\"api.add_star\", source_uuid=uuid),\n            url_for(\"api.flag\", source_uuid=uuid),\n        ]\n\n    with journalist_app.test_client() as app:\n        for protected_route in protected_routes:\n            response = app.post(\n                protected_route,\n                headers=get_api_headers(\"\"),\n                data=json.dumps({\"some\": \"stuff\"}),\n            )\n            assert response.status_code == 403",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_api_error_handlers_defined",
          "code": "def test_api_error_handlers_defined(journalist_app):\n    \"\"\"Ensure the expected error handler is defined in the API blueprint\"\"\"\n    for status_code in [400, 401, 403, 404, 500]:\n        result = journalist_app.error_handler_spec[\"api\"][status_code]\n\n        expected_error_handler = \"_handle_api_http_exception\"\n        assert list(result.values())[0].__name__ == expected_error_handler",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_api_error_handler_404",
          "code": "def test_api_error_handler_404(journalist_app, journalist_api_token):\n    with journalist_app.test_client() as app:\n        response = app.get(\"/api/v1/invalidendpoint\", headers=get_api_headers(journalist_api_token))\n\n        assert response.status_code == 404\n        assert response.json[\"error\"] == \"Not Found\"",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_trailing_slash_cleanly_404s",
          "code": "def test_trailing_slash_cleanly_404s(journalist_app, test_source, journalist_api_token):\n    with journalist_app.test_client() as app:\n        uuid = test_source[\"source\"].uuid\n        response = app.get(\n            url_for(\"api.single_source\", source_uuid=uuid) + \"/\",\n            headers=get_api_headers(journalist_api_token),\n        )\n\n        assert response.status_code == 404\n        assert response.json[\"error\"] == \"Not Found\"",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_gets_single_source",
          "code": "def test_authorized_user_gets_single_source(journalist_app, test_source, journalist_api_token):\n    with journalist_app.test_client() as app:\n        uuid = test_source[\"source\"].uuid\n        response = app.get(\n            url_for(\"api.single_source\", source_uuid=uuid),\n            headers=get_api_headers(journalist_api_token),\n        )\n\n        assert response.status_code == 200\n\n        assert response.json[\"uuid\"] == test_source[\"source\"].uuid\n        assert response.json[\"key\"][\"fingerprint\"] == test_source[\"source\"].fingerprint\n        assert redwood.is_valid_public_key(response.json[\"key\"][\"public\"])",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_get_non_existant_source_404s",
          "code": "def test_get_non_existant_source_404s(journalist_app, journalist_api_token):\n    with journalist_app.test_client() as app:\n        response = app.get(\n            url_for(\"api.single_source\", source_uuid=1),\n            headers=get_api_headers(journalist_api_token),\n        )\n\n        assert response.status_code == 404",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_can_star_a_source",
          "code": "def test_authorized_user_can_star_a_source(journalist_app, test_source, journalist_api_token):\n    with journalist_app.test_client() as app:\n        uuid = test_source[\"source\"].uuid\n        source_id = test_source[\"source\"].id\n        response = app.post(\n            url_for(\"api.add_star\", source_uuid=uuid),\n            headers=get_api_headers(journalist_api_token),\n        )\n\n        assert response.status_code == 201\n\n        # Verify that the source was starred.\n        assert SourceStar.query.filter(SourceStar.source_id == source_id).one().starred\n\n        # API should also report is_starred is true\n        response = app.get(\n            url_for(\"api.single_source\", source_uuid=uuid),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.json[\"is_starred\"] is True",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_can_unstar_a_source",
          "code": "def test_authorized_user_can_unstar_a_source(journalist_app, test_source, journalist_api_token):\n    with journalist_app.test_client() as app:\n        uuid = test_source[\"source\"].uuid\n        source_id = test_source[\"source\"].id\n        response = app.post(\n            url_for(\"api.add_star\", source_uuid=uuid),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.status_code == 201\n\n        response = app.delete(\n            url_for(\"api.remove_star\", source_uuid=uuid),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.status_code == 200\n\n        # Verify that the source is gone.\n        assert SourceStar.query.filter(SourceStar.source_id == source_id).one().starred is False\n\n        # API should also report is_starred is false\n        response = app.get(\n            url_for(\"api.single_source\", source_uuid=uuid),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.json[\"is_starred\"] is False",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_disallowed_methods_produces_405",
          "code": "def test_disallowed_methods_produces_405(journalist_app, test_source, journalist_api_token):\n    with journalist_app.test_client() as app:\n        uuid = test_source[\"source\"].uuid\n        response = app.delete(\n            url_for(\"api.add_star\", source_uuid=uuid),\n            headers=get_api_headers(journalist_api_token),\n        )\n\n        assert response.status_code == 405\n        assert response.json[\"error\"] == \"Method Not Allowed\"",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_can_get_all_submissions",
          "code": "def test_authorized_user_can_get_all_submissions(\n    journalist_app, test_submissions, journalist_api_token\n):\n    with journalist_app.test_client() as app:\n        response = app.get(\n            url_for(\"api.get_all_submissions\"),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.status_code == 200\n\n        observed_submissions = [\n            submission[\"filename\"] for submission in response.json[\"submissions\"]\n        ]\n\n        expected_submissions = [submission.filename for submission in Submission.query.all()]\n        assert observed_submissions == expected_submissions",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_get_all_submissions_with_disconnected_submissions",
          "code": "def test_authorized_user_get_all_submissions_with_disconnected_submissions(\n    journalist_app, test_submissions, journalist_api_token\n):\n    with journalist_app.test_client() as app:\n        db.session.execute(\n            \"DELETE FROM sources WHERE id = :id\", {\"id\": test_submissions[\"source\"].id}\n        )\n        response = app.get(\n            url_for(\"api.get_all_submissions\"),\n            headers=get_api_headers(journalist_api_token),\n        )\n\n        assert response.status_code == 200",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_get_source_submissions",
          "code": "def test_authorized_user_get_source_submissions(\n    journalist_app, test_submissions, journalist_api_token\n):\n    with journalist_app.test_client() as app:\n        uuid = test_submissions[\"source\"].uuid\n        response = app.get(\n            url_for(\"api.all_source_submissions\", source_uuid=uuid),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.status_code == 200\n\n        observed_submissions = [\n            submission[\"filename\"] for submission in response.json[\"submissions\"]\n        ]\n\n        expected_submissions = [\n            submission.filename for submission in test_submissions[\"source\"].submissions\n        ]\n        assert observed_submissions == expected_submissions",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_can_get_single_submission",
          "code": "def test_authorized_user_can_get_single_submission(\n    journalist_app, test_submissions, journalist_api_token\n):\n    with journalist_app.test_client() as app:\n        submission_uuid = test_submissions[\"source\"].submissions[0].uuid\n        uuid = test_submissions[\"source\"].uuid\n        response = app.get(\n            url_for(\n                \"api.single_submission\",\n                source_uuid=uuid,\n                submission_uuid=submission_uuid,\n            ),\n            headers=get_api_headers(journalist_api_token),\n        )\n\n        assert response.status_code == 200\n\n        assert response.json[\"uuid\"] == submission_uuid\n        assert response.json[\"is_read\"] is False\n        assert response.json[\"filename\"] == test_submissions[\"source\"].submissions[0].filename\n        assert response.json[\"size\"] == test_submissions[\"source\"].submissions[0].size",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_can_get_all_replies_with_disconnected_replies",
          "code": "def test_authorized_user_can_get_all_replies_with_disconnected_replies(\n    journalist_app, test_files, journalist_api_token\n):\n    with journalist_app.test_client() as app:\n        db.session.execute(\"DELETE FROM sources WHERE id = :id\", {\"id\": test_files[\"source\"].id})\n        response = app.get(\n            url_for(\"api.get_all_replies\"),\n            headers=get_api_headers(journalist_api_token),\n        )\n\n        assert response.status_code == 200",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_can_get_all_replies",
          "code": "def test_authorized_user_can_get_all_replies(journalist_app, test_files, journalist_api_token):\n    with journalist_app.test_client() as app:\n        response = app.get(\n            url_for(\"api.get_all_replies\"),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.status_code == 200\n\n        observed_replies = [reply[\"filename\"] for reply in response.json[\"replies\"]]\n\n        expected_replies = [reply.filename for reply in Reply.query.all()]\n        assert observed_replies == expected_replies",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_get_source_replies",
          "code": "def test_authorized_user_get_source_replies(journalist_app, test_files, journalist_api_token):\n    with journalist_app.test_client() as app:\n        uuid = test_files[\"source\"].uuid\n        response = app.get(\n            url_for(\"api.all_source_replies\", source_uuid=uuid),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.status_code == 200\n\n        observed_replies = [reply[\"filename\"] for reply in response.json[\"replies\"]]\n\n        expected_replies = [reply.filename for reply in test_files[\"source\"].replies]\n        assert observed_replies == expected_replies",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_can_get_single_reply",
          "code": "def test_authorized_user_can_get_single_reply(journalist_app, test_files, journalist_api_token):\n    with journalist_app.test_client() as app:\n        reply_uuid = test_files[\"source\"].replies[0].uuid\n        uuid = test_files[\"source\"].uuid\n        response = app.get(\n            url_for(\"api.single_reply\", source_uuid=uuid, reply_uuid=reply_uuid),\n            headers=get_api_headers(journalist_api_token),\n        )\n\n        assert response.status_code == 200\n\n        reply = Reply.query.filter(Reply.uuid == reply_uuid).one()\n\n        assert response.json[\"uuid\"] == reply_uuid\n        assert response.json[\"journalist_username\"] == reply.journalist.username\n        assert response.json[\"journalist_uuid\"] == reply.journalist.uuid\n        assert response.json[\"journalist_first_name\"] == (reply.journalist.first_name or \"\")\n        assert response.json[\"journalist_last_name\"] == (reply.journalist.last_name or \"\")\n        assert response.json[\"is_deleted_by_source\"] is False\n        assert response.json[\"filename\"] == test_files[\"source\"].replies[0].filename\n        assert response.json[\"size\"] == test_files[\"source\"].replies[0].size",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_reply_of_deleted_journalist",
          "code": "def test_reply_of_deleted_journalist(\n    journalist_app, test_files_deleted_journalist, journalist_api_token\n):\n    with journalist_app.test_client() as app:\n        reply_uuid = test_files_deleted_journalist[\"source\"].replies[0].uuid\n        uuid = test_files_deleted_journalist[\"source\"].uuid\n        response = app.get(\n            url_for(\"api.single_reply\", source_uuid=uuid, reply_uuid=reply_uuid),\n            headers=get_api_headers(journalist_api_token),\n        )\n        deleted_uuid = Journalist.get_deleted().uuid\n        assert response.status_code == 200\n\n        assert response.json[\"uuid\"] == reply_uuid\n        assert response.json[\"journalist_username\"] == \"deleted\"\n        assert response.json[\"journalist_uuid\"] == deleted_uuid\n        assert response.json[\"journalist_first_name\"] == \"\"\n        assert response.json[\"journalist_last_name\"] == \"\"\n        assert response.json[\"is_deleted_by_source\"] is False\n        assert (\n            response.json[\"filename\"] == test_files_deleted_journalist[\"source\"].replies[0].filename\n        )\n        assert response.json[\"size\"] == test_files_deleted_journalist[\"source\"].replies[0].size",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_can_delete_single_submission",
          "code": "def test_authorized_user_can_delete_single_submission(\n    journalist_app, test_submissions, journalist_api_token\n):\n    with journalist_app.test_client() as app:\n        submission_uuid = test_submissions[\"source\"].submissions[0].uuid\n        uuid = test_submissions[\"source\"].uuid\n        response = app.delete(\n            url_for(\n                \"api.single_submission\",\n                source_uuid=uuid,\n                submission_uuid=submission_uuid,\n            ),\n            headers=get_api_headers(journalist_api_token),\n        )\n\n        assert response.status_code == 200\n\n        # Submission now should be gone.\n        assert Submission.query.filter(Submission.uuid == submission_uuid).all() == []",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_can_delete_single_reply",
          "code": "def test_authorized_user_can_delete_single_reply(journalist_app, test_files, journalist_api_token):\n    with journalist_app.test_client() as app:\n        reply_uuid = test_files[\"source\"].replies[0].uuid\n        uuid = test_files[\"source\"].uuid\n        response = app.delete(\n            url_for(\"api.single_reply\", source_uuid=uuid, reply_uuid=reply_uuid),\n            headers=get_api_headers(journalist_api_token),\n        )\n\n        assert response.status_code == 200\n\n        # Reply should now be gone.\n        assert Reply.query.filter(Reply.uuid == reply_uuid).all() == []",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_can_delete_source_conversation",
          "code": "def test_authorized_user_can_delete_source_conversation(\n    journalist_app, test_files, journalist_api_token\n):\n    with journalist_app.test_client() as app:\n        uuid = test_files[\"source\"].uuid\n        source_id = test_files[\"source\"].id\n\n        # Submissions and Replies both exist\n        assert Submission.query.filter(source_id == source_id).all() != []\n        assert Reply.query.filter(source_id == source_id).all() != []\n\n        response = app.delete(\n            url_for(\"api.source_conversation\", source_uuid=uuid),\n            headers=get_api_headers(journalist_api_token),\n        )\n\n        assert response.status_code == 200\n\n        # Submissions and Replies do not exist\n        assert Submission.query.filter(source_id == source_id).all() == []\n        assert Reply.query.filter(source_id == source_id).all() == []\n\n        # Source still exists\n        assert Source.query.filter(uuid == uuid).all() != []",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_source_conversation_does_not_support_get",
          "code": "def test_source_conversation_does_not_support_get(\n    journalist_app, test_source, journalist_api_token\n):\n    with journalist_app.test_client() as app:\n        uuid = test_source[\"source\"].uuid\n        response = app.get(\n            url_for(\"api.source_conversation\", source_uuid=uuid),\n            headers=get_api_headers(journalist_api_token),\n        )\n\n        assert response.status_code == 405",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_can_delete_source_collection",
          "code": "def test_authorized_user_can_delete_source_collection(\n    journalist_app, test_source, journalist_api_token\n):\n    with journalist_app.test_client() as app:\n        uuid = test_source[\"source\"].uuid\n        response = app.delete(\n            url_for(\"api.single_source\", source_uuid=uuid),\n            headers=get_api_headers(journalist_api_token),\n        )\n\n        assert response.status_code == 200\n\n        # Source does not exist\n        assert Source.query.all() == []",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_can_download_submission",
          "code": "def test_authorized_user_can_download_submission(\n    journalist_app, test_submissions, journalist_api_token\n):\n    with journalist_app.test_client() as app:\n        submission_uuid = test_submissions[\"source\"].submissions[0].uuid\n        uuid = test_submissions[\"source\"].uuid\n\n        response = app.get(\n            url_for(\n                \"api.download_submission\",\n                source_uuid=uuid,\n                submission_uuid=submission_uuid,\n            ),\n            headers=get_api_headers(journalist_api_token),\n        )\n\n        assert response.status_code == 200\n\n        # Response should be a PGP encrypted download\n        assert response.mimetype == \"application/pgp-encrypted\"\n\n        # Response should have Etag field with hash\n        assert response.headers[\"ETag\"].startswith(\"sha256:\")",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_can_download_reply",
          "code": "def test_authorized_user_can_download_reply(journalist_app, test_files, journalist_api_token):\n    with journalist_app.test_client() as app:\n        reply_uuid = test_files[\"source\"].replies[0].uuid\n        uuid = test_files[\"source\"].uuid\n\n        response = app.get(\n            url_for(\"api.download_reply\", source_uuid=uuid, reply_uuid=reply_uuid),\n            headers=get_api_headers(journalist_api_token),\n        )\n\n        assert response.status_code == 200\n\n        # Response should be a PGP encrypted download\n        assert response.mimetype == \"application/pgp-encrypted\"\n\n        # Response should have Etag field with hash\n        assert response.headers[\"ETag\"].startswith(\"sha256:\")",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_can_get_current_user_endpoint",
          "code": "def test_authorized_user_can_get_current_user_endpoint(\n    journalist_app, test_journo, journalist_api_token\n):\n    with journalist_app.test_client() as app:\n        response = app.get(\n            url_for(\"api.get_current_user\"),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.status_code == 200\n\n        assert response.json[\"is_admin\"] is False\n        assert response.json[\"username\"] == test_journo[\"username\"]\n        assert response.json[\"uuid\"] == test_journo[\"journalist\"].uuid\n        assert response.json[\"first_name\"] == test_journo[\"journalist\"].first_name\n        assert response.json[\"last_name\"] == test_journo[\"journalist\"].last_name",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_can_get_all_users",
          "code": "def test_authorized_user_can_get_all_users(\n    journalist_app, test_journo, test_admin, journalist_api_token\n):\n    with journalist_app.test_client() as app:\n        response = app.get(\n            url_for(\"api.get_all_users\"), headers=get_api_headers(journalist_api_token)\n        )\n\n        assert response.status_code == 200\n\n        # Ensure that all the users in the database are returned\n        observed_users = [user[\"uuid\"] for user in response.json[\"users\"]]\n        expected_users = [user.uuid for user in Journalist.query.all()]\n        assert observed_users == expected_users\n\n        # Ensure that no fields other than the expected ones are returned\n        expected_fields = [\"first_name\", \"last_name\", \"username\", \"uuid\"]\n        for user in response.json[\"users\"]:\n            assert sorted(user.keys()) == expected_fields",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_request_with_missing_auth_header_triggers_403",
          "code": "def test_request_with_missing_auth_header_triggers_403(journalist_app):\n    with journalist_app.test_client() as app:\n        response = app.get(\n            url_for(\"api.get_current_user\"),\n            headers={\"Accept\": \"application/json\", \"Content-Type\": \"application/json\"},\n        )\n        assert response.status_code == 403",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_request_with_auth_header_but_no_token_triggers_403",
          "code": "def test_request_with_auth_header_but_no_token_triggers_403(journalist_app):\n    with journalist_app.test_client() as app:\n        response = app.get(\n            url_for(\"api.get_current_user\"),\n            headers={\n                \"Authorization\": \"\",\n                \"Accept\": \"application/json\",\n                \"Content-Type\": \"application/json\",\n            },\n        )\n        assert response.status_code == 403",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_unencrypted_replies_get_rejected",
          "code": "def test_unencrypted_replies_get_rejected(\n    journalist_app, journalist_api_token, test_source, test_journo\n):\n    with journalist_app.test_client() as app:\n        uuid = test_source[\"source\"].uuid\n        reply_content = \"This is a plaintext reply\"\n        response = app.post(\n            url_for(\"api.all_source_replies\", source_uuid=uuid),\n            data=json.dumps({\"reply\": reply_content}),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.status_code == 400",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_authorized_user_can_add_reply",
          "code": "def test_authorized_user_can_add_reply(\n    journalist_app, journalist_api_token, test_source, test_journo, app_storage, tmp_path\n):\n    with journalist_app.test_client() as app:\n        source_id = test_source[\"source\"].id\n        uuid = test_source[\"source\"].uuid\n\n        # First we must encrypt the reply, or it will get rejected\n        # by the server.\n        reply_path = tmp_path / \"message.gpg\"\n        # Use redwood directly, so we can generate an armored message.\n        redwood.encrypt_message(\n            recipients=[\n                test_source[\"source\"].public_key,\n                EncryptionManager.get_default().get_journalist_public_key(),\n            ],\n            plaintext=\"This is an encrypted reply\",\n            destination=reply_path,\n            armor=True,\n        )\n        reply_content = reply_path.read_text()\n\n        response = app.post(\n            url_for(\"api.all_source_replies\", source_uuid=uuid),\n            data=json.dumps({\"reply\": reply_content}),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.status_code == 201\n\n    # ensure the uuid is present and valid\n    reply_uuid = UUID(response.json[\"uuid\"])\n\n    # check that the uuid has a matching db object\n    reply = Reply.query.filter_by(uuid=str(reply_uuid)).one_or_none()\n    assert reply is not None\n\n    # check that the filename is present and correct (#4047)\n    assert response.json[\"filename\"] == reply.filename\n\n    with journalist_app.app_context():  # Now verify everything was saved.\n        assert reply.journalist_id == test_journo[\"id\"]\n        assert reply.source_id == source_id\n\n        # regression test for #3918\n        assert \"/\" not in reply.filename\n\n        source = Source.query.get(source_id)\n\n        expected_filename = f\"{source.interaction_count}-{source.journalist_filename}-reply.gpg\"\n\n        expected_filepath = Path(app_storage.path(source.filesystem_id, expected_filename))\n\n        saved_content = expected_filepath.read_text()\n\n        assert reply_content == saved_content",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_reply_without_content_400",
          "code": "def test_reply_without_content_400(journalist_app, journalist_api_token, test_source, test_journo):\n    with journalist_app.test_client() as app:\n        uuid = test_source[\"source\"].uuid\n        response = app.post(\n            url_for(\"api.all_source_replies\", source_uuid=uuid),\n            data=json.dumps({\"reply\": \"\"}),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.status_code == 400",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_reply_without_reply_field_400",
          "code": "def test_reply_without_reply_field_400(\n    journalist_app, journalist_api_token, test_source, test_journo\n):\n    with journalist_app.test_client() as app:\n        uuid = test_source[\"source\"].uuid\n        response = app.post(\n            url_for(\"api.all_source_replies\", source_uuid=uuid),\n            data=json.dumps({\"other\": \"stuff\"}),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.status_code == 400",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_reply_without_json_400",
          "code": "def test_reply_without_json_400(journalist_app, journalist_api_token, test_source, test_journo):\n    with journalist_app.test_client() as app:\n        uuid = test_source[\"source\"].uuid\n        response = app.post(\n            url_for(\"api.all_source_replies\", source_uuid=uuid),\n            data=\"invalid\",\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.status_code == 400",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_reply_with_valid_curly_json_400",
          "code": "def test_reply_with_valid_curly_json_400(\n    journalist_app, journalist_api_token, test_source, test_journo\n):\n    with journalist_app.test_client() as app:\n        uuid = test_source[\"source\"].uuid\n        response = app.post(\n            url_for(\"api.all_source_replies\", source_uuid=uuid),\n            data=\"{}\",\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.status_code == 400\n\n        assert response.json[\"message\"] == \"reply not found in request body\"",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_reply_with_valid_square_json_400",
          "code": "def test_reply_with_valid_square_json_400(\n    journalist_app, journalist_api_token, test_source, test_journo\n):\n    with journalist_app.test_client() as app:\n        uuid = test_source[\"source\"].uuid\n        response = app.post(\n            url_for(\"api.all_source_replies\", source_uuid=uuid),\n            data=\"[]\",\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.status_code == 400\n\n        assert response.json[\"message\"] == \"reply not found in request body\"",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_malformed_json_400",
          "code": "def test_malformed_json_400(journalist_app, journalist_api_token, test_journo, test_source):\n    with journalist_app.app_context():\n        uuid = test_source[\"source\"].uuid\n        protected_routes = [\n            url_for(\"api.get_token\"),\n            url_for(\"api.all_source_replies\", source_uuid=uuid),\n            url_for(\"api.add_star\", source_uuid=uuid),\n            url_for(\"api.flag\", source_uuid=uuid),\n        ]\n    with journalist_app.test_client() as app:\n        for protected_route in protected_routes:\n            response = app.post(\n                protected_route,\n                data=\"{this is invalid {json!\",\n                headers=get_api_headers(journalist_api_token),\n            )\n\n            assert response.status_code == 400\n            assert response.json[\"error\"] == \"Bad Request\"",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_empty_json_400",
          "code": "def test_empty_json_400(journalist_app, journalist_api_token, test_journo, test_source):\n    with journalist_app.app_context():\n        uuid = test_source[\"source\"].uuid\n        protected_routes = [\n            url_for(\"api.get_token\"),\n            url_for(\"api.all_source_replies\", source_uuid=uuid),\n        ]\n    with journalist_app.test_client() as app:\n        for protected_route in protected_routes:\n            response = app.post(\n                protected_route, data=\"\", headers=get_api_headers(journalist_api_token)\n            )\n\n            assert response.status_code == 400\n            assert response.json[\"error\"] == \"Bad Request\"",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_empty_json_20X",
          "code": "def test_empty_json_20X(journalist_app, journalist_api_token, test_journo, test_source):\n    with journalist_app.app_context():\n        uuid = test_source[\"source\"].uuid\n        protected_routes = [\n            url_for(\"api.add_star\", source_uuid=uuid),\n            url_for(\"api.flag\", source_uuid=uuid),\n        ]\n    with journalist_app.test_client() as app:\n        for protected_route in protected_routes:\n            response = app.post(\n                protected_route, data=\"\", headers=get_api_headers(journalist_api_token)\n            )\n\n            assert response.status_code in (200, 201)",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_set_reply_uuid",
          "code": "def test_set_reply_uuid(journalist_app, journalist_api_token, test_source):\n    msg = \"-----BEGIN PGP MESSAGE-----\\nwat\\n-----END PGP MESSAGE-----\"\n    reply_uuid = str(uuid4())\n    req_data = {\"uuid\": reply_uuid, \"reply\": msg}\n\n    with journalist_app.test_client() as app:\n        # first check that we can set a valid UUID\n        source_uuid = test_source[\"uuid\"]\n        resp = app.post(\n            url_for(\"api.all_source_replies\", source_uuid=source_uuid),\n            data=json.dumps(req_data),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert resp.status_code == 201\n        assert resp.json[\"uuid\"] == reply_uuid\n\n        reply = Reply.query.filter_by(uuid=reply_uuid).one_or_none()\n        assert reply is not None\n\n        len_of_replies = len(Source.query.get(test_source[\"id\"]).replies)\n\n        # next check that requesting with the same UUID does not succeed\n        source_uuid = test_source[\"uuid\"]\n        resp = app.post(\n            url_for(\"api.all_source_replies\", source_uuid=source_uuid),\n            data=json.dumps(req_data),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert resp.status_code == 409\n\n        new_len_of_replies = len(Source.query.get(test_source[\"id\"]).replies)\n\n        assert new_len_of_replies == len_of_replies\n\n        # check setting null for the uuid field doesn't break\n        req_data[\"uuid\"] = None\n        source_uuid = test_source[\"uuid\"]\n        resp = app.post(\n            url_for(\"api.all_source_replies\", source_uuid=source_uuid),\n            data=json.dumps(req_data),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert resp.status_code == 201\n\n        new_uuid = resp.json[\"uuid\"]\n        reply = Reply.query.filter_by(uuid=new_uuid).one_or_none()\n        assert reply is not None\n\n        # check setting invalid values for the uuid field doesn't break\n        req_data[\"uuid\"] = \"not a uuid\"\n        source_uuid = test_source[\"uuid\"]\n        resp = app.post(\n            url_for(\"api.all_source_replies\", source_uuid=source_uuid),\n            data=json.dumps(req_data),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert resp.status_code == 400",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_api_does_not_set_cookie_headers",
          "code": "def test_api_does_not_set_cookie_headers(journalist_app, test_journo):\n    with journalist_app.test_client() as app:\n        response = app.get(url_for(\"api.get_endpoints\"))\n\n        observed_headers = response.headers\n        assert \"Set-Cookie\" not in list(observed_headers.keys())\n        if \"Vary\" in list(observed_headers.keys()):\n            assert \"Cookie\" not in observed_headers[\"Vary\"]",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_malformed_auth_token",
          "code": "def test_malformed_auth_token(journalist_app, journalist_api_token):\n    with journalist_app.app_context():\n        # we know this endpoint requires an auth header\n        url = url_for(\"api.get_all_sources\")\n\n    with journalist_app.test_client() as app:\n        # precondition to ensure token is even valid\n        resp = app.get(url, headers={\"Authorization\": f\"Token {journalist_api_token}\"})\n        assert resp.status_code == 200\n\n        resp = app.get(url, headers={\"Authorization\": f\"not-token {journalist_api_token}\"})\n        assert resp.status_code == 403\n\n        resp = app.get(url, headers={\"Authorization\": journalist_api_token})\n        assert resp.status_code == 403\n\n        resp = app.get(url, headers={\"Authorization\": f\"too many {journalist_api_token}\"})\n        assert resp.status_code == 403",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_submission_download_generates_checksum",
          "code": "def test_submission_download_generates_checksum(\n    journalist_app, journalist_api_token, test_source, test_submissions, mocker\n):\n    submission = test_submissions[\"submissions\"][0]\n    assert submission.checksum is None  # precondition\n\n    with journalist_app.test_client() as app:\n        response = app.get(\n            url_for(\n                \"api.download_submission\",\n                source_uuid=test_source[\"uuid\"],\n                submission_uuid=submission.uuid,\n            ),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.status_code == 200\n        assert response.headers[\"ETag\"]\n\n    # check that the submission checksum was added\n    fetched_submission = Submission.query.get(submission.id)\n    assert fetched_submission.checksum\n\n    mock_add_checksum = mocker.patch(\"journalist_app.utils.add_checksum_for_file\")\n    with journalist_app.test_client() as app:\n        response = app.get(\n            url_for(\n                \"api.download_submission\",\n                source_uuid=test_source[\"uuid\"],\n                submission_uuid=submission.uuid,\n            ),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.status_code == 200\n        assert response.headers[\"ETag\"]\n\n    fetched_submission = Submission.query.get(submission.id)\n    assert fetched_submission.checksum\n    # we don't want to recalculat this value\n    assert not mock_add_checksum.called",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_reply_download_generates_checksum",
          "code": "def test_reply_download_generates_checksum(\n    journalist_app, journalist_api_token, test_source, test_files, mocker\n):\n    reply = test_files[\"replies\"][0]\n    assert reply.checksum is None  # precondition\n\n    with journalist_app.test_client() as app:\n        response = app.get(\n            url_for(\n                \"api.download_reply\",\n                source_uuid=test_source[\"uuid\"],\n                reply_uuid=reply.uuid,\n            ),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.status_code == 200\n        assert response.headers[\"ETag\"]\n\n    # check that the reply checksum was added\n    fetched_reply = Reply.query.get(reply.id)\n    assert fetched_reply.checksum\n\n    mock_add_checksum = mocker.patch(\"journalist_app.utils.add_checksum_for_file\")\n    with journalist_app.test_client() as app:\n        response = app.get(\n            url_for(\n                \"api.download_reply\",\n                source_uuid=test_source[\"uuid\"],\n                reply_uuid=reply.uuid,\n            ),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert response.status_code == 200\n        assert response.headers[\"ETag\"]\n\n    fetched_reply = Reply.query.get(reply.id)\n    assert fetched_reply.checksum\n    # we don't want to recalculat this value\n    assert not mock_add_checksum.called",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_seen",
          "code": "def test_seen(journalist_app, journalist_api_token, test_files, test_journo, test_submissions):\n    \"\"\"\n    Happy path for seen: marking things seen works.\n    \"\"\"\n    with journalist_app.test_client() as app:\n        replies_url = url_for(\"api.get_all_replies\")\n        seen_url = url_for(\"api.seen\")\n        submissions_url = url_for(\"api.get_all_submissions\")\n        headers = get_api_headers(journalist_api_token)\n\n        # check that /submissions contains no seen items\n        response = app.get(submissions_url, headers=headers)\n        assert response.status_code == 200\n        assert not any([s[\"seen_by\"] for s in response.json[\"submissions\"]])\n\n        # check that /replies only contains seen items\n        response = app.get(replies_url, headers=headers)\n        assert response.status_code == 200\n        assert all([r[\"seen_by\"] for r in response.json[\"replies\"]])\n\n        # now mark one of each type of conversation item seen\n        file_uuid = test_files[\"submissions\"][0].uuid\n        msg_uuid = test_submissions[\"submissions\"][0].uuid\n        reply_uuid = test_files[\"replies\"][0].uuid\n        data = {\n            \"files\": [file_uuid],\n            \"messages\": [msg_uuid],\n            \"replies\": [reply_uuid],\n        }\n        response = app.post(seen_url, data=json.dumps(data), headers=headers)\n        assert response.status_code == 200\n        assert response.json[\"message\"] == \"resources marked seen\"\n\n        # check that /submissions now contains the seen items\n        response = app.get(submissions_url, headers=headers)\n        assert response.status_code == 200\n        assert [\n            s\n            for s in response.json[\"submissions\"]\n            if s[\"is_file\"] and s[\"uuid\"] == file_uuid and test_journo[\"uuid\"] in s[\"seen_by\"]\n        ]\n        assert [\n            s\n            for s in response.json[\"submissions\"]\n            if s[\"is_message\"] and s[\"uuid\"] == msg_uuid and test_journo[\"uuid\"] in s[\"seen_by\"]\n        ]\n\n        # check that /replies still only contains one seen reply\n        response = app.get(replies_url, headers=headers)\n        assert response.status_code == 200\n        assert len(response.json[\"replies\"]) == 1\n        assert all([r[\"seen_by\"] for r in response.json[\"replies\"]])\n\n        # now mark the same things seen. this should be fine.\n        response = app.post(seen_url, data=json.dumps(data), headers=headers)\n        assert response.status_code == 200\n        assert response.json[\"message\"] == \"resources marked seen\"\n\n        # check that /submissions still only has the test journalist\n        # once in the seen_by list of the submissions we marked\n        response = app.get(submissions_url, headers=headers)\n        assert response.status_code == 200\n        assert [\n            s\n            for s in response.json[\"submissions\"]\n            if s[\"uuid\"] in [file_uuid, msg_uuid] and s[\"seen_by\"] == [test_journo[\"uuid\"]]\n        ]\n\n        # check that /replies still only contains one seen reply\n        response = app.get(replies_url, headers=headers)\n        assert response.status_code == 200\n        assert len(response.json[\"replies\"]) == 1\n        assert all([r[\"seen_by\"] for r in response.json[\"replies\"]])",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_seen_bad_requests",
          "code": "def test_seen_bad_requests(journalist_app, journalist_api_token):\n    \"\"\"\n    Check that /seen rejects invalid requests.\n    \"\"\"\n    with journalist_app.test_client() as app:\n        seen_url = url_for(\"api.seen\")\n        headers = get_api_headers(journalist_api_token)\n\n        # invalid JSON\n        data = \"not a mapping\"\n        response = app.post(seen_url, data=json.dumps(data), headers=headers)\n        assert response.status_code == 400\n        assert response.json[\"message\"] == \"Please send requests in valid JSON.\"\n\n        # valid JSON, but with invalid content\n        data = {\"valid mapping\": False}\n        response = app.post(seen_url, data=json.dumps(data), headers=headers)\n        assert response.status_code == 400\n        assert response.json[\"message\"] == \"Please specify the resources to mark seen.\"\n\n        # unsupported HTTP method\n        response = app.head(seen_url, headers=headers)\n        assert response.status_code == 405\n\n        # nonexistent file\n        data = {\n            \"files\": [\"not-a-file\"],\n        }\n        response = app.post(seen_url, data=json.dumps(data), headers=headers)\n        assert response.status_code == 404\n        assert response.json[\"message\"] == \"file not found: not-a-file\"\n\n        # nonexistent message\n        data = {\n            \"messages\": [\"not-a-message\"],\n        }\n        response = app.post(seen_url, data=json.dumps(data), headers=headers)\n        assert response.status_code == 404\n        assert response.json[\"message\"] == \"message not found: not-a-message\"\n\n        # nonexistent reply\n        data = {\n            \"replies\": [\"not-a-reply\"],\n        }\n        response = app.post(seen_url, data=json.dumps(data), headers=headers)\n        assert response.status_code == 404\n        assert response.json[\"message\"] == \"reply not found: not-a-reply\"",
          "file": "test_journalist_api.py"
        },
        {
          "type": "function",
          "name": "test_download_submission_range",
          "code": "def test_download_submission_range(journalist_app, test_files, journalist_api_token):\n    with journalist_app.test_client() as app:\n        submission_uuid = test_files[\"submissions\"][0].uuid\n        uuid = test_files[\"source\"].uuid\n\n        # Download the full file\n        full_response = app.get(\n            url_for(\n                \"api.download_submission\",\n                source_uuid=uuid,\n                submission_uuid=submission_uuid,\n            ),\n            headers=get_api_headers(journalist_api_token),\n        )\n        assert full_response.status_code == 200\n        assert full_response.mimetype == \"application/pgp-encrypted\"\n\n        # Verify the etag header is actually the sha256 hash\n        hasher = hashlib.sha256()\n        hasher.update(full_response.data)\n        digest = binascii.hexlify(hasher.digest()).decode(\"utf-8\")\n        digest_str = \"sha256:\" + digest\n        assert full_response.headers.get(\"ETag\") == digest_str\n\n        # Verify the Content-Length header matches the length of the content\n        assert full_response.headers.get(\"Content-Length\") == str(len(full_response.data))\n\n        # Verify that range requests are accepted\n        assert full_response.headers.get(\"Accept-Ranges\") == \"bytes\"\n\n        # Download the first 10 bytes of data\n        headers = get_api_headers(journalist_api_token)\n        headers[\"Range\"] = \"bytes=0-9\"\n        partial_response = app.get(\n            url_for(\n                \"api.download_submission\",\n                source_uuid=uuid,\n                submission_uuid=submission_uuid,\n            ),\n            headers=headers,\n        )\n        assert partial_response.status_code == 206  # HTTP/1.1 206 Partial Content\n        assert partial_response.mimetype == \"application/pgp-encrypted\"\n        assert partial_response.headers.get(\"Content-Length\") == \"10\"\n        assert len(partial_response.data) == 10\n        assert full_response.data[0:10] == partial_response.data\n\n        # Download the second half of the data\n        range_start = int(len(full_response.data) / 2)\n        range_end = len(full_response.data)\n        headers = get_api_headers(journalist_api_token)\n        headers[\"Range\"] = f\"bytes={range_start}-{range_end}\"\n        partial_response = app.get(\n            url_for(\n                \"api.download_submission\",\n                source_uuid=uuid,\n                submission_uuid=submission_uuid,\n            ),\n            headers=headers,\n        )\n        assert partial_response.status_code == 206  # HTTP/1.1 206 Partial Content\n        assert partial_response.mimetype == \"application/pgp-encrypted\"\n        assert partial_response.headers.get(\"Content-Length\") == str(range_end - range_start)\n        assert len(partial_response.data) == range_end - range_start\n        assert full_response.data[range_start:] == partial_response.data",
          "file": "test_journalist_api.py"
        }
      ],
      "test_encryption.py": [
        {
          "type": "class",
          "name": "TestEncryptionManager",
          "code": "class TestEncryptionManager:\n    def test_get_default(self, config):\n        # Given an encryption manager\n        encryption_mgr = EncryptionManager.get_default()\n        assert encryption_mgr\n        # When using the encryption manager to fetch the journalist public key\n        # It succeeds\n        assert redwood.is_valid_public_key(encryption_mgr.get_journalist_public_key())\n\n    def test_get_gpg_source_public_key(self, test_source):\n        # Given a source user with a key pair in the gpg keyring\n        source_user = test_source[\"source_user\"]\n        encryption_mgr = EncryptionManager.get_default()\n        utils.create_legacy_gpg_key(encryption_mgr, source_user, test_source[\"source\"])\n\n        # When using the encryption manager to fetch the source user's public key\n        # It succeeds\n        source_pub_key = encryption_mgr.get_source_public_key(source_user.filesystem_id)\n        assert redwood.is_valid_public_key(source_pub_key)\n\n        # And the key's fingerprint was saved to Redis\n        source_key_fingerprint = encryption_mgr._redis.hget(\n            encryption_mgr.REDIS_FINGERPRINT_HASH, source_user.filesystem_id\n        )\n        assert source_key_fingerprint\n\n        # And the public key was saved to Redis\n        assert encryption_mgr._redis.hget(encryption_mgr.REDIS_KEY_HASH, source_key_fingerprint)\n\n    def test_get_gpg_source_public_key_wrong_id(self, test_source):\n        # Given an encryption manager\n        encryption_mgr = EncryptionManager.get_default()\n\n        # When using the encryption manager to fetch a key for an invalid filesystem id\n        # It fails\n        with pytest.raises(GpgKeyNotFoundError):\n            encryption_mgr.get_source_public_key(\"1234test\")\n\n    def test_delete_gpg_source_key_pair(self, source_app, test_source):\n        # Given a source user with a key pair in the gpg keyring\n        source_user = test_source[\"source_user\"]\n        encryption_mgr = EncryptionManager.get_default()\n        utils.create_legacy_gpg_key(encryption_mgr, source_user, test_source[\"source\"])\n        assert encryption_mgr.get_source_public_key(source_user.filesystem_id)\n\n        # When using the encryption manager to delete this source user's key pair\n        # It succeeds\n        encryption_mgr.delete_source_key_pair(source_user.filesystem_id)\n\n        # And the user's key information can no longer be retrieved\n        with pytest.raises(GpgKeyNotFoundError):\n            encryption_mgr.get_source_public_key(source_user.filesystem_id)\n\n        with pytest.raises(GpgKeyNotFoundError):\n            encryption_mgr.get_source_key_fingerprint(source_user.filesystem_id)\n\n        with pytest.raises(GpgKeyNotFoundError):\n            encryption_mgr._get_source_key_details(source_user.filesystem_id)\n\n    def test_delete_source_key_pair_pinentry_status_is_handled(\n        self, source_app, test_source, mocker, capsys\n    ):\n        \"\"\"\n        Regression test for https://github.com/freedomofpress/securedrop/issues/4294\n        \"\"\"\n        # Given a source user with a key pair in the gpg keyring\n        source_user = test_source[\"source_user\"]\n        encryption_mgr = EncryptionManager.get_default()\n        utils.create_legacy_gpg_key(encryption_mgr, source_user, test_source[\"source\"])\n        assert encryption_mgr.get_source_public_key(source_user.filesystem_id)\n\n        # And a gpg binary that will trigger the issue described in #4294\n        mocker.patch(\n            \"pretty_bad_protocol._util._separate_keyword\",\n            return_value=(\"PINENTRY_LAUNCHED\", \"does not matter\"),\n        )\n\n        # When using the encryption manager to delete this source user's key pair\n        # It succeeds\n        encryption_mgr.delete_source_key_pair(source_user.filesystem_id)\n\n        # And the user's key information can no longer be retrieved\n        with pytest.raises(GpgKeyNotFoundError):\n            encryption_mgr.get_source_key_fingerprint(source_user.filesystem_id)\n\n        # And the bug fix was properly triggered\n        captured = capsys.readouterr()\n        assert \"ValueError: Unknown status message: 'PINENTRY_LAUNCHED'\" not in captured.err\n\n    def test_encrypt_source_message(self, config, tmp_path):\n        # Given an encryption manager\n        encryption_mgr = EncryptionManager.get_default()\n\n        # And a message to be submitted by a source\n        message = \"s3cr3t message\"\n\n        # When the source tries to encrypt the message\n        # It succeeds\n        encrypted_message_path = tmp_path / \"message.gpg\"\n        encryption_mgr.encrypt_source_message(\n            message_in=message, encrypted_message_path_out=encrypted_message_path\n        )\n\n        # And the output file doesn't contain the message plaintext\n        encrypted_message = encrypted_message_path.read_bytes()\n        assert message.encode() not in encrypted_message\n\n        # And the journalist is able to decrypt the message\n        decrypted_message = utils.decrypt_as_journalist(encrypted_message).decode()\n        assert decrypted_message == message\n\n    def test_encrypt_source_file(self, config, tmp_path):\n        # Given an encryption manager\n        encryption_mgr = EncryptionManager.get_default()\n\n        # And a file to be submitted by a source - we use this python file\n        file_to_encrypt_path = Path(__file__)\n\n        # When the source tries to encrypt the file\n        # It succeeds\n        encrypted_file_path = tmp_path / \"file.gpg\"\n        with file_to_encrypt_path.open(\"rb\") as fh:\n            encryption_mgr.encrypt_source_file(\n                file_in=fh,\n                encrypted_file_path_out=encrypted_file_path,\n            )\n\n        # And the output file doesn't contain the file plaintext\n        encrypted_file = encrypted_file_path.read_bytes()\n        assert file_to_encrypt_path.read_bytes() not in encrypted_file\n\n        # And the journalist is able to decrypt the file\n        decrypted_file = utils.decrypt_as_journalist(encrypted_file)\n        assert decrypted_file == file_to_encrypt_path.read_bytes()\n\n    def test_encrypt_and_decrypt_journalist_reply(\n        self, source_app, test_source, tmp_path, app_storage\n    ):\n        # Given a source user\n        source_user1 = test_source[\"source_user\"]\n        source1 = test_source[\"source\"]\n        encryption_mgr = EncryptionManager.get_default()\n\n        # And another source user\n        with source_app.app_context():\n            source_user2 = create_source_user(\n                db_session=db.session,\n                source_passphrase=PassphraseGenerator.get_default().generate_passphrase(),\n                source_app_storage=app_storage,\n            )\n        source_user2.get_db_record()\n\n        # When the journalist tries to encrypt a reply to source1\n        # It succeeds\n        journalist_reply = \"s3cr3t message\"\n        encrypted_reply_path = tmp_path / \"reply.gpg\"\n        encryption_mgr.encrypt_journalist_reply(\n            for_source=source1,\n            reply_in=journalist_reply,\n            encrypted_reply_path_out=encrypted_reply_path,\n        )\n\n        # And the output file doesn't contain the reply plaintext\n        encrypted_reply = encrypted_reply_path.read_bytes()\n        assert journalist_reply.encode() not in encrypted_reply\n\n        # And source1 is able to decrypt the reply\n        decrypted_reply = encryption_mgr.decrypt_journalist_reply(\n            for_source_user=source_user1,\n            ciphertext_in=encrypted_reply,\n        )\n        assert decrypted_reply == journalist_reply\n\n        # And source2 is NOT able to decrypt the reply\n        with pytest.raises(RedwoodError):\n            encryption_mgr.decrypt_journalist_reply(\n                for_source_user=source_user2,\n                ciphertext_in=encrypted_reply,\n            )\n\n        # And the journalist is able to decrypt their own reply\n        decrypted_reply_for_journalist = utils.decrypt_as_journalist(encrypted_reply)\n        assert decrypted_reply_for_journalist.decode() == journalist_reply\n\n    def test_gpg_encrypt_and_decrypt_journalist_reply(\n        self, source_app, test_source, tmp_path, app_storage\n    ):\n        # Given a source user with a key pair in the gpg keyring\n        source_user1 = test_source[\"source_user\"]\n        source1 = test_source[\"source\"]\n        encryption_mgr = EncryptionManager.get_default()\n        utils.create_legacy_gpg_key(encryption_mgr, source_user1, source1)\n\n        # And another source with a key pair in the gpg keyring\n        with source_app.app_context():\n            source_user2 = create_source_user(\n                db_session=db.session,\n                source_passphrase=PassphraseGenerator.get_default().generate_passphrase(),\n                source_app_storage=app_storage,\n            )\n            source2 = source_user2.get_db_record()\n            utils.create_legacy_gpg_key(encryption_mgr, source_user2, source2)\n\n            # When the journalist tries to encrypt a reply to source1\n            # It succeeds\n            journalist_reply = \"s3cr3t message\"\n            encrypted_reply_path = tmp_path / \"reply.gpg\"\n            encryption_mgr.encrypt_journalist_reply(\n                for_source=source1,\n                reply_in=journalist_reply,\n                encrypted_reply_path_out=encrypted_reply_path,\n            )\n\n            # And the output file doesn't contain the reply plaintext\n            encrypted_reply = encrypted_reply_path.read_bytes()\n            assert journalist_reply.encode() not in encrypted_reply\n\n            # And source1 is able to decrypt the reply\n            decrypted_reply = encryption_mgr.decrypt_journalist_reply(\n                for_source_user=source_user1,\n                ciphertext_in=encrypted_reply,\n            )\n            assert decrypted_reply == journalist_reply\n\n            # And source2 is NOT able to decrypt the reply\n            with pytest.raises(GpgDecryptError):\n                encryption_mgr.decrypt_journalist_reply(\n                    for_source_user=source_user2,\n                    ciphertext_in=encrypted_reply,\n                )\n\n        # Amd the reply can't be decrypted without providing the source1's gpg secret\n        result = encryption_mgr.gpg().decrypt(\n            # For GPG 2.1+, a non-null passphrase _must_ be passed to decrypt()\n            encrypted_reply,\n            passphrase=\"test 123\",\n        )\n        assert not result.ok\n\n        # And the journalist is able to decrypt their reply\n        decrypted_reply_for_journalist = utils.decrypt_as_journalist(encrypted_reply).decode()\n        assert decrypted_reply_for_journalist == journalist_reply\n\n    def test_get_source_secret_key_from_gpg(self, test_source, tmp_path, config):\n        source_user = test_source[\"source_user\"]\n        source = test_source[\"source\"]\n\n        encryption_mgr = EncryptionManager(\n            gpg_key_dir=tmp_path,\n            journalist_pub_key=(config.SECUREDROP_DATA_ROOT / \"journalist.pub\"),\n            redis=Redis(decode_responses=True, **config.REDIS_KWARGS),\n        )\n        new_fingerprint = utils.create_legacy_gpg_key(encryption_mgr, source_user, source)\n        secret_key = encryption_mgr.get_source_secret_key_from_gpg(\n            new_fingerprint, source_user.gpg_secret\n        )\n        assert secret_key.splitlines()[0] == \"-----BEGIN PGP PRIVATE KEY BLOCK-----\"\n        # Now try an invalid passphrase\n        with pytest.raises(GpgKeyNotFoundError):\n            encryption_mgr.get_source_secret_key_from_gpg(new_fingerprint, \"not correct passphrase\")\n        # Now if we get a garbage response from GPG\n        mock_gpg = MagicMock()\n        mock_gpg.export_keys.return_value = \"definitely not a gpg secret key\"\n        encryption_mgr._gpg = mock_gpg\n        with pytest.raises(GpgKeyNotFoundError):\n            encryption_mgr.get_source_secret_key_from_gpg(new_fingerprint, source_user.gpg_secret)",
          "file": "test_encryption.py"
        },
        {
          "type": "function",
          "name": "test_get_default",
          "code": "def test_get_default(self, config):\n        # Given an encryption manager\n        encryption_mgr = EncryptionManager.get_default()\n        assert encryption_mgr\n        # When using the encryption manager to fetch the journalist public key\n        # It succeeds\n        assert redwood.is_valid_public_key(encryption_mgr.get_journalist_public_key())",
          "file": "test_encryption.py"
        },
        {
          "type": "function",
          "name": "test_get_gpg_source_public_key",
          "code": "def test_get_gpg_source_public_key(self, test_source):\n        # Given a source user with a key pair in the gpg keyring\n        source_user = test_source[\"source_user\"]\n        encryption_mgr = EncryptionManager.get_default()\n        utils.create_legacy_gpg_key(encryption_mgr, source_user, test_source[\"source\"])\n\n        # When using the encryption manager to fetch the source user's public key\n        # It succeeds\n        source_pub_key = encryption_mgr.get_source_public_key(source_user.filesystem_id)\n        assert redwood.is_valid_public_key(source_pub_key)\n\n        # And the key's fingerprint was saved to Redis\n        source_key_fingerprint = encryption_mgr._redis.hget(\n            encryption_mgr.REDIS_FINGERPRINT_HASH, source_user.filesystem_id\n        )\n        assert source_key_fingerprint\n\n        # And the public key was saved to Redis\n        assert encryption_mgr._redis.hget(encryption_mgr.REDIS_KEY_HASH, source_key_fingerprint)",
          "file": "test_encryption.py"
        },
        {
          "type": "function",
          "name": "test_get_gpg_source_public_key_wrong_id",
          "code": "def test_get_gpg_source_public_key_wrong_id(self, test_source):\n        # Given an encryption manager\n        encryption_mgr = EncryptionManager.get_default()\n\n        # When using the encryption manager to fetch a key for an invalid filesystem id\n        # It fails\n        with pytest.raises(GpgKeyNotFoundError):\n            encryption_mgr.get_source_public_key(\"1234test\")",
          "file": "test_encryption.py"
        },
        {
          "type": "function",
          "name": "test_delete_gpg_source_key_pair",
          "code": "def test_delete_gpg_source_key_pair(self, source_app, test_source):\n        # Given a source user with a key pair in the gpg keyring\n        source_user = test_source[\"source_user\"]\n        encryption_mgr = EncryptionManager.get_default()\n        utils.create_legacy_gpg_key(encryption_mgr, source_user, test_source[\"source\"])\n        assert encryption_mgr.get_source_public_key(source_user.filesystem_id)\n\n        # When using the encryption manager to delete this source user's key pair\n        # It succeeds\n        encryption_mgr.delete_source_key_pair(source_user.filesystem_id)\n\n        # And the user's key information can no longer be retrieved\n        with pytest.raises(GpgKeyNotFoundError):\n            encryption_mgr.get_source_public_key(source_user.filesystem_id)\n\n        with pytest.raises(GpgKeyNotFoundError):\n            encryption_mgr.get_source_key_fingerprint(source_user.filesystem_id)\n\n        with pytest.raises(GpgKeyNotFoundError):\n            encryption_mgr._get_source_key_details(source_user.filesystem_id)",
          "file": "test_encryption.py"
        },
        {
          "type": "function",
          "name": "test_delete_source_key_pair_pinentry_status_is_handled",
          "code": "def test_delete_source_key_pair_pinentry_status_is_handled(\n        self, source_app, test_source, mocker, capsys\n    ):\n        \"\"\"\n        Regression test for https://github.com/freedomofpress/securedrop/issues/4294\n        \"\"\"\n        # Given a source user with a key pair in the gpg keyring\n        source_user = test_source[\"source_user\"]\n        encryption_mgr = EncryptionManager.get_default()\n        utils.create_legacy_gpg_key(encryption_mgr, source_user, test_source[\"source\"])\n        assert encryption_mgr.get_source_public_key(source_user.filesystem_id)\n\n        # And a gpg binary that will trigger the issue described in #4294\n        mocker.patch(\n            \"pretty_bad_protocol._util._separate_keyword\",\n            return_value=(\"PINENTRY_LAUNCHED\", \"does not matter\"),\n        )\n\n        # When using the encryption manager to delete this source user's key pair\n        # It succeeds\n        encryption_mgr.delete_source_key_pair(source_user.filesystem_id)\n\n        # And the user's key information can no longer be retrieved\n        with pytest.raises(GpgKeyNotFoundError):\n            encryption_mgr.get_source_key_fingerprint(source_user.filesystem_id)\n\n        # And the bug fix was properly triggered\n        captured = capsys.readouterr()\n        assert \"ValueError: Unknown status message: 'PINENTRY_LAUNCHED'\" not in captured.err",
          "file": "test_encryption.py"
        },
        {
          "type": "function",
          "name": "test_encrypt_source_message",
          "code": "def test_encrypt_source_message(self, config, tmp_path):\n        # Given an encryption manager\n        encryption_mgr = EncryptionManager.get_default()\n\n        # And a message to be submitted by a source\n        message = \"s3cr3t message\"\n\n        # When the source tries to encrypt the message\n        # It succeeds\n        encrypted_message_path = tmp_path / \"message.gpg\"\n        encryption_mgr.encrypt_source_message(\n            message_in=message, encrypted_message_path_out=encrypted_message_path\n        )\n\n        # And the output file doesn't contain the message plaintext\n        encrypted_message = encrypted_message_path.read_bytes()\n        assert message.encode() not in encrypted_message\n\n        # And the journalist is able to decrypt the message\n        decrypted_message = utils.decrypt_as_journalist(encrypted_message).decode()\n        assert decrypted_message == message",
          "file": "test_encryption.py"
        },
        {
          "type": "function",
          "name": "test_encrypt_source_file",
          "code": "def test_encrypt_source_file(self, config, tmp_path):\n        # Given an encryption manager\n        encryption_mgr = EncryptionManager.get_default()\n\n        # And a file to be submitted by a source - we use this python file\n        file_to_encrypt_path = Path(__file__)\n\n        # When the source tries to encrypt the file\n        # It succeeds\n        encrypted_file_path = tmp_path / \"file.gpg\"\n        with file_to_encrypt_path.open(\"rb\") as fh:\n            encryption_mgr.encrypt_source_file(\n                file_in=fh,\n                encrypted_file_path_out=encrypted_file_path,\n            )\n\n        # And the output file doesn't contain the file plaintext\n        encrypted_file = encrypted_file_path.read_bytes()\n        assert file_to_encrypt_path.read_bytes() not in encrypted_file\n\n        # And the journalist is able to decrypt the file\n        decrypted_file = utils.decrypt_as_journalist(encrypted_file)\n        assert decrypted_file == file_to_encrypt_path.read_bytes()",
          "file": "test_encryption.py"
        },
        {
          "type": "function",
          "name": "test_encrypt_and_decrypt_journalist_reply",
          "code": "def test_encrypt_and_decrypt_journalist_reply(\n        self, source_app, test_source, tmp_path, app_storage\n    ):\n        # Given a source user\n        source_user1 = test_source[\"source_user\"]\n        source1 = test_source[\"source\"]\n        encryption_mgr = EncryptionManager.get_default()\n\n        # And another source user\n        with source_app.app_context():\n            source_user2 = create_source_user(\n                db_session=db.session,\n                source_passphrase=PassphraseGenerator.get_default().generate_passphrase(),\n                source_app_storage=app_storage,\n            )\n        source_user2.get_db_record()\n\n        # When the journalist tries to encrypt a reply to source1\n        # It succeeds\n        journalist_reply = \"s3cr3t message\"\n        encrypted_reply_path = tmp_path / \"reply.gpg\"\n        encryption_mgr.encrypt_journalist_reply(\n            for_source=source1,\n            reply_in=journalist_reply,\n            encrypted_reply_path_out=encrypted_reply_path,\n        )\n\n        # And the output file doesn't contain the reply plaintext\n        encrypted_reply = encrypted_reply_path.read_bytes()\n        assert journalist_reply.encode() not in encrypted_reply\n\n        # And source1 is able to decrypt the reply\n        decrypted_reply = encryption_mgr.decrypt_journalist_reply(\n            for_source_user=source_user1,\n            ciphertext_in=encrypted_reply,\n        )\n        assert decrypted_reply == journalist_reply\n\n        # And source2 is NOT able to decrypt the reply\n        with pytest.raises(RedwoodError):\n            encryption_mgr.decrypt_journalist_reply(\n                for_source_user=source_user2,\n                ciphertext_in=encrypted_reply,\n            )\n\n        # And the journalist is able to decrypt their own reply\n        decrypted_reply_for_journalist = utils.decrypt_as_journalist(encrypted_reply)\n        assert decrypted_reply_for_journalist.decode() == journalist_reply",
          "file": "test_encryption.py"
        },
        {
          "type": "function",
          "name": "test_gpg_encrypt_and_decrypt_journalist_reply",
          "code": "def test_gpg_encrypt_and_decrypt_journalist_reply(\n        self, source_app, test_source, tmp_path, app_storage\n    ):\n        # Given a source user with a key pair in the gpg keyring\n        source_user1 = test_source[\"source_user\"]\n        source1 = test_source[\"source\"]\n        encryption_mgr = EncryptionManager.get_default()\n        utils.create_legacy_gpg_key(encryption_mgr, source_user1, source1)\n\n        # And another source with a key pair in the gpg keyring\n        with source_app.app_context():\n            source_user2 = create_source_user(\n                db_session=db.session,\n                source_passphrase=PassphraseGenerator.get_default().generate_passphrase(),\n                source_app_storage=app_storage,\n            )\n            source2 = source_user2.get_db_record()\n            utils.create_legacy_gpg_key(encryption_mgr, source_user2, source2)\n\n            # When the journalist tries to encrypt a reply to source1\n            # It succeeds\n            journalist_reply = \"s3cr3t message\"\n            encrypted_reply_path = tmp_path / \"reply.gpg\"\n            encryption_mgr.encrypt_journalist_reply(\n                for_source=source1,\n                reply_in=journalist_reply,\n                encrypted_reply_path_out=encrypted_reply_path,\n            )\n\n            # And the output file doesn't contain the reply plaintext\n            encrypted_reply = encrypted_reply_path.read_bytes()\n            assert journalist_reply.encode() not in encrypted_reply\n\n            # And source1 is able to decrypt the reply\n            decrypted_reply = encryption_mgr.decrypt_journalist_reply(\n                for_source_user=source_user1,\n                ciphertext_in=encrypted_reply,\n            )\n            assert decrypted_reply == journalist_reply\n\n            # And source2 is NOT able to decrypt the reply\n            with pytest.raises(GpgDecryptError):\n                encryption_mgr.decrypt_journalist_reply(\n                    for_source_user=source_user2,\n                    ciphertext_in=encrypted_reply,\n                )\n\n        # Amd the reply can't be decrypted without providing the source1's gpg secret\n        result = encryption_mgr.gpg().decrypt(\n            # For GPG 2.1+, a non-null passphrase _must_ be passed to decrypt()\n            encrypted_reply,\n            passphrase=\"test 123\",\n        )\n        assert not result.ok\n\n        # And the journalist is able to decrypt their reply\n        decrypted_reply_for_journalist = utils.decrypt_as_journalist(encrypted_reply).decode()\n        assert decrypted_reply_for_journalist == journalist_reply",
          "file": "test_encryption.py"
        },
        {
          "type": "function",
          "name": "test_get_source_secret_key_from_gpg",
          "code": "def test_get_source_secret_key_from_gpg(self, test_source, tmp_path, config):\n        source_user = test_source[\"source_user\"]\n        source = test_source[\"source\"]\n\n        encryption_mgr = EncryptionManager(\n            gpg_key_dir=tmp_path,\n            journalist_pub_key=(config.SECUREDROP_DATA_ROOT / \"journalist.pub\"),\n            redis=Redis(decode_responses=True, **config.REDIS_KWARGS),\n        )\n        new_fingerprint = utils.create_legacy_gpg_key(encryption_mgr, source_user, source)\n        secret_key = encryption_mgr.get_source_secret_key_from_gpg(\n            new_fingerprint, source_user.gpg_secret\n        )\n        assert secret_key.splitlines()[0] == \"-----BEGIN PGP PRIVATE KEY BLOCK-----\"\n        # Now try an invalid passphrase\n        with pytest.raises(GpgKeyNotFoundError):\n            encryption_mgr.get_source_secret_key_from_gpg(new_fingerprint, \"not correct passphrase\")\n        # Now if we get a garbage response from GPG\n        mock_gpg = MagicMock()\n        mock_gpg.export_keys.return_value = \"definitely not a gpg secret key\"\n        encryption_mgr._gpg = mock_gpg\n        with pytest.raises(GpgKeyNotFoundError):\n            encryption_mgr.get_source_secret_key_from_gpg(new_fingerprint, source_user.gpg_secret)",
          "file": "test_encryption.py"
        }
      ],
      "test_secure_tempfile.py": [
        {
          "type": "function",
          "name": "test_read_before_writing",
          "code": "def test_read_before_writing():\n    f = SecureTemporaryFile(\"/tmp\")\n    with pytest.raises(AssertionError) as err:\n        f.read()\n    assert \"You must write before reading!\" in str(err)",
          "file": "test_secure_tempfile.py"
        },
        {
          "type": "function",
          "name": "test_write_then_read_once",
          "code": "def test_write_then_read_once():\n    f = SecureTemporaryFile(\"/tmp\")\n    f.write(MESSAGE)\n    assert f.read().decode(\"utf-8\") == MESSAGE",
          "file": "test_secure_tempfile.py"
        },
        {
          "type": "function",
          "name": "test_write_twice_then_read_once",
          "code": "def test_write_twice_then_read_once():\n    f = SecureTemporaryFile(\"/tmp\")\n    f.write(MESSAGE)\n    f.write(MESSAGE)\n    assert f.read().decode(\"utf-8\") == MESSAGE * 2",
          "file": "test_secure_tempfile.py"
        },
        {
          "type": "function",
          "name": "test_write_then_read_twice",
          "code": "def test_write_then_read_twice():\n    f = SecureTemporaryFile(\"/tmp\")\n    f.write(MESSAGE)\n    assert f.read().decode(\"utf-8\") == MESSAGE\n    assert f.read() == b\"\"",
          "file": "test_secure_tempfile.py"
        },
        {
          "type": "function",
          "name": "test_write_then_read_then_write",
          "code": "def test_write_then_read_then_write():\n    f = SecureTemporaryFile(\"/tmp\")\n    f.write(MESSAGE)\n    f.read()\n\n    with pytest.raises(AssertionError) as err:\n        f.write(\"be gentle to each other so we can be dangerous together\")\n    assert \"You cannot write after reading!\" in str(err)",
          "file": "test_secure_tempfile.py"
        },
        {
          "type": "function",
          "name": "test_read_write_unicode",
          "code": "def test_read_write_unicode():\n    f = SecureTemporaryFile(\"/tmp\")\n    unicode_msg = \"鬼神 Kill Em All 1989\"\n    f.write(unicode_msg)\n    assert f.read().decode(\"utf-8\") == unicode_msg",
          "file": "test_secure_tempfile.py"
        },
        {
          "type": "function",
          "name": "test_file_seems_encrypted",
          "code": "def test_file_seems_encrypted():\n    f = SecureTemporaryFile(\"/tmp\")\n    f.write(MESSAGE)\n    with open(f.filepath, \"rb\") as fh:\n        contents = fh.read()\n\n    assert MESSAGE.encode(\"utf-8\") not in contents\n    assert MESSAGE not in contents.decode()",
          "file": "test_secure_tempfile.py"
        },
        {
          "type": "function",
          "name": "test_file_is_removed_from_disk",
          "code": "def test_file_is_removed_from_disk():\n    # once without reading the contents\n    f = SecureTemporaryFile(\"/tmp\")\n    f.write(MESSAGE)\n    assert os.path.exists(f.filepath)\n    f.close()\n    assert not os.path.exists(f.filepath)\n\n    # once with reading the contents\n    f = SecureTemporaryFile(\"/tmp\")\n    f.write(MESSAGE)\n    f.read()\n    assert os.path.exists(f.filepath)\n    f.close()\n    assert not os.path.exists(f.filepath)",
          "file": "test_secure_tempfile.py"
        },
        {
          "type": "function",
          "name": "test_buffered_read",
          "code": "def test_buffered_read():\n    f = SecureTemporaryFile(\"/tmp\")\n    msg = MESSAGE * 1000\n    f.write(msg)\n    out = b\"\"\n    while True:\n        chars = f.read(1024)\n        if chars:\n            out += chars\n        else:\n            break\n\n    assert out.decode(\"utf-8\") == msg",
          "file": "test_secure_tempfile.py"
        },
        {
          "type": "function",
          "name": "test_tmp_file_id_omits_invalid_chars",
          "code": "def test_tmp_file_id_omits_invalid_chars():\n    \"\"\"The `SecureTempFile.tmp_file_id` instance attribute is used as the filename\n    for the secure temporary file. This attribute should not contain\n    invalid characters such as '/' and '\\0' (null).\"\"\"\n    f = SecureTemporaryFile(\"/tmp\")\n    assert \"/\" not in f.tmp_file_id\n    assert \"\\0\" not in f.tmp_file_id",
          "file": "test_secure_tempfile.py"
        }
      ],
      "test_journalist.py": [
        {
          "type": "function",
          "name": "test_user_with_whitespace_in_username_can_login",
          "code": "def test_user_with_whitespace_in_username_can_login(journalist_app):\n    # Create a user with whitespace at the end of the username\n    with get_database_session(journalist_app.config[\"SQLALCHEMY_DATABASE_URI\"]) as db_session:\n        username_with_whitespace = \"journalist \"\n        password = PassphraseGenerator.get_default().generate_passphrase()\n        user = Journalist(\n            username=username_with_whitespace,\n            password=password,\n        )\n        db_session.add(user)\n        db_session.commit()\n        otp_secret = user.otp_secret\n\n    # Verify that user is able to login successfully\n    with journalist_app.test_client() as app:\n        login_journalist(app, username_with_whitespace, password, otp_secret)",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_reply_error_logging",
          "code": "def test_reply_error_logging(journalist_app, test_journo, test_source):\n    exception_class = StaleDataError\n    exception_msg = \"Potentially sensitive content!\"\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n        with patch.object(journalist_app.logger, \"error\") as mocked_error_logger:\n            with patch.object(db.session, \"commit\", side_effect=exception_class(exception_msg)):\n                resp = app.post(\n                    url_for(\"main.reply\"),\n                    data={\n                        \"filesystem_id\": test_source[\"filesystem_id\"],\n                        \"message\": \"_\",\n                    },\n                    follow_redirects=True,\n                )\n                assert resp.status_code == 200\n\n    # Notice the \"potentially sensitive\" exception_msg is not present in\n    # the log event.\n    mocked_error_logger.assert_called_once_with(\n        \"Reply from '{}' (ID {}) failed: {}!\".format(\n            test_journo[\"username\"], test_journo[\"id\"], exception_class\n        )\n    )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_reply_error_flashed_message",
          "code": "def test_reply_error_flashed_message(config, journalist_app, test_journo, test_source, locale):\n    exception_class = StaleDataError\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n\n        with InstrumentedApp(app) as ins:\n            with patch.object(db.session, \"commit\", side_effect=exception_class()):\n                resp = app.post(\n                    url_for(\"main.reply\", l=locale),\n                    data={\n                        \"filesystem_id\": test_source[\"filesystem_id\"],\n                        \"message\": \"_\",\n                    },\n                    follow_redirects=True,\n                )\n\n                assert page_language(resp.data) == language_tag(locale)\n                msgids = [\"An unexpected error occurred! Please inform your admin.\"]\n                with xfail_untranslated_messages(config, locale, msgids):\n                    ins.assert_message_flashed(gettext(msgids[0]), \"error\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_empty_replies_are_rejected",
          "code": "def test_empty_replies_are_rejected(config, journalist_app, test_journo, test_source, locale):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n        resp = app.post(\n            url_for(\"main.reply\", l=locale),\n            data={\"filesystem_id\": test_source[\"filesystem_id\"], \"message\": \"\"},\n            follow_redirects=True,\n        )\n\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"You cannot send an empty reply.\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert gettext(msgids[0]) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_nonempty_replies_are_accepted",
          "code": "def test_nonempty_replies_are_accepted(config, journalist_app, test_journo, test_source, locale):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n        resp = app.post(\n            url_for(\"main.reply\", l=locale),\n            data={\"filesystem_id\": test_source[\"filesystem_id\"], \"message\": \"_\"},\n            follow_redirects=True,\n        )\n\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"You cannot send an empty reply.\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert gettext(msgids[0]) not in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_successful_reply_marked_as_seen_by_sender",
          "code": "def test_successful_reply_marked_as_seen_by_sender(\n    config, journalist_app, test_journo, test_source, locale\n):\n    with journalist_app.test_client() as app:\n        journo = test_journo[\"journalist\"]\n        login_journalist(app, journo.username, test_journo[\"password\"], test_journo[\"otp_secret\"])\n\n        seen_reply = SeenReply.query.filter_by(journalist_id=journo.id).one_or_none()\n        assert not seen_reply\n\n        resp = app.post(\n            url_for(\"main.reply\", l=locale),\n            data={\"filesystem_id\": test_source[\"filesystem_id\"], \"message\": \"_\"},\n            follow_redirects=True,\n        )\n\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"You cannot send an empty reply.\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert gettext(msgids[0]) not in resp.data.decode(\"utf-8\")\n        seen_reply = SeenReply.query.filter_by(journalist_id=journo.id).one_or_none()\n        assert seen_reply",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_unauthorized_access_redirects_to_login",
          "code": "def test_unauthorized_access_redirects_to_login(journalist_app):\n    with journalist_app.test_client() as app:\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.get(url_for(\"main.index\"))\n            ins.assert_redirects(resp, url_for(\"main.login\"))",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_login_throttle",
          "code": "def test_login_throttle(config, journalist_app, test_journo, locale):\n    with journalist_app.test_client() as app:\n        with InstrumentedApp(app) as ins:\n            for _ in range(Journalist._MAX_LOGIN_ATTEMPTS_PER_PERIOD):\n                resp = app.post(\n                    url_for(\"main.login\"),\n                    data=dict(\n                        username=test_journo[\"username\"],\n                        password=\"invalid\",\n                        token=\"invalid\",\n                    ),\n                )\n                assert resp.status_code == 200\n                text = resp.data.decode(\"utf-8\")\n                assert \"Login failed\" in text\n\n            resp = app.post(\n                url_for(\"main.login\", l=locale),\n                data=dict(\n                    username=test_journo[\"username\"],\n                    password=\"invalid\",\n                    token=\"invalid\",\n                ),\n            )\n            assert page_language(resp.data) == language_tag(locale)\n            msgids = [\n                \"Login failed.\",\n                \"Please wait at least {num} second before logging in again.\",\n            ]\n            with xfail_untranslated_messages(config, locale, msgids):\n                ins.assert_message_flashed(\n                    \"{} {}\".format(\n                        gettext(msgids[0]),\n                        ngettext(\n                            msgids[1],\n                            \"Please wait at least {num} seconds before logging in again.\",\n                            Journalist._LOGIN_ATTEMPT_PERIOD,\n                        ).format(num=Journalist._LOGIN_ATTEMPT_PERIOD),\n                    ),\n                    \"error\",\n                )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_login_throttle_is_not_global",
          "code": "def test_login_throttle_is_not_global(config, journalist_app, test_journo, test_admin, locale):\n    \"\"\"The login throttling should be per-user, not global. Global login\n    throttling can prevent all users logging into the application.\"\"\"\n    with journalist_app.test_client() as app:\n        with InstrumentedApp(app) as ins:\n            for _ in range(Journalist._MAX_LOGIN_ATTEMPTS_PER_PERIOD):\n                resp = app.post(\n                    url_for(\"main.login\", l=locale),\n                    data=dict(\n                        username=test_journo[\"username\"],\n                        password=\"invalid\",\n                        token=\"invalid\",\n                    ),\n                )\n                assert page_language(resp.data) == language_tag(locale)\n                msgids = [\"Login failed.\"]\n                with xfail_untranslated_messages(config, locale, msgids):\n                    assert gettext(msgids[0]) in resp.data.decode(\"utf-8\")\n\n            resp = app.post(\n                url_for(\"main.login\", l=locale),\n                data=dict(\n                    username=test_journo[\"username\"],\n                    password=\"invalid\",\n                    token=\"invalid\",\n                ),\n            )\n            assert page_language(resp.data) == language_tag(locale)\n            msgids = [\n                \"Login failed.\",\n                \"Please wait at least {num} second before logging in again.\",\n            ]\n            with xfail_untranslated_messages(config, locale, msgids):\n                ins.assert_message_flashed(\n                    \"{} {}\".format(\n                        gettext(msgids[0]),\n                        ngettext(\n                            msgids[1],\n                            \"Please wait at least {num} seconds before logging in again.\",\n                            Journalist._LOGIN_ATTEMPT_PERIOD,\n                        ).format(num=Journalist._LOGIN_ATTEMPT_PERIOD),\n                    ),\n                    \"error\",\n                )\n\n        # A different user should be able to login\n        resp = app.post(\n            url_for(\"main.login\", l=locale),\n            data=dict(\n                username=test_admin[\"username\"],\n                password=test_admin[\"password\"],\n                token=TOTP(test_admin[\"otp_secret\"]).now(),\n            ),\n            follow_redirects=True,\n        )\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"All Sources\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert gettext(msgids[0]) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_login_invalid_credentials",
          "code": "def test_login_invalid_credentials(config, journalist_app, test_journo, locale):\n    with journalist_app.test_client() as app:\n        resp = app.post(\n            url_for(\"main.login\", l=locale),\n            data=dict(username=test_journo[\"username\"], password=\"invalid\", token=\"mocked\"),\n        )\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"Login failed.\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert gettext(msgids[0]) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_validate_redirect",
          "code": "def test_validate_redirect(config, journalist_app, locale):\n    with journalist_app.test_client() as app:\n        resp = app.post(url_for(\"main.index\", l=locale), follow_redirects=True)\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"Log in to access the journalist interface\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert gettext(msgids[0]) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_login_valid_credentials",
          "code": "def test_login_valid_credentials(config, journalist_app, test_journo, locale):\n    with journalist_app.test_client() as app:\n        resp = app.post(\n            url_for(\"main.login\", l=locale),\n            data=dict(\n                username=test_journo[\"username\"],\n                password=test_journo[\"password\"],\n                token=TOTP(test_journo[\"otp_secret\"]).now(),\n            ),\n            follow_redirects=True,\n        )\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"All Sources\", \"There are no submissions.\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            resp_text = resp.data.decode(\"utf-8\")\n            for msgid in msgids:\n                assert gettext(msgid) in resp_text",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_login_redirects_to_index",
          "code": "def test_admin_login_redirects_to_index(journalist_app, test_admin):\n    with journalist_app.test_client() as app:\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.post(\n                url_for(\"main.login\"),\n                data=dict(\n                    username=test_admin[\"username\"],\n                    password=test_admin[\"password\"],\n                    token=TOTP(test_admin[\"otp_secret\"]).now(),\n                ),\n                follow_redirects=False,\n            )\n            ins.assert_redirects(resp, url_for(\"main.index\"))",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_user_login_redirects_to_index",
          "code": "def test_user_login_redirects_to_index(journalist_app, test_journo):\n    with journalist_app.test_client() as app:\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.post(\n                url_for(\"main.login\"),\n                data=dict(\n                    username=test_journo[\"username\"],\n                    password=test_journo[\"password\"],\n                    token=TOTP(test_journo[\"otp_secret\"]).now(),\n                ),\n                follow_redirects=False,\n            )\n            ins.assert_redirects(resp, url_for(\"main.index\"))",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_has_link_to_edit_account_page_in_index_page",
          "code": "def test_admin_has_link_to_edit_account_page_in_index_page(journalist_app, test_admin):\n    with journalist_app.test_client() as app:\n        resp = app.post(\n            url_for(\"main.login\"),\n            data=dict(\n                username=test_admin[\"username\"],\n                password=test_admin[\"password\"],\n                token=TOTP(test_admin[\"otp_secret\"]).now(),\n            ),\n            follow_redirects=True,\n        )\n    edit_account_link = '<a href=\"/account/account\" ' 'id=\"link-edit-account\">'\n    text = resp.data.decode(\"utf-8\")\n    assert edit_account_link in text",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_user_has_link_to_edit_account_page_in_index_page",
          "code": "def test_user_has_link_to_edit_account_page_in_index_page(journalist_app, test_journo):\n    with journalist_app.test_client() as app:\n        resp = app.post(\n            url_for(\"main.login\"),\n            data=dict(\n                username=test_journo[\"username\"],\n                password=test_journo[\"password\"],\n                token=TOTP(test_journo[\"otp_secret\"]).now(),\n            ),\n            follow_redirects=True,\n        )\n    edit_account_link = '<a href=\"/account/account\" ' 'id=\"link-edit-account\">'\n    text = resp.data.decode(\"utf-8\")\n    assert edit_account_link in text",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_has_link_to_admin_index_page_in_index_page",
          "code": "def test_admin_has_link_to_admin_index_page_in_index_page(journalist_app, test_admin):\n    with journalist_app.test_client() as app:\n        resp = app.post(\n            url_for(\"main.login\"),\n            data=dict(\n                username=test_admin[\"username\"],\n                password=test_admin[\"password\"],\n                token=TOTP(test_admin[\"otp_secret\"]).now(),\n            ),\n            follow_redirects=True,\n        )\n    text = resp.data.decode(\"utf-8\")\n    assert '<a href=\"/admin/\" id=\"link-admin-index\">' in text",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_user_lacks_link_to_admin_index_page_in_index_page",
          "code": "def test_user_lacks_link_to_admin_index_page_in_index_page(journalist_app, test_journo):\n    with journalist_app.test_client() as app:\n        resp = app.post(\n            url_for(\"main.login\"),\n            data=dict(\n                username=test_journo[\"username\"],\n                password=test_journo[\"password\"],\n                token=TOTP(test_journo[\"otp_secret\"]).now(),\n            ),\n            follow_redirects=True,\n        )\n    text = resp.data.decode(\"utf-8\")\n    assert '<a href=\"/admin/\" id=\"link-admin-index\">' not in text",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_logout_redirects_to_index",
          "code": "def test_admin_logout_redirects_to_index(config, journalist_app, test_admin):\n    with journalist_app.test_client() as app:\n        with InstrumentedApp(journalist_app) as ins:\n            login_journalist(\n                app,\n                test_admin[\"username\"],\n                test_admin[\"password\"],\n                test_admin[\"otp_secret\"],\n            )\n            resp = app.post(url_for(\"main.logout\"))\n            ins.assert_redirects(resp, url_for(\"main.index\"))",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_user_logout_redirects_to_index",
          "code": "def test_user_logout_redirects_to_index(config, journalist_app, test_journo):\n    with journalist_app.test_client() as app:\n        with InstrumentedApp(journalist_app) as ins:\n            login_journalist(\n                app,\n                test_journo[\"username\"],\n                test_journo[\"password\"],\n                test_journo[\"otp_secret\"],\n            )\n            resp = app.post(url_for(\"main.logout\"))\n            ins.assert_redirects(resp, url_for(\"main.index\"))",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_index",
          "code": "def test_admin_index(config, journalist_app, test_admin, locale):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        resp = app.get(url_for(\"admin.index\", l=locale))\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"Admin Interface\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert gettext(msgids[0]) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_delete_user",
          "code": "def test_admin_delete_user(config, journalist_app, test_admin, test_journo, locale):\n    # Verify journalist is in the database\n    with journalist_app.app_context():\n        assert Journalist.query.get(test_journo[\"id\"]) is not None\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.post(\n                url_for(\"admin.delete_user\", user_id=test_journo[\"id\"], l=locale),\n                follow_redirects=True,\n            )\n\n            assert page_language(resp.data) == language_tag(locale)\n            msgids = [\"Deleted user '{user}'.\"]\n            with xfail_untranslated_messages(config, locale, msgids):\n                ins.assert_message_flashed(\n                    gettext(msgids[0]).format(user=test_journo[\"username\"]),\n                    \"notification\",\n                )\n\n    # Verify journalist is no longer in the database\n    with journalist_app.app_context():\n        assert Journalist.query.get(test_journo[\"id\"]) is None",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_cannot_delete_self",
          "code": "def test_admin_cannot_delete_self(config, journalist_app, test_admin, test_journo, locale):\n    # Verify journalist is in the database\n    with journalist_app.app_context():\n        assert Journalist.query.get(test_journo[\"id\"]) is not None\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        resp = app.post(\n            url_for(\"admin.delete_user\", user_id=test_admin[\"id\"], l=locale),\n            follow_redirects=True,\n        )\n\n        # Assert correct interface behavior\n        assert resp.status_code == 403\n\n        resp = app.get(url_for(\"admin.index\", l=locale), follow_redirects=True)\n        assert resp.status_code == 200\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"Admin Interface\", \"Edit user {username}\", \"Delete user {username}\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            resp_text = resp.data.decode(\"utf-8\")\n            assert gettext(msgids[0]) in resp_text\n\n            # The user can be edited and deleted\n            assert (\n                escape(gettext(\"Edit user {username}\").format(username=test_journo[\"username\"]))\n                in resp_text\n            )\n            assert (\n                escape(gettext(\"Delete user {username}\").format(username=test_journo[\"username\"]))\n                in resp_text\n            )\n            # The admin can be edited but cannot deleted\n            assert (\n                escape(gettext(\"Edit user {username}\").format(username=test_admin[\"username\"]))\n                in resp_text\n            )\n            assert (\n                escape(gettext(\"Delete user {username}\").format(username=test_admin[\"username\"]))\n                not in resp_text\n            )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_cannot_edit_own_password_without_validation",
          "code": "def test_admin_cannot_edit_own_password_without_validation(\n    config, journalist_app, test_admin, locale, mocker\n):\n    mocked_error_logger = mocker.patch(\"journalist.app.logger.error\")\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app, test_admin[\"username\"], test_admin[\"password\"], test_admin[\"otp_secret\"]\n        )\n        utils.prepare_password_change(app, test_admin[\"id\"], VALID_PASSWORD)\n\n        resp = app.post(\n            url_for(\"admin.new_password\", user_id=test_admin[\"id\"], l=locale),\n            data=dict(password=VALID_PASSWORD),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 403\n\n    mocked_error_logger.assert_called_once_with(\n        \"Admin {} tried to change their password without validation.\".format(test_admin[\"username\"])\n    )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_edits_user_password_success_response",
          "code": "def test_admin_edits_user_password_success_response(\n    config, journalist_app, test_admin, test_journo, locale\n):\n    \"\"\"Verify the flow of an admin resetting another journalist's password\"\"\"\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        # First obtain a randomly generated password from the server\n        first = app.get(url_for(\"admin.edit_user\", user_id=test_journo[\"id\"], l=locale))\n        password = utils.extract_password(first.data)\n\n        # Then send it back\n        resp = app.post(\n            url_for(\"admin.new_password\", user_id=test_journo[\"id\"], l=locale),\n            data=dict(password=password),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\n            \"Password updated. Don't forget to save it in your KeePassX database. New password:\"\n        ]\n        with xfail_untranslated_messages(config, locale, msgids):\n            resp_text = resp.data.decode(\"utf-8\")\n            assert escape(gettext(msgids[0])) in resp_text\n            assert password in resp_text",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_edits_user_password_session_invalidate",
          "code": "def test_admin_edits_user_password_session_invalidate(\n    config, journalist_app, test_admin, test_journo, locale\n):\n    # Start the journalist session.\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n\n        # Change the journalist password via an admin session.\n        with journalist_app.test_client() as admin_app:\n            login_journalist(\n                admin_app,\n                test_admin[\"username\"],\n                test_admin[\"password\"],\n                test_admin[\"otp_secret\"],\n            )\n            utils.prepare_password_change(admin_app, test_journo[\"id\"], VALID_PASSWORD_2)\n\n            resp = admin_app.post(\n                url_for(\"admin.new_password\", user_id=test_journo[\"id\"], l=locale),\n                data=dict(password=VALID_PASSWORD_2),\n                follow_redirects=True,\n            )\n            assert resp.status_code == 200\n            assert page_language(resp.data) == language_tag(locale)\n            msgids = [\n                \"Password updated. Don't forget to save it in your KeePassX database. New password:\"\n            ]\n            with xfail_untranslated_messages(config, locale, msgids):\n                resp_text = resp.data.decode(\"utf-8\")\n                assert escape(gettext(msgids[0])) in resp_text\n                assert VALID_PASSWORD_2 in resp_text",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_edits_user_password_invalid_different",
          "code": "def test_admin_edits_user_password_invalid_different(\n    config, journalist_app, test_admin, test_journo, locale\n):\n    \"\"\"Test if the admin submits a different password than the one they received\"\"\"\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        # First obtain a randomly generated password from the server\n        first = app.get(url_for(\"admin.edit_user\", user_id=test_journo[\"id\"], l=locale))\n        password = utils.extract_password(first.data)\n\n        # No freak random collisions\n        assert password != VALID_PASSWORD_2\n\n        # Then send back a different password\n        resp = app.post(\n            url_for(\"admin.new_password\", user_id=test_journo[\"id\"], l=locale),\n            data=dict(password=VALID_PASSWORD_2),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"The password you submitted is invalid. Password not changed.\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            resp_text = resp.data.decode(\"utf-8\")\n            assert escape(gettext(msgids[0])) in resp_text",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_edits_user_password_invalid_nopending",
          "code": "def test_admin_edits_user_password_invalid_nopending(\n    config, journalist_app, test_admin, test_journo, locale\n):\n    \"\"\"Test if the user submits a password without receiving one from the server first\"\"\"\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n\n        # We explicitly do not fetch a password from the server nor set a pending one\n        resp = app.post(\n            url_for(\"admin.new_password\", user_id=test_journo[\"id\"], l=locale),\n            data=dict(password=VALID_PASSWORD_2),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"The password you submitted is invalid. Password not changed.\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            resp_text = resp.data.decode(\"utf-8\")\n            assert escape(gettext(msgids[0])) in resp_text",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_deletes_invalid_user_404",
          "code": "def test_admin_deletes_invalid_user_404(journalist_app, test_admin):\n    with journalist_app.app_context():\n        invalid_id = db.session.query(func.max(Journalist.id)).scalar() + 1\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        resp = app.post(url_for(\"admin.delete_user\", user_id=invalid_id))\n        assert resp.status_code == 404",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_deletes_deleted_user_403",
          "code": "def test_admin_deletes_deleted_user_403(journalist_app, test_admin):\n    with journalist_app.app_context():\n        deleted = Journalist.get_deleted()\n        db.session.commit()\n        deleted_id = deleted.id\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        resp = app.post(url_for(\"admin.delete_user\", user_id=deleted_id))\n        assert resp.status_code == 403",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_edits_user_password_error_response",
          "code": "def test_admin_edits_user_password_error_response(\n    config, journalist_app, test_admin, test_journo, locale\n):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        utils.prepare_password_change(app, test_journo[\"id\"], VALID_PASSWORD_2)\n\n        with patch(\"sqlalchemy.orm.scoping.scoped_session.commit\", side_effect=Exception()):\n            with InstrumentedApp(journalist_app) as ins:\n                resp = app.post(\n                    url_for(\"admin.new_password\", user_id=test_journo[\"id\"], l=locale),\n                    data=dict(password=VALID_PASSWORD_2),\n                    follow_redirects=True,\n                )\n                assert page_language(resp.data) == language_tag(locale)\n                msgids = [\n                    \"There was an error, and the new password might not have been \"\n                    \"saved correctly. To prevent you from getting locked \"\n                    \"out of your account, you should reset your password again.\"\n                ]\n                with xfail_untranslated_messages(config, locale, msgids):\n                    ins.assert_message_flashed(gettext(msgids[0]), \"error\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_user_edits_password_success_response",
          "code": "def test_user_edits_password_success_response(config, journalist_app, test_journo, locale):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n        utils.prepare_password_change(app, test_journo[\"id\"], VALID_PASSWORD_2)\n        token = TOTP(test_journo[\"otp_secret\"]).now()\n        resp = app.post(\n            url_for(\"account.new_password\", l=locale),\n            data=dict(\n                current_password=test_journo[\"password\"],\n                token=token,\n                password=VALID_PASSWORD_2,\n            ),\n            follow_redirects=True,\n        )\n\n        msgids = [\n            \"Password updated. Don't forget to save it in your KeePassX database. New password:\"\n        ]\n        with xfail_untranslated_messages(config, locale, msgids):\n            resp_text = resp.data.decode(\"utf-8\")\n            assert escape(gettext(msgids[0])) in resp_text\n            assert VALID_PASSWORD_2 in resp_text",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_user_edits_password_expires_session",
          "code": "def test_user_edits_password_expires_session(journalist_app, test_journo):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n        assert \"uid\" in session\n        utils.prepare_password_change(app, test_journo[\"id\"], VALID_PASSWORD_2)\n\n        with InstrumentedApp(journalist_app) as ins:\n            token = TOTP(test_journo[\"otp_secret\"]).now()\n            resp = app.post(\n                url_for(\"account.new_password\"),\n                data=dict(\n                    current_password=test_journo[\"password\"],\n                    token=token,\n                    password=VALID_PASSWORD_2,\n                ),\n            )\n\n            ins.assert_redirects(resp, url_for(\"main.login\"))\n\n        # verify the session was expired after the password was changed\n        assert session.uid is None\n        assert session.user is None",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_user_edits_password_error_response",
          "code": "def test_user_edits_password_error_response(config, journalist_app, test_journo, locale):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n        utils.prepare_password_change(app, test_journo[\"id\"], VALID_PASSWORD_2)\n\n        # patch token verification because there are multiple commits\n        # to the database and this isolates the one we want to fail\n        with patch.object(Journalist, \"verify_2fa_token\", return_value=\"token\"):\n            with patch.object(db.session, \"commit\", side_effect=[None, None, Exception()]):\n                with InstrumentedApp(journalist_app) as ins:\n                    resp = app.post(\n                        url_for(\"account.new_password\", l=locale),\n                        data=dict(\n                            current_password=test_journo[\"password\"],\n                            token=\"mocked\",\n                            password=VALID_PASSWORD_2,\n                        ),\n                        follow_redirects=True,\n                    )\n\n                    assert page_language(resp.data) == language_tag(locale)\n                    msgids = [\n                        (\n                            \"There was an error, and the new password might not have been \"\n                            \"saved correctly. To prevent you from getting locked \"\n                            \"out of your account, you should reset your password again.\"\n                        )\n                    ]\n                    with xfail_untranslated_messages(config, locale, msgids):\n                        ins.assert_message_flashed(gettext(msgids[0]), \"error\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_add_user_when_username_already_taken",
          "code": "def test_admin_add_user_when_username_already_taken(config, journalist_app, test_admin, locale):\n    with journalist_app.test_client() as client:\n        login_journalist(\n            client,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        utils.prepare_password_change(client, \"new\", VALID_PASSWORD)\n        with InstrumentedApp(journalist_app) as ins:\n            resp = client.post(\n                url_for(\"admin.add_user\", l=locale),\n                data=dict(\n                    username=test_admin[\"username\"],\n                    first_name=\"\",\n                    last_name=\"\",\n                    password=VALID_PASSWORD,\n                    otp_secret=\"\",\n                    is_admin=None,\n                ),\n            )\n            assert page_language(resp.data) == language_tag(locale)\n\n            msgids = ['Username \"{username}\" already taken.']\n            with xfail_untranslated_messages(config, locale, msgids):\n                ins.assert_message_flashed(\n                    gettext(msgids[0]).format(username=test_admin[\"username\"]), \"error\"\n                )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_max_password_length",
          "code": "def test_max_password_length():\n    \"\"\"Creating a Journalist with a password that is greater than the\n    maximum password length should raise an exception\"\"\"\n    overly_long_password = VALID_PASSWORD + \"a\" * (\n        Journalist.MAX_PASSWORD_LEN - len(VALID_PASSWORD) + 1\n    )\n    with pytest.raises(InvalidPasswordLength):\n        Journalist(username=\"My Password is Too Big!\", password=overly_long_password)",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_login_password_too_long",
          "code": "def test_login_password_too_long(journalist_app, test_journo, mocker):\n    mocked_error_logger = mocker.patch(\"journalist.app.logger.error\")\n    with journalist_app.test_client() as app:\n        resp = app.post(\n            url_for(\"main.login\"),\n            data=dict(\n                username=test_journo[\"username\"],\n                password=\"a\" * (Journalist.MAX_PASSWORD_LEN + 1),\n                token=TOTP(test_journo[\"otp_secret\"]).now(),\n            ),\n        )\n    assert resp.status_code == 200\n    text = resp.data.decode(\"utf-8\")\n    assert \"Login failed\" in text\n    mocked_error_logger.assert_called_once_with(\n        \"Login for '{}' failed: Password is too long.\".format(test_journo[\"username\"])\n    )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_min_password_length",
          "code": "def test_min_password_length():\n    \"\"\"Creating a Journalist with a password that is smaller than the\n    minimum password length should raise an exception. This uses the\n    magic number 7 below to get around the \"diceware-like\" requirement\n    that may cause a failure before the length check.\n    \"\"\"\n    password = (\"a \" * 7)[0 : (Journalist.MIN_PASSWORD_LEN - 1)]\n    with pytest.raises(InvalidPasswordLength):\n        Journalist(username=\"My Password is Too Small!\", password=password)",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_edits_user_password_too_long_warning",
          "code": "def test_admin_edits_user_password_too_long_warning(journalist_app, test_admin, test_journo):\n    # append a bunch of a's to a diceware password to keep it \"diceware-like\"\n    overly_long_password = VALID_PASSWORD + \"a\" * (\n        Journalist.MAX_PASSWORD_LEN - len(VALID_PASSWORD) + 1\n    )\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        utils.prepare_password_change(app, test_journo[\"id\"], overly_long_password)\n\n        with InstrumentedApp(journalist_app) as ins:\n            app.post(\n                url_for(\"admin.new_password\", user_id=test_journo[\"id\"]),\n                data=dict(\n                    username=test_journo[\"username\"],\n                    first_name=\"\",\n                    last_name=\"\",\n                    is_admin=None,\n                    password=overly_long_password,\n                ),\n                follow_redirects=True,\n            )\n\n            ins.assert_message_flashed(\n                \"The password you submitted is invalid. \" \"Password not changed.\",\n                \"error\",\n            )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_user_edits_password_too_long_warning",
          "code": "def test_user_edits_password_too_long_warning(config, journalist_app, test_journo):\n    overly_long_password = VALID_PASSWORD + \"a\" * (\n        Journalist.MAX_PASSWORD_LEN - len(VALID_PASSWORD) + 1\n    )\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n        utils.prepare_password_change(app, test_journo[\"id\"], overly_long_password)\n\n        with InstrumentedApp(journalist_app) as ins:\n            app.post(\n                url_for(\"account.new_password\"),\n                data=dict(\n                    username=test_journo[\"username\"],\n                    first_name=\"\",\n                    last_name=\"\",\n                    is_admin=None,\n                    token=TOTP(test_journo[\"otp_secret\"]).now(),\n                    current_password=test_journo[\"password\"],\n                    password=overly_long_password,\n                ),\n                follow_redirects=True,\n            )\n\n            ins.assert_message_flashed(\n                \"The password you submitted is invalid. \" \"Password not changed.\",\n                \"error\",\n            )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_add_user_password_too_long_warning",
          "code": "def test_admin_add_user_password_too_long_warning(config, journalist_app, test_admin, locale):\n    overly_long_password = VALID_PASSWORD + \"a\" * (\n        Journalist.MAX_PASSWORD_LEN - len(VALID_PASSWORD) + 1\n    )\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        utils.prepare_password_change(app, \"new\", overly_long_password)\n\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.post(\n                url_for(\"admin.add_user\", l=locale),\n                data=dict(\n                    username=\"dellsberg\",\n                    first_name=\"\",\n                    last_name=\"\",\n                    password=overly_long_password,\n                    otp_secret=\"\",\n                    is_admin=None,\n                ),\n            )\n\n            assert page_language(resp.data) == language_tag(locale)\n            msgids = [\n                \"There was an error with the autogenerated password. User not \"\n                \"created. Please try again.\"\n            ]\n            with xfail_untranslated_messages(config, locale, msgids):\n                ins.assert_message_flashed(gettext(msgids[0]), \"error\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_add_user_first_name_too_long_warning",
          "code": "def test_admin_add_user_first_name_too_long_warning(config, journalist_app, test_admin, locale):\n    with journalist_app.test_client() as app:\n        overly_long_name = \"a\" * (Journalist.MAX_NAME_LEN + 1)\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        utils.prepare_password_change(app, \"new\", VALID_PASSWORD)\n\n        resp = app.post(\n            url_for(\"admin.add_user\", l=locale),\n            data=dict(\n                username=test_admin[\"username\"],\n                first_name=overly_long_name,\n                last_name=\"\",\n                password=VALID_PASSWORD,\n                otp_secret=\"\",\n                is_admin=None,\n            ),\n        )\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"Cannot be longer than {num} character.\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert ngettext(\n                \"Cannot be longer than {num} character.\",\n                \"Cannot be longer than {num} characters.\",\n                Journalist.MAX_NAME_LEN,\n            ).format(num=Journalist.MAX_NAME_LEN) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_add_user_last_name_too_long_warning",
          "code": "def test_admin_add_user_last_name_too_long_warning(config, journalist_app, test_admin, locale):\n    with journalist_app.test_client() as app:\n        overly_long_name = \"a\" * (Journalist.MAX_NAME_LEN + 1)\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        utils.prepare_password_change(app, \"new\", VALID_PASSWORD)\n\n        resp = app.post(\n            url_for(\"admin.add_user\", l=locale),\n            data=dict(\n                username=test_admin[\"username\"],\n                first_name=\"\",\n                last_name=overly_long_name,\n                password=VALID_PASSWORD,\n                otp_secret=\"\",\n                is_admin=None,\n            ),\n        )\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"Cannot be longer than {num} character.\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert ngettext(\n                \"Cannot be longer than {num} character.\",\n                \"Cannot be longer than {num} characters.\",\n                Journalist.MAX_NAME_LEN,\n            ).format(num=Journalist.MAX_NAME_LEN) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_edits_user_invalid_username_deleted",
          "code": "def test_admin_edits_user_invalid_username_deleted(\n    config, journalist_app, test_admin, test_journo, locale\n):\n    \"\"\"Test expected error message when admin attempts to change a user's\n    username to deleted\"\"\"\n    new_username = \"deleted\"\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.post(\n                url_for(\"admin.edit_user\", user_id=test_admin[\"id\"], l=locale),\n                data=dict(username=new_username, first_name=\"\", last_name=\"\", is_admin=None),\n                follow_redirects=True,\n            )\n\n            assert page_language(resp.data) == language_tag(locale)\n            msgids = [\n                \"Invalid username: {message}\",\n                \"This username is invalid because it is reserved for internal use by the software.\",\n            ]\n            with xfail_untranslated_messages(config, locale, msgids):\n                ins.assert_message_flashed(\n                    gettext(msgids[0]).format(message=gettext(msgids[1])), \"error\"\n                )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_resets_user_hotp_format_non_hexa",
          "code": "def test_admin_resets_user_hotp_format_non_hexa(journalist_app, test_admin, test_journo):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n\n        journo = test_journo[\"journalist\"]\n        # guard to ensure check below tests the correct condition\n        assert journo.is_totp\n\n        old_secret = journo.otp_secret\n\n        non_hexa_secret = \"0123456789ABCDZZ0123456789ABCDEF01234567\"\n        with InstrumentedApp(journalist_app) as ins:\n            app.post(\n                url_for(\"admin.reset_two_factor_hotp\"),\n                data=dict(uid=test_journo[\"id\"], otp_secret=non_hexa_secret),\n            )\n\n            # fetch altered DB object\n            journo = Journalist.query.get(journo.id)\n\n            new_secret = journo.otp_secret\n            assert old_secret == new_secret\n\n            # ensure we didn't accidentally enable hotp\n            assert journo.is_totp\n\n            ins.assert_message_flashed(\n                \"Invalid HOTP secret format: please only submit letters A-F and \" \"numbers 0-9.\",\n                \"error\",\n            )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_resets_user_hotp_format_too_short",
          "code": "def test_admin_resets_user_hotp_format_too_short(\n    journalist_app, test_admin, test_journo, the_secret\n):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n\n        journo = test_journo[\"journalist\"]\n        # guard to ensure check below tests the correct condition\n        assert journo.is_totp\n\n        old_secret = journo.otp_secret\n\n        with InstrumentedApp(journalist_app) as ins:\n            app.post(\n                url_for(\"admin.reset_two_factor_hotp\"),\n                data=dict(uid=test_journo[\"id\"], otp_secret=the_secret),\n            )\n\n            # fetch altered DB object\n            journo = Journalist.query.get(journo.id)\n\n            new_secret = journo.otp_secret\n            assert old_secret == new_secret\n\n            # ensure we didn't accidentally enable hotp\n            assert journo.is_totp\n\n            ins.assert_message_flashed(\n                \"HOTP secrets are 40 characters long\" \" - you have entered {num}.\".format(\n                    num=len(the_secret.replace(\" \", \"\"))\n                ),\n                \"error\",\n            )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_resets_user_hotp",
          "code": "def test_admin_resets_user_hotp(journalist_app, test_admin, test_journo):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n\n        journo = test_journo[\"journalist\"]\n        old_secret = journo.otp_secret\n\n        valid_secret = \"DEADBEEF01234567DEADBEEF01234567DEADBEEF\"\n        resp = app.post(\n            url_for(\"admin.reset_two_factor_hotp\"),\n            data=dict(uid=test_journo[\"id\"], otp_secret=valid_secret),\n        )\n        assert resp.status_code == 200\n        assert (\n            '<form id=\"check-token\" method=\"post\" action=\"/admin/verify-2fa-hotp\">'\n            in resp.data.decode()\n        )\n\n        # fetch altered DB object\n        journo = Journalist.query.get(journo.id)\n\n        new_secret = journo.otp_secret\n        assert old_secret != new_secret\n        assert not journo.is_totp",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_resets_user_hotp_error",
          "code": "def test_admin_resets_user_hotp_error(mocker, journalist_app, test_admin, test_journo):\n    bad_secret = \"0123456789ABCDZZ0123456789ABCDZZ01234567\"\n    error_message = \"SOMETHING WRONG!\"\n    mocked_error_logger = mocker.patch(\"journalist.app.logger.error\")\n    old_secret = test_journo[\"otp_secret\"]\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n\n        mocker.patch(\n            \"models.Journalist.set_hotp_secret\",\n            side_effect=binascii.Error(error_message),\n        )\n\n        with InstrumentedApp(journalist_app) as ins:\n            app.post(\n                url_for(\"admin.reset_two_factor_hotp\"),\n                data=dict(uid=test_journo[\"id\"], otp_secret=bad_secret),\n            )\n            ins.assert_message_flashed(\n                \"An unexpected error occurred! \" \"Please inform your admin.\", \"error\"\n            )\n\n    # Re-fetch journalist to get fresh DB instance\n    user = Journalist.query.get(test_journo[\"id\"])\n    new_secret = user.otp_secret\n\n    assert new_secret == old_secret\n\n    mocked_error_logger.assert_called_once_with(\n        \"set_hotp_secret '{}' (id {}) failed: {}\".format(\n            bad_secret, test_journo[\"id\"], error_message\n        )\n    )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_user_resets_hotp",
          "code": "def test_user_resets_hotp(journalist_app, test_journo):\n    old_secret = test_journo[\"otp_secret\"]\n    new_secret = \"0123456789ABCDEF0123456789ABCDEF01234567\"\n\n    # Precondition\n    assert new_secret != old_secret\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n\n        resp = app.post(\n            url_for(\"account.reset_two_factor_hotp\"),\n            data=dict(otp_secret=new_secret),\n        )\n        assert resp.status_code == 200\n        assert (\n            '<form id=\"check-token\" method=\"post\" action=\"/account/verify-2fa-hotp\">'\n            in resp.data.decode()\n        )\n\n    # Re-fetch journalist to get fresh DB instance\n    user = Journalist.query.get(test_journo[\"id\"])\n    new_secret = user.otp_secret\n\n    assert old_secret != new_secret",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_user_resets_user_hotp_format_non_hexa",
          "code": "def test_user_resets_user_hotp_format_non_hexa(journalist_app, test_journo):\n    old_secret = test_journo[\"otp_secret\"]\n\n    non_hexa_secret = \"0123456789ABCDZZ0123456789ABCDEF01234567\"\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n\n        with InstrumentedApp(journalist_app) as ins:\n            app.post(\n                url_for(\"account.reset_two_factor_hotp\"),\n                data=dict(otp_secret=non_hexa_secret),\n            )\n            ins.assert_message_flashed(\n                \"Invalid HOTP secret format: \" \"please only submit letters A-F and numbers 0-9.\",\n                \"error\",\n            )\n\n    # Re-fetch journalist to get fresh DB instance\n    user = Journalist.query.get(test_journo[\"id\"])\n    new_secret = user.otp_secret\n\n    assert old_secret == new_secret",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_user_resets_user_hotp_error",
          "code": "def test_user_resets_user_hotp_error(mocker, journalist_app, test_journo):\n    bad_secret = \"0123456789ABCDZZ0123456789ABCDZZ01234567\"\n    old_secret = test_journo[\"otp_secret\"]\n    error_message = \"SOMETHING WRONG!\"\n    mocked_error_logger = mocker.patch(\"journalist.app.logger.error\")\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n\n        mocker.patch(\n            \"models.Journalist.set_hotp_secret\",\n            side_effect=binascii.Error(error_message),\n        )\n\n        with InstrumentedApp(journalist_app) as ins:\n            app.post(\n                url_for(\"account.reset_two_factor_hotp\"),\n                data=dict(otp_secret=bad_secret),\n            )\n            ins.assert_message_flashed(\n                \"An unexpected error occurred! Please inform your \" \"admin.\", \"error\"\n            )\n\n    # Re-fetch journalist to get fresh DB instance\n    user = Journalist.query.get(test_journo[\"id\"])\n    new_secret = user.otp_secret\n\n    assert old_secret == new_secret\n    mocked_error_logger.assert_called_once_with(\n        \"set_hotp_secret '{}' (id {}) failed: {}\".format(\n            bad_secret, test_journo[\"id\"], error_message\n        )\n    )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_resets_user_totp",
          "code": "def test_admin_resets_user_totp(journalist_app, test_admin, test_journo):\n    old_secret = test_journo[\"otp_secret\"]\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n\n        resp = app.post(url_for(\"admin.reset_two_factor_totp\"), data=dict(uid=test_journo[\"id\"]))\n        assert resp.status_code == 200\n        assert (\n            '<form id=\"check-token\" method=\"post\" action=\"/admin/verify-2fa-totp\">'\n            in resp.data.decode()\n        )\n\n    # Re-fetch journalist to get fresh DB instance\n    user = Journalist.query.get(test_journo[\"id\"])\n    new_secret = user.otp_secret\n\n    assert new_secret != old_secret",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_user_resets_totp",
          "code": "def test_user_resets_totp(journalist_app, test_journo):\n    old_secret = test_journo[\"otp_secret\"]\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n\n        resp = app.post(url_for(\"account.reset_two_factor_totp\"))\n        assert resp.status_code == 200\n        assert (\n            '<form id=\"check-token\" method=\"post\" action=\"/account/verify-2fa-totp\">'\n            in resp.data.decode()\n        )\n\n    # Re-fetch journalist to get fresh DB instance\n    user = Journalist.query.get(test_journo[\"id\"])\n    new_secret = user.otp_secret\n\n    assert new_secret != old_secret",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_resets_hotp_with_missing_otp_secret_key",
          "code": "def test_admin_resets_hotp_with_missing_otp_secret_key(config, journalist_app, test_admin, locale):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        resp = app.post(\n            url_for(\"admin.reset_two_factor_hotp\", l=locale),\n            data=dict(uid=test_admin[\"id\"]),\n        )\n\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"Change HOTP Secret\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert gettext(msgids[0]) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_new_user_2fa_redirect",
          "code": "def test_admin_new_user_2fa_redirect(journalist_app, test_admin, test_journo):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.post(\n                url_for(\"admin.new_user_two_factor_totp\"),\n                data=dict(\n                    token=TOTP(test_journo[\"otp_secret\"]).now(),\n                    otp_secret=test_journo[\"otp_secret\"],\n                    userid=test_journo[\"id\"],\n                ),\n            )\n            ins.assert_redirects(resp, url_for(\"admin.index\"))",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_http_get_on_admin_add_user_page",
          "code": "def test_http_get_on_admin_add_user_page(config, journalist_app, test_admin, locale):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        resp = app.get(url_for(\"admin.add_user\", l=locale))\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"ADD USER\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert gettext(msgids[0]) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_add_user",
          "code": "def test_admin_add_user(journalist_app, test_admin):\n    \"\"\"Test the workflow of adding a new journalist\"\"\"\n    username = \"dellsberg\"\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n\n        # Get the autogenerated password\n        first = app.get(url_for(\"admin.add_user\"))\n        assert first.status_code == 200\n        password = utils.extract_password(first.data)\n\n        resp = app.post(\n            url_for(\"admin.add_user\"),\n            data=dict(\n                username=username,\n                first_name=\"\",\n                last_name=\"\",\n                password=password,\n                otp_secret=\"\",\n                is_admin=None,\n            ),\n        )\n        assert resp.status_code == 200\n        assert (\n            '<form id=\"check-token\" method=\"post\" action=\"/admin/verify-2fa-totp\">'\n            in resp.data.decode()\n        )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_add_user_invalid_different_password",
          "code": "def test_admin_add_user_invalid_different_password(config, journalist_app, test_admin, locale):\n    \"\"\"The admin submits a different password than the one given\"\"\"\n    username = \"dellsberg\"\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n\n        # Get the autogenerated password\n        first = app.get(url_for(\"admin.add_user\", l=locale))\n        assert first.status_code == 200\n        password = utils.extract_password(first.data)\n\n        # No freak random collisions\n        assert password != VALID_PASSWORD\n\n        resp = app.post(\n            url_for(\"admin.add_user\", l=locale),\n            data=dict(\n                username=username,\n                first_name=\"\",\n                last_name=\"\",\n                password=VALID_PASSWORD,\n                otp_secret=\"\",\n                is_admin=None,\n            ),\n        )\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\n            \"There was an error with the autogenerated password. \"\n            \"User not created. Please try again.\"\n        ]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert escape(gettext(msgids[0])) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_add_user_invalid_nopending",
          "code": "def test_admin_add_user_invalid_nopending(config, journalist_app, test_admin, locale):\n    \"\"\"The admin does not get a random password from the server\"\"\"\n    username = \"dellsberg\"\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n\n        # We explicitly do not fetch a password from the server nor set a pending one\n        resp = app.post(\n            url_for(\"admin.add_user\", l=locale),\n            data=dict(\n                username=username,\n                first_name=\"\",\n                last_name=\"\",\n                password=VALID_PASSWORD,\n                otp_secret=\"\",\n                is_admin=None,\n            ),\n        )\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\n            \"There was an error with the autogenerated password. \"\n            \"User not created. Please try again.\"\n        ]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert escape(gettext(msgids[0])) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_add_user_with_invalid_username",
          "code": "def test_admin_add_user_with_invalid_username(config, journalist_app, test_admin, locale):\n    username = \"deleted\"\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        utils.prepare_password_change(app, \"new\", VALID_PASSWORD)\n\n        resp = app.post(\n            url_for(\"admin.add_user\", l=locale),\n            data=dict(\n                username=username,\n                first_name=\"\",\n                last_name=\"\",\n                password=VALID_PASSWORD,\n                otp_secret=\"\",\n                is_admin=None,\n            ),\n        )\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\n            \"This username is invalid because it is reserved for internal use by the software.\"\n        ]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert gettext(msgids[0]) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_deleted_user_cannot_login",
          "code": "def test_deleted_user_cannot_login(config, journalist_app, locale):\n    with journalist_app.app_context():\n        user = Journalist.get_deleted()\n        password = PassphraseGenerator.get_default().generate_passphrase()\n        user.set_password(password)\n        db.session.commit()\n        otp_secret = user.otp_secret\n\n    with InstrumentedApp(journalist_app) as ins:\n        # Verify that deleted user is not able to login\n        with journalist_app.test_client() as app:\n            resp = app.post(\n                url_for(\"main.login\", l=locale),\n                data=dict(username=\"deleted\", password=password, token=TOTP(otp_secret).now()),\n            )\n            assert page_language(resp.data) == language_tag(locale)\n            msgids = [\n                \"Login failed.\",\n                (\n                    \"Please wait for a new code from your two-factor mobile\"\n                    \" app or security key before trying again.\"\n                ),\n            ]\n            with xfail_untranslated_messages(config, locale, msgids):\n                ins.assert_message_flashed(\n                    f\"{gettext(msgids[0])} {gettext(msgids[1])}\",\n                    \"error\",\n                )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_deleted_user_cannot_login_exception",
          "code": "def test_deleted_user_cannot_login_exception(journalist_app, locale):\n    with journalist_app.app_context():\n        user = Journalist.get_deleted()\n        password = PassphraseGenerator.get_default().generate_passphrase()\n        user.set_password(password)\n        db.session.commit()\n        otp_secret = user.otp_secret\n\n    with journalist_app.test_request_context(\"/\"):\n        with pytest.raises(InvalidUsernameException):\n            Journalist.login(\"deleted\", password, TOTP(otp_secret).now())",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_add_user_without_username",
          "code": "def test_admin_add_user_without_username(config, journalist_app, test_admin, locale):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        utils.prepare_password_change(app, \"new\", VALID_PASSWORD)\n\n        resp = app.post(\n            url_for(\"admin.add_user\", l=locale),\n            data=dict(\n                username=\"\",\n                first_name=\"\",\n                last_name=\"\",\n                password=VALID_PASSWORD,\n                otp_secret=\"\",\n                is_admin=None,\n            ),\n        )\n\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"This field is required.\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert gettext(msgids[0]) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_add_user_too_short_username",
          "code": "def test_admin_add_user_too_short_username(config, journalist_app, test_admin, locale):\n    username = \"a\" * (Journalist.MIN_USERNAME_LEN - 1)\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        # this will never be possible in real circumstances, but verify that later\n        # checks would reject such a password\n        utils.prepare_password_change(app, \"new\", \"pentagonpapers\")\n\n        resp = app.post(\n            url_for(\"admin.add_user\", l=locale),\n            data=dict(\n                username=username,\n                first_name=\"\",\n                last_name=\"\",\n                password=\"pentagonpapers\",\n                password_again=\"pentagonpapers\",\n                otp_secret=\"\",\n                is_admin=None,\n            ),\n        )\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"Must be at least {num} character long.\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert ngettext(\n                \"Must be at least {num} character long.\",\n                \"Must be at least {num} characters long.\",\n                Journalist.MIN_USERNAME_LEN,\n            ).format(num=Journalist.MIN_USERNAME_LEN) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_add_user_yubikey_odd_length",
          "code": "def test_admin_add_user_yubikey_odd_length(config, journalist_app, test_admin, locale, secret):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        utils.prepare_password_change(app, \"new\", VALID_PASSWORD)\n\n        resp = app.post(\n            url_for(\"admin.add_user\", l=locale),\n            data=dict(\n                username=\"dellsberg\",\n                first_name=\"\",\n                last_name=\"\",\n                password=VALID_PASSWORD,\n                password_again=VALID_PASSWORD,\n                is_admin=None,\n                is_hotp=True,\n                otp_secret=secret,\n            ),\n        )\n        journalist_app.logger.critical(\"response: %s\", resp.data)\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"HOTP secrets are 40 characters long - you have entered {num}.\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert ngettext(\n                \"HOTP secrets are 40 characters long - you have entered {num}.\",\n                \"HOTP secrets are 40 characters long - you have entered {num}.\",\n                len(secret),\n            ).format(num=len(secret)) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_add_user_yubikey_blank_secret",
          "code": "def test_admin_add_user_yubikey_blank_secret(config, journalist_app, test_admin, locale, secret):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        utils.prepare_password_change(app, \"new\", VALID_PASSWORD)\n\n        resp = app.post(\n            url_for(\"admin.add_user\", l=locale),\n            data=dict(\n                username=\"dellsberg\",\n                first_name=\"\",\n                last_name=\"\",\n                password=VALID_PASSWORD,\n                password_again=VALID_PASSWORD,\n                is_admin=None,\n                is_hotp=True,\n                otp_secret=secret,\n            ),\n        )\n\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"The &#34;otp_secret&#34; field is required when &#34;is_hotp&#34; is set.\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            # Should redirect to the token verification page\n            assert gettext(msgids[0]) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_add_user_yubikey_valid_length",
          "code": "def test_admin_add_user_yubikey_valid_length(config, journalist_app, test_admin, locale):\n    otp = \"1234567890123456789012345678901234567890\"\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        utils.prepare_password_change(app, \"new\", VALID_PASSWORD)\n\n        resp = app.post(\n            url_for(\"admin.add_user\", l=locale),\n            data=dict(\n                username=\"dellsberg\",\n                first_name=\"\",\n                last_name=\"\",\n                password=VALID_PASSWORD,\n                password_again=VALID_PASSWORD,\n                is_admin=None,\n                is_hotp=True,\n                otp_secret=otp,\n            ),\n            follow_redirects=True,\n        )\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"Enable YubiKey (OATH-HOTP)\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            # Should redirect to the token verification page\n            assert gettext(msgids[0]) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_add_user_yubikey_correct_length_with_whitespace",
          "code": "def test_admin_add_user_yubikey_correct_length_with_whitespace(\n    config, journalist_app, test_admin, locale\n):\n    otp = \"12 34 56 78 90 12 34 56 78 90 12 34 56 78 90 12 34 56 78 90\"\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        utils.prepare_password_change(app, \"new\", VALID_PASSWORD)\n\n        resp = app.post(\n            url_for(\"admin.add_user\", l=locale),\n            data=dict(\n                username=\"dellsberg\",\n                first_name=\"\",\n                last_name=\"\",\n                password=VALID_PASSWORD,\n                password_again=VALID_PASSWORD,\n                is_admin=None,\n                is_hotp=True,\n                otp_secret=otp,\n            ),\n            follow_redirects=True,\n        )\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"Enable YubiKey (OATH-HOTP)\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            # Should redirect to the token verification page\n            assert gettext(msgids[0]) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_sets_user_to_admin",
          "code": "def test_admin_sets_user_to_admin(journalist_app, test_admin):\n    new_user = \"admin-set-user-to-admin-test\"\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        utils.prepare_password_change(app, \"new\", VALID_PASSWORD)\n\n        resp = app.post(\n            url_for(\"admin.add_user\"),\n            data=dict(\n                username=new_user,\n                first_name=\"\",\n                last_name=\"\",\n                otp_secret=\"\",\n                password=VALID_PASSWORD,\n                is_admin=None,\n            ),\n        )\n        print(resp.data.decode(\"utf-8\"))\n        assert resp.status_code in (200, 302)\n\n        journo = Journalist.query.filter_by(username=new_user).one()\n        # precondition check\n        assert journo.is_admin is False\n\n        resp = app.post(\n            url_for(\"admin.edit_user\", user_id=journo.id),\n            data=dict(first_name=\"\", last_name=\"\", is_admin=True),\n        )\n        assert resp.status_code in (200, 302)\n\n        journo = Journalist.query.filter_by(username=new_user).one()\n        assert journo.is_admin is True",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_renames_user",
          "code": "def test_admin_renames_user(journalist_app, test_admin):\n    new_user = \"admin-renames-user-test\"\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        utils.prepare_password_change(app, \"new\", VALID_PASSWORD)\n\n        resp = app.post(\n            url_for(\"admin.add_user\"),\n            data=dict(\n                username=new_user,\n                first_name=\"\",\n                last_name=\"\",\n                password=VALID_PASSWORD,\n                otp_secret=\"\",\n                is_admin=None,\n            ),\n        )\n        assert resp.status_code in (200, 302)\n        journo = Journalist.query.filter(Journalist.username == new_user).one()\n\n        new_user = new_user + \"a\"\n        resp = app.post(\n            url_for(\"admin.edit_user\", user_id=journo.id),\n            data=dict(username=new_user, first_name=\"\", last_name=\"\"),\n        )\n    assert resp.status_code in (200, 302), resp.data.decode(\"utf-8\")\n\n    # the following will throw an exception if new_user is not found\n    # therefore asserting it has been created\n    Journalist.query.filter(Journalist.username == new_user).one()",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_adds_first_name_last_name_to_user",
          "code": "def test_admin_adds_first_name_last_name_to_user(journalist_app, test_admin):\n    new_user = \"admin-first-name-last-name-user-test\"\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        utils.prepare_password_change(app, \"new\", VALID_PASSWORD)\n\n        resp = app.post(\n            url_for(\"admin.add_user\"),\n            data=dict(\n                username=new_user,\n                first_name=\"\",\n                last_name=\"\",\n                password=VALID_PASSWORD,\n                otp_secret=\"\",\n                is_admin=None,\n            ),\n        )\n        assert resp.status_code in (200, 302)\n        journo = Journalist.query.filter(Journalist.username == new_user).one()\n\n        resp = app.post(\n            url_for(\"admin.edit_user\", user_id=journo.id),\n            data=dict(username=new_user, first_name=\"test name\", last_name=\"test name\"),\n        )\n    assert resp.status_code in (200, 302)\n\n    # the following will throw an exception if new_user is not found\n    # therefore asserting it has been created\n    Journalist.query.filter(Journalist.username == new_user).one()",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_adds_invalid_first_last_name_to_user",
          "code": "def test_admin_adds_invalid_first_last_name_to_user(config, journalist_app, test_admin, locale):\n    with journalist_app.test_client() as client:\n        new_user = \"admin-invalid-first-name-last-name-user-test\"\n\n        login_journalist(\n            client,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        utils.prepare_password_change(client, \"new\", VALID_PASSWORD)\n\n        resp = client.post(\n            url_for(\"admin.add_user\"),\n            data=dict(\n                username=new_user,\n                first_name=\"\",\n                last_name=\"\",\n                password=VALID_PASSWORD,\n                otp_secret=\"\",\n                is_admin=None,\n            ),\n        )\n        assert resp.status_code == 200\n        journo = Journalist.query.filter(Journalist.username == new_user).one()\n\n        overly_long_name = \"a\" * (Journalist.MAX_NAME_LEN + 1)\n\n        with InstrumentedApp(journalist_app) as ins:\n            resp = client.post(\n                url_for(\"admin.edit_user\", user_id=journo.id, l=locale),\n                data=dict(\n                    username=new_user,\n                    first_name=overly_long_name,\n                    last_name=\"test name\",\n                ),\n                follow_redirects=True,\n            )\n            assert resp.status_code == 200\n            assert page_language(resp.data) == language_tag(locale)\n\n            msgids = [\"Name not updated: {message}\", \"Name too long\"]\n            with xfail_untranslated_messages(config, locale, msgids):\n                ins.assert_message_flashed(\n                    gettext(msgids[0]).format(message=gettext(\"Name too long\")), \"error\"\n                )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_add_user_integrity_error",
          "code": "def test_admin_add_user_integrity_error(config, journalist_app, test_admin, mocker, locale):\n    mocked_error_logger = mocker.patch(\"journalist_app.admin.current_app.logger.error\")\n    mocker.patch(\n        \"journalist_app.admin.Journalist\",\n        side_effect=IntegrityError(\"STATEMENT\", \"PARAMETERS\", None),\n    )\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        utils.prepare_password_change(app, \"new\", VALID_PASSWORD)\n\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.post(\n                url_for(\"admin.add_user\", l=locale),\n                data=dict(\n                    username=\"username\",\n                    first_name=\"\",\n                    last_name=\"\",\n                    password=VALID_PASSWORD,\n                    otp_secret=\"\",\n                    is_admin=None,\n                ),\n            )\n            assert page_language(resp.data) == language_tag(locale)\n            msgids = [\n                \"An error occurred saving this user to the database. \" \"Please inform your admin.\"\n            ]\n            with xfail_untranslated_messages(config, locale, msgids):\n                ins.assert_message_flashed(gettext(msgids[0]), \"error\")\n\n    log_event = mocked_error_logger.call_args[0][0]\n    assert (\n        \"Adding user 'username' failed: (builtins.NoneType) \"\n        \"None\\n[SQL: STATEMENT]\\n[parameters: 'PARAMETERS']\"\n    ) in log_event",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_prevent_document_uploads",
          "code": "def test_prevent_document_uploads(config, journalist_app, test_admin, locale):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        form = journalist_app_module.forms.SubmissionPreferencesForm(\n            prevent_document_uploads=True, min_message_length=0\n        )\n        app.post(\n            url_for(\"admin.update_submission_preferences\"),\n            data=form.data,\n            follow_redirects=True,\n        )\n        assert InstanceConfig.get_current().allow_document_uploads is False\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.post(\n                url_for(\"admin.update_submission_preferences\", l=locale),\n                data=form.data,\n                follow_redirects=True,\n            )\n            assert page_language(resp.data) == language_tag(locale)\n            msgids = [\"Preferences saved.\"]\n            with xfail_untranslated_messages(config, locale, msgids):\n                ins.assert_message_flashed(gettext(msgids[0]), \"submission-preferences-success\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_no_prevent_document_uploads",
          "code": "def test_no_prevent_document_uploads(config, journalist_app, test_admin, locale):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        form = journalist_app_module.forms.SubmissionPreferencesForm(min_message_length=0)\n        app.post(\n            url_for(\"admin.update_submission_preferences\"),\n            data=form.data,\n            follow_redirects=True,\n        )\n        assert InstanceConfig.get_current().allow_document_uploads is True\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.post(\n                url_for(\"admin.update_submission_preferences\", l=locale),\n                data=form.data,\n                follow_redirects=True,\n            )\n            assert InstanceConfig.get_current().allow_document_uploads is True\n            assert page_language(resp.data) == language_tag(locale)\n            msgids = [\"Preferences saved.\"]\n            with xfail_untranslated_messages(config, locale, msgids):\n                ins.assert_message_flashed(gettext(msgids[0]), \"submission-preferences-success\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_prevent_document_uploads_invalid",
          "code": "def test_prevent_document_uploads_invalid(journalist_app, test_admin):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        form_true = journalist_app_module.forms.SubmissionPreferencesForm(\n            prevent_document_uploads=True, min_message_length=0\n        )\n        app.post(\n            url_for(\"admin.update_submission_preferences\"),\n            data=form_true.data,\n            follow_redirects=True,\n        )\n        assert InstanceConfig.get_current().allow_document_uploads is False\n\n        with patch(\"flask_wtf.FlaskForm.validate_on_submit\") as fMock:\n            fMock.return_value = False\n            form_false = journalist_app_module.forms.SubmissionPreferencesForm(\n                prevent_document_uploads=False\n            )\n            app.post(\n                url_for(\"admin.update_submission_preferences\"),\n                data=form_false.data,\n                follow_redirects=True,\n            )\n            assert InstanceConfig.get_current().allow_document_uploads is False",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_message_filtering",
          "code": "def test_message_filtering(journalist_app, test_admin):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        # Assert status quo\n        assert InstanceConfig.get_current().initial_message_min_len == 0\n\n        # Try to set min length to 10, but don't tick the \"prevent short messages\" checkbox\n        form = journalist_app_module.forms.SubmissionPreferencesForm(\n            prevent_short_messages=False, min_message_length=10\n        )\n        app.post(\n            url_for(\"admin.update_submission_preferences\"),\n            data=form.data,\n            follow_redirects=True,\n        )\n        # Still 0\n        assert InstanceConfig.get_current().initial_message_min_len == 0\n\n        # Inverse, tick the \"prevent short messages\" checkbox but leave min length at 0\n        form = journalist_app_module.forms.SubmissionPreferencesForm(\n            prevent_short_messages=True, min_message_length=0\n        )\n        resp = app.post(\n            url_for(\"admin.update_submission_preferences\"),\n            data=form.data,\n            follow_redirects=True,\n        )\n        # Still 0\n        assert InstanceConfig.get_current().initial_message_min_len == 0\n        html = resp.data.decode(\"utf-8\")\n        assert \"To configure a minimum message length, you must set the required\" in html\n\n        # Now tick the \"prevent short messages\" checkbox\n        form = journalist_app_module.forms.SubmissionPreferencesForm(\n            prevent_short_messages=True, min_message_length=10\n        )\n        app.post(\n            url_for(\"admin.update_submission_preferences\"),\n            data=form.data,\n            follow_redirects=True,\n        )\n        assert InstanceConfig.get_current().initial_message_min_len == 10\n\n        # Submit junk data for min_message_length\n        resp = app.post(\n            url_for(\"admin.update_submission_preferences\"),\n            data={**form.data, \"min_message_length\": \"abcdef\"},\n            follow_redirects=True,\n        )\n        html = resp.data.decode(\"utf-8\")\n        assert \"To configure a minimum message length, you must set the required\" in html\n        # Now rejecting codenames\n        assert InstanceConfig.get_current().reject_message_with_codename is False\n        form = journalist_app_module.forms.SubmissionPreferencesForm(reject_codename_messages=True)\n        app.post(\n            url_for(\"admin.update_submission_preferences\"),\n            data=form.data,\n            follow_redirects=True,\n        )\n        assert InstanceConfig.get_current().reject_message_with_codename is True",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_orgname_default_set",
          "code": "def test_orgname_default_set(journalist_app, test_admin):\n    class dummy_current:\n        organization_name = None\n\n    with patch.object(InstanceConfig, \"get_current\") as iMock:\n        with journalist_app.test_client() as app:\n            iMock.return_value = dummy_current()\n            login_journalist(\n                app,\n                test_admin[\"username\"],\n                test_admin[\"password\"],\n                test_admin[\"otp_secret\"],\n            )\n            assert g.organization_name == \"SecureDrop\"",
          "file": "test_journalist.py"
        },
        {
          "type": "class",
          "name": "dummy_current",
          "code": "class dummy_current:\n        organization_name = None",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_orgname_valid_succeeds",
          "code": "def test_orgname_valid_succeeds(config, journalist_app, test_admin, locale):\n    test_name = \"Walden Inquirer\"\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        form = journalist_app_module.forms.OrgNameForm(organization_name=test_name)\n        assert InstanceConfig.get_current().organization_name == \"SecureDrop\"\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.post(\n                url_for(\"admin.update_org_name\", l=locale),\n                data=form.data,\n                follow_redirects=True,\n            )\n            assert page_language(resp.data) == language_tag(locale)\n            msgids = [\"Preferences saved.\"]\n            with xfail_untranslated_messages(config, locale, msgids):\n                ins.assert_message_flashed(gettext(msgids[0]), \"org-name-success\")\n            assert InstanceConfig.get_current().organization_name == test_name",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_orgname_null_fails",
          "code": "def test_orgname_null_fails(config, journalist_app, test_admin, locale):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        form = journalist_app_module.forms.OrgNameForm(organization_name=\"\")\n        assert InstanceConfig.get_current().organization_name == \"SecureDrop\"\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.post(\n                url_for(\"admin.update_org_name\", l=locale),\n                data=form.data,\n                follow_redirects=True,\n            )\n            assert page_language(resp.data) == language_tag(locale)\n            msgids = [\"This field is required.\"]\n            with xfail_untranslated_messages(config, locale, msgids):\n                ins.assert_message_flashed(gettext(msgids[0]), \"org-name-error\")\n            assert InstanceConfig.get_current().organization_name == \"SecureDrop\"",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_orgname_oversized_fails",
          "code": "def test_orgname_oversized_fails(config, journalist_app, test_admin, locale):\n    test_name = \"1234567812345678123456781234567812345678123456781234567812345678a\"\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        form = journalist_app_module.forms.OrgNameForm(organization_name=test_name)\n        assert InstanceConfig.get_current().organization_name == \"SecureDrop\"\n        resp = app.post(\n            url_for(\"admin.update_org_name\", l=locale),\n            data=form.data,\n            follow_redirects=True,\n        )\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"Cannot be longer than {num} character.\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert ngettext(\n                \"Cannot be longer than {num} character.\",\n                \"Cannot be longer than {num} characters.\",\n                InstanceConfig.MAX_ORG_NAME_LEN,\n            ).format(num=InstanceConfig.MAX_ORG_NAME_LEN) in resp.data.decode(\"utf-8\")\n        assert InstanceConfig.get_current().organization_name == \"SecureDrop\"",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_logo_default_available",
          "code": "def test_logo_default_available(journalist_app, config):\n    # if the custom image is available, this test will fail\n    custom_image_location = os.path.join(config.SECUREDROP_ROOT, \"static/i/custom_logo.png\")\n    if os.path.exists(custom_image_location):\n        os.remove(custom_image_location)\n\n    with journalist_app.test_client() as app:\n        logo_url = journalist_app_module.get_logo_url(journalist_app)\n        assert logo_url.endswith(\"/static/i/logo.png\")\n        response = app.get(logo_url, follow_redirects=False)\n        assert response.status_code == 200",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_logo_upload_with_valid_image_succeeds",
          "code": "def test_logo_upload_with_valid_image_succeeds(config, journalist_app, test_admin, locale):\n    # Save original logo to restore after test run\n    logo_image_location = os.path.join(config.SECUREDROP_ROOT, \"static/i/logo.png\")\n    with open(logo_image_location, \"rb\") as logo_file:\n        original_image = logo_file.read()\n\n    try:\n        logo_bytes = base64.decodebytes(\n            b\"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQ\"\n            b\"VR42mP8/x8AAwMCAO+ip1sAAAAASUVORK5CYII=\"\n        )\n\n        with journalist_app.test_client() as app:\n            login_journalist(\n                app,\n                test_admin[\"username\"],\n                test_admin[\"password\"],\n                test_admin[\"otp_secret\"],\n            )\n            # Create 1px * 1px 'white' PNG file from its base64 string\n            form = journalist_app_module.forms.LogoForm(logo=(BytesIO(logo_bytes), \"test.png\"))\n            # Create 1px * 1px 'white' PNG file from its base64 string\n            form = journalist_app_module.forms.LogoForm(logo=(BytesIO(logo_bytes), \"test.png\"))\n            with InstrumentedApp(journalist_app) as ins:\n                resp = app.post(\n                    url_for(\"admin.manage_config\", l=locale),\n                    data=form.data,\n                    follow_redirects=True,\n                )\n                assert page_language(resp.data) == language_tag(locale)\n                msgids = [\"Image updated.\"]\n                with xfail_untranslated_messages(config, locale, msgids):\n                    ins.assert_message_flashed(gettext(msgids[0]), \"logo-success\")\n\n        with journalist_app.test_client() as app:\n            logo_url = journalist_app_module.get_logo_url(journalist_app)\n            assert logo_url.endswith(\"/static/i/custom_logo.png\")\n            response = app.get(logo_url, follow_redirects=False)\n            assert response.status_code == 200\n            assert response.data == logo_bytes\n    finally:\n        # Restore original image to logo location for subsequent tests\n        with open(logo_image_location, \"wb\") as logo_file:\n            logo_file.write(original_image)",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_logo_upload_with_invalid_filetype_fails",
          "code": "def test_logo_upload_with_invalid_filetype_fails(config, journalist_app, test_admin, locale):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n\n        form = journalist_app_module.forms.LogoForm(logo=(BytesIO(b\"filedata\"), \"bad.exe\"))\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.post(\n                url_for(\"admin.manage_config\", l=locale),\n                data=form.data,\n                follow_redirects=True,\n            )\n\n            assert page_language(resp.data) == language_tag(locale)\n            msgids = [\"You can only upload PNG image files.\"]\n            with xfail_untranslated_messages(config, locale, msgids):\n                ins.assert_message_flashed(gettext(msgids[0]), \"logo-error\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_logo_upload_save_fails",
          "code": "def test_logo_upload_save_fails(config, journalist_app, test_admin, locale):\n    # Save original logo to restore after test run\n    logo_image_location = os.path.join(config.SECUREDROP_ROOT, \"static/i/logo.png\")\n    with open(logo_image_location, \"rb\") as logo_file:\n        original_image = logo_file.read()\n\n    try:\n        with journalist_app.test_client() as app:\n            login_journalist(\n                app,\n                test_admin[\"username\"],\n                test_admin[\"password\"],\n                test_admin[\"otp_secret\"],\n            )\n            # Create 1px * 1px 'white' PNG file from its base64 string\n            form = journalist_app_module.forms.LogoForm(\n                logo=(\n                    BytesIO(\n                        base64.decodebytes(\n                            b\"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQ\"\n                            b\"VR42mP8/x8AAwMCAO+ip1sAAAAASUVORK5CYII=\"\n                        )\n                    ),\n                    \"test.png\",\n                )\n            )\n            with InstrumentedApp(journalist_app) as ins:\n                with patch(\"werkzeug.datastructures.FileStorage.save\") as sMock:\n                    sMock.side_effect = Exception\n                    resp = app.post(\n                        url_for(\"admin.manage_config\", l=locale),\n                        data=form.data,\n                        follow_redirects=True,\n                    )\n\n                    assert page_language(resp.data) == language_tag(locale)\n                    msgids = [\"Unable to process the image file. Please try another one.\"]\n                    with xfail_untranslated_messages(config, locale, msgids):\n                        ins.assert_message_flashed(gettext(msgids[0]), \"logo-error\")\n    finally:\n        # Restore original image to logo location for subsequent tests\n        with open(logo_image_location, \"wb\") as logo_file:\n            logo_file.write(original_image)",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_creation_of_ossec_test_log_event",
          "code": "def test_creation_of_ossec_test_log_event(journalist_app, test_admin, mocker):\n    mocked_error_logger = mocker.patch(\"journalist.app.logger.error\")\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n        app.post(url_for(\"admin.ossec_test\"))\n\n    mocked_error_logger.assert_called_once_with(\"This is a test OSSEC alert\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_logo_upload_with_empty_input_field_fails",
          "code": "def test_logo_upload_with_empty_input_field_fails(config, journalist_app, test_admin, locale):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_admin[\"username\"],\n            test_admin[\"password\"],\n            test_admin[\"otp_secret\"],\n        )\n\n        form = journalist_app_module.forms.LogoForm(logo=(BytesIO(b\"\"), \"\"))\n\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.post(\n                url_for(\"admin.manage_config\", l=locale),\n                data=form.data,\n                follow_redirects=True,\n            )\n\n            assert page_language(resp.data) == language_tag(locale)\n            msgids = [\"File required.\"]\n            with xfail_untranslated_messages(config, locale, msgids):\n                ins.assert_message_flashed(gettext(msgids[0]), \"logo-error\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_page_restriction_http_gets",
          "code": "def test_admin_page_restriction_http_gets(journalist_app, test_journo):\n    admin_urls = [\n        url_for(\"admin.index\"),\n        url_for(\"admin.add_user\"),\n        url_for(\"admin.edit_user\", user_id=test_journo[\"id\"]),\n    ]\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n        for admin_url in admin_urls:\n            resp = app.get(admin_url)\n            assert resp.status_code == 302",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_admin_page_restriction_http_posts",
          "code": "def test_admin_page_restriction_http_posts(journalist_app, test_journo):\n    admin_urls = [\n        url_for(\"admin.reset_two_factor_totp\"),\n        url_for(\"admin.reset_two_factor_hotp\"),\n        url_for(\"admin.add_user\", user_id=test_journo[\"id\"]),\n        url_for(\"admin.new_user_two_factor_totp\"),\n        url_for(\"admin.new_user_two_factor_hotp\"),\n        url_for(\"admin.reset_two_factor_totp\"),\n        url_for(\"admin.reset_two_factor_hotp\"),\n        url_for(\"admin.edit_user\", user_id=test_journo[\"id\"]),\n        url_for(\"admin.delete_user\", user_id=test_journo[\"id\"]),\n    ]\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n        for admin_url in admin_urls:\n            resp = app.post(admin_url)\n            assert resp.status_code == 302",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_user_authorization_for_gets",
          "code": "def test_user_authorization_for_gets(journalist_app):\n    urls = [\n        url_for(\"main.index\"),\n        url_for(\"col.col\", filesystem_id=\"1\"),\n        url_for(\"col.download_single_file\", filesystem_id=\"1\", fn=\"1\"),\n        url_for(\"account.edit\"),\n    ]\n\n    with journalist_app.test_client() as app:\n        for url in urls:\n            resp = app.get(url)\n            assert resp.status_code == 302",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_user_authorization_for_posts",
          "code": "def test_user_authorization_for_posts(journalist_app):\n    urls = [\n        url_for(\"col.add_star\", filesystem_id=\"1\"),\n        url_for(\"col.remove_star\", filesystem_id=\"1\"),\n        url_for(\"col.process\"),\n        url_for(\"col.delete_single\", filesystem_id=\"1\"),\n        url_for(\"main.reply\"),\n        url_for(\"main.bulk\"),\n        url_for(\"account.new_two_factor_totp\"),\n        url_for(\"account.new_two_factor_hotp\"),\n        url_for(\"account.reset_two_factor_totp\"),\n        url_for(\"account.reset_two_factor_hotp\"),\n        url_for(\"account.change_name\"),\n    ]\n    with journalist_app.test_client() as app:\n        for url in urls:\n            resp = app.post(url)\n            assert resp.status_code == 302",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_incorrect_current_password_change",
          "code": "def test_incorrect_current_password_change(config, journalist_app, test_journo, locale):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n        utils.prepare_password_change(app, test_journo[\"id\"], VALID_PASSWORD)\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.post(\n                url_for(\"account.new_password\", l=locale),\n                data=dict(password=VALID_PASSWORD, token=\"mocked\", current_password=\"badpw\"),\n                follow_redirects=True,\n            )\n            assert page_language(resp.data) == language_tag(locale)\n            msgids = [\n                \"Incorrect password or two-factor code.\",\n                (\n                    \"Please wait for a new code from your two-factor mobile\"\n                    \" app or security key before trying again.\"\n                ),\n            ]\n            with xfail_untranslated_messages(config, locale, msgids):\n                ins.assert_message_flashed(gettext(msgids[0]) + \" \" + gettext(msgids[1]), \"error\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_passphrase_migration_on_verification",
          "code": "def test_passphrase_migration_on_verification(journalist_app):\n    salt = b64decode(\"+mGOQmD5Nnb+mH9gwBoxKRhKZmmJ6BzpmD5YArPHZsY=\")\n    journalist = Journalist(\"test\", VALID_PASSWORD)\n\n    # manually set the params\n    hash = journalist._scrypt_hash(VALID_PASSWORD, salt)\n    journalist.passphrase_hash = None\n    journalist.pw_salt = salt\n    journalist.pw_hash = hash\n\n    assert journalist.valid_password(VALID_PASSWORD)\n\n    # check that the migration happened\n    assert journalist.passphrase_hash is not None\n    assert journalist.passphrase_hash.startswith(\"$argon2\")\n    assert journalist.pw_salt is None\n    assert journalist.pw_hash is None\n\n    # check that a verification post-migration works\n    assert journalist.valid_password(VALID_PASSWORD)",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_passphrase_migration_on_reset",
          "code": "def test_passphrase_migration_on_reset(journalist_app):\n    salt = b64decode(\"+mGOQmD5Nnb+mH9gwBoxKRhKZmmJ6BzpmD5YArPHZsY=\")\n    journalist = Journalist(\"test\", VALID_PASSWORD)\n\n    # manually set the params\n    hash = journalist._scrypt_hash(VALID_PASSWORD, salt)\n    journalist.passphrase_hash = None\n    journalist.pw_salt = salt\n    journalist.pw_hash = hash\n\n    journalist.set_password(VALID_PASSWORD)\n\n    # check that the migration happened\n    assert journalist.passphrase_hash is not None\n    assert journalist.passphrase_hash.startswith(\"$argon2\")\n    assert journalist.pw_salt is None\n    assert journalist.pw_hash is None\n\n    # check that a verification post-migration works\n    assert journalist.valid_password(VALID_PASSWORD)",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_passphrase_argon2i_migration",
          "code": "def test_passphrase_argon2i_migration(test_journo):\n    \"\"\"verify argon2i hashes work and then are migrated to argon2id\"\"\"\n    journalist = test_journo[\"journalist\"]\n    # But use our password hash\n    journalist.passphrase_hash = (\n        \"$argon2i$v=19$m=65536,t=4,p=2$JfFkLIJ2ogPUDI19XiBzHA$kaKNVckLLQNNBnmllMWqXg\"\n    )\n    db.session.add(journalist)\n    db.session.commit()\n    assert journalist.valid_password(\"correct horse battery staple profanity oil chewy\")\n    assert journalist.passphrase_hash.startswith(\"$argon2id$\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_journalist_reply_view",
          "code": "def test_journalist_reply_view(journalist_app, test_source, test_journo, app_storage):\n    source, _ = utils.db_helper.init_source(app_storage)\n    journalist, _ = utils.db_helper.init_journalist()\n    submissions = utils.db_helper.submit(app_storage, source, 1)\n    replies = utils.db_helper.reply(app_storage, journalist, source, 1)\n\n    subm_url = url_for(\n        \"col.download_single_file\",\n        filesystem_id=submissions[0].source.filesystem_id,\n        fn=submissions[0].filename,\n    )\n    reply_url = url_for(\n        \"col.download_single_file\",\n        filesystem_id=replies[0].source.filesystem_id,\n        fn=replies[0].filename,\n    )\n\n    with journalist_app.test_client() as app:\n        resp = app.get(subm_url)\n        assert resp.status_code == 302\n        resp = app.get(reply_url)\n        assert resp.status_code == 302",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_too_long_user_password_change",
          "code": "def test_too_long_user_password_change(config, journalist_app, test_journo, locale):\n    overly_long_password = VALID_PASSWORD + \"a\" * (\n        Journalist.MAX_PASSWORD_LEN - len(VALID_PASSWORD) + 1\n    )\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n        utils.prepare_password_change(app, test_journo[\"id\"], overly_long_password)\n\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.post(\n                url_for(\"account.new_password\", l=locale),\n                data=dict(\n                    password=overly_long_password,\n                    token=TOTP(test_journo[\"otp_secret\"]).now(),\n                    current_password=test_journo[\"password\"],\n                ),\n                follow_redirects=True,\n            )\n\n            assert page_language(resp.data) == language_tag(locale)\n            msgids = [\"The password you submitted is invalid. Password not changed.\"]\n            with xfail_untranslated_messages(config, locale, msgids):\n                ins.assert_message_flashed(gettext(msgids[0]), \"error\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_valid_user_password_change",
          "code": "def test_valid_user_password_change(config, journalist_app, test_journo, locale):\n    \"\"\"Test a successful password change flow\"\"\"\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n\n        # First obtain a randomly generated password from the server\n        first = app.get(url_for(\"account.edit\", l=locale))\n        assert first.status_code == 200\n        password = utils.extract_password(first.data)\n\n        # Then send it back\n        resp = app.post(\n            url_for(\"account.new_password\", l=locale),\n            data=dict(\n                password=password,\n                token=TOTP(test_journo[\"otp_secret\"]).now(),\n                current_password=test_journo[\"password\"],\n            ),\n            follow_redirects=True,\n        )\n\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\n            \"Password updated. Don't forget to save it in your KeePassX database. New password:\"\n        ]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert escape(gettext(msgids[0])) in resp.data.decode(\"utf-8\")\n            assert password in resp.data.decode()",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_invalid_user_password_change_different",
          "code": "def test_invalid_user_password_change_different(config, journalist_app, test_journo, locale):\n    \"\"\"Test if the user submits a different password than the one they received\"\"\"\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n\n        # First obtain a randomly generated password from the server\n        first = app.get(url_for(\"account.edit\", l=locale))\n        assert first.status_code == 200\n        password = utils.extract_password(first.data)\n\n        # No freak random collisions\n        assert password != VALID_PASSWORD\n\n        # Then send back a different password\n        resp = app.post(\n            url_for(\"account.new_password\", l=locale),\n            data=dict(\n                password=VALID_PASSWORD,\n                token=TOTP(test_journo[\"otp_secret\"]).now(),\n                current_password=test_journo[\"password\"],\n            ),\n            follow_redirects=True,\n        )\n\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"The password you submitted is invalid. Password not changed.\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert escape(gettext(msgids[0])) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_invalid_user_password_change_nopending",
          "code": "def test_invalid_user_password_change_nopending(config, journalist_app, test_journo, locale):\n    \"\"\"Test if the user submits a password without receiving one from the server first\"\"\"\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n\n        # We explicitly do not fetch a password from the server nor set a pending one\n        resp = app.post(\n            url_for(\"account.new_password\", l=locale),\n            data=dict(\n                password=VALID_PASSWORD,\n                token=TOTP(test_journo[\"otp_secret\"]).now(),\n                current_password=test_journo[\"password\"],\n            ),\n            follow_redirects=True,\n        )\n\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"The password you submitted is invalid. Password not changed.\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert escape(gettext(msgids[0])) in resp.data.decode(\"utf-8\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_valid_user_first_last_name_change",
          "code": "def test_valid_user_first_last_name_change(config, journalist_app, test_journo, locale):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.post(\n                url_for(\"account.change_name\", l=locale),\n                data=dict(first_name=\"test\", last_name=\"test\"),\n                follow_redirects=True,\n            )\n\n            assert page_language(resp.data) == language_tag(locale)\n            msgids = [\"Name updated.\", \"Name too long\"]\n            with xfail_untranslated_messages(config, locale, msgids):\n                ins.assert_message_flashed(\n                    gettext(msgids[0]).format(gettext(\"Name too long\")), \"success\"\n                )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_valid_user_invalid_first_last_name_change",
          "code": "def test_valid_user_invalid_first_last_name_change(config, journalist_app, test_journo, locale):\n    with journalist_app.test_client() as app:\n        overly_long_name = \"a\" * (Journalist.MAX_NAME_LEN + 1)\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.post(\n                url_for(\"account.change_name\", l=locale),\n                data=dict(first_name=overly_long_name, last_name=overly_long_name),\n                follow_redirects=True,\n            )\n            assert page_language(resp.data) == language_tag(locale)\n            msgids = [\"Name not updated: {message}\", \"Name too long\"]\n            with xfail_untranslated_messages(config, locale, msgids):\n                ins.assert_message_flashed(\n                    gettext(msgids[0]).format(message=gettext(\"Name too long\")), \"error\"\n                )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_regenerate_totp",
          "code": "def test_regenerate_totp(journalist_app, test_journo):\n    old_secret = test_journo[\"otp_secret\"]\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n\n        resp = app.post(url_for(\"account.reset_two_factor_totp\"))\n        assert resp.status_code == 200\n        assert (\n            '<form id=\"check-token\" method=\"post\" action=\"/account/verify-2fa-totp\">'\n            in resp.data.decode()\n        )\n\n        new_secret = Journalist.query.get(test_journo[\"id\"]).otp_secret\n\n        # check that totp is different\n        assert new_secret != old_secret",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_edit_hotp",
          "code": "def test_edit_hotp(journalist_app, test_journo):\n    old_secret = test_journo[\"otp_secret\"]\n    valid_secret = \"DEADBEEF01234567DEADBEEF01234567DADEFEEB\"\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n\n        resp = app.post(\n            url_for(\"account.reset_two_factor_hotp\"),\n            data=dict(otp_secret=valid_secret),\n        )\n        assert resp.status_code == 200\n        assert (\n            '<form id=\"check-token\" method=\"post\" action=\"/account/verify-2fa-hotp\">'\n            in resp.data.decode()\n        )\n\n        new_secret = Journalist.query.get(test_journo[\"id\"]).otp_secret\n\n        # check that totp is different\n        assert new_secret != old_secret",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_delete_data_deletes_submissions_retaining_source",
          "code": "def test_delete_data_deletes_submissions_retaining_source(\n    journalist_app, test_journo, test_source, app_storage\n):\n    \"\"\"Verify that when only a source's data is deleted, the submissions\n    are deleted but the source is not.\"\"\"\n\n    with journalist_app.app_context():\n        source = Source.query.get(test_source[\"id\"])\n        journo = Journalist.query.get(test_journo[\"id\"])\n\n        utils.db_helper.submit(app_storage, source, 2)\n        utils.db_helper.reply(app_storage, journo, source, 2)\n\n        assert len(source.collection) == 4\n\n        journalist_app_module.utils.delete_source_files(test_source[\"filesystem_id\"])\n\n        res = Source.query.filter_by(id=test_source[\"id\"]).one_or_none()\n        assert res is not None\n\n        assert len(source.collection) == 0",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_delete_source_deletes_submissions",
          "code": "def test_delete_source_deletes_submissions(journalist_app, test_journo, test_source, app_storage):\n    \"\"\"Verify that when a source is deleted, the submissions that\n    correspond to them are also deleted.\"\"\"\n\n    with journalist_app.app_context():\n        source = Source.query.get(test_source[\"id\"])\n        journo = Journalist.query.get(test_journo[\"id\"])\n\n        utils.db_helper.submit(app_storage, source, 2)\n        utils.db_helper.reply(app_storage, journo, source, 2)\n\n        journalist_app_module.utils.delete_collection(test_source[\"filesystem_id\"])\n\n        res = Source.query.filter_by(id=test_source[\"id\"]).one_or_none()\n        assert res is None",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_delete_collection_updates_db",
          "code": "def test_delete_collection_updates_db(journalist_app, test_journo, test_source, app_storage):\n    \"\"\"\n    Verify that when a source is deleted, the Source record is deleted and all records associated\n    with the source are deleted.\n    \"\"\"\n\n    with journalist_app.app_context():\n        source = Source.query.get(test_source[\"id\"])\n        journo = Journalist.query.get(test_journo[\"id\"])\n        files = utils.db_helper.submit(app_storage, source, 2)\n        mark_seen(files, journo)\n        messages = utils.db_helper.submit(app_storage, source, 2)\n        mark_seen(messages, journo)\n        replies = utils.db_helper.reply(app_storage, journo, source, 2)\n        mark_seen(replies, journo)\n\n        journalist_app_module.utils.delete_collection(test_source[\"filesystem_id\"])\n\n        # Verify no source record exists\n        source_result = Source.query.filter_by(id=source.id).all()\n        assert not source_result\n\n        # Verify no submission from the deleted source exist\n        submissions_result = Submission.query.filter_by(source_id=source.id).all()\n        assert not submissions_result\n\n        # Verify no replies to the deleted source exist\n        replies_result = Reply.query.filter_by(source_id=source.id).all()\n        assert not replies_result\n\n        # Verify no seen records about the deleted files, messages, or replies exist\n        for file in files:\n            seen_file = SeenFile.query.filter_by(\n                file_id=file.id, journalist_id=journo.id\n            ).one_or_none()\n            assert not seen_file\n\n        for message in messages:\n            seen_message = SeenMessage.query.filter_by(\n                message_id=message.id, journalist_id=journo.id\n            ).one_or_none()\n            assert not seen_message\n\n        for reply in replies:\n            seen_reply = SeenReply.query.filter_by(\n                reply_id=reply.id, journalist_id=journo.id\n            ).one_or_none()\n            assert not seen_reply",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_delete_source_deletes_gpg_source_key",
          "code": "def test_delete_source_deletes_gpg_source_key(\n    journalist_app, test_source, test_journo, app_storage\n):\n    \"\"\"Verify that when a legacy source is deleted, the GPG key that corresponds\n    to them is also deleted.\"\"\"\n\n    encryption_mgr = EncryptionManager.get_default()\n    create_legacy_gpg_key(encryption_mgr, test_source[\"source_user\"], test_source[\"source\"])\n\n    with journalist_app.app_context():\n        source = Source.query.get(test_source[\"id\"])\n        journo = Journalist.query.get(test_journo[\"id\"])\n\n        utils.db_helper.submit(app_storage, source, 2)\n        utils.db_helper.reply(app_storage, journo, source, 2)\n\n        # Source key exists\n        assert encryption_mgr.get_source_key_fingerprint(test_source[\"filesystem_id\"])\n\n        journalist_app_module.utils.delete_collection(test_source[\"filesystem_id\"])\n\n        # Source key no longer exists\n        with pytest.raises(GpgKeyNotFoundError):\n            encryption_mgr.get_source_key_fingerprint(test_source[\"filesystem_id\"])",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_delete_source_deletes_docs_on_disk",
          "code": "def test_delete_source_deletes_docs_on_disk(\n    journalist_app, test_source, test_journo, config, app_storage\n):\n    \"\"\"Verify that when a source is deleted, the encrypted documents that\n    exist on disk is also deleted.\"\"\"\n\n    with journalist_app.app_context():\n        source = Source.query.get(test_source[\"id\"])\n        journo = Journalist.query.get(test_journo[\"id\"])\n\n        utils.db_helper.submit(app_storage, source, 2)\n        utils.db_helper.reply(app_storage, journo, source, 2)\n\n        dir_source_docs = os.path.join(config.STORE_DIR, test_source[\"filesystem_id\"])\n        assert os.path.exists(dir_source_docs)\n\n        journalist_app_module.utils.delete_collection(test_source[\"filesystem_id\"])\n\n        def assertion():\n            assert not os.path.exists(dir_source_docs)\n\n        utils.asynchronous.wait_for_assertion(assertion)",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "assertion",
          "code": "def assertion():\n            assert not os.path.exists(dir_source_docs)",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_bulk_delete_deletes_db_entries",
          "code": "def test_bulk_delete_deletes_db_entries(\n    journalist_app, test_source, test_journo, config, app_storage\n):\n    \"\"\"\n    Verify that when files are deleted, the corresponding db entries are\n    also deleted.\n    \"\"\"\n\n    with journalist_app.app_context():\n        source = Source.query.get(test_source[\"id\"])\n        journo = Journalist.query.get(test_journo[\"id\"])\n\n        utils.db_helper.submit(app_storage, source, 2)\n        utils.db_helper.reply(app_storage, journo, source, 2)\n\n        dir_source_docs = os.path.join(config.STORE_DIR, test_source[\"filesystem_id\"])\n        assert os.path.exists(dir_source_docs)\n\n        subs = Submission.query.filter_by(source_id=source.id).all()\n        assert subs\n\n        replies = Reply.query.filter_by(source_id=source.id).all()\n        assert replies\n\n        file_list = []\n        file_list.extend(subs)\n        file_list.extend(replies)\n\n        with journalist_app.test_request_context(\"/\"):\n            journalist_app_module.utils.bulk_delete(test_source[\"filesystem_id\"], file_list)\n\n        def db_assertion():\n            subs = Submission.query.filter_by(source_id=source.id).all()\n            assert not subs\n\n            replies = Reply.query.filter_by(source_id=source.id).all()\n            assert not replies\n\n        utils.asynchronous.wait_for_assertion(db_assertion)",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "db_assertion",
          "code": "def db_assertion():\n            subs = Submission.query.filter_by(source_id=source.id).all()\n            assert not subs\n\n            replies = Reply.query.filter_by(source_id=source.id).all()\n            assert not replies",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_bulk_delete_works_when_files_absent",
          "code": "def test_bulk_delete_works_when_files_absent(\n    journalist_app, test_source, test_journo, config, app_storage\n):\n    \"\"\"\n    Verify that when files are deleted but are already missing,\n    the corresponding db entries are still deleted\n    \"\"\"\n\n    with journalist_app.app_context():\n        source = Source.query.get(test_source[\"id\"])\n        journo = Journalist.query.get(test_journo[\"id\"])\n\n        utils.db_helper.submit(app_storage, source, 2)\n        utils.db_helper.reply(app_storage, journo, source, 2)\n\n        dir_source_docs = os.path.join(config.STORE_DIR, test_source[\"filesystem_id\"])\n        assert os.path.exists(dir_source_docs)\n\n        subs = Submission.query.filter_by(source_id=source.id).all()\n        assert subs\n\n        replies = Reply.query.filter_by(source_id=source.id).all()\n        assert replies\n\n        file_list = []\n        file_list.extend(subs)\n        file_list.extend(replies)\n\n        with journalist_app.test_request_context(\"/\"):\n            with patch(\"store.Storage.move_to_shredder\") as delMock:\n                delMock.side_effect = ValueError\n                journalist_app_module.utils.bulk_delete(test_source[\"filesystem_id\"], file_list)\n\n        def db_assertion():\n            subs = Submission.query.filter_by(source_id=source.id).all()\n            assert not subs\n\n            replies = Reply.query.filter_by(source_id=source.id).all()\n            assert not replies\n\n        utils.asynchronous.wait_for_assertion(db_assertion)",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "db_assertion",
          "code": "def db_assertion():\n            subs = Submission.query.filter_by(source_id=source.id).all()\n            assert not subs\n\n            replies = Reply.query.filter_by(source_id=source.id).all()\n            assert not replies",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_login_with_invalid_password_doesnt_call_argon2",
          "code": "def test_login_with_invalid_password_doesnt_call_argon2(mocker, test_journo):\n    mock_argon2 = mocker.patch(\"models.argon2.PasswordHasher\")\n    invalid_pw = \"a\" * (Journalist.MAX_PASSWORD_LEN + 1)\n\n    with pytest.raises(InvalidPasswordLength):\n        Journalist.login(test_journo[\"username\"], invalid_pw, TOTP(test_journo[\"otp_secret\"]).now())\n    assert not mock_argon2.called",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_render_locales",
          "code": "def test_render_locales(\n    setup_journalist_key_and_gpg_folder: Tuple[str, Path],\n    setup_rqworker: Tuple[str, str],\n) -> None:\n    \"\"\"the locales.html template must collect both request.args (l=XX) and\n    request.view_args (/<filesystem_id>) to build the URL to\n    change the locale\n    \"\"\"\n    journalist_key_fingerprint, gpg_key_dir = setup_journalist_key_and_gpg_folder\n    worker_name, _ = setup_rqworker\n    config_with_fr_locale = SecureDropConfigFactory.create(\n        SECUREDROP_DATA_ROOT=Path(\"/tmp/sd-tests/render_locales\"),\n        GPG_KEY_DIR=gpg_key_dir,\n        JOURNALIST_KEY=journalist_key_fingerprint,\n        SUPPORTED_LOCALES=[\"en_US\", \"fr_FR\"],\n        RQ_WORKER_NAME=worker_name,\n    )\n    app = journalist_app_module.create_app(config_with_fr_locale)\n    app.config[\"SERVER_NAME\"] = \"localhost.localdomain\"  # needed for url_for\n    with app.app_context():\n        journo_user, journo_pw = utils.db_helper.init_journalist(is_admin=False)\n        source_user = create_source_user(\n            db_session=db.session,\n            source_passphrase=PassphraseGenerator.get_default().generate_passphrase(),\n            source_app_storage=Storage(\n                str(config_with_fr_locale.STORE_DIR), str(config_with_fr_locale.TEMP_DIR)\n            ),\n        )\n\n        url = url_for(\"col.col\", filesystem_id=source_user.filesystem_id)\n        # we need the relative URL, not the full url including proto / localhost\n        url_end = url.replace(\"http://\", \"\")\n        url_end = url_end[url_end.index(\"/\") + 1 :]\n\n        with app.test_client() as app:\n            login_journalist(app, journo_user.username, journo_pw, journo_user.otp_secret)\n            resp = app.get(url + \"?l=fr_FR\")\n\n        # check that links to i18n URLs are/aren't present\n        text = resp.data.decode(\"utf-8\")\n        assert \"?l=fr_FR\" not in text, text\n        assert url_end + \"?l=en_US\" in text, text",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_download_selected_submissions_and_replies",
          "code": "def test_download_selected_submissions_and_replies(\n    journalist_app, test_journo, test_source, app_storage\n):\n    journo = Journalist.query.get(test_journo[\"id\"])\n    source = Source.query.get(test_source[\"id\"])\n    submissions = utils.db_helper.submit(app_storage, source, 4)\n    replies = utils.db_helper.reply(app_storage, journo, source, 4)\n    selected_submissions = random.sample(submissions, 2)\n    selected_replies = random.sample(replies, 2)\n    selected = [submission.filename for submission in selected_submissions + selected_replies]\n    selected.sort()\n\n    with journalist_app.test_client() as app:\n        login_journalist(app, journo.username, test_journo[\"password\"], test_journo[\"otp_secret\"])\n        resp = app.post(\n            \"/bulk\",\n            data=dict(\n                action=\"download\",\n                filesystem_id=test_source[\"filesystem_id\"],\n                doc_names_selected=selected,\n            ),\n        )\n\n    # The download request was succesful, and the app returned a zipfile\n    assert resp.status_code == 200\n    assert resp.content_type == \"application/zip\"\n    assert zipfile.is_zipfile(BytesIO(resp.data))\n\n    # The items selected are in the zipfile and items are marked seen\n    for item in selected_submissions + selected_replies:\n        zipinfo = zipfile.ZipFile(BytesIO(resp.data)).getinfo(\n            os.path.join(\n                source.journalist_filename,\n                \"{}_{}\".format(item.filename.split(\"-\")[0], source.last_updated.date()),\n                item.filename,\n            )\n        )\n        assert zipinfo\n\n        seen_file = SeenFile.query.filter_by(file_id=item.id, journalist_id=journo.id).one_or_none()\n        seen_message = SeenMessage.query.filter_by(\n            message_id=item.id, journalist_id=journo.id\n        ).one_or_none()\n        seen_reply = SeenReply.query.filter_by(\n            reply_id=item.id, journalist_id=journo.id\n        ).one_or_none()\n\n        if not seen_file and not seen_message and not seen_reply:\n            pytest.fail(\"no seen_file and no seen_message and no seen_reply\")\n\n    # The items not selected are absent from the zipfile\n    not_selected_submissions = set(submissions).difference(selected_submissions)\n    not_selected_replies = set(replies).difference(selected_replies)\n    not_selected = [i.filename for i in not_selected_submissions.union(not_selected_replies)]\n    for filename in not_selected:\n        with pytest.raises(KeyError):\n            zipfile.ZipFile(BytesIO(resp.data)).getinfo(\n                os.path.join(\n                    source.journalist_filename,\n                    source.journalist_designation,\n                    \"{}_{}\".format(filename.split(\"-\")[0], source.last_updated.date()),\n                    filename,\n                )\n            )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_download_selected_submissions_and_replies_previously_seen",
          "code": "def test_download_selected_submissions_and_replies_previously_seen(\n    journalist_app, test_journo, test_source, app_storage\n):\n    journo = Journalist.query.get(test_journo[\"id\"])\n    source = Source.query.get(test_source[\"id\"])\n    submissions = utils.db_helper.submit(app_storage, source, 4)\n    replies = utils.db_helper.reply(app_storage, journo, source, 4)\n    selected_submissions = random.sample(submissions, 2)\n    selected_replies = random.sample(replies, 2)\n    selected = [submission.filename for submission in selected_submissions + selected_replies]\n    selected.sort()\n\n    # Mark selected files, messages, and replies as seen\n    seen_file = SeenFile(file_id=selected_submissions[0].id, journalist_id=journo.id)\n    db.session.add(seen_file)\n    seen_message = SeenMessage(message_id=selected_submissions[1].id, journalist_id=journo.id)\n    db.session.add(seen_message)\n    mark_seen(selected_replies, journo)\n    db.session.commit()\n\n    with journalist_app.test_client() as app:\n        login_journalist(app, journo.username, test_journo[\"password\"], test_journo[\"otp_secret\"])\n        resp = app.post(\n            \"/bulk\",\n            data=dict(\n                action=\"download\",\n                filesystem_id=test_source[\"filesystem_id\"],\n                doc_names_selected=selected,\n            ),\n        )\n\n    # The download request was succesful, and the app returned a zipfile\n    assert resp.status_code == 200\n    assert resp.content_type == \"application/zip\"\n    assert zipfile.is_zipfile(BytesIO(resp.data))\n\n    # The items selected are in the zipfile and items are marked seen\n    for item in selected_submissions + selected_replies:\n        zipinfo = zipfile.ZipFile(BytesIO(resp.data)).getinfo(\n            os.path.join(\n                source.journalist_filename,\n                \"{}_{}\".format(item.filename.split(\"-\")[0], source.last_updated.date()),\n                item.filename,\n            )\n        )\n        assert zipinfo\n\n        seen_file = SeenFile.query.filter_by(file_id=item.id, journalist_id=journo.id).one_or_none()\n        seen_message = SeenMessage.query.filter_by(\n            message_id=item.id, journalist_id=journo.id\n        ).one_or_none()\n        seen_reply = SeenReply.query.filter_by(\n            reply_id=item.id, journalist_id=journo.id\n        ).one_or_none()\n\n        if not seen_file and not seen_message and not seen_reply:\n            pytest.fail(\"no seen_file and no seen_message and no seen_reply\")\n\n    # The items not selected are absent from the zipfile\n    not_selected_submissions = set(submissions).difference(selected_submissions)\n    not_selected_replies = set(replies).difference(selected_replies)\n    not_selected = [i.filename for i in not_selected_submissions.union(not_selected_replies)]\n    for filename in not_selected:\n        with pytest.raises(KeyError):\n            zipfile.ZipFile(BytesIO(resp.data)).getinfo(\n                os.path.join(\n                    source.journalist_filename,\n                    source.journalist_designation,\n                    \"{}_{}\".format(filename.split(\"-\")[0], source.last_updated.date()),\n                    filename,\n                )\n            )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_download_selected_submissions_previously_downloaded",
          "code": "def test_download_selected_submissions_previously_downloaded(\n    journalist_app, test_journo, test_source, app_storage\n):\n    journo = Journalist.query.get(test_journo[\"id\"])\n    source = Source.query.get(test_source[\"id\"])\n    submissions = utils.db_helper.submit(app_storage, source, 4)\n    replies = utils.db_helper.reply(app_storage, journo, source, 4)\n    selected_submissions = random.sample(submissions, 2)\n    selected_replies = random.sample(replies, 2)\n    selected = [submission.filename for submission in selected_submissions + selected_replies]\n    selected.sort()\n\n    # Mark selected submissions as downloaded\n    for submission in selected_submissions:\n        submission.downloaded = True\n        db.session.commit()\n\n    with journalist_app.test_client() as app:\n        login_journalist(app, journo.username, test_journo[\"password\"], test_journo[\"otp_secret\"])\n        resp = app.post(\n            \"/bulk\",\n            data=dict(\n                action=\"download\",\n                filesystem_id=test_source[\"filesystem_id\"],\n                doc_names_selected=selected,\n            ),\n        )\n\n    # The download request was succesful, and the app returned a zipfile\n    assert resp.status_code == 200\n    assert resp.content_type == \"application/zip\"\n    assert zipfile.is_zipfile(BytesIO(resp.data))\n\n    # The items selected are in the zipfile\n    for filename in selected:\n        zipinfo = zipfile.ZipFile(BytesIO(resp.data)).getinfo(\n            os.path.join(\n                source.journalist_filename,\n                \"{}_{}\".format(filename.split(\"-\")[0], source.last_updated.date()),\n                filename,\n            )\n        )\n        assert zipinfo\n\n    # The items not selected are absent from the zipfile\n    not_selected_submissions = set(submissions).difference(selected_submissions)\n    not_selected_replies = set(replies).difference(selected_replies)\n    not_selected = [i.filename for i in not_selected_submissions.union(not_selected_replies)]\n    for filename in not_selected:\n        with pytest.raises(KeyError):\n            zipfile.ZipFile(BytesIO(resp.data)).getinfo(\n                os.path.join(\n                    source.journalist_filename,\n                    source.journalist_designation,\n                    \"{}_{}\".format(filename.split(\"-\")[0], source.last_updated.date()),\n                    filename,\n                )\n            )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "selected_missing_files",
          "code": "def selected_missing_files(journalist_app, test_source, app_storage):\n    \"\"\"Fixture for the download tests with missing files in storage.\"\"\"\n    source = Source.query.get(test_source[\"id\"])\n    submissions = utils.db_helper.submit(app_storage, source, 2)\n    selected = sorted([s.filename for s in submissions])\n\n    storage_path = Path(app_storage.storage_path)\n    msg_files = sorted([p for p in storage_path.rglob(\"*\") if p.is_file()])\n    assert len(msg_files) == 2\n    for file in msg_files:\n        file.unlink()\n\n    return selected",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_download_selected_submissions_missing_files",
          "code": "def test_download_selected_submissions_missing_files(\n    journalist_app,\n    test_journo,\n    test_source,\n    mocker,\n    selected_missing_files,\n    app_storage,\n):\n    \"\"\"Tests download of selected submissions with missing files in storage.\"\"\"\n    mocked_error_logger = mocker.patch(\"journalist.app.logger.error\")\n    journo = Journalist.query.get(test_journo[\"id\"])\n\n    with journalist_app.test_client() as app:\n        login_journalist(app, journo.username, test_journo[\"password\"], test_journo[\"otp_secret\"])\n        resp = app.post(\n            url_for(\"main.bulk\"),\n            data=dict(\n                action=\"download\",\n                filesystem_id=test_source[\"filesystem_id\"],\n                doc_names_selected=selected_missing_files,\n            ),\n        )\n\n    assert resp.status_code == 302\n\n    expected_calls = []\n    for file in selected_missing_files:\n        missing_file = (\n            Path(app_storage.storage_path)\n            .joinpath(test_source[\"filesystem_id\"])\n            .joinpath(file)\n            .as_posix()\n        )\n        expected_calls.append(call(f\"File {missing_file} not found\"))\n\n    mocked_error_logger.assert_has_calls(expected_calls)",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_download_single_submission_missing_file",
          "code": "def test_download_single_submission_missing_file(\n    journalist_app,\n    test_journo,\n    test_source,\n    mocker,\n    selected_missing_files,\n    app_storage,\n):\n    \"\"\"Tests download of single submissions with missing files in storage.\"\"\"\n    mocked_error_logger = mocker.patch(\"journalist.app.logger.error\")\n    journo = Journalist.query.get(test_journo[\"id\"])\n    missing_file = selected_missing_files[0]\n\n    with journalist_app.test_client() as app:\n        login_journalist(app, journo.username, test_journo[\"password\"], test_journo[\"otp_secret\"])\n        resp = app.get(\n            url_for(\n                \"col.download_single_file\",\n                filesystem_id=test_source[\"filesystem_id\"],\n                fn=missing_file,\n            )\n        )\n\n    assert resp.status_code == 302\n\n    missing_file = (\n        Path(app_storage.storage_path)\n        .joinpath(test_source[\"filesystem_id\"])\n        .joinpath(missing_file)\n        .as_posix()\n    )\n\n    mocked_error_logger.assert_called_once_with(f\"File {missing_file} not found\")",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_download_unread_all_sources",
          "code": "def test_download_unread_all_sources(journalist_app, test_journo, app_storage):\n    \"\"\"\n    Test that downloading all unread creates a zip that contains all unread submissions from the\n    selected sources and marks these submissions as seen.\n    \"\"\"\n    journo = Journalist.query.get(test_journo[\"id\"])\n\n    bulk = utils.db_helper.bulk_setup_for_seen_only(journo, app_storage)\n\n    with journalist_app.test_client() as app:\n        login_journalist(app, journo.username, test_journo[\"password\"], test_journo[\"otp_secret\"])\n\n        # Select all sources supplied from bulk_download_setup\n        selected = []\n        for i in bulk:\n            source = i[\"source\"]\n            selected.append(source.filesystem_id)\n\n        # Download all unread submissions (not replies) from all sources selected\n        resp = app.post(\n            url_for(\"col.process\"),\n            data=dict(action=\"download-unread\", cols_selected=selected),\n        )\n\n    # The download request was succesful, and the app returned a zipfile\n    assert resp.status_code == 200\n    assert resp.content_type == \"application/zip\"\n    assert zipfile.is_zipfile(BytesIO(resp.data))\n\n    for i in bulk:\n        source = i[\"source\"]\n        seen_files = i[\"seen_files\"]\n        seen_messages = i[\"seen_messages\"]\n        seen_replies = i[\"seen_replies\"]\n        unseen_files = i[\"unseen_files\"]\n        unseen_messages = i[\"unseen_messages\"]\n        unseen_replies = i[\"unseen_replies\"]\n        not_downloaded = i[\"not_downloaded\"]\n\n        # Check that the zip file contains all submissions for the source that haven't been marked\n        # as downloaded and that they are now marked as seen in the database\n        for item in not_downloaded + unseen_files + unseen_messages:\n            zipinfo = zipfile.ZipFile(BytesIO(resp.data)).getinfo(\n                os.path.join(\n                    \"unread\",\n                    source.journalist_designation,\n                    \"{}_{}\".format(item.filename.split(\"-\")[0], source.last_updated.date()),\n                    item.filename,\n                )\n            )\n            assert zipinfo\n\n            seen_file = SeenFile.query.filter_by(\n                file_id=item.id, journalist_id=journo.id\n            ).one_or_none()\n            seen_message = SeenMessage.query.filter_by(\n                message_id=item.id, journalist_id=journo.id\n            ).one_or_none()\n\n            if not seen_file and not seen_message:\n                pytest.fail(\"no seen_file and no seen_message\")\n\n        # Check that the zip file does not contain any seen data or replies\n        zipf = zipfile.ZipFile(BytesIO(resp.data))\n        for item in seen_files + seen_messages + seen_replies + unseen_replies:\n            path = os.path.join(\n                \"unread\",\n                source.journalist_designation,\n                \"{}_{}\".format(item.filename.split(\"-\")[0], source.last_updated.date()),\n                item.filename,\n            )\n            with pytest.raises(KeyError):\n                zipf.getinfo(path)",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_download_all_selected_sources",
          "code": "def test_download_all_selected_sources(journalist_app, test_journo, app_storage):\n    \"\"\"\n    Test that downloading all selected sources creates zip that contains all submissions from the\n    selected sources and marks these submissions as seen.\n    \"\"\"\n    journo = Journalist.query.get(test_journo[\"id\"])\n\n    bulk = utils.db_helper.bulk_setup_for_seen_only(journo, app_storage)\n\n    with journalist_app.test_client() as app:\n        login_journalist(app, journo.username, test_journo[\"password\"], test_journo[\"otp_secret\"])\n\n        # Select all sources supplied from bulk_download_setup\n        selected = []\n        for i in bulk:\n            source = i[\"source\"]\n            selected.append(source.filesystem_id)\n\n        # Download all submissions from all sources selected\n        resp = app.post(\n            url_for(\"col.process\"),\n            data=dict(action=\"download-all\", cols_selected=selected),\n        )\n\n    # The download request was succesful, and the app returned a zipfile\n    assert resp.status_code == 200\n    assert resp.content_type == \"application/zip\"\n    assert zipfile.is_zipfile(BytesIO(resp.data))\n\n    for i in bulk:\n        source = i[\"source\"]\n        seen_files = i[\"seen_files\"]\n        seen_messages = i[\"seen_messages\"]\n        seen_replies = i[\"seen_replies\"]\n        unseen_files = i[\"unseen_files\"]\n        unseen_messages = i[\"unseen_messages\"]\n        unseen_replies = i[\"unseen_replies\"]\n        not_downloaded = i[\"not_downloaded\"]\n\n        # Check that the zip file contains all submissions for the source\n        for item in not_downloaded + unseen_files + unseen_messages + seen_files + seen_messages:\n            zipinfo = zipfile.ZipFile(BytesIO(resp.data)).getinfo(\n                os.path.join(\n                    \"all\",\n                    source.journalist_designation,\n                    \"{}_{}\".format(item.filename.split(\"-\")[0], source.last_updated.date()),\n                    item.filename,\n                )\n            )\n            assert zipinfo\n\n            seen_file = SeenFile.query.filter_by(\n                file_id=item.id, journalist_id=journo.id\n            ).one_or_none()\n            seen_message = SeenMessage.query.filter_by(\n                message_id=item.id, journalist_id=journo.id\n            ).one_or_none()\n\n            if not seen_file and not seen_message:\n                pytest.fail(\"no seen_file and no seen_message\")\n\n        # Check that the zip file does not contain any replies\n        zipf = zipfile.ZipFile(BytesIO(resp.data))\n        for item in seen_replies + unseen_replies:\n            path = os.path.join(\n                \"unread\",\n                source.journalist_designation,\n                \"{}_{}\".format(item.filename.split(\"-\")[0], source.last_updated.date()),\n                item.filename,\n            )\n            with pytest.raises(KeyError):\n                zipf.getinfo(path)",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_single_source_is_successfully_starred",
          "code": "def test_single_source_is_successfully_starred(journalist_app, test_journo, test_source):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.post(url_for(\"col.add_star\", filesystem_id=test_source[\"filesystem_id\"]))\n\n            ins.assert_redirects(resp, url_for(\"main.index\"))\n\n    source = Source.query.get(test_source[\"id\"])\n    assert source.star.starred",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_single_source_is_successfully_unstarred",
          "code": "def test_single_source_is_successfully_unstarred(journalist_app, test_journo, test_source):\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n        # First star the source\n        app.post(url_for(\"col.add_star\", filesystem_id=test_source[\"filesystem_id\"]))\n\n        with InstrumentedApp(journalist_app) as ins:\n            # Now unstar the source\n            resp = app.post(url_for(\"col.remove_star\", filesystem_id=test_source[\"filesystem_id\"]))\n\n            ins.assert_redirects(resp, url_for(\"main.index\"))\n\n        source = Source.query.get(test_source[\"id\"])\n        assert not source.star.starred",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_journalist_session_expiration",
          "code": "def test_journalist_session_expiration(journalist_app, test_journo, locale):\n    # set the expiration to be very short\n    journalist_app.session_interface.lifetime = 1\n    with journalist_app.test_client() as app:\n        with InstrumentedApp(journalist_app) as ins:\n            login_data = {\n                \"username\": test_journo[\"username\"],\n                \"password\": test_journo[\"password\"],\n                \"token\": TOTP(test_journo[\"otp_secret\"]).now(),\n            }\n            resp = app.post(url_for(\"main.login\"), data=login_data)\n            ins.assert_redirects(resp, url_for(\"main.index\"))\n        assert \"uid\" in session\n\n        # Wait 2s for the redis key to expire\n        time.sleep(2)\n        resp = app.get(url_for(\"account.edit\"), follow_redirects=True)\n        # because the session is being cleared when it expires, the\n        # response should always be in English.\n        assert page_language(resp.data) == \"en-US\"\n        assert \"Log in to access the journalist interface\" in resp.data.decode(\"utf-8\")\n\n        # check that the session was cleared (apart from 'expires'\n        # which is always present and 'csrf_token' which leaks no info)\n        session.pop(\"expires\", None)\n        session.pop(\"csrf_token\", None)\n        session.pop(\"locale\", None)\n        session.pop(\"renew_count\", None)\n        assert not session, session",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_csrf_error_page",
          "code": "def test_csrf_error_page(config, journalist_app, locale):\n    # get the locale into the session\n    with journalist_app.test_client() as app:\n        resp = app.get(url_for(\"main.login\", l=locale))\n        assert page_language(resp.data) == language_tag(locale)\n        msgids = [\"Show password\"]\n        with xfail_untranslated_messages(config, locale, msgids):\n            assert gettext(msgids[0]) in resp.data.decode(\"utf-8\")\n\n    journalist_app.config[\"WTF_CSRF_ENABLED\"] = True\n    with journalist_app.test_client() as app:\n        # /login without a CSRF token should redirect...\n        with InstrumentedApp(journalist_app) as ins:\n            resp = app.post(url_for(\"main.login\"))\n            ins.assert_redirects(resp, url_for(\"main.login\"))\n\n        resp = app.post(url_for(\"main.login\"), follow_redirects=True)\n\n        # ...and show an error.  (Because the session is being cleared when it\n        # expires, the response should always be in English.)\n        assert page_language(resp.data) == \"en-US\"\n        assert (\n            \"You have been logged out due to inactivity or a problem with your session.\"\n            in resp.data.decode(\"utf-8\")\n        )\n\n        # /logout without a CSRF token should also error.\n        resp = app.post(url_for(\"main.logout\"), follow_redirects=True)\n        assert (\n            \"You have been logged out due to inactivity or a problem with your session.\"\n            in resp.data.decode(\"utf-8\")\n        )",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_col_process_aborts_with_bad_action",
          "code": "def test_col_process_aborts_with_bad_action(journalist_app, test_journo):\n    \"\"\"If the action is not a valid choice, a 500 should occur\"\"\"\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n\n        form_data = {\n            \"cols_selected\": \"does not matter\",\n            \"action\": \"this action does not exist\",\n        }\n\n        resp = app.post(url_for(\"col.process\"), data=form_data)\n        assert resp.status_code == 500",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_col_process_successfully_deletes_multiple_sources",
          "code": "def test_col_process_successfully_deletes_multiple_sources(\n    journalist_app, test_journo, app_storage\n):\n    # Create two sources with one submission each\n    source_1, _ = utils.db_helper.init_source(app_storage)\n    utils.db_helper.submit(app_storage, source_1, 1)\n    source_2, _ = utils.db_helper.init_source(app_storage)\n    utils.db_helper.submit(app_storage, source_2, 1)\n    source_3, _ = utils.db_helper.init_source(app_storage)\n    utils.db_helper.submit(app_storage, source_3, 1)\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n\n        form_data = {\n            \"cols_selected\": [source_1.filesystem_id, source_2.filesystem_id],\n            \"action\": \"delete\",\n        }\n\n        resp = app.post(url_for(\"col.process\"), data=form_data, follow_redirects=True)\n\n        assert resp.status_code == 200\n\n    # simulate the source_deleter's work\n    journalist_app_module.utils.purge_deleted_sources()\n\n    # Verify that all of the specified sources were deleted, but no others\n    remaining_sources = Source.query.all()\n    assert len(remaining_sources) == 1\n    assert remaining_sources[0].uuid == source_3.uuid",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_col_process_successfully_stars_sources",
          "code": "def test_col_process_successfully_stars_sources(\n    journalist_app, test_journo, test_source, app_storage\n):\n    utils.db_helper.submit(app_storage, test_source[\"source\"], 1)\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n\n        form_data = {\"cols_selected\": [test_source[\"filesystem_id\"]], \"action\": \"star\"}\n\n        resp = app.post(url_for(\"col.process\"), data=form_data, follow_redirects=True)\n        assert resp.status_code == 200\n\n    source = Source.query.get(test_source[\"id\"])\n    assert source.star.starred",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_col_process_successfully_unstars_sources",
          "code": "def test_col_process_successfully_unstars_sources(\n    journalist_app, test_journo, test_source, app_storage\n):\n    utils.db_helper.submit(app_storage, test_source[\"source\"], 1)\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n\n        # First star the source\n        form_data = {\"cols_selected\": [test_source[\"filesystem_id\"]], \"action\": \"star\"}\n        app.post(url_for(\"col.process\"), data=form_data, follow_redirects=True)\n\n        # Now unstar the source\n        form_data = {\n            \"cols_selected\": [test_source[\"filesystem_id\"]],\n            \"action\": \"un-star\",\n        }\n        resp = app.post(url_for(\"col.process\"), data=form_data, follow_redirects=True)\n\n    assert resp.status_code == 200\n\n    source = Source.query.get(test_source[\"id\"])\n    assert not source.star.starred",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_source_with_null_last_updated",
          "code": "def test_source_with_null_last_updated(journalist_app, test_journo, test_files):\n    \"\"\"Regression test for issues #3862\"\"\"\n\n    source = test_files[\"source\"]\n    source.last_updated = None\n    db.session.add(source)\n    db.session.commit()\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app,\n            test_journo[\"username\"],\n            test_journo[\"password\"],\n            test_journo[\"otp_secret\"],\n        )\n        resp = app.get(url_for(\"main.index\"))\n        assert resp.status_code == 200",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_does_set_cookie_headers",
          "code": "def test_does_set_cookie_headers(journalist_app, test_journo):\n    with journalist_app.test_client() as app:\n        response = app.get(url_for(\"main.login\"))\n\n        observed_headers = response.headers\n        assert \"Set-Cookie\" in list(observed_headers.keys())\n        assert \"Cookie\" in observed_headers[\"Vary\"]",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_app_error_handlers_defined",
          "code": "def test_app_error_handlers_defined(journalist_app):\n    for status_code in [400, 401, 403, 404, 500]:\n        # This will raise KeyError if an app-wide error handler is not defined\n        assert journalist_app.error_handler_spec[None][status_code]",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_lazy_deleted_journalist_creation",
          "code": "def test_lazy_deleted_journalist_creation(journalist_app):\n    \"\"\"test lazy creation of \"deleted\" journalist works\"\"\"\n    not_found = Journalist.query.filter_by(username=\"deleted\").one_or_none()\n    assert not_found is None, \"deleted journalist doesn't exist yet\"\n    deleted = Journalist.get_deleted()\n    db.session.commit()\n    # Can be found as a normal Journalist object\n    found = Journalist.query.filter_by(username=\"deleted\").one()\n    assert deleted.uuid == found.uuid\n    assert found.is_deleted_user() is True\n    # And get_deleted() now returns the same instance\n    deleted2 = Journalist.get_deleted()\n    assert deleted.uuid == deleted2.uuid",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_journalist_deletion",
          "code": "def test_journalist_deletion(journalist_app, app_storage):\n    \"\"\"test deleting a journalist and see data reassociated to \"deleted\" journalist\"\"\"\n    # Create a journalist that's seen two replies and has a login attempt\n    source, _ = utils.db_helper.init_source(app_storage)\n    journalist, _ = utils.db_helper.init_journalist()\n    db.session.add(JournalistLoginAttempt(journalist))\n    replies = utils.db_helper.reply(app_storage, journalist, source, 2)\n    # Create a second journalist that's seen those replies\n    journalist2, _ = utils.db_helper.init_journalist()\n    for reply in replies:\n        db.session.add(SeenReply(reply=reply, journalist=journalist2))\n    db.session.commit()\n    # Only one login attempt in the table\n    assert len(JournalistLoginAttempt.query.all()) == 1\n    # And four SeenReply instances\n    assert len(SeenReply.query.all()) == 4\n    # Delete the journalists\n    journalist.delete()\n    journalist2.delete()\n    db.session.commit()\n    # Verify the \"deleted\" journalist has 2 associated rows of both types\n    deleted = Journalist.get_deleted()\n    assert len(Reply.query.filter_by(journalist_id=deleted.id).all()) == 2\n    assert len(SeenReply.query.filter_by(journalist_id=deleted.id).all()) == 2\n    # And there are no login attempts\n    assert JournalistLoginAttempt.query.all() == []",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_user_sees_os_warning_if_server_past_eol",
          "code": "def test_user_sees_os_warning_if_server_past_eol(config, journalist_app, test_journo):\n    journalist_app.config.update(OS_PAST_EOL=True, OS_NEAR_EOL=False)\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n\n        resp = app.get(url_for(\"main.index\"))\n\n    text = resp.data.decode(\"utf-8\")\n    assert 'id=\"os-past-eol\"' in text, text\n    assert 'id=\"os-near-eol\"' not in text, text",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_user_sees_os_warning_if_migration_fixes",
          "code": "def test_user_sees_os_warning_if_migration_fixes(config, journalist_app, test_journo):\n    journalist_app.config.update(OS_PAST_EOL=False, OS_NEEDS_MIGRATION_FIXES=True)\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n\n        resp = app.get(url_for(\"main.index\"))\n\n    text = resp.data.decode(\"utf-8\")\n    assert 'id=\"os-past-eol\"' not in text, text\n    assert 'id=\"os-near-eol\"' in text, text",
          "file": "test_journalist.py"
        },
        {
          "type": "function",
          "name": "test_user_does_not_see_os_warning_if_server_is_current",
          "code": "def test_user_does_not_see_os_warning_if_server_is_current(config, journalist_app, test_journo):\n    journalist_app.config.update(OS_PAST_EOL=False, OS_NEEDS_MIGRATION_FIXES=False)\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n\n        resp = app.get(url_for(\"main.index\"))\n\n    text = resp.data.decode(\"utf-8\")\n    assert 'id=\"os-past-eol\"' not in text, text\n    assert 'id=\"os-near-eol\"' not in text, text",
          "file": "test_journalist.py"
        }
      ],
      "test_config.py": [
        {
          "type": "function",
          "name": "test_parse_2014_config",
          "code": "def test_parse_2014_config():\n    # Given a config file from 2014\n    config_module_from_2014 = \"tests.config_from_2014\"\n\n    # When trying to parse it, it succeeds\n    assert _parse_config_from_file(config_module_from_2014)",
          "file": "test_config.py"
        },
        {
          "type": "function",
          "name": "test_parse_current_config",
          "code": "def test_parse_current_config():\n    # Given a config file that is current; copy the example file to a proper Python module\n    current_sample_config = Path(__file__).absolute().parent.parent / \"config.py.example\"\n    current_config_module = \"config_current\"\n    current_config_file = Path(__file__).absolute().parent / f\"{current_config_module}.py\"\n    try:\n        current_config_file.write_text(current_sample_config.read_text())\n\n        # When trying to parse it, it succeeds...\n        parsed_config = _parse_config_from_file(f\"tests.{current_config_module}\")\n        assert parsed_config\n\n        # ...and has one of our `env` values rather than one of Flask's:\n        assert parsed_config.env in [\"prod\", \"dev\", \"test\"]\n\n    finally:\n        current_config_file.unlink(missing_ok=True)",
          "file": "test_config.py"
        }
      ],
      "test_integration.py": [
        {
          "type": "function",
          "name": "test_submit_message",
          "code": "def test_submit_message(journalist_app, source_app, test_journo, app_storage):\n    \"\"\"When a source creates an account, test that a new entry appears\n    in the journalist interface\"\"\"\n    test_msg = \"This is a test message.\"\n\n    with source_app.test_client() as app:\n        app.post(\"/generate\", data=GENERATE_DATA)\n        tab_id = next(iter(session[\"codenames\"].keys()))\n        app.post(\"/create\", data={\"tab_id\": tab_id}, follow_redirects=True)\n        source_user = SessionManager.get_logged_in_user(db_session=db.session)\n        filesystem_id = source_user.filesystem_id\n\n        # redirected to submission form\n        resp = app.post(\n            \"/submit\",\n            data=dict(\n                msg=test_msg,\n                fh=(BytesIO(b\"\"), \"\"),\n            ),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        resp = app.post(\"/logout\")\n        assert resp.status_code == 200\n\n    # Request the Journalist Interface index\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n        resp = app.get(\"/\")\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Sources\" in text\n        soup = BeautifulSoup(text, \"html.parser\")\n\n        # The source should have a \"download unread\" link that\n        # says \"1 unread\"\n        col = soup.select(\"table#collections tr.source\")[0]\n        unread_span = col.select(\"td.unread a\")[0]\n        assert \"1 unread\" in unread_span.get_text()\n\n        col_url = soup.select(\"table#collections th.designation a\")[0][\"href\"]\n        resp = app.get(col_url)\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        soup = BeautifulSoup(text, \"html.parser\")\n        submission_url = soup.select(\"table#submissions th.filename a\")[0][\"href\"]\n        assert \"-msg\" in submission_url\n        size = soup.select(\"table#submissions td.info\")[0]\n        assert re.compile(r\"\\d+ bytes\").match(size[\"title\"])\n\n        resp = app.get(submission_url)\n        assert resp.status_code == 200\n\n        decryption_result = utils.decrypt_as_journalist(resp.data).decode()\n        assert decryption_result == test_msg\n\n        # delete submission\n        resp = app.get(col_url)\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        soup = BeautifulSoup(text, \"html.parser\")\n        doc_name = soup.select(\n            'table#submissions > tr.submission > td.status input[name=\"doc_names_selected\"]'\n        )[0][\"value\"]\n        resp = app.post(\n            \"/bulk\",\n            data=dict(\n                action=\"delete\",\n                filesystem_id=filesystem_id,\n                doc_names_selected=doc_name,\n            ),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        soup = BeautifulSoup(text, \"html.parser\")\n        assert \"The item has been deleted.\" in text\n\n        # confirm that submission deleted and absent in list of submissions\n        resp = app.get(col_url)\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"No submissions to display.\" in text\n\n        # the file should be deleted from the filesystem\n        # since file deletion is handled by a polling worker, this test\n        # needs to wait for the worker to get the job and execute it\n        def assertion():\n            assert not (os.path.exists(app_storage.path(filesystem_id, doc_name)))\n\n        utils.asynchronous.wait_for_assertion(assertion)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "assertion",
          "code": "def assertion():\n            assert not (os.path.exists(app_storage.path(filesystem_id, doc_name)))",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "test_submit_file",
          "code": "def test_submit_file(journalist_app, source_app, test_journo, app_storage):\n    \"\"\"When a source creates an account, test that a new entry appears\n    in the journalist interface\"\"\"\n    test_file_contents = b\"This is a test file.\"\n    test_filename = \"test.txt\"\n\n    with source_app.test_client() as app:\n        app.post(\"/generate\", data=GENERATE_DATA)\n        tab_id = next(iter(session[\"codenames\"].keys()))\n        app.post(\"/create\", data={\"tab_id\": tab_id}, follow_redirects=True)\n        source_user = SessionManager.get_logged_in_user(db_session=db.session)\n        filesystem_id = source_user.filesystem_id\n\n        # redirected to submission form\n        resp = app.post(\n            \"/submit\",\n            data=dict(\n                msg=\"\",\n                fh=(BytesIO(test_file_contents), test_filename),\n            ),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        resp = app.post(\"/logout\")\n        assert resp.status_code == 200\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n        resp = app.get(\"/\")\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Sources\" in text\n        soup = BeautifulSoup(text, \"html.parser\")\n\n        # The source should have a \"download unread\" link that says\n        # \"1 unread\"\n        col = soup.select(\"table#collections tr.source\")[0]\n        unread_span = col.select(\"td.unread a\")[0]\n        assert \"1 unread\" in unread_span.get_text()\n\n        col_url = soup.select(\"table#collections th.designation a\")[0][\"href\"]\n        resp = app.get(col_url)\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        soup = BeautifulSoup(text, \"html.parser\")\n        submission_url = soup.select(\"table#submissions th.filename a\")[0][\"href\"]\n        assert \"-doc\" in submission_url\n        size = soup.select(\"table#submissions td.info\")[0]\n        assert re.compile(r\"\\d+ bytes\").match(size[\"title\"])\n\n        resp = app.get(submission_url)\n        assert resp.status_code == 200\n\n        decrypted_data = utils.decrypt_as_journalist(resp.data)\n        sio = BytesIO(decrypted_data)\n        with gzip.GzipFile(mode=\"rb\", fileobj=sio) as gzip_file:\n            unzipped_decrypted_data = gzip_file.read()\n            mtime = gzip_file.mtime\n        assert unzipped_decrypted_data == test_file_contents\n        # Verify gzip file metadata and ensure timestamp is not present.\n        assert mtime == 0\n\n        # delete submission\n        resp = app.get(col_url)\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        soup = BeautifulSoup(text, \"html.parser\")\n        doc_name = soup.select(\n            'table#submissions > tr.submission > td.status input[name=\"doc_names_selected\"]'\n        )[0][\"value\"]\n        resp = app.post(\n            \"/bulk\",\n            data=dict(\n                action=\"delete\",\n                filesystem_id=filesystem_id,\n                doc_names_selected=doc_name,\n            ),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"The item has been deleted.\" in text\n        soup = BeautifulSoup(resp.data, \"html.parser\")\n\n        # confirm that submission deleted and absent in list of submissions\n        resp = app.get(col_url)\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"No submissions to display.\" in text\n\n        # the file should be deleted from the filesystem\n        # since file deletion is handled by a polling worker, this test\n        # needs to wait for the worker to get the job and execute it\n        def assertion():\n            assert not (os.path.exists(app_storage.path(filesystem_id, doc_name)))\n\n        utils.asynchronous.wait_for_assertion(assertion)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "assertion",
          "code": "def assertion():\n            assert not (os.path.exists(app_storage.path(filesystem_id, doc_name)))",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "_helper_test_reply",
          "code": "def _helper_test_reply(journalist_app, source_app, test_journo, test_reply):\n    test_msg = \"This is a test message.\"\n\n    with source_app.test_client() as app:\n        app.post(\"/generate\", data=GENERATE_DATA)\n        tab_id, codename = next(iter(session[\"codenames\"].items()))\n        app.post(\"/create\", data={\"tab_id\": tab_id}, follow_redirects=True)\n        # redirected to submission form\n        resp = app.post(\n            \"/submit\",\n            data=dict(\n                msg=test_msg,\n                fh=(BytesIO(b\"\"), \"\"),\n            ),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        source_user = SessionManager.get_logged_in_user(db_session=db.session)\n        filesystem_id = source_user.filesystem_id\n        resp = app.post(\"/logout\")\n        assert resp.status_code == 200\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n        resp = app.get(\"/\")\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Sources\" in text\n        soup = BeautifulSoup(resp.data, \"html.parser\")\n        col_url = soup.select(\"table#collections tr.source > th.designation a\")[0][\"href\"]\n\n        resp = app.get(col_url)\n        assert resp.status_code == 200\n\n    # Create 2 replies to test deleting on journalist and source interface\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n        for _i in range(2):\n            resp = app.post(\n                \"/reply\",\n                data=dict(filesystem_id=filesystem_id, message=test_reply),\n                follow_redirects=True,\n            )\n            assert resp.status_code == 200\n\n        text = resp.data.decode(\"utf-8\")\n        assert \"The source will receive your reply\" in text\n\n        resp = app.get(col_url)\n        text = resp.data.decode(\"utf-8\")\n        assert \"reply-\" in text\n\n    soup = BeautifulSoup(text, \"html.parser\")\n\n    # Download the reply and verify that it can be decrypted with the\n    # journalist's key as well as the source's reply key\n    filesystem_id = soup.select('input[name=\"filesystem_id\"]')[0][\"value\"]\n    checkbox_values = [soup.select('input[name=\"doc_names_selected\"]')[1][\"value\"]]\n    resp = app.post(\n        \"/bulk\",\n        data=dict(\n            filesystem_id=filesystem_id,\n            action=\"download\",\n            doc_names_selected=checkbox_values,\n        ),\n        follow_redirects=True,\n    )\n    assert resp.status_code == 200\n\n    zf = zipfile.ZipFile(BytesIO(resp.data), \"r\")\n    data = zf.read(zf.namelist()[0])\n    journalist_decrypted = utils.decrypt_as_journalist(data).decode()\n    assert journalist_decrypted == test_reply\n    source_decrypted = EncryptionManager.get_default().decrypt_journalist_reply(source_user, data)\n    assert source_decrypted == test_reply\n\n    # Test deleting reply on the journalist interface\n    last_reply_number = len(soup.select('input[name=\"doc_names_selected\"]')) - 1\n    _helper_filenames_delete(app, soup, last_reply_number)\n\n    with source_app.test_client() as app:\n        resp = app.post(\"/login\", data=dict(codename=codename), follow_redirects=True)\n        assert resp.status_code == 200\n        resp = app.get(\"/lookup\")\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n\n        assert \"You have received a reply. To protect your identity\" in text\n        assert test_reply in text, text\n        soup = BeautifulSoup(text, \"html.parser\")\n        msgid = soup.select('form > input[name=\"reply_filename\"]')[0][\"value\"]\n        resp = app.post(\n            \"/delete\",\n            data=dict(filesystem_id=filesystem_id, reply_filename=msgid),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert \"Reply deleted\" in text\n\n        resp = app.post(\"/logout\")\n        assert resp.status_code == 200",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "_helper_filenames_delete",
          "code": "def _helper_filenames_delete(journalist_app, soup, i):\n    filesystem_id = soup.select('input[name=\"filesystem_id\"]')[0][\"value\"]\n    checkbox_values = [soup.select('input[name=\"doc_names_selected\"]')[i][\"value\"]]\n\n    # delete\n    resp = journalist_app.post(\n        \"/bulk\",\n        data=dict(\n            filesystem_id=filesystem_id,\n            action=\"delete\",\n            doc_names_selected=checkbox_values,\n        ),\n        follow_redirects=True,\n    )\n    assert resp.status_code == 200\n    assert \"The item has been deleted.\" in resp.data.decode(\"utf-8\")\n\n    # Make sure the files were deleted from the filesystem\n    def assertion():\n        assert not any(\n            [\n                os.path.exists(Storage.get_default().path(filesystem_id, doc_name))\n                for doc_name in checkbox_values\n            ]\n        )\n\n    utils.asynchronous.wait_for_assertion(assertion)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "assertion",
          "code": "def assertion():\n        assert not any(\n            [\n                os.path.exists(Storage.get_default().path(filesystem_id, doc_name))\n                for doc_name in checkbox_values\n            ]\n        )",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "test_reply_normal",
          "code": "def test_reply_normal(journalist_app, source_app, test_journo):\n    \"\"\"Test for regression on #1360 (failure to encode bytes before calling\n    gpg functions).\n    \"\"\"\n    encryption_mgr = EncryptionManager.get_default()\n    encryption_mgr.gpg()  # lazily initialize GPG\n    with mock.patch.object(encryption_mgr._gpg, \"_encoding\", \"ansi_x3.4_1968\"):\n        _helper_test_reply(\n            journalist_app,\n            source_app,\n            test_journo,\n            \"This is a test reply.\",\n        )",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "test_unicode_reply_with_ansi_env",
          "code": "def test_unicode_reply_with_ansi_env(journalist_app, source_app, test_journo):\n    # This makes python-gnupg handle encoding equivalent to if we were\n    # running SD in an environment where os.getenv(\"LANG\") == \"C\".\n    # Unfortunately, with the way our test suite is set up simply setting\n    # that env var here will not have the desired effect. Instead we\n    # monkey-patch the GPG object that is called crypto_util to imitate the\n    # _encoding attribute it would have had it been initialized in a \"C\"\n    # environment. See\n    # https://github.com/freedomofpress/securedrop/issues/1360 for context.\n    encryption_mgr = EncryptionManager.get_default()\n    encryption_mgr.gpg()  # lazily initialize GPG\n    with mock.patch.object(encryption_mgr._gpg, \"_encoding\", \"ansi_x3.4_1968\"):\n        _helper_test_reply(\n            journalist_app,\n            source_app,\n            test_journo,\n            \"ᚠᛇᚻ᛫ᛒᛦᚦ᛫ᚠᚱᚩᚠᚢᚱ᛫ᚠᛁᚱᚪ᛫ᚷᛖᚻᚹᛦᛚᚳᚢᛗ\",\n        )",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "test_delete_collection",
          "code": "def test_delete_collection(mocker, source_app, journalist_app, test_journo):\n    \"\"\"Test the \"delete collection\" button on each collection page\"\"\"\n\n    # first, add a source\n    with source_app.test_client() as app:\n        app.post(\"/generate\", data=GENERATE_DATA)\n        tab_id = next(iter(session[\"codenames\"].keys()))\n        app.post(\"/create\", data={\"tab_id\": tab_id})\n        resp = app.post(\n            \"/submit\",\n            data=dict(\n                msg=\"This is a test.\",\n                fh=(BytesIO(b\"\"), \"\"),\n            ),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n        resp = app.get(\"/\")\n        # navigate to the collection page\n        soup = BeautifulSoup(resp.data.decode(\"utf-8\"), \"html.parser\")\n        first_col_url = soup.select(\"table#collections tr.source > th.designation a\")[0][\"href\"]\n        resp = app.get(first_col_url)\n        assert resp.status_code == 200\n\n        # find the delete form and extract the post parameters\n        soup = BeautifulSoup(resp.data.decode(\"utf-8\"), \"html.parser\")\n        delete_form_inputs = soup.select(\"form#delete-collection\")[0](\"input\")\n        filesystem_id = delete_form_inputs[1][\"value\"]\n        col_name = delete_form_inputs[2][\"value\"]\n\n        resp = app.post(\"/col/delete/\" + filesystem_id, follow_redirects=True)\n        assert resp.status_code == 200\n\n        text = resp.data.decode(\"utf-8\")\n        assert escape(f\"The account and data for the source {col_name} have been deleted.\") in text\n\n        assert \"There are no submissions.\" in text\n\n        # Make sure the collection is deleted from the filesystem\n        def assertion():\n            assert not os.path.exists(Storage.get_default().path(filesystem_id))\n\n        utils.asynchronous.wait_for_assertion(assertion)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "assertion",
          "code": "def assertion():\n            assert not os.path.exists(Storage.get_default().path(filesystem_id))",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "test_delete_collections",
          "code": "def test_delete_collections(mocker, journalist_app, source_app, test_journo):\n    \"\"\"Test the \"delete selected\" checkboxes on the index page that can be\n    used to delete multiple collections\"\"\"\n\n    # first, add some sources\n    with source_app.test_client() as app:\n        num_sources = 2\n        for i in range(num_sources):\n            app.post(\"/generate\", data=GENERATE_DATA)\n            tab_id = next(iter(session[\"codenames\"].keys()))\n            app.post(\"/create\", data={\"tab_id\": tab_id})\n            app.post(\n                \"/submit\",\n                data=dict(\n                    msg=\"This is a test \" + str(i) + \".\",\n                    fh=(BytesIO(b\"\"), \"\"),\n                ),\n                follow_redirects=True,\n            )\n            resp = app.post(\"/logout\")\n            assert resp.status_code == 200\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n        resp = app.get(\"/\")\n        # get all the checkbox values\n        soup = BeautifulSoup(resp.data.decode(\"utf-8\"), \"html.parser\")\n        checkbox_values = [\n            checkbox[\"value\"] for checkbox in soup.select('input[name=\"cols_selected\"]')\n        ]\n\n        resp = app.post(\n            \"/col/process\",\n            data=dict(action=\"delete\", cols_selected=checkbox_values),\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n        text = resp.data.decode(\"utf-8\")\n        assert f\"The accounts and all data for {num_sources} sources\" in text\n\n        # simulate the source_deleter's work\n        journalist_app_module.utils.purge_deleted_sources()\n\n        # Make sure the collections are deleted from the filesystem\n        def assertion():\n            assert not (\n                any(\n                    [\n                        os.path.exists(Storage.get_default().path(filesystem_id))\n                        for filesystem_id in checkbox_values\n                    ]\n                )\n            )\n\n        utils.asynchronous.wait_for_assertion(assertion)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "assertion",
          "code": "def assertion():\n            assert not (\n                any(\n                    [\n                        os.path.exists(Storage.get_default().path(filesystem_id))\n                        for filesystem_id in checkbox_values\n                    ]\n                )\n            )",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "_helper_filenames_submit",
          "code": "def _helper_filenames_submit(app):\n    app.post(\n        \"/submit\",\n        data=dict(\n            msg=\"This is a test.\",\n            fh=(BytesIO(b\"\"), \"\"),\n        ),\n        follow_redirects=True,\n    )\n    app.post(\n        \"/submit\",\n        data=dict(\n            msg=\"This is a test.\",\n            fh=(BytesIO(b\"This is a test\"), \"test.txt\"),\n        ),\n        follow_redirects=True,\n    )\n    app.post(\n        \"/submit\",\n        data=dict(\n            msg=\"\",\n            fh=(BytesIO(b\"This is a test\"), \"test.txt\"),\n        ),\n        follow_redirects=True,\n    )",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "test_filenames",
          "code": "def test_filenames(source_app, journalist_app, test_journo):\n    \"\"\"Test pretty, sequential filenames when source uploads messages\n    and files\"\"\"\n    # add a source and submit stuff\n    with source_app.test_client() as app:\n        app.post(\"/generate\", data=GENERATE_DATA)\n        tab_id = next(iter(session[\"codenames\"].keys()))\n        app.post(\"/create\", data={\"tab_id\": tab_id})\n        _helper_filenames_submit(app)\n\n    # navigate to the collection page\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n        resp = app.get(\"/\")\n        soup = BeautifulSoup(resp.data.decode(\"utf-8\"), \"html.parser\")\n        first_col_url = soup.select(\"table#collections tr.source > th.designation a\")[0][\"href\"]\n        resp = app.get(first_col_url)\n        assert resp.status_code == 200\n\n        # test filenames and sort order\n        soup = BeautifulSoup(resp.data.decode(\"utf-8\"), \"html.parser\")\n        submission_filename_re = r\"^{0}-[a-z0-9-_]+(-msg|-doc\\.gz)\\.gpg$\"\n        for i, submission_link in enumerate(\n            soup.select(\"table#submissions tr.submission > th.filename a\")\n        ):\n            filename = str(submission_link.contents[0])\n            assert re.match(submission_filename_re.format(i + 1), filename)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "test_filenames_delete",
          "code": "def test_filenames_delete(journalist_app, source_app, test_journo):\n    \"\"\"Test pretty, sequential filenames when journalist deletes files\"\"\"\n    # add a source and submit stuff\n    with source_app.test_client() as app:\n        app.post(\"/generate\", data=GENERATE_DATA)\n        tab_id = next(iter(session[\"codenames\"].keys()))\n        app.post(\"/create\", data={\"tab_id\": tab_id})\n        _helper_filenames_submit(app)\n\n    # navigate to the collection page\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n        resp = app.get(\"/\")\n        soup = BeautifulSoup(resp.data.decode(\"utf-8\"), \"html.parser\")\n        first_col_url = soup.select(\"table#collections tr.source > th.designation a\")[0][\"href\"]\n        resp = app.get(first_col_url)\n        assert resp.status_code == 200\n        soup = BeautifulSoup(resp.data.decode(\"utf-8\"), \"html.parser\")\n\n        # delete file #2\n        _helper_filenames_delete(app, soup, 1)\n        resp = app.get(first_col_url)\n        soup = BeautifulSoup(resp.data.decode(\"utf-8\"), \"html.parser\")\n\n        # test filenames and sort order\n        submission_filename_re = r\"^{0}-[a-z0-9-_]+(-msg|-doc\\.gz)\\.gpg$\"\n        filename = str(\n            soup.select(\"table#submissions tr.submission > th.filename a\")[0].contents[0]\n        )\n        assert re.match(submission_filename_re.format(1), filename)\n        filename = str(\n            soup.select(\"table#submissions tr.submission > th.filename a\")[1].contents[0]\n        )\n        assert re.match(submission_filename_re.format(3), filename)\n        filename = str(\n            soup.select(\"table#submissions tr.submission > th.filename a\")[2].contents[0]\n        )\n        assert re.match(submission_filename_re.format(4), filename)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "test_user_change_password",
          "code": "def test_user_change_password(journalist_app, test_journo):\n    \"\"\"Test that a journalist can successfully login after changing\n    their password\"\"\"\n\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app, test_journo[\"username\"], test_journo[\"password\"], test_journo[\"otp_secret\"]\n        )\n        # change password\n        new_pw = \"another correct horse battery staply long password\"\n        assert new_pw != test_journo[\"password\"]  # precondition\n        utils.prepare_password_change(app, test_journo[\"id\"], new_pw)\n\n        app.post(\n            \"/account/new-password\",\n            data=dict(\n                password=new_pw,\n                current_password=test_journo[\"password\"],\n                token=TOTP(test_journo[\"otp_secret\"]).now(),\n            ),\n        )\n        # logout\n        resp = app.post(\"/logout\")\n        assert resp.status_code == 302\n\n    # start a new client/context to be sure we've cleared the session\n    with journalist_app.test_client() as app:\n        # login with new credentials\n        login_journalist(app, test_journo[\"username\"], new_pw, test_journo[\"otp_secret\"])",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "test_prevent_document_uploads",
          "code": "def test_prevent_document_uploads(source_app, journalist_app, test_admin):\n    \"\"\"Test that the source interface accepts only messages when\n    allow_document_uploads == False.\n\n    \"\"\"\n\n    # Set allow_document_uploads = False:\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app, test_admin[\"username\"], test_admin[\"password\"], test_admin[\"otp_secret\"]\n        )\n        form = journalist_app_module.forms.SubmissionPreferencesForm(\n            prevent_document_uploads=True, min_message_length=0\n        )\n        resp = app.post(\n            \"/admin/update-submission-preferences\",\n            data=form.data,\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n\n    # Check that the source interface accepts only messages:\n    with source_app.test_client() as app:\n        app.post(\"/generate\", data=GENERATE_DATA)\n        tab_id = next(iter(session[\"codenames\"].keys()))\n        resp = app.post(\"/create\", data={\"tab_id\": tab_id}, follow_redirects=True)\n        assert resp.status_code == 200\n\n        text = resp.data.decode(\"utf-8\")\n        soup = BeautifulSoup(text, \"html.parser\")\n        assert \"Submit Messages\" in text\n        assert len(soup.select('input[type=\"file\"]')) == 0",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "test_no_prevent_document_uploads",
          "code": "def test_no_prevent_document_uploads(source_app, journalist_app, test_admin):\n    \"\"\"Test that the source interface accepts both files and messages when\n    allow_document_uploads == True.\n\n    \"\"\"\n\n    # Set allow_document_uploads = True:\n    with journalist_app.test_client() as app:\n        login_journalist(\n            app, test_admin[\"username\"], test_admin[\"password\"], test_admin[\"otp_secret\"]\n        )\n        form = journalist_app_module.forms.SubmissionPreferencesForm(\n            prevent_document_uploads=False, min_message_length=0\n        )\n        resp = app.post(\n            \"/admin/update-submission-preferences\",\n            data=form.data,\n            follow_redirects=True,\n        )\n        assert resp.status_code == 200\n\n    # Check that the source interface accepts both files and messages:\n    with source_app.test_client() as app:\n        app.post(\"/generate\", data=GENERATE_DATA)\n        tab_id = next(iter(session[\"codenames\"].keys()))\n        resp = app.post(\"/create\", data={\"tab_id\": tab_id}, follow_redirects=True)\n        assert resp.status_code == 200\n\n        text = resp.data.decode(\"utf-8\")\n        soup = BeautifulSoup(text, \"html.parser\")\n        assert \"Submit Files or Messages\" in text\n        assert len(soup.select('input[type=\"file\"]')) == 1",
          "file": "test_integration.py"
        }
      ],
      "test_manage.py": [
        {
          "type": "function",
          "name": "test_parse_args",
          "code": "def test_parse_args():\n    # just test that the arg parser is stable\n    manage.get_args()",
          "file": "test_manage.py"
        },
        {
          "type": "function",
          "name": "test_not_verbose",
          "code": "def test_not_verbose(caplog):\n    args = manage.get_args().parse_args([\"run\"])\n    manage.setup_verbosity(args)\n    manage.log.debug(\"INVISIBLE\")\n    assert \"INVISIBLE\" not in caplog.text",
          "file": "test_manage.py"
        },
        {
          "type": "function",
          "name": "test_verbose",
          "code": "def test_verbose(caplog):\n    args = manage.get_args().parse_args([\"--verbose\", \"run\"])\n    manage.setup_verbosity(args)\n    manage.log.debug(\"VISIBLE\")\n    assert \"VISIBLE\" in caplog.text",
          "file": "test_manage.py"
        },
        {
          "type": "function",
          "name": "test_get_username_success",
          "code": "def test_get_username_success():\n    with mock.patch(\"manage.obtain_input\", return_value=\"jen\"):\n        assert manage._get_username() == \"jen\"",
          "file": "test_manage.py"
        },
        {
          "type": "function",
          "name": "test_get_username_fail",
          "code": "def test_get_username_fail():\n    bad_username = \"a\" * (Journalist.MIN_USERNAME_LEN - 1)\n    with mock.patch(\"manage.obtain_input\", side_effect=[bad_username, \"jen\"]):\n        assert manage._get_username() == \"jen\"",
          "file": "test_manage.py"
        },
        {
          "type": "function",
          "name": "test_get_yubikey_usage_yes",
          "code": "def test_get_yubikey_usage_yes():\n    with mock.patch(\"manage.obtain_input\", return_value=\"y\"):\n        assert manage._get_yubikey_usage()",
          "file": "test_manage.py"
        },
        {
          "type": "function",
          "name": "test_get_yubikey_usage_no",
          "code": "def test_get_yubikey_usage_no():\n    with mock.patch(\"manage.obtain_input\", return_value=\"n\"):\n        assert not manage._get_yubikey_usage()",
          "file": "test_manage.py"
        },
        {
          "type": "function",
          "name": "test_handle_invalid_secret",
          "code": "def test_handle_invalid_secret(journalist_app, config, mocker, capsys):\n    \"\"\"Regression test for bad secret logic in manage.py\"\"\"\n\n    mocker.patch(\"manage._get_username\", return_value=\"ntoll\")\n    mocker.patch(\"manage._get_first_name\", return_value=\"\")\n    mocker.patch(\"manage._get_last_name\", return_value=\"\")\n    mocker.patch(\"manage._get_yubikey_usage\", return_value=True)\n    mocker.patch(\"manage.obtain_input\", side_effect=YUBIKEY_HOTP)\n\n    with journalist_app.app_context() as context:\n        # We will try to provide one invalid and one valid secret\n        return_value = manage._add_user(context=context)\n        out, err = capsys.readouterr()\n\n        assert return_value == 0\n        assert \"Try again.\" in out\n        assert \"successfully added\" in out",
          "file": "test_manage.py"
        },
        {
          "type": "function",
          "name": "test_exception_handling_when_duplicate_username",
          "code": "def test_exception_handling_when_duplicate_username(journalist_app, config, mocker, capsys):\n    \"\"\"Regression test for duplicate username logic in manage.py\"\"\"\n\n    mocker.patch(\"manage._get_username\", return_value=\"foo-bar-baz\")\n    mocker.patch(\"manage._get_first_name\", return_value=\"\")\n    mocker.patch(\"manage._get_last_name\", return_value=\"\")\n    mocker.patch(\"manage._get_yubikey_usage\", return_value=False)\n\n    with journalist_app.app_context() as context:\n        # Inserting the user for the first time should succeed\n        return_value = manage._add_user(context=context)\n        out, err = capsys.readouterr()\n\n        assert return_value == 0\n        assert \"successfully added\" in out\n\n        # Inserting the user for a second time should fail\n        return_value = manage._add_user(context=context)\n        out, err = capsys.readouterr()\n        assert return_value == 1\n        assert \"ERROR: That username is already taken!\" in out",
          "file": "test_manage.py"
        },
        {
          "type": "function",
          "name": "test_delete_user",
          "code": "def test_delete_user(journalist_app, config, mocker):\n    mocker.patch(\"manage._get_username\", return_value=\"test-user-56789\")\n    mocker.patch(\"manage._get_first_name\", return_value=\"\")\n    mocker.patch(\"manage._get_last_name\", return_value=\"\")\n    mocker.patch(\"manage._get_yubikey_usage\", return_value=False)\n    mocker.patch(\"manage._get_username_to_delete\", return_value=\"test-user-56789\")\n    mocker.patch(\"manage._get_delete_confirmation\", return_value=True)\n\n    with journalist_app.app_context() as context:\n        return_value = manage._add_user(context=context)\n        assert return_value == 0\n\n        return_value = manage.delete_user(args=None, context=context)\n        assert return_value == 0",
          "file": "test_manage.py"
        },
        {
          "type": "function",
          "name": "test_delete_non_existent_user",
          "code": "def test_delete_non_existent_user(journalist_app, config, mocker, capsys):\n    mocker.patch(\"manage._get_username_to_delete\", return_value=\"does-not-exist\")\n    mocker.patch(\"manage._get_delete_confirmation\", return_value=True)\n\n    with journalist_app.app_context() as context:\n        return_value = manage.delete_user(args=None, context=context)\n        out, err = capsys.readouterr()\n        assert return_value == 0\n        assert \"ERROR: That user was not found!\" in out",
          "file": "test_manage.py"
        },
        {
          "type": "function",
          "name": "test_get_username_to_delete",
          "code": "def test_get_username_to_delete(mocker):\n    mocker.patch(\"manage.obtain_input\", return_value=\"test-user-12345\")\n    return_value = manage._get_username_to_delete()\n    assert return_value == \"test-user-12345\"",
          "file": "test_manage.py"
        },
        {
          "type": "function",
          "name": "test_reset",
          "code": "def test_reset(journalist_app, test_journo, alembic_config, config):\n    with journalist_app.app_context() as context:\n        args = argparse.Namespace(store_dir=config.STORE_DIR)\n        # We have to override the hardcoded alembic .ini file because during testing\n        # the value in the .ini doesn't exist.\n        return_value = manage.reset(args=args, alembic_ini_path=alembic_config, context=context)\n\n        assert return_value == 0\n        assert config.DATABASE_FILE.exists()\n        assert config.STORE_DIR.exists()\n\n        # Verify journalist user present in the database is gone\n        res = Journalist.query.filter_by(username=test_journo[\"username\"]).one_or_none()\n        assert res is None",
          "file": "test_manage.py"
        },
        {
          "type": "function",
          "name": "test_get_username",
          "code": "def test_get_username(mocker):\n    mocker.patch(\"manage.obtain_input\", return_value=\"foo-bar-baz\")\n    assert manage._get_username() == \"foo-bar-baz\"",
          "file": "test_manage.py"
        },
        {
          "type": "function",
          "name": "test_get_first_name",
          "code": "def test_get_first_name(mocker):\n    mocker.patch(\"manage.obtain_input\", return_value=\"foo-bar-baz\")\n    assert manage._get_first_name() == \"foo-bar-baz\"",
          "file": "test_manage.py"
        },
        {
          "type": "function",
          "name": "test_get_last_name",
          "code": "def test_get_last_name(mocker):\n    mocker.patch(\"manage.obtain_input\", return_value=\"foo-bar-baz\")\n    assert manage._get_last_name() == \"foo-bar-baz\"",
          "file": "test_manage.py"
        },
        {
          "type": "function",
          "name": "test_clean_tmp_do_nothing",
          "code": "def test_clean_tmp_do_nothing(caplog):\n    args = argparse.Namespace(\n        days=0, directory=\" UNLIKELY::::::::::::::::: \", verbose=logging.DEBUG\n    )\n    manage.setup_verbosity(args)\n    manage.clean_tmp(args)\n    assert \"does not exist, do nothing\" in caplog.text",
          "file": "test_manage.py"
        },
        {
          "type": "function",
          "name": "test_clean_tmp_too_young",
          "code": "def test_clean_tmp_too_young(config, caplog):\n    args = argparse.Namespace(days=24 * 60 * 60, directory=config.TEMP_DIR, verbose=logging.DEBUG)\n    # create a file\n    open(os.path.join(config.TEMP_DIR, \"FILE\"), \"a\").close()\n\n    manage.setup_verbosity(args)\n    manage.clean_tmp(args)\n    assert \"modified less than\" in caplog.text",
          "file": "test_manage.py"
        },
        {
          "type": "function",
          "name": "test_clean_tmp_removed",
          "code": "def test_clean_tmp_removed(config, caplog):\n    args = argparse.Namespace(days=0, directory=config.TEMP_DIR, verbose=logging.DEBUG)\n    fname = os.path.join(config.TEMP_DIR, \"FILE\")\n    with open(fname, \"a\"):\n        old = time.time() - 24 * 60 * 60\n        os.utime(fname, (old, old))\n    manage.setup_verbosity(args)\n    manage.clean_tmp(args)\n    assert \"FILE removed\" in caplog.text",
          "file": "test_manage.py"
        },
        {
          "type": "function",
          "name": "test_were_there_submissions_today",
          "code": "def test_were_there_submissions_today(source_app, config, app_storage):\n    with source_app.app_context() as context:\n        # We need to override the config to point at the per-test DB\n        data_root = config.SECUREDROP_DATA_ROOT\n        args = argparse.Namespace(data_root=data_root, verbose=logging.DEBUG)\n\n        count_file = os.path.join(data_root, \"submissions_today.txt\")\n        source_user = create_source_user(\n            db_session=db.session,\n            source_passphrase=PassphraseGenerator.get_default().generate_passphrase(),\n            source_app_storage=app_storage,\n        )\n        source = source_user.get_db_record()\n        source.last_updated = datetime.datetime.utcnow() - datetime.timedelta(hours=24 * 2)\n        db.session.commit()\n        submissions.were_there_submissions_today(args, context)\n        assert open(count_file).read() == \"0\"\n        source.last_updated = datetime.datetime.utcnow()\n        db.session.commit()\n        submissions.were_there_submissions_today(args, context)\n        assert open(count_file).read() == \"1\"",
          "file": "test_manage.py"
        }
      ],
      "test_passphrases.py": [
        {
          "type": "class",
          "name": "TestPassphrasesGenerator",
          "code": "class TestPassphrasesGenerator:\n    def test_default_generator(self):\n        # Given the default generator for the Securedrop app\n        generator = PassphraseGenerator.get_default()\n        assert generator.available_languages == {\"en\", \"fr\"}\n\n        # When using it to generate a passphrase\n        # It succeeds\n        passphrase = generator.generate_passphrase()\n\n        # And a reasonably-secure passphrase was generated\n        assert passphrase\n        assert len(passphrase) >= 20\n        assert len(passphrase.split(\" \")) >= 7\n\n    def test_default_generator_passphrases_are_random(self):\n        # Given the default generator for the Securedrop app\n        generator = PassphraseGenerator.get_default()\n\n        # When using it to generate two passphrases\n        # It succeeds\n        passphrase1 = generator.generate_passphrase()\n        passphrase2 = generator.generate_passphrase()\n\n        # And the two passphrases are different because they are randomly-generated\n        assert passphrase1 != passphrase2\n\n    @mock.patch.object(PassphraseGenerator, \"_WORD_LIST_MINIMUM_SIZE\", 1)\n    def test_generate_passphrase_with_specific_language(self):\n        # Given a generator that supports two languages\n        generator = PassphraseGenerator(language_to_words={\"en\": [\"boat\"], \"fr\": [\"bateau\"]})\n        assert generator.available_languages == {\"en\", \"fr\"}\n\n        # When using it to create a passphrase for one of the two languages\n        # It succeeds\n        passphrase = generator.generate_passphrase(preferred_language=\"fr\")\n\n        # And the passphrase is in the chosen language\n        assert \"bateau\" in passphrase\n        assert \"boat\" not in passphrase\n\n    @mock.patch.object(PassphraseGenerator, \"_WORD_LIST_MINIMUM_SIZE\", 1)\n    def test_generate_passphrase_with_specific_language_that_is_not_available(self):\n        # Given a generator that supports two languages\n        generator = PassphraseGenerator(\n            language_to_words={\"en\": [\"boat\"], \"fr\": [\"bateau\"]},\n            # With english as the fallback language\n            fallback_language=\"en\",\n        )\n        assert generator.available_languages == {\"en\", \"fr\"}\n\n        # When using it to create a passphrase for another, non-supported language\n        # It succeeds\n        passphrase = generator.generate_passphrase(preferred_language=\"es\")\n\n        # And the passphrase is in the default/fallback language, english\n        assert \"boat\" in passphrase\n        assert \"bateau\" not in passphrase\n\n    def test_word_list_does_not_have_enough_words(self):\n        with pytest.raises(InvalidWordListError, match=\"long-enough words\"):\n            PassphraseGenerator(language_to_words={\"en\": [\"only\", \"three\", \"words\"]})\n\n    @mock.patch.object(PassphraseGenerator, \"_WORD_LIST_MINIMUM_SIZE\", 1)\n    def test_word_list_will_generate_overly_long_passphrase(self):\n        with pytest.raises(InvalidWordListError, match=\"over the maximum length\"):\n            PassphraseGenerator(language_to_words={\"en\": [\"overlylongwordtogetoverthelimit\"]})\n\n    @mock.patch.object(PassphraseGenerator, \"_WORD_LIST_MINIMUM_SIZE\", 1)\n    def test_word_list_will_generate_overly_short_passphrase(self):\n        with pytest.raises(InvalidWordListError, match=\"under the minimum length\"):\n            PassphraseGenerator(language_to_words={\"en\": [\"b\", \"a\"]})\n\n    @mock.patch.object(PassphraseGenerator, \"_WORD_LIST_MINIMUM_SIZE\", 1)\n    def test_word_list_has_non_ascii_string(self):\n        with pytest.raises(InvalidWordListError, match=\"non-ASCII words\"):\n            PassphraseGenerator(language_to_words={\"en\": [\"word\", \"éoèô\"]})",
          "file": "test_passphrases.py"
        },
        {
          "type": "function",
          "name": "test_default_generator",
          "code": "def test_default_generator(self):\n        # Given the default generator for the Securedrop app\n        generator = PassphraseGenerator.get_default()\n        assert generator.available_languages == {\"en\", \"fr\"}\n\n        # When using it to generate a passphrase\n        # It succeeds\n        passphrase = generator.generate_passphrase()\n\n        # And a reasonably-secure passphrase was generated\n        assert passphrase\n        assert len(passphrase) >= 20\n        assert len(passphrase.split(\" \")) >= 7",
          "file": "test_passphrases.py"
        },
        {
          "type": "function",
          "name": "test_default_generator_passphrases_are_random",
          "code": "def test_default_generator_passphrases_are_random(self):\n        # Given the default generator for the Securedrop app\n        generator = PassphraseGenerator.get_default()\n\n        # When using it to generate two passphrases\n        # It succeeds\n        passphrase1 = generator.generate_passphrase()\n        passphrase2 = generator.generate_passphrase()\n\n        # And the two passphrases are different because they are randomly-generated\n        assert passphrase1 != passphrase2",
          "file": "test_passphrases.py"
        },
        {
          "type": "function",
          "name": "test_generate_passphrase_with_specific_language",
          "code": "def test_generate_passphrase_with_specific_language(self):\n        # Given a generator that supports two languages\n        generator = PassphraseGenerator(language_to_words={\"en\": [\"boat\"], \"fr\": [\"bateau\"]})\n        assert generator.available_languages == {\"en\", \"fr\"}\n\n        # When using it to create a passphrase for one of the two languages\n        # It succeeds\n        passphrase = generator.generate_passphrase(preferred_language=\"fr\")\n\n        # And the passphrase is in the chosen language\n        assert \"bateau\" in passphrase\n        assert \"boat\" not in passphrase",
          "file": "test_passphrases.py"
        },
        {
          "type": "function",
          "name": "test_generate_passphrase_with_specific_language_that_is_not_available",
          "code": "def test_generate_passphrase_with_specific_language_that_is_not_available(self):\n        # Given a generator that supports two languages\n        generator = PassphraseGenerator(\n            language_to_words={\"en\": [\"boat\"], \"fr\": [\"bateau\"]},\n            # With english as the fallback language\n            fallback_language=\"en\",\n        )\n        assert generator.available_languages == {\"en\", \"fr\"}\n\n        # When using it to create a passphrase for another, non-supported language\n        # It succeeds\n        passphrase = generator.generate_passphrase(preferred_language=\"es\")\n\n        # And the passphrase is in the default/fallback language, english\n        assert \"boat\" in passphrase\n        assert \"bateau\" not in passphrase",
          "file": "test_passphrases.py"
        },
        {
          "type": "function",
          "name": "test_word_list_does_not_have_enough_words",
          "code": "def test_word_list_does_not_have_enough_words(self):\n        with pytest.raises(InvalidWordListError, match=\"long-enough words\"):\n            PassphraseGenerator(language_to_words={\"en\": [\"only\", \"three\", \"words\"]})",
          "file": "test_passphrases.py"
        },
        {
          "type": "function",
          "name": "test_word_list_will_generate_overly_long_passphrase",
          "code": "def test_word_list_will_generate_overly_long_passphrase(self):\n        with pytest.raises(InvalidWordListError, match=\"over the maximum length\"):\n            PassphraseGenerator(language_to_words={\"en\": [\"overlylongwordtogetoverthelimit\"]})",
          "file": "test_passphrases.py"
        },
        {
          "type": "function",
          "name": "test_word_list_will_generate_overly_short_passphrase",
          "code": "def test_word_list_will_generate_overly_short_passphrase(self):\n        with pytest.raises(InvalidWordListError, match=\"under the minimum length\"):\n            PassphraseGenerator(language_to_words={\"en\": [\"b\", \"a\"]})",
          "file": "test_passphrases.py"
        },
        {
          "type": "function",
          "name": "test_word_list_has_non_ascii_string",
          "code": "def test_word_list_has_non_ascii_string(self):\n        with pytest.raises(InvalidWordListError, match=\"non-ASCII words\"):\n            PassphraseGenerator(language_to_words={\"en\": [\"word\", \"éoèô\"]})",
          "file": "test_passphrases.py"
        }
      ],
      "test_wsgi.py": [
        {
          "type": "function",
          "name": "_journalist_pubkey",
          "code": "def _journalist_pubkey():\n    \"\"\"provision a valid public key for the JI startup check\"\"\"\n    path = Path(\"/tmp/securedrop/journalist.pub\")\n    if not path.exists():\n        created_dir = False\n        try:\n            if not path.parent.exists():\n                path.parent.mkdir()\n                created_dir = True\n            shutil.copy(Path(__file__).parent / \"files/test_journalist_key.pub\", path)\n            yield\n        finally:\n            if created_dir:\n                shutil.rmtree(path.parent)\n            else:\n                path.unlink()",
          "file": "test_wsgi.py"
        },
        {
          "type": "function",
          "name": "test_wsgi",
          "code": "def test_wsgi(filename, _journalist_pubkey):\n    \"\"\"\n    Verify that all setup code and imports in the wsgi files work\n\n    This is slightly hacky because it executes the wsgi files using\n    the current virtualenv, and we hack the paths so it works out.\n    \"\"\"\n    sd_dir = Path(__file__).parent.parent\n    wsgi = sd_dir / \"debian/app-code/var/www\" / filename\n    python_path = os.getenv(\"PYTHONPATH\", \"\")\n    subprocess.check_call(\n        [sys.executable, str(wsgi)],\n        env={\"PYTHONPATH\": f\"{python_path}:{sd_dir}\", \"SECUREDROP_ENV\": \"test\"},\n    )",
          "file": "test_wsgi.py"
        }
      ],
      "test_submission_cleanup.py": [
        {
          "type": "function",
          "name": "test_delete_disconnected_db_submissions",
          "code": "def test_delete_disconnected_db_submissions(journalist_app, app_storage, config):\n    \"\"\"\n    Test that Submission records without corresponding files are deleted.\n    \"\"\"\n    with journalist_app.app_context():\n        source, _ = utils.db_helper.init_source(app_storage)\n        source_id = source.id\n\n        # make two submissions\n        utils.db_helper.submit(app_storage, source, 2)\n        submission_id = source.submissions[0].id\n\n        # remove one submission's file\n        f1 = os.path.join(config.STORE_DIR, source.filesystem_id, source.submissions[0].filename)\n        assert os.path.exists(f1)\n        os.remove(f1)\n        assert os.path.exists(f1) is False\n\n        # check that the single disconnect is seen\n        disconnects = submissions.find_disconnected_db_submissions(config.STORE_DIR)\n        assert len(disconnects) == 1\n        assert disconnects[0].filename == source.submissions[0].filename\n\n        # remove the disconnected Submission\n        args = argparse.Namespace(force=True, store_dir=config.STORE_DIR)\n        submissions.delete_disconnected_db_submissions(args)\n\n        assert db.session.query(Submission).filter(Submission.id == submission_id).count() == 0\n        assert db.session.query(Submission).filter(Submission.source_id == source_id).count() == 1",
          "file": "test_submission_cleanup.py"
        },
        {
          "type": "function",
          "name": "test_delete_disconnected_fs_submissions",
          "code": "def test_delete_disconnected_fs_submissions(journalist_app, app_storage, config):\n    \"\"\"\n    Test that files in the store without corresponding Submission records are deleted.\n    \"\"\"\n    source, _ = utils.db_helper.init_source(app_storage)\n\n    # make two submissions\n    utils.db_helper.submit(app_storage, source, 2)\n    source_filesystem_id = source.filesystem_id\n    submission_filename = source.submissions[0].filename\n    disconnect_path = os.path.join(config.STORE_DIR, source_filesystem_id, submission_filename)\n\n    # make two replies, to make sure that their files are not seen\n    # as disconnects\n    journalist, _ = utils.db_helper.init_journalist(\"Mary\", \"Lane\")\n    utils.db_helper.reply(app_storage, journalist, source, 2)\n\n    # delete the first Submission record\n    db.session.delete(source.submissions[0])\n    db.session.commit()\n\n    disconnects = submissions.find_disconnected_fs_submissions(config.STORE_DIR)\n    assert len(disconnects) == 1\n    assert disconnects[0] == disconnect_path\n    assert os.path.exists(disconnect_path)\n\n    args = argparse.Namespace(force=True, store_dir=config.STORE_DIR)\n    submissions.delete_disconnected_fs_submissions(args)\n\n    assert os.path.exists(disconnect_path) is False",
          "file": "test_submission_cleanup.py"
        }
      ],
      "test_pretty_bad_protocol.py": [
        {
          "type": "function",
          "name": "test_gpg_export_keys",
          "code": "def test_gpg_export_keys(tmp_path):\n    gpg = gnupg.GPG(\n        binary=\"gpg2\",\n        homedir=str(tmp_path),\n        options=[\"--pinentry-mode loopback\", \"--trust-model direct\"],\n    )\n    passphrase = \"correcthorsebatterystaple\"\n    gen_key_input = gpg.gen_key_input(\n        passphrase=passphrase,\n        name_email=\"example@example.org\",\n        key_type=\"RSA\",\n        key_length=4096,\n        name_real=\"example\",\n    )\n    fingerprint = gpg.gen_key(gen_key_input)\n    print(fingerprint)\n    public_key = gpg.export_keys(fingerprint)\n    assert redwood.is_valid_public_key(public_key)\n    secret_key = gpg.export_keys(fingerprint, secret=True, passphrase=passphrase)\n    assert secret_key.startswith(\"-----BEGIN PGP PRIVATE KEY BLOCK-----\")\n\n    # Now verify the exported key pair is usable by Sequoia\n    message = \"GPG to Sequoia-PGP, yippe!\"\n    ciphertext = tmp_path / \"encrypted.asc\"\n    redwood.encrypt_message([public_key], message, ciphertext)\n    decrypted = redwood.decrypt(ciphertext.read_bytes(), secret_key, passphrase).decode()\n    assert decrypted == message\n\n    # Test some failure cases for exporting the secret key:\n    # bad passphrase\n    assert gpg.export_keys(fingerprint, secret=True, passphrase=\"wrong\") == \"\"\n    # exporting a non-existent secret key (import just the public key and try to export)\n    journalist_public_key = (\n        Path(__file__).parent / \"files\" / \"test_journalist_key.pub\"\n    ).read_text()\n    journalist_fingerprint = gpg.import_keys(journalist_public_key).fingerprints[0]\n    assert gpg.export_keys(journalist_fingerprint, secret=True, passphrase=passphrase) == \"\"",
          "file": "test_pretty_bad_protocol.py"
        }
      ],
      "test_session_manager.py": [
        {
          "type": "class",
          "name": "TestSessionManager",
          "code": "class TestSessionManager:\n    def test_log_user_in(self, source_app, app_storage):\n        # Given a source user\n        passphrase = PassphraseGenerator.get_default().generate_passphrase()\n        source_user = create_source_user(\n            db_session=db.session,\n            source_passphrase=passphrase,\n            source_app_storage=app_storage,\n        )\n\n        with source_app.test_request_context():\n            # When they log in, it succeeds\n            SessionManager.log_user_in(db_session=db.session, supplied_passphrase=passphrase)\n\n            # And the SessionManager returns them as the current user\n            assert SessionManager.is_user_logged_in(db_session=db.session)\n            logged_in_user = SessionManager.get_logged_in_user(db_session=db.session)\n            assert logged_in_user.db_record_id == source_user.db_record_id\n\n    def test_log_user_out(self, source_app, app_storage):\n        # Given a source user\n        passphrase = PassphraseGenerator.get_default().generate_passphrase()\n        create_source_user(\n            db_session=db.session,\n            source_passphrase=passphrase,\n            source_app_storage=app_storage,\n        )\n\n        with source_app.test_request_context():\n            # Who previously logged in\n            SessionManager.log_user_in(db_session=db.session, supplied_passphrase=passphrase)\n\n            # When they log out, it succeeds\n            SessionManager.log_user_out()\n\n            # And the SessionManager no longer returns a current user\n            assert not SessionManager.is_user_logged_in(db_session=db.session)\n            with pytest.raises(UserNotLoggedIn):\n                SessionManager.get_logged_in_user(db_session=db.session)\n\n    def test_get_logged_in_user_but_session_expired(self, source_app, app_storage):\n        # Given a source user\n        passphrase = PassphraseGenerator.get_default().generate_passphrase()\n        create_source_user(\n            db_session=db.session,\n            source_passphrase=passphrase,\n            source_app_storage=app_storage,\n        )\n\n        with source_app.test_request_context():\n            # Who previously logged in\n            SessionManager.log_user_in(db_session=db.session, supplied_passphrase=passphrase)\n\n            # But we're now 6 hours later hence their session expired\n            with mock.patch(\"source_app.session_manager.datetime\") as mock_datetime:\n                six_hours_later = datetime.now(timezone.utc) + timedelta(hours=6)\n                mock_datetime.now.return_value = six_hours_later\n\n                # When querying the current user from the SessionManager\n                # it fails with the right error\n                with pytest.raises(UserSessionExpired):\n                    SessionManager.get_logged_in_user(db_session=db.session)\n\n    def test_get_logged_in_user_but_user_deleted(self, source_app, app_storage):\n        # Given a source user\n        passphrase = PassphraseGenerator.get_default().generate_passphrase()\n        source_user = create_source_user(\n            db_session=db.session,\n            source_passphrase=passphrase,\n            source_app_storage=app_storage,\n        )\n\n        with source_app.test_request_context():\n            # Who previously logged in\n            SessionManager.log_user_in(db_session=db.session, supplied_passphrase=passphrase)\n            # But since then their account was deleted\n            source_in_db = source_user.get_db_record()\n            source_in_db.deleted_at = datetime.utcnow()\n            db.session.commit()\n\n            # When querying the current user from the SessionManager, it fails with the right error\n            with pytest.raises(UserHasBeenDeleted):\n                SessionManager.get_logged_in_user(db_session=db.session)",
          "file": "test_session_manager.py"
        },
        {
          "type": "function",
          "name": "test_log_user_in",
          "code": "def test_log_user_in(self, source_app, app_storage):\n        # Given a source user\n        passphrase = PassphraseGenerator.get_default().generate_passphrase()\n        source_user = create_source_user(\n            db_session=db.session,\n            source_passphrase=passphrase,\n            source_app_storage=app_storage,\n        )\n\n        with source_app.test_request_context():\n            # When they log in, it succeeds\n            SessionManager.log_user_in(db_session=db.session, supplied_passphrase=passphrase)\n\n            # And the SessionManager returns them as the current user\n            assert SessionManager.is_user_logged_in(db_session=db.session)\n            logged_in_user = SessionManager.get_logged_in_user(db_session=db.session)\n            assert logged_in_user.db_record_id == source_user.db_record_id",
          "file": "test_session_manager.py"
        },
        {
          "type": "function",
          "name": "test_log_user_out",
          "code": "def test_log_user_out(self, source_app, app_storage):\n        # Given a source user\n        passphrase = PassphraseGenerator.get_default().generate_passphrase()\n        create_source_user(\n            db_session=db.session,\n            source_passphrase=passphrase,\n            source_app_storage=app_storage,\n        )\n\n        with source_app.test_request_context():\n            # Who previously logged in\n            SessionManager.log_user_in(db_session=db.session, supplied_passphrase=passphrase)\n\n            # When they log out, it succeeds\n            SessionManager.log_user_out()\n\n            # And the SessionManager no longer returns a current user\n            assert not SessionManager.is_user_logged_in(db_session=db.session)\n            with pytest.raises(UserNotLoggedIn):\n                SessionManager.get_logged_in_user(db_session=db.session)",
          "file": "test_session_manager.py"
        },
        {
          "type": "function",
          "name": "test_get_logged_in_user_but_session_expired",
          "code": "def test_get_logged_in_user_but_session_expired(self, source_app, app_storage):\n        # Given a source user\n        passphrase = PassphraseGenerator.get_default().generate_passphrase()\n        create_source_user(\n            db_session=db.session,\n            source_passphrase=passphrase,\n            source_app_storage=app_storage,\n        )\n\n        with source_app.test_request_context():\n            # Who previously logged in\n            SessionManager.log_user_in(db_session=db.session, supplied_passphrase=passphrase)\n\n            # But we're now 6 hours later hence their session expired\n            with mock.patch(\"source_app.session_manager.datetime\") as mock_datetime:\n                six_hours_later = datetime.now(timezone.utc) + timedelta(hours=6)\n                mock_datetime.now.return_value = six_hours_later\n\n                # When querying the current user from the SessionManager\n                # it fails with the right error\n                with pytest.raises(UserSessionExpired):\n                    SessionManager.get_logged_in_user(db_session=db.session)",
          "file": "test_session_manager.py"
        },
        {
          "type": "function",
          "name": "test_get_logged_in_user_but_user_deleted",
          "code": "def test_get_logged_in_user_but_user_deleted(self, source_app, app_storage):\n        # Given a source user\n        passphrase = PassphraseGenerator.get_default().generate_passphrase()\n        source_user = create_source_user(\n            db_session=db.session,\n            source_passphrase=passphrase,\n            source_app_storage=app_storage,\n        )\n\n        with source_app.test_request_context():\n            # Who previously logged in\n            SessionManager.log_user_in(db_session=db.session, supplied_passphrase=passphrase)\n            # But since then their account was deleted\n            source_in_db = source_user.get_db_record()\n            source_in_db.deleted_at = datetime.utcnow()\n            db.session.commit()\n\n            # When querying the current user from the SessionManager, it fails with the right error\n            with pytest.raises(UserHasBeenDeleted):\n                SessionManager.get_logged_in_user(db_session=db.session)",
          "file": "test_session_manager.py"
        }
      ],
      "test_rm.py": [
        {
          "type": "function",
          "name": "test_secure_delete_capability",
          "code": "def test_secure_delete_capability(config):\n    assert rm.check_secure_delete_capability() is True\n\n    path = os.environ[\"PATH\"]\n    try:\n        os.environ[\"PATH\"] = f\"{config.TEMP_DIR}\"\n        assert rm.check_secure_delete_capability() is False\n        fakeshred = os.path.join(config.TEMP_DIR, \"shred\")\n        with open(fakeshred, \"w\") as f:\n            f.write(\"#!/bin/bash\\nexit 1\\n\")\n        os.chmod(fakeshred, 0o700)\n        assert rm.check_secure_delete_capability() is False\n    finally:\n        os.environ[\"PATH\"] = path",
          "file": "test_rm.py"
        },
        {
          "type": "function",
          "name": "test_shred",
          "code": "def test_shred(config):\n    testfile = \"test_shred.txt\"\n    content = \"abc123\\n\"\n\n    # nonexistent target should raise an exception\n    with pytest.raises(EnvironmentError):\n        rm.shred(os.path.abspath(os.path.join(config.TEMP_DIR, \"nonexistentshredtarget\")))\n\n    # a non-file target should raise an exception\n    d = os.path.abspath(os.path.join(config.TEMP_DIR, \"nonexistentshredtarget\"))\n    os.makedirs(d)\n    with pytest.raises(ValueError):\n        rm.shred(d)\n    os.rmdir(d)\n\n    with open(testfile, \"w\") as f:\n        f.write(content)\n\n    with open(testfile) as f:\n        read_content = f.read()\n        assert read_content == content\n\n    # Shred without deleting, so we can check the new content\n    rm.shred(testfile, delete=False)\n\n    with open(testfile) as f:\n        read_content = f.read()\n        assert read_content != content\n\n    # Shred and delete\n    rm.shred(testfile)\n    assert os.path.exists(testfile) is False",
          "file": "test_rm.py"
        },
        {
          "type": "function",
          "name": "test_secure_delete",
          "code": "def test_secure_delete(config):\n    content = \"abc123\\n\"\n    testfile = \"test_shred.txt\"\n\n    # Shred a file\n    testfile1 = os.path.abspath(os.path.join(config.TEMP_DIR, testfile))\n    with open(testfile1, \"w\") as f:\n        f.write(content)\n\n    assert os.path.exists(testfile1)\n    rm.secure_delete(testfile1)\n    assert os.path.exists(testfile1) is False\n\n    # Shred a directory\n    testdir = os.path.abspath(os.path.join(config.TEMP_DIR, \"shredtest1\"))\n    testsubdir1 = os.path.abspath(os.path.join(testdir, \"shredtest1.1\"))\n    testsubdir2 = os.path.abspath(os.path.join(testdir, \"shredtest1.2\"))\n\n    os.makedirs(testsubdir1)\n    os.makedirs(testsubdir2)\n\n    testfile1 = os.path.abspath(os.path.join(testdir, testfile))\n    with open(testfile1, \"w\") as f:\n        f.write(content)\n\n    testfile2 = os.path.abspath(os.path.join(testsubdir1, testfile))\n    with open(testfile2, \"w\") as f:\n        f.write(content)\n\n    assert os.path.exists(testfile1)\n    assert os.path.exists(testfile2)\n\n    rm.secure_delete(testdir)\n\n    assert os.path.exists(testfile1) is False\n    assert os.path.exists(testfile2) is False\n    assert os.path.exists(testsubdir1) is False\n    assert os.path.exists(testsubdir2) is False\n    assert os.path.exists(testdir) is False",
          "file": "test_rm.py"
        }
      ],
      "test_source_user.py": [
        {
          "type": "class",
          "name": "TestSourceUser",
          "code": "class TestSourceUser:\n    def test_create_source_user(self, source_app, app_storage):\n        # Given a passphrase\n        passphrase = PassphraseGenerator.get_default().generate_passphrase()\n\n        # When trying to create a new source user with this passphrase, it succeeds\n        source_user = create_source_user(\n            db_session=db.session,\n            source_passphrase=passphrase,\n            source_app_storage=app_storage,\n        )\n        assert source_user\n        assert source_user.get_db_record()\n\n    def test_create_source_user_passphrase_collision(self, source_app, app_storage):\n        # Given a source in the DB\n        passphrase = PassphraseGenerator.get_default().generate_passphrase()\n        create_source_user(\n            db_session=db.session,\n            source_passphrase=passphrase,\n            source_app_storage=app_storage,\n        )\n\n        # When trying to create another with the same passphrase, it fails\n        with pytest.raises(SourcePassphraseCollisionError):\n            create_source_user(\n                db_session=db.session,\n                source_passphrase=passphrase,\n                source_app_storage=app_storage,\n            )\n\n    def test_create_source_user_designation_collision(self, source_app, app_storage):\n        # Given a source in the DB\n        existing_source = create_source_user(\n            db_session=db.session,\n            source_passphrase=PassphraseGenerator.get_default().generate_passphrase(),\n            source_app_storage=app_storage,\n        )\n        existing_designation = existing_source.get_db_record().journalist_designation\n\n        # And the next generated journalist designation will be identical to this source's\n        with mock.patch.object(\n            source_user._DesignationGenerator,\n            \"generate_journalist_designation\",\n            return_value=existing_designation,\n        ):\n            # When trying to create another source, it fails, because the designation is the same\n            with pytest.raises(SourceDesignationCollisionError):\n                create_source_user(\n                    db_session=db.session,\n                    source_passphrase=PassphraseGenerator.get_default().generate_passphrase(),\n                    source_app_storage=app_storage,\n                )\n\n    def test_authenticate_source_user(self, source_app, app_storage):\n        # Given a source in the DB\n        passphrase = PassphraseGenerator.get_default().generate_passphrase()\n        source_user = create_source_user(\n            db_session=db.session,\n            source_passphrase=passphrase,\n            source_app_storage=app_storage,\n        )\n\n        # When they try to authenticate using their passphrase\n        authenticated_user = authenticate_source_user(\n            db_session=db.session, supplied_passphrase=passphrase\n        )\n\n        # It succeeds and the user is mapped to the right source in the DB\n        assert authenticated_user\n        assert authenticated_user.db_record_id == source_user.db_record_id\n\n    def test_authenticate_source_user_wrong_passphrase(self, source_app, app_storage):\n        # Given a source in the DB\n        create_source_user(\n            db_session=db.session,\n            source_passphrase=PassphraseGenerator.get_default().generate_passphrase(),\n            source_app_storage=app_storage,\n        )\n\n        # When a user tries to authenticate using a wrong passphrase, it fails\n        wrong_passphrase = \"rehydrate flaring study raven fence extenuate linguist\"\n        with pytest.raises(InvalidPassphraseError):\n            authenticate_source_user(db_session=db.session, supplied_passphrase=wrong_passphrase)",
          "file": "test_source_user.py"
        },
        {
          "type": "function",
          "name": "test_create_source_user",
          "code": "def test_create_source_user(self, source_app, app_storage):\n        # Given a passphrase\n        passphrase = PassphraseGenerator.get_default().generate_passphrase()\n\n        # When trying to create a new source user with this passphrase, it succeeds\n        source_user = create_source_user(\n            db_session=db.session,\n            source_passphrase=passphrase,\n            source_app_storage=app_storage,\n        )\n        assert source_user\n        assert source_user.get_db_record()",
          "file": "test_source_user.py"
        },
        {
          "type": "function",
          "name": "test_create_source_user_passphrase_collision",
          "code": "def test_create_source_user_passphrase_collision(self, source_app, app_storage):\n        # Given a source in the DB\n        passphrase = PassphraseGenerator.get_default().generate_passphrase()\n        create_source_user(\n            db_session=db.session,\n            source_passphrase=passphrase,\n            source_app_storage=app_storage,\n        )\n\n        # When trying to create another with the same passphrase, it fails\n        with pytest.raises(SourcePassphraseCollisionError):\n            create_source_user(\n                db_session=db.session,\n                source_passphrase=passphrase,\n                source_app_storage=app_storage,\n            )",
          "file": "test_source_user.py"
        },
        {
          "type": "function",
          "name": "test_create_source_user_designation_collision",
          "code": "def test_create_source_user_designation_collision(self, source_app, app_storage):\n        # Given a source in the DB\n        existing_source = create_source_user(\n            db_session=db.session,\n            source_passphrase=PassphraseGenerator.get_default().generate_passphrase(),\n            source_app_storage=app_storage,\n        )\n        existing_designation = existing_source.get_db_record().journalist_designation\n\n        # And the next generated journalist designation will be identical to this source's\n        with mock.patch.object(\n            source_user._DesignationGenerator,\n            \"generate_journalist_designation\",\n            return_value=existing_designation,\n        ):\n            # When trying to create another source, it fails, because the designation is the same\n            with pytest.raises(SourceDesignationCollisionError):\n                create_source_user(\n                    db_session=db.session,\n                    source_passphrase=PassphraseGenerator.get_default().generate_passphrase(),\n                    source_app_storage=app_storage,\n                )",
          "file": "test_source_user.py"
        },
        {
          "type": "function",
          "name": "test_authenticate_source_user",
          "code": "def test_authenticate_source_user(self, source_app, app_storage):\n        # Given a source in the DB\n        passphrase = PassphraseGenerator.get_default().generate_passphrase()\n        source_user = create_source_user(\n            db_session=db.session,\n            source_passphrase=passphrase,\n            source_app_storage=app_storage,\n        )\n\n        # When they try to authenticate using their passphrase\n        authenticated_user = authenticate_source_user(\n            db_session=db.session, supplied_passphrase=passphrase\n        )\n\n        # It succeeds and the user is mapped to the right source in the DB\n        assert authenticated_user\n        assert authenticated_user.db_record_id == source_user.db_record_id",
          "file": "test_source_user.py"
        },
        {
          "type": "function",
          "name": "test_authenticate_source_user_wrong_passphrase",
          "code": "def test_authenticate_source_user_wrong_passphrase(self, source_app, app_storage):\n        # Given a source in the DB\n        create_source_user(\n            db_session=db.session,\n            source_passphrase=PassphraseGenerator.get_default().generate_passphrase(),\n            source_app_storage=app_storage,\n        )\n\n        # When a user tries to authenticate using a wrong passphrase, it fails\n        wrong_passphrase = \"rehydrate flaring study raven fence extenuate linguist\"\n        with pytest.raises(InvalidPassphraseError):\n            authenticate_source_user(db_session=db.session, supplied_passphrase=wrong_passphrase)",
          "file": "test_source_user.py"
        },
        {
          "type": "class",
          "name": "TestSourceScryptManager",
          "code": "class TestSourceScryptManager:\n    def test(self):\n        # Given a passphrase\n        passphrase = \"rehydrate flaring study raven fence extenuate linguist\"\n\n        # When deriving the passphrase's filesystem ID and GPG secret\n        scrypt_mgr = _SourceScryptManager(\n            salt_for_gpg_secret=TEST_SALT_GPG_SECRET.encode(),\n            salt_for_filesystem_id=TEST_SALT_FOR_FILESYSTEM_ID.encode(),\n            scrypt_n=2**1,\n            scrypt_r=1,\n            scrypt_p=1,\n        )\n        filesystem_id = scrypt_mgr.derive_source_filesystem_id(passphrase)\n        gpg_secret = scrypt_mgr.derive_source_gpg_secret(passphrase)\n\n        # It succeeds and the right values are returned\n        expected_filesystem_id = \"7A7N4GSAB6NRZLUYOTHVYWJGOYIFS24TRC5FQQUSSXCWTF7MJQ7W3QTQLHUFHTKHYO3ONKJ6RSWPS6OI2PFCIW3KI4UZVKGZ3GAIKXI=\"  # noqa: E501\n        assert expected_filesystem_id == filesystem_id\n\n        expected_gpg_secret = \"AWCRZVPA6YTQ2A3552LZJW3VO7L3ZONDFT6A6VPRGPGQQSNENRLA3EVRW4LZYNSUV5QIKNFZMJ2BMOVORG43ZETV5ZCRQKLJNOC2BXY=\"  # noqa: E501\n        assert expected_gpg_secret == gpg_secret\n\n    def test_get_default(self):\n        scrypt_mgr = _SourceScryptManager.get_default()\n        assert scrypt_mgr",
          "file": "test_source_user.py"
        },
        {
          "type": "function",
          "name": "test",
          "code": "def test(self):\n        # Given a passphrase\n        passphrase = \"rehydrate flaring study raven fence extenuate linguist\"\n\n        # When deriving the passphrase's filesystem ID and GPG secret\n        scrypt_mgr = _SourceScryptManager(\n            salt_for_gpg_secret=TEST_SALT_GPG_SECRET.encode(),\n            salt_for_filesystem_id=TEST_SALT_FOR_FILESYSTEM_ID.encode(),\n            scrypt_n=2**1,\n            scrypt_r=1,\n            scrypt_p=1,\n        )\n        filesystem_id = scrypt_mgr.derive_source_filesystem_id(passphrase)\n        gpg_secret = scrypt_mgr.derive_source_gpg_secret(passphrase)\n\n        # It succeeds and the right values are returned\n        expected_filesystem_id = \"7A7N4GSAB6NRZLUYOTHVYWJGOYIFS24TRC5FQQUSSXCWTF7MJQ7W3QTQLHUFHTKHYO3ONKJ6RSWPS6OI2PFCIW3KI4UZVKGZ3GAIKXI=\"  # noqa: E501\n        assert expected_filesystem_id == filesystem_id\n\n        expected_gpg_secret = \"AWCRZVPA6YTQ2A3552LZJW3VO7L3ZONDFT6A6VPRGPGQQSNENRLA3EVRW4LZYNSUV5QIKNFZMJ2BMOVORG43ZETV5ZCRQKLJNOC2BXY=\"  # noqa: E501\n        assert expected_gpg_secret == gpg_secret",
          "file": "test_source_user.py"
        },
        {
          "type": "function",
          "name": "test_get_default",
          "code": "def test_get_default(self):\n        scrypt_mgr = _SourceScryptManager.get_default()\n        assert scrypt_mgr",
          "file": "test_source_user.py"
        },
        {
          "type": "class",
          "name": "TestDesignationGenerator",
          "code": "class TestDesignationGenerator:\n    def test(self):\n        # Given a designation generator\n        nouns = [\"ability\", \"accent\", \"academia\"]\n        adjectives = [\"tonic\", \"trivial\", \"tropical\"]\n        generator = _DesignationGenerator(nouns=nouns, adjectives=adjectives)\n\n        # When using it to generate a journalist designation\n        designation = generator.generate_journalist_designation()\n\n        # It succeeds\n        assert designation\n\n        # And the designation is correctly formatted\n        designation_words = designation.split()\n        assert len(designation_words) == 2\n        assert designation_words[0] in adjectives\n        assert designation_words[1] in nouns\n\n    def test_nouns_list_is_not_empty(self):\n        with pytest.raises(ValueError):\n            _DesignationGenerator(nouns=[], adjectives=[\"hello\"])\n\n    def test_adjectives_list_is_not_empty(self):\n        with pytest.raises(ValueError):\n            _DesignationGenerator(nouns=[\"hello\"], adjectives=[])\n\n    def test_nouns_list_does_not_contain_empty_strings(self):\n        with pytest.raises(ValueError):\n            _DesignationGenerator(nouns=[\"hello\", \"\"], adjectives=[\"hello\"])\n\n    def test_adjectives_list_does_not_contain_empty_strings(self):\n        with pytest.raises(ValueError):\n            _DesignationGenerator(nouns=[\"hello\"], adjectives=[\"hello\", \"\"])\n\n    def test_get_default(self):\n        designation_generator = _DesignationGenerator.get_default()\n        assert designation_generator\n        assert designation_generator.generate_journalist_designation()",
          "file": "test_source_user.py"
        },
        {
          "type": "function",
          "name": "test",
          "code": "def test(self):\n        # Given a designation generator\n        nouns = [\"ability\", \"accent\", \"academia\"]\n        adjectives = [\"tonic\", \"trivial\", \"tropical\"]\n        generator = _DesignationGenerator(nouns=nouns, adjectives=adjectives)\n\n        # When using it to generate a journalist designation\n        designation = generator.generate_journalist_designation()\n\n        # It succeeds\n        assert designation\n\n        # And the designation is correctly formatted\n        designation_words = designation.split()\n        assert len(designation_words) == 2\n        assert designation_words[0] in adjectives\n        assert designation_words[1] in nouns",
          "file": "test_source_user.py"
        },
        {
          "type": "function",
          "name": "test_nouns_list_is_not_empty",
          "code": "def test_nouns_list_is_not_empty(self):\n        with pytest.raises(ValueError):\n            _DesignationGenerator(nouns=[], adjectives=[\"hello\"])",
          "file": "test_source_user.py"
        },
        {
          "type": "function",
          "name": "test_adjectives_list_is_not_empty",
          "code": "def test_adjectives_list_is_not_empty(self):\n        with pytest.raises(ValueError):\n            _DesignationGenerator(nouns=[\"hello\"], adjectives=[])",
          "file": "test_source_user.py"
        },
        {
          "type": "function",
          "name": "test_nouns_list_does_not_contain_empty_strings",
          "code": "def test_nouns_list_does_not_contain_empty_strings(self):\n        with pytest.raises(ValueError):\n            _DesignationGenerator(nouns=[\"hello\", \"\"], adjectives=[\"hello\"])",
          "file": "test_source_user.py"
        },
        {
          "type": "function",
          "name": "test_adjectives_list_does_not_contain_empty_strings",
          "code": "def test_adjectives_list_does_not_contain_empty_strings(self):\n        with pytest.raises(ValueError):\n            _DesignationGenerator(nouns=[\"hello\"], adjectives=[\"hello\", \"\"])",
          "file": "test_source_user.py"
        },
        {
          "type": "function",
          "name": "test_get_default",
          "code": "def test_get_default(self):\n        designation_generator = _DesignationGenerator.get_default()\n        assert designation_generator\n        assert designation_generator.generate_journalist_designation()",
          "file": "test_source_user.py"
        }
      ],
      "test_template_filters.py": [
        {
          "type": "function",
          "name": "verify_rel_datetime_format",
          "code": "def verify_rel_datetime_format(app):\n    with app.test_client() as c:\n        c.get(\"/\")\n        assert session.get(\"locale\") == \"en_US\"\n        result = template_filters.rel_datetime_format(datetime(2016, 1, 1, 1, 1, 1))\n        assert result == \"January 1, 2016 at 1:01:01 AM UTC\"\n\n        result = template_filters.rel_datetime_format(datetime(2016, 1, 1, 1, 1, 1), fmt=\"yyyy\")\n        assert result == \"2016\"\n\n        test_time = datetime.utcnow() - timedelta(hours=2)\n        result = template_filters.rel_datetime_format(test_time, relative=True)\n        assert result == \"2 hours ago\"\n\n        c.get(\"/?l=fr_FR\")\n        assert session.get(\"locale\") == \"fr_FR\"\n        result = template_filters.rel_datetime_format(datetime(2016, 1, 1, 1, 1, 1))\n        assert result == \"1 janvier 2016 à 01:01:01 TU\"\n\n        result = template_filters.rel_datetime_format(datetime(2016, 1, 1, 1, 1, 1), fmt=\"yyyy\")\n        assert result == \"2016\"\n\n        test_time = datetime.utcnow() - timedelta(hours=2)\n        result = template_filters.rel_datetime_format(test_time, relative=True)\n        assert \"2\\xa0heures\" in result",
          "file": "test_template_filters.py"
        },
        {
          "type": "function",
          "name": "verify_filesizeformat",
          "code": "def verify_filesizeformat(app):\n    with app.test_client() as c:\n        c.get(\"/\")\n        assert session.get(\"locale\") == \"en_US\"\n        assert template_filters.filesizeformat(1) == \"1 byte\"\n        assert template_filters.filesizeformat(2) == \"2 bytes\"\n        value = 1024 * 3\n        assert template_filters.filesizeformat(value) == \"3 kB\"\n        value *= 1024\n        assert template_filters.filesizeformat(value) == \"3 MB\"\n        value *= 1024\n        assert template_filters.filesizeformat(value) == \"3 GB\"\n        value *= 1024\n        assert template_filters.filesizeformat(value) == \"3 TB\"\n        value *= 1024\n        assert template_filters.filesizeformat(value) == \"3,072 TB\"\n\n        c.get(\"/?l=fr_FR\")\n        assert session.get(\"locale\") == \"fr_FR\"\n        assert template_filters.filesizeformat(1) == \"1\\xa0octet\"\n        assert template_filters.filesizeformat(2) == \"2\\xa0octets\"\n        value = 1024 * 3\n        assert template_filters.filesizeformat(value) == \"3\\u202fko\"\n        value *= 1024\n        assert template_filters.filesizeformat(value) == \"3\\u202fMo\"\n        value *= 1024\n        assert template_filters.filesizeformat(value) == \"3\\u202fGo\"\n        value *= 1024\n        assert template_filters.filesizeformat(value) == \"3\\u202fTo\"\n        value *= 1024\n        assert \"072\\u202fTo\" in template_filters.filesizeformat(value)",
          "file": "test_template_filters.py"
        },
        {
          "type": "function",
          "name": "test_source_filters",
          "code": "def test_source_filters():\n    do_test(source_app.create_app)",
          "file": "test_template_filters.py"
        },
        {
          "type": "function",
          "name": "test_journalist_filters",
          "code": "def test_journalist_filters():\n    do_test(journalist_app.create_app)",
          "file": "test_template_filters.py"
        },
        {
          "type": "function",
          "name": "do_test",
          "code": "def do_test(create_app):\n    test_config = create_config_for_i18n_test(supported_locales=[\"en_US\", \"fr_FR\"])\n\n    i18n_dir = Path(__file__).absolute().parent / \"i18n\"\n    pot = Path(test_config.TEMP_DIR) / \"messages.pot\"\n    subprocess.check_call(\n        [\n            \"pybabel\",\n            \"extract\",\n            \"--mapping\",\n            str(i18n_dir / \"babel.cfg\"),\n            \"--output\",\n            pot,\n            str(i18n_dir / \"code.py\"),\n        ]\n    )\n\n    # To be able to test template filters for a given language, its message\n    # catalog must exist, but it doesn't have to contain any actual\n    # translations.  So we can just initialize it based on the template created\n    # by \"pybabel extract\".\n    for lang in (\"en_US\", \"fr_FR\"):\n        subprocess.check_call(\n            [\"pybabel\", \"init\", \"-i\", pot, \"-d\", test_config.TEMP_DIR, \"-l\", lang]\n        )\n\n    app = create_app(test_config)\n    with app.app_context():\n        db.create_all()\n\n    assert list(app.config[\"LOCALES\"].keys()) == test_config.SUPPORTED_LOCALES\n    verify_filesizeformat(app)\n    verify_rel_datetime_format(app)",
          "file": "test_template_filters.py"
        }
      ],
      "test_source_utils.py": [
        {
          "type": "function",
          "name": "test_check_url_file",
          "code": "def test_check_url_file(config):\n    assert check_url_file(\"nosuchfile\", \"whatever\") is None\n\n    try:\n\n        def write_url_file(path, content):\n            url_file = open(path, \"w\")\n            url_file.write(f\"{content}\\n\")\n\n        url_path = \"test_source_url\"\n\n        onion_test_url = \"abcdabcdabcdabcd.onion\"\n        write_url_file(url_path, onion_test_url)\n        assert check_url_file(url_path, r\"^[a-z0-9]{16}\\.onion$\") == onion_test_url\n\n        onion_test_url = \"abcdefghabcdefghabcdefghabcdefghabcdefghabcdefghabcdefgh.onion\"\n        write_url_file(url_path, onion_test_url)\n        assert check_url_file(url_path, r\"^[a-z0-9]{56}\\.onion$\") == onion_test_url\n\n        write_url_file(url_path, \"NO.onion\")\n        assert check_url_file(url_path, r\"^[a-z0-9]{56}\\.onion$\") is None\n    finally:\n        if os.path.exists(url_path):\n            os.unlink(url_path)",
          "file": "test_source_utils.py"
        },
        {
          "type": "function",
          "name": "write_url_file",
          "code": "def write_url_file(path, content):\n            url_file = open(path, \"w\")\n            url_file.write(f\"{content}\\n\")",
          "file": "test_source_utils.py"
        },
        {
          "type": "function",
          "name": "test_fit_codenames_into_cookie",
          "code": "def test_fit_codenames_into_cookie(config):\n    # A single codename should never be truncated.\n    codenames = {\"a\": VALID_PASSWORD}\n    assert fit_codenames_into_cookie(codenames) == codenames\n\n    # A reasonable number of codenames should never be truncated.\n    codenames = {\n        \"a\": VALID_PASSWORD,\n        \"b\": VALID_PASSWORD,\n        \"c\": VALID_PASSWORD,\n    }\n    assert fit_codenames_into_cookie(codenames) == codenames\n\n    # A single gargantuan codename is undefined behavior---but also should not\n    # be truncated.\n    codenames = {\"a\": werkzeug.Response.max_cookie_size * VALID_PASSWORD}\n    assert fit_codenames_into_cookie(codenames) == codenames\n\n    # Too many codenames of the expected length should be truncated.\n    codenames = {}\n    too_many = 2 * (werkzeug.Response.max_cookie_size // len(VALID_PASSWORD))\n    for i in range(too_many):\n        codenames[i] = VALID_PASSWORD\n    serialized = json.dumps(codenames).encode()\n    assert len(serialized) > werkzeug.Response.max_cookie_size\n    serialized = json.dumps(fit_codenames_into_cookie(codenames)).encode()\n    assert len(serialized) < werkzeug.Response.max_cookie_size",
          "file": "test_source_utils.py"
        },
        {
          "type": "function",
          "name": "test_codename_detected",
          "code": "def test_codename_detected(message, expected):\n    assert codename_detected(message, \"codename\") is expected",
          "file": "test_source_utils.py"
        }
      ],
      "config_from_2014.py": [
        {
          "type": "class",
          "name": "FlaskConfig",
          "code": "class FlaskConfig:\n    DEBUG = False\n    TESTING = False\n    WTF_CSRF_ENABLED = True",
          "file": "config_from_2014.py"
        },
        {
          "type": "class",
          "name": "SourceInterfaceFlaskConfig",
          "code": "class SourceInterfaceFlaskConfig(FlaskConfig):\n    SECRET_KEY = \"{{ source_secret_key }}\"",
          "file": "config_from_2014.py"
        },
        {
          "type": "class",
          "name": "JournalistInterfaceFlaskConfig",
          "code": "class JournalistInterfaceFlaskConfig(FlaskConfig):\n    SECRET_KEY = \"{{ journalist_secret_key }}\"",
          "file": "config_from_2014.py"
        }
      ],
      "test_worker.py": [
        {
          "type": "function",
          "name": "layabout",
          "code": "def layabout():\n    \"\"\"\n    Function that just sleeps for an hour.\n    \"\"\"\n    time.sleep(3600)",
          "file": "test_worker.py"
        },
        {
          "type": "function",
          "name": "start_rq_worker",
          "code": "def start_rq_worker(config, queue_name):\n    \"\"\"\n    Launches an rq worker process.\n    \"\"\"\n    return subprocess.Popen(\n        [\n            \"/opt/venvs/securedrop-app-code/bin/rqworker\",\n            \"--path\",\n            config.SECUREDROP_ROOT,\n            \"-c\",\n            \"rq_config\",\n            queue_name,\n        ],\n        preexec_fn=os.setsid,  # noqa: PLW1509\n    )",
          "file": "test_worker.py"
        },
        {
          "type": "function",
          "name": "test_no_interrupted_jobs",
          "code": "def test_no_interrupted_jobs(config, caplog):\n    \"\"\"\n    Tests requeue_interrupted_jobs when there are no interrupted jobs.\n    \"\"\"\n    caplog.set_level(logging.DEBUG)\n\n    q = worker.create_queue(config.RQ_WORKER_NAME)\n    try:\n        assert len(q.get_job_ids()) == 0\n        worker.requeue_interrupted_jobs(config.RQ_WORKER_NAME)\n        assert \"No interrupted jobs found in started job registry.\" in caplog.text\n    finally:\n        q.delete()",
          "file": "test_worker.py"
        },
        {
          "type": "function",
          "name": "test_job_interruption",
          "code": "def test_job_interruption(config, caplog):\n    \"\"\"\n    Tests that a job is requeued unless it is already being run.\n    \"\"\"\n    caplog.set_level(logging.DEBUG)\n\n    queue_name = \"test_job_interruption\"\n    q = worker_process = None\n    try:\n        q = worker.create_queue(queue_name)\n\n        # submit a job that sleeps for an hour\n        job = q.enqueue(layabout)\n        assert len(q.get_job_ids()) == 1\n\n        # launch worker processes\n        worker_process = start_rq_worker(config, queue_name)\n\n        i = 0\n        while i < 20:\n            if len(worker.rq_workers(q)) == 1:\n                break\n            time.sleep(0.1)\n\n        assert len(worker.rq_workers(q)) == 1\n\n        i = 0\n        while i < 20:\n            w = worker.worker_for_job(job.id)\n            if w:\n                break\n            i += 1\n            time.sleep(0.1)\n        assert w is not None\n\n        # the running job should not be requeued\n        worker.requeue_interrupted_jobs(queue_name)\n        skipped = f\"Skipping job {job.id}, which is already being run by worker {w.key}\"\n        assert skipped in caplog.text\n\n        # kill the process group, to kill the worker and its workhorse\n        os.killpg(worker_process.pid, signal.SIGKILL)\n        worker_process.wait()\n        caplog.clear()\n\n        # after killing the worker, the interrupted job should be requeued\n        worker.requeue_interrupted_jobs(queue_name)\n        print(caplog.text)\n        assert f\"Requeuing job {job}\" in caplog.text\n        assert len(q.get_job_ids()) == 1\n    finally:\n        q.delete()\n        if worker_process:\n            try:\n                os.killpg(worker_process.pid, 0)\n                os.killpg(worker_process.pid, signal.SIGKILL)\n            except OSError:\n                logging.debug(\"worker_process already gone.\")",
          "file": "test_worker.py"
        },
        {
          "type": "function",
          "name": "test_worker_for_job",
          "code": "def test_worker_for_job(config):\n    \"\"\"\n    Tests that worker_for_job works when there are multiple workers.\n    \"\"\"\n\n    queue_name = \"test_worker_for_job\"\n    q = worker_process = second_process = None\n    try:\n        q = worker.create_queue(queue_name)\n        assert len(worker.rq_workers(q)) == 0\n\n        # launch worker processes\n        worker_process = start_rq_worker(config, queue_name)\n        second_process = start_rq_worker(config, queue_name)\n\n        i = 0\n        while i < 20:\n            if len(worker.rq_workers(q)) == 2:\n                break\n            time.sleep(0.1)\n\n        assert len(worker.rq_workers(q)) == 2\n\n        worker.rq_workers(q)[0].set_state(WorkerStatus.SUSPENDED)\n\n        logging.debug(\n            [\n                f\"{w.pid}: state={w.get_state()}, job={w.get_current_job_id()}\"\n                for w in worker.rq_workers(q)\n            ]\n        )\n\n        # submit a job that sleeps for an hour\n        job = q.enqueue(layabout)\n\n        i = 0\n        while i < 20:\n            w = worker.worker_for_job(job.id)\n            if w:\n                break\n            i += 1\n            time.sleep(0.1)\n        assert w is not None\n\n    finally:\n        q.delete()\n        if worker_process:\n            try:\n                os.killpg(worker_process.pid, 0)\n                os.killpg(worker_process.pid, signal.SIGKILL)\n            except OSError:\n                logging.debug(\"worker_process already gone.\")\n\n        if second_process:\n            try:\n                os.killpg(second_process.pid, 0)\n                os.killpg(second_process.pid, signal.SIGKILL)\n            except OSError:\n                logging.debug(\"second_process already gone.\")",
          "file": "test_worker.py"
        }
      ],
      "test_alembic.py": [
        {
          "type": "function",
          "name": "_reset_db",
          "code": "def _reset_db(config: SecureDropConfig) -> None:\n    # The config fixture creates all the models in the DB, but most alembic tests expect an\n    #  empty DB, so we reset the DB via this fixture\n    reset_database(config.DATABASE_FILE)",
          "file": "test_alembic.py"
        },
        {
          "type": "function",
          "name": "list_migrations",
          "code": "def list_migrations(cfg_path, head):\n    cfg = AlembicConfig(cfg_path)\n    script = ScriptDirectory.from_config(cfg)\n    migrations = [x.revision for x in script.walk_revisions(base=\"base\", head=head)]\n    migrations.reverse()\n    return migrations",
          "file": "test_alembic.py"
        },
        {
          "type": "function",
          "name": "upgrade",
          "code": "def upgrade(alembic_config, migration):\n    subprocess.check_call([\"alembic\", \"upgrade\", migration], cwd=alembic_config.parent)",
          "file": "test_alembic.py"
        },
        {
          "type": "function",
          "name": "downgrade",
          "code": "def downgrade(alembic_config, migration):\n    subprocess.check_call([\"alembic\", \"downgrade\", migration], cwd=alembic_config.parent)",
          "file": "test_alembic.py"
        },
        {
          "type": "function",
          "name": "get_schema",
          "code": "def get_schema(app):\n    with app.app_context():\n        result = list(\n            db.engine.execute(\n                text(\n                    \"\"\"\n            SELECT type, name, tbl_name, sql\n            FROM sqlite_master\n            ORDER BY type, name, tbl_name\n            \"\"\"\n                )\n            )\n        )\n\n    return {(x[0], x[1], x[2]): x[3] for x in result}",
          "file": "test_alembic.py"
        },
        {
          "type": "function",
          "name": "assert_schemas_equal",
          "code": "def assert_schemas_equal(left, right):\n    assert list(left) == list(right), \"Left and right do not contain same list of tables\"\n    for table, left_schema in list(left.items()):\n        assert_ddl_equal(left_schema, right[table])",
          "file": "test_alembic.py"
        },
        {
          "type": "function",
          "name": "assert_ddl_equal",
          "code": "def assert_ddl_equal(left, right):\n    \"\"\"Check the \"tokenized\" DDL is equivalent because, because sometimes\n    Alembic schemas append columns on the same line to the DDL comes out\n    like:\n\n    column1 TEXT NOT NULL, column2 TEXT NOT NULL\n\n    and SQLAlchemy comes out:\n\n    column1 TEXT NOT NULL,\n    column2 TEXT NOT NULL\n\n    Also, sometimes CHECK constraints are duplicated by alembic, like:\n    CHECK (column IN (0, 1)),\n    CHECK (column IN (0, 1)),\n    So dedupe alembic's output as well\n\n    \"\"\"\n    # ignore the autoindex cases\n    if left is None and right is None:\n        return\n\n    left_schema = left\n    right_schema = right\n\n    # dedupe output by line\n    left = \"\\n\".join(list(OrderedDict.fromkeys(left.split(\"\\n\"))))\n    right = \"\\n\".join(list(OrderedDict.fromkeys(right.split(\"\\n\"))))\n\n    left = [x for x in WHITESPACE_REGEX.split(left) if x]\n    right = [x for x in WHITESPACE_REGEX.split(right) if x]\n\n    # Strip commas and quotes\n    left = [x.replace('\"', \"\").replace(\",\", \"\") for x in left]\n    left.sort()\n    right = [x.replace('\"', \"\").replace(\",\", \"\") for x in right]\n    right.sort()\n\n    assert left == right, f\"Schemas don't match:\\nLeft\\n{left_schema}\\nRight:\\n{right_schema}\"",
          "file": "test_alembic.py"
        },
        {
          "type": "function",
          "name": "test_alembic_head_matches_db_models",
          "code": "def test_alembic_head_matches_db_models(journalist_app, alembic_config, config):\n    \"\"\"This test is to make sure that our database models in `models.py` are\n    always in sync with the schema generated by `alembic upgrade head`.\n    \"\"\"\n    models_schema = get_schema(journalist_app)\n\n    reset_database(config.DATABASE_FILE)\n    upgrade(alembic_config, \"head\")\n\n    # Recreate the app to get a new SQLALCHEMY_DATABASE_URI\n    app = create_app(config)\n    alembic_schema = get_schema(app)\n\n    # The initial migration creates the table 'alembic_version', but this is\n    # not present in the schema created by `db.create_all()`.\n    alembic_schema = {k: v for k, v in list(alembic_schema.items()) if k[2] != \"alembic_version\"}\n\n    assert_schemas_equal(alembic_schema, models_schema)",
          "file": "test_alembic.py"
        },
        {
          "type": "function",
          "name": "test_alembic_migration_up_and_down",
          "code": "def test_alembic_migration_up_and_down(alembic_config, config, use_config_py, migration, _reset_db):\n    with use_config(use_config_py):\n        upgrade(alembic_config, migration)\n        downgrade(alembic_config, \"base\")",
          "file": "test_alembic.py"
        },
        {
          "type": "function",
          "name": "test_schema_unchanged_after_up_then_downgrade",
          "code": "def test_schema_unchanged_after_up_then_downgrade(alembic_config, config, migration, _reset_db):\n    if migration == \"811334d7105f\":\n        pytest.skip(\"811334d7105f_sequoia_pgp doesn't delete columns on downgrade\")\n    # Create the app here. Using a fixture will init the database.\n    app = create_app(config)\n\n    migrations = list_migrations(alembic_config, migration)\n\n    if len(migrations) > 1:\n        target = migrations[-2]\n        upgrade(alembic_config, target)\n    else:\n        # The first migration is the degenerate case where we don't need to\n        # get the database to some base state.\n        pass\n\n    original_schema = get_schema(app)\n\n    upgrade(alembic_config, \"+1\")\n    downgrade(alembic_config, \"-1\")\n\n    reverted_schema = get_schema(app)\n\n    # The initial migration is a degenerate case because it creates the table\n    # 'alembic_version', but rolling back the migration doesn't clear it.\n    if len(migrations) == 1:\n        reverted_schema = {\n            k: v for k, v in list(reverted_schema.items()) if k[2] != \"alembic_version\"\n        }\n\n    assert_schemas_equal(reverted_schema, original_schema)",
          "file": "test_alembic.py"
        },
        {
          "type": "function",
          "name": "test_upgrade_with_data",
          "code": "def test_upgrade_with_data(alembic_config, config, migration, _reset_db):\n    migrations = list_migrations(alembic_config, migration)\n    if len(migrations) == 1:\n        # Degenerate case where there is no data for the first migration\n        return\n\n    # Upgrade to one migration before the target stored in `migration`\n    last_migration = migrations[-2]\n    upgrade(alembic_config, last_migration)\n\n    # Dynamic module import\n    mod_name = f\"tests.migrations.migration_{migration}\"\n    mod = __import__(mod_name, fromlist=[\"UpgradeTester\"])\n\n    # Load the test data\n    upgrade_tester = mod.UpgradeTester(config=config)\n    upgrade_tester.load_data()\n\n    # Upgrade to the target\n    upgrade(alembic_config, migration)\n\n    # Make sure it applied \"cleanly\" for some definition of clean\n    upgrade_tester.check_upgrade()",
          "file": "test_alembic.py"
        },
        {
          "type": "function",
          "name": "test_downgrade_with_data",
          "code": "def test_downgrade_with_data(alembic_config, config, migration, _reset_db):\n    # Upgrade to the target\n    upgrade(alembic_config, migration)\n\n    # Dynamic module import\n    mod_name = f\"tests.migrations.migration_{migration}\"\n    mod = __import__(mod_name, fromlist=[\"DowngradeTester\"])\n\n    # Load the test data\n    downgrade_tester = mod.DowngradeTester(config=config)\n    downgrade_tester.load_data()\n\n    # Downgrade to previous migration\n    downgrade(alembic_config, \"-1\")\n\n    # Make sure it applied \"cleanly\" for some definition of clean\n    downgrade_tester.check_downgrade()",
          "file": "test_alembic.py"
        },
        {
          "type": "function",
          "name": "use_config",
          "code": "def use_config(use: bool) -> Generator:\n    if not use:\n        shutil.move(\"config.py\", \"config.py.moved\")\n    try:\n        yield\n    finally:\n        if not use:\n            shutil.move(\"config.py.moved\", \"config.py\")",
          "file": "test_alembic.py"
        }
      ],
      "test_redwood.py": [
        {
          "type": "function",
          "name": "key_pair",
          "code": "def key_pair():\n    return redwood.generate_source_key_pair(PASSPHRASE, \"foo@example.org\")",
          "file": "test_redwood.py"
        },
        {
          "type": "function",
          "name": "test_encrypt_stream",
          "code": "def test_encrypt_stream(tmp_path, key_pair):\n    (public_key, secret_key, fingerprint) = key_pair\n    iterations = 100_000  # force a good amount of chunks\n    file = tmp_path / \"file.asc\"\n    with SecureTemporaryFile(\"/tmp\") as stf:\n        for _ in range(iterations):\n            stf.write(SECRET_MESSAGE.encode())\n\n        redwood.encrypt_stream([public_key], stf, file)\n    ciphertext = file.read_bytes()\n    actual = redwood.decrypt(ciphertext, secret_key, PASSPHRASE)\n    assert (SECRET_MESSAGE * iterations) == actual.decode()",
          "file": "test_redwood.py"
        },
        {
          "type": "class",
          "name": "DummyReadable",
          "code": "class DummyReadable:\n    \"\"\"A fake class with a read() method that fails\"\"\"\n\n    def read(self, _len: int) -> None:\n        raise RuntimeError(\"uhoh\")",
          "file": "test_redwood.py"
        },
        {
          "type": "function",
          "name": "read",
          "code": "def read(self, _len: int) -> None:\n        raise RuntimeError(\"uhoh\")",
          "file": "test_redwood.py"
        },
        {
          "type": "function",
          "name": "test_encrypt_stream_bad",
          "code": "def test_encrypt_stream_bad(tmp_path, key_pair):\n    (public_key, secret_key, fingerprint) = key_pair\n    not_stream = SECRET_MESSAGE.encode()\n    # Passing in a type/object that has no `.read()`\n    with pytest.raises(\n        redwood.RedwoodError, match=\"AttributeError: 'bytes' object has no attribute 'read'\"\n    ):\n        redwood.encrypt_stream([public_key], not_stream, tmp_path / \"file1.asc\")\n    # When `.read()` returns something other than bytes\n    with pytest.raises(\n        redwood.RedwoodError, match=\"TypeError: 'str' object cannot be converted to 'PyBytes'\"\n    ):\n        redwood.encrypt_stream([public_key], StringIO(SECRET_MESSAGE), tmp_path / \"file2.asc\")\n    with pytest.raises(redwood.RedwoodError, match='error: \"RuntimeError: uhoh\"'):\n        redwood.encrypt_stream([public_key], DummyReadable(), tmp_path / \"file3.asc\")",
          "file": "test_redwood.py"
        }
      ],
      "functional": {
        "test_submit_and_retrieve_message.py": [
          {
            "type": "class",
            "name": "TestSubmitAndRetrieveMessage",
            "code": "class TestSubmitAndRetrieveMessage:\n    def test_submit_and_retrieve_happy_path(\n        self, sd_servers_with_clean_state, tor_browser_web_driver, firefox_web_driver\n    ):\n        # Given a source user accessing the app from their browser\n        source_app_nav = SourceAppNavigator(\n            source_app_base_url=sd_servers_with_clean_state.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        # And they created an account\n        source_app_nav.source_visits_source_homepage()\n        source_app_nav.source_clicks_submit_documents_on_homepage()\n        source_app_nav.source_continues_to_submit_page()\n\n        # And the source user submitted a message\n        submitted_message = \"Confidential message with some international characters: éèö\"\n        source_app_nav.source_submits_a_message(message=submitted_message)\n        source_app_nav.source_logs_out()\n\n        # When a journalist logs in\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n        journ_app_nav.journalist_checks_messages()\n\n        #  And they try to download the message\n        #  Then it succeeds and the journalist sees correct message\n        retrieved_message = journ_app_nav.journalist_downloads_first_message()\n        assert retrieved_message == submitted_message",
            "file": "test_submit_and_retrieve_message.py"
          },
          {
            "type": "function",
            "name": "test_submit_and_retrieve_happy_path",
            "code": "def test_submit_and_retrieve_happy_path(\n        self, sd_servers_with_clean_state, tor_browser_web_driver, firefox_web_driver\n    ):\n        # Given a source user accessing the app from their browser\n        source_app_nav = SourceAppNavigator(\n            source_app_base_url=sd_servers_with_clean_state.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        # And they created an account\n        source_app_nav.source_visits_source_homepage()\n        source_app_nav.source_clicks_submit_documents_on_homepage()\n        source_app_nav.source_continues_to_submit_page()\n\n        # And the source user submitted a message\n        submitted_message = \"Confidential message with some international characters: éèö\"\n        source_app_nav.source_submits_a_message(message=submitted_message)\n        source_app_nav.source_logs_out()\n\n        # When a journalist logs in\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n        journ_app_nav.journalist_checks_messages()\n\n        #  And they try to download the message\n        #  Then it succeeds and the journalist sees correct message\n        retrieved_message = journ_app_nav.journalist_downloads_first_message()\n        assert retrieved_message == submitted_message",
            "file": "test_submit_and_retrieve_message.py"
          }
        ],
        "conftest.py": [
          {
            "type": "function",
            "name": "firefox_web_driver",
            "code": "def firefox_web_driver(request) -> WebDriver:  # type: ignore\n    locale = request.param.replace(\"_\", \"-\")\n    with get_web_driver(\n        web_driver_type=WebDriverTypeEnum.FIREFOX, accept_languages=locale\n    ) as web_driver:\n        yield web_driver",
            "file": "conftest.py"
          },
          {
            "type": "function",
            "name": "tor_browser_web_driver",
            "code": "def tor_browser_web_driver(request) -> WebDriver:  # type: ignore\n    locale = request.param.replace(\"_\", \"-\")\n    with get_web_driver(\n        web_driver_type=WebDriverTypeEnum.TOR_BROWSER, accept_languages=locale\n    ) as web_driver:\n        yield web_driver",
            "file": "conftest.py"
          },
          {
            "type": "function",
            "name": "_get_unused_port",
            "code": "def _get_unused_port() -> int:\n    s = socket.socket()\n    s.bind((\"127.0.0.1\", 0))\n    port = s.getsockname()[1]\n    s.close()\n    return port",
            "file": "conftest.py"
          },
          {
            "type": "function",
            "name": "_start_source_server",
            "code": "def _start_source_server(port: int, config_to_use: SecureDropConfig) -> None:\n    # This function will be called in a separate Process that runs the source app\n    # Modify the sdconfig module in the app's memory so that it mirrors the supplied config\n    import sdconfig\n    from source_app import create_app\n\n    sdconfig._current_config = config_to_use\n\n    # Then start the source app\n    source_app = create_app(config_to_use)\n    source_app.run(port=port, debug=True, use_reloader=False, threaded=True)",
            "file": "conftest.py"
          },
          {
            "type": "function",
            "name": "_start_journalist_server",
            "code": "def _start_journalist_server(\n    port: int,\n    config_to_use: SecureDropConfig,\n    journalist_app_setup_callback: Optional[Callable[[SecureDropConfig], None]],\n) -> None:\n    # This function will be called in a separate Process that runs the journalist app\n    # Modify the sdconfig module in the app's memory so that it mirrors the supplied config\n    import sdconfig\n    from journalist_app import create_app\n\n    sdconfig._current_config = config_to_use\n\n    # Some tests require a specific state to be set (such as having a submission)\n    if journalist_app_setup_callback:\n        journalist_app_setup_callback(config_to_use)\n\n    # Then start the journalist app\n    journalist_app = create_app(config_to_use)\n    journalist_app.run(port=port, debug=True, use_reloader=False, threaded=True)",
            "file": "conftest.py"
          },
          {
            "type": "class",
            "name": "SdServersFixtureResult",
            "code": "class SdServersFixtureResult:\n    source_app_base_url: str\n    journalist_app_base_url: str\n\n    # The config that's being used by the source app and journalist app\n    config_in_use: SecureDropConfig\n\n    # Credentials to log into the journalist app as a journalist/admin\n    journalist_username: str\n    journalist_password: str\n    journalist_otp_secret: str\n    journalist_is_admin: bool",
            "file": "conftest.py"
          },
          {
            "type": "function",
            "name": "spawn_sd_servers",
            "code": "def spawn_sd_servers(\n    config_to_use: SecureDropConfig,\n    journalist_app_setup_callback: Optional[Callable[[SecureDropConfig], Any]] = None,\n) -> Generator[SdServersFixtureResult, None, None]:\n    \"\"\"Spawn the source and journalist apps as separate processes with the supplied config.\n\n    The journalist_app_setup_callback can be used to run a setup function within the journalist\n    app's process right before the app starts (for setting up state needed by the test).\n    \"\"\"\n    journalist_app_process = None\n    source_app_process = None\n    try:\n        # Add a test journalist\n        with get_database_session(\n            database_uri=config_to_use.DATABASE_URI\n        ) as db_session_for_sd_servers:\n            journalist_password = \"correct horse battery staple profanity oil chewy\"\n            journalist_username = \"journalist\"\n            journalist_otp_secret = \"JHCOGO7VCER3EJ4L\"\n            journalist_is_admin = True\n            journalist = Journalist(\n                username=journalist_username,\n                password=journalist_password,\n                is_admin=journalist_is_admin,\n            )\n            journalist.otp_secret = journalist_otp_secret\n            db_session_for_sd_servers.add(journalist)\n            db_session_for_sd_servers.commit()\n\n        # Spawn the source and journalist web apps in separate processes\n        source_port = _get_unused_port()\n        journalist_port = _get_unused_port()\n\n        # Start the server subprocesses using the \"spawn\" method instead of \"fork\".\n        # This is needed for the config_to_use argument to work; if \"fork\" is used, the subprocess\n        # will inherit all the globals from the parent process, which will include the Python\n        # variables declared as \"global\" in the SD code base\n        # (example: see _DesignationGenerator.get_default()).\n        # This means the config_to_use will be ignored if these globals have already been\n        # initialized (for example by tests running before the code here).\n        mp_spawn_ctx = multiprocessing.get_context(\"spawn\")\n\n        source_app_process = mp_spawn_ctx.Process(  # type: ignore\n            target=_start_source_server, args=(source_port, config_to_use)\n        )\n        source_app_process.start()\n        journalist_app_process = mp_spawn_ctx.Process(  # type: ignore\n            target=_start_journalist_server,\n            args=(journalist_port, config_to_use, journalist_app_setup_callback),\n        )\n        journalist_app_process.start()\n        source_app_base_url = f\"http://127.0.0.1:{source_port}\"\n        journalist_app_base_url = f\"http://127.0.0.1:{journalist_port}\"\n\n        # Sleep until the source and journalist web apps are up and running\n        attempts_count = 30\n        seconds_between_attempts = 0.25\n        response_source_status_code = None\n        for _ in range(attempts_count):\n            try:\n                response_source = requests.get(source_app_base_url, timeout=1)\n                response_source_status_code = response_source.status_code\n                break\n            except (requests.ConnectionError, requests.Timeout):\n                time.sleep(seconds_between_attempts)\n        assert response_source_status_code == 200\n\n        response_journalist_status_code = None\n        for _ in range(attempts_count):\n            try:\n                response_journalist = requests.get(journalist_app_base_url, timeout=1)\n                response_journalist_status_code = response_journalist.status_code\n                break\n            except (requests.ConnectionError, requests.Timeout):\n                time.sleep(seconds_between_attempts)\n        assert response_journalist_status_code == 200\n\n        # Ready for the tests\n        yield SdServersFixtureResult(\n            source_app_base_url=source_app_base_url,\n            journalist_app_base_url=journalist_app_base_url,\n            config_in_use=config_to_use,\n            journalist_username=journalist_username,\n            journalist_password=journalist_password,\n            journalist_otp_secret=journalist_otp_secret,\n            journalist_is_admin=journalist_is_admin,\n        )\n\n    # Clean everything up\n    finally:\n        if source_app_process:\n            source_app_process.terminate()\n            source_app_process.join()\n        if journalist_app_process:\n            journalist_app_process.terminate()\n            journalist_app_process.join()",
            "file": "conftest.py"
          },
          {
            "type": "function",
            "name": "sd_servers",
            "code": "def sd_servers(\n    setup_journalist_key_and_gpg_folder: Tuple[str, Path],\n    setup_rqworker: Tuple[str, Path],\n) -> Generator[SdServersFixtureResult, None, None]:\n    \"\"\"Spawn the source and journalist apps as separate processes with a default config.\n\n    This fixture is session-scoped so the apps are only spawned once during the test session, and\n    shared between the different unit tests. If your test needs to modify the state of the apps\n    (example: a source submits a message), use the sd_servers_with_clean_state fixture, which is\n    slower.\n    \"\"\"\n    journalist_key_fingerprint, gpg_key_dir = setup_journalist_key_and_gpg_folder\n    worker_name, _ = setup_rqworker\n    default_config = SecureDropConfigFactory.create(\n        SECUREDROP_DATA_ROOT=Path(\"/tmp/sd-tests/functional\"),\n        GPG_KEY_DIR=gpg_key_dir,\n        JOURNALIST_KEY=journalist_key_fingerprint,\n        RQ_WORKER_NAME=worker_name,\n        SUPPORTED_LOCALES=get_test_locales(),\n    )\n\n    # Spawn the apps in separate processes\n    with spawn_sd_servers(config_to_use=default_config) as sd_servers_result:\n        yield sd_servers_result",
            "file": "conftest.py"
          },
          {
            "type": "function",
            "name": "sd_servers_with_clean_state",
            "code": "def sd_servers_with_clean_state(\n    setup_journalist_key_and_gpg_folder: Tuple[str, Path],\n    setup_rqworker: Tuple[str, Path],\n) -> Generator[SdServersFixtureResult, None, None]:\n    \"\"\"Same as sd_servers but spawns the apps with a clean state.\n\n    Slower than sd_servers as it is function-scoped.\n    \"\"\"\n    journalist_key_fingerprint, gpg_key_dir = setup_journalist_key_and_gpg_folder\n    worker_name, _ = setup_rqworker\n    default_config = SecureDropConfigFactory.create(\n        SECUREDROP_DATA_ROOT=Path(f\"/tmp/sd-tests/functional-clean-state-{uuid4()}\"),\n        GPG_KEY_DIR=gpg_key_dir,\n        JOURNALIST_KEY=journalist_key_fingerprint,\n        RQ_WORKER_NAME=worker_name,\n        SUPPORTED_LOCALES=get_test_locales(),\n    )\n\n    # Spawn the apps in separate processes\n    with spawn_sd_servers(config_to_use=default_config) as sd_servers_result:\n        yield sd_servers_result",
            "file": "conftest.py"
          },
          {
            "type": "function",
            "name": "sd_servers_with_submitted_file",
            "code": "def sd_servers_with_submitted_file(\n    setup_journalist_key_and_gpg_folder: Tuple[str, Path],\n    setup_rqworker: Tuple[str, Path],\n) -> Generator[SdServersFixtureResult, None, None]:\n    \"\"\"Same as sd_servers but spawns the apps with an already-submitted source file.\n\n    Slower than sd_servers as it is function-scoped.\n    \"\"\"\n    journalist_key_fingerprint, gpg_key_dir = setup_journalist_key_and_gpg_folder\n    worker_name, _ = setup_rqworker\n    default_config = SecureDropConfigFactory.create(\n        SECUREDROP_DATA_ROOT=Path(f\"/tmp/sd-tests/functional-with-submitted-file-{uuid4()}\"),\n        GPG_KEY_DIR=gpg_key_dir,\n        JOURNALIST_KEY=journalist_key_fingerprint,\n        RQ_WORKER_NAME=worker_name,\n        SUPPORTED_LOCALES=get_test_locales(),\n    )\n\n    # Spawn the apps in separate processes with a callback to create a submission\n    with spawn_sd_servers(\n        config_to_use=default_config, journalist_app_setup_callback=create_source_and_submission\n    ) as sd_servers_result:\n        yield sd_servers_result",
            "file": "conftest.py"
          },
          {
            "type": "function",
            "name": "create_source_and_submission",
            "code": "def create_source_and_submission(config_in_use: SecureDropConfig) -> Tuple[SourceUser, Path]:\n    \"\"\"Directly create a source and a submission within the app.\n\n    Some tests for the journalist app require a submission to already be present, and this\n    function is used to create the source user and submission when the journalist app starts.\n\n    This implementation is much faster than using Selenium to navigate the source app in order\n    to create a submission: it takes 0.2s to run, while the Selenium implementation takes 7s.\n    \"\"\"\n    # This function will be called in a separate Process that runs the app\n    # Hence the late imports\n    from models import Submission\n    from passphrases import PassphraseGenerator\n    from source_user import create_source_user\n    from store import Storage, add_checksum_for_file\n    from tests.functional.db_session import get_database_session\n\n    # Create a source\n    passphrase = PassphraseGenerator.get_default().generate_passphrase()\n    with get_database_session(database_uri=config_in_use.DATABASE_URI) as db_session:\n        source_user = create_source_user(\n            db_session=db_session,\n            source_passphrase=passphrase,\n            source_app_storage=Storage.get_default(),\n        )\n        source_db_record = source_user.get_db_record()\n\n        # Create a file submission from this source\n        source_db_record.interaction_count += 1\n        app_storage = Storage.get_default()\n        encrypted_file_name = app_storage.save_file_submission(\n            filesystem_id=source_user.filesystem_id,\n            count=source_db_record.interaction_count,\n            journalist_filename=source_db_record.journalist_filename,\n            filename=\"filename.txt\",\n            stream=BytesIO(b\"File with S3cr3t content\"),\n        )\n        submission = Submission(source_db_record, encrypted_file_name, app_storage)\n        db_session.add(submission)\n        source_db_record.pending = False\n        source_db_record.last_updated = datetime.now(timezone.utc)\n        db_session.commit()\n\n        submission_file_path = app_storage.path(source_user.filesystem_id, submission.filename)\n        add_checksum_for_file(\n            session=db_session,\n            db_obj=submission,\n            file_path=submission_file_path,\n        )\n\n        return source_user, Path(submission_file_path)",
            "file": "conftest.py"
          }
        ],
        "tor_utils.py": [
          {
            "type": "function",
            "name": "proxies_for_url",
            "code": "def proxies_for_url(url: str) -> Optional[Dict[str, str]]:\n    \"\"\"Generate the right proxies argument to pass to requests.get() for supporting Tor.\"\"\"\n    proxies = None\n    if \".onion\" in url:\n        proxies = {\"http\": \"socks5h://127.0.0.1:9150\", \"https\": \"socks5h://127.0.0.1:9150\"}\n    return proxies",
            "file": "tor_utils.py"
          }
        ],
        "test_source_designation_collision.py": [
          {
            "type": "function",
            "name": "sd_servers_with_designation_collisions",
            "code": "def sd_servers_with_designation_collisions(setup_journalist_key_and_gpg_folder, setup_rqworker):\n    \"\"\"Spawn source and journalist apps that can only generate a single journalist designation.\"\"\"\n    # Generate a config that can only generate a single journalist designation\n    folder_for_fixture_path = Path(\"/tmp/sd-tests/functional-designation-collisions\")\n    folder_for_fixture_path.mkdir(parents=True, exist_ok=True)\n    nouns_path = folder_for_fixture_path / \"nouns.txt\"\n    nouns_path.touch(exist_ok=True)\n    nouns_path.write_text(\"accent\")\n    adjectives_path = folder_for_fixture_path / \"adjectives.txt\"\n    adjectives_path.touch(exist_ok=True)\n    adjectives_path.write_text(\"tonic\")\n\n    worker_name, _ = setup_rqworker\n    journalist_key_fingerprint, gpg_key_dir = setup_journalist_key_and_gpg_folder\n    config_for_collisions = SecureDropConfigFactory.create(\n        SECUREDROP_DATA_ROOT=folder_for_fixture_path / \"sd_data_root\",\n        NOUNS=nouns_path,\n        ADJECTIVES=adjectives_path,\n        GPG_KEY_DIR=gpg_key_dir,\n        JOURNALIST_KEY=journalist_key_fingerprint,\n        RQ_WORKER_NAME=worker_name,\n    )\n\n    # Spawn the apps in separate processes\n    with spawn_sd_servers(config_to_use=config_for_collisions) as sd_servers_result:\n        yield sd_servers_result",
            "file": "test_source_designation_collision.py"
          },
          {
            "type": "class",
            "name": "TestSourceAppDesignationCollision",
            "code": "class TestSourceAppDesignationCollision:\n    def test(self, sd_servers_with_designation_collisions, tor_browser_web_driver):\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers_with_designation_collisions.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        # Given a source user who created an account\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n        navigator.source_continues_to_submit_page()\n        navigator.source_logs_out()\n\n        # When another source user creates an account but gets the same journalist designation\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n\n        # Then the right error message is displayed\n        navigator.nav_helper.safe_click_by_css_selector(\"#create-form button\")\n        navigator.nav_helper.wait_for(\n            lambda: navigator.driver.find_element(By.CSS_SELECTOR, \".error\")\n        )\n        flash_error = navigator.driver.find_element(By.CSS_SELECTOR, \".error\")\n        assert \"There was a temporary problem creating your account\" in flash_error.text",
            "file": "test_source_designation_collision.py"
          },
          {
            "type": "function",
            "name": "test",
            "code": "def test(self, sd_servers_with_designation_collisions, tor_browser_web_driver):\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers_with_designation_collisions.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        # Given a source user who created an account\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n        navigator.source_continues_to_submit_page()\n        navigator.source_logs_out()\n\n        # When another source user creates an account but gets the same journalist designation\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n\n        # Then the right error message is displayed\n        navigator.nav_helper.safe_click_by_css_selector(\"#create-form button\")\n        navigator.nav_helper.wait_for(\n            lambda: navigator.driver.find_element(By.CSS_SELECTOR, \".error\")\n        )\n        flash_error = navigator.driver.find_element(By.CSS_SELECTOR, \".error\")\n        assert \"There was a temporary problem creating your account\" in flash_error.text",
            "file": "test_source_designation_collision.py"
          }
        ],
        "test_source.py": [
          {
            "type": "class",
            "name": "TestSourceAppCodenameHints",
            "code": "class TestSourceAppCodenameHints:\n    FIRST_SUBMISSION_TEXT = \"Please check back later for replies\"\n\n    def test_no_codename_hint_on_second_login(self, sd_servers, tor_browser_web_driver):\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        # Given a source user who creates an account\n        # When they first login\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n        navigator.source_continues_to_submit_page()\n\n        # Then they are able to retrieve their codename from the UI\n        source_codename = navigator.source_retrieves_codename_from_hint()\n        assert source_codename\n\n        # And they are able to close the codename hint UI\n        content = navigator.driver.find_element(By.ID, \"codename-show-checkbox\")\n        assert content.get_attribute(\"checked\") is not None\n        navigator.nav_helper.safe_click_by_id(\"codename-show\")\n        assert content.get_attribute(\"checked\") is None\n\n        # And on their second login\n        navigator.source_logs_out()\n        navigator.source_visits_source_homepage()\n        navigator.source_chooses_to_login()\n        navigator.source_proceeds_to_login(codename=source_codename)\n\n        # The codename hint UI is no longer present\n        codename = navigator.driver.find_elements(By.CSS_SELECTOR, \"#codename-reminder\")\n        assert len(codename) == 0\n\n    def test_submission_notifications_on_first_login(self, sd_servers, tor_browser_web_driver):\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        # Given a source user who creates an account\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n        navigator.source_continues_to_submit_page()\n\n        # When they submit a message during their first login\n        # Then it succeeds\n        confirmation_text_first_submission = navigator.source_submits_a_message()\n\n        # And they see the expected confirmation messages for a first submission on first login\n        assert self.FIRST_SUBMISSION_TEXT in confirmation_text_first_submission\n\n        # And when they submit a second message\n        confirmation_text_second_submission = navigator.source_submits_a_message()\n\n        # Then they don't see the messages since it's not their first submission\n        assert self.FIRST_SUBMISSION_TEXT not in confirmation_text_second_submission\n\n    def test_submission_notifications_on_second_login(self, sd_servers, tor_browser_web_driver):\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        # Given a source user who creates an account\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n        navigator.source_continues_to_submit_page()\n        source_codename = navigator.source_retrieves_codename_from_hint()\n        assert source_codename\n\n        # When they submit a message during their second login\n        navigator.source_logs_out()\n        navigator.source_visits_source_homepage()\n        navigator.source_chooses_to_login()\n        navigator.source_proceeds_to_login(codename=source_codename)\n\n        # Then it succeeds\n        confirmation_text_first_submission = navigator.source_submits_a_message()\n\n        # And they see the expected confirmation messages for a first submission on second login\n        assert self.FIRST_SUBMISSION_TEXT in confirmation_text_first_submission\n\n        # And when they submit a second message\n        confirmation_text_second_submission = navigator.source_submits_a_message()\n\n        # Then they don't see the messages since it's not their first submission\n        assert self.FIRST_SUBMISSION_TEXT not in confirmation_text_second_submission",
            "file": "test_source.py"
          },
          {
            "type": "function",
            "name": "test_no_codename_hint_on_second_login",
            "code": "def test_no_codename_hint_on_second_login(self, sd_servers, tor_browser_web_driver):\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        # Given a source user who creates an account\n        # When they first login\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n        navigator.source_continues_to_submit_page()\n\n        # Then they are able to retrieve their codename from the UI\n        source_codename = navigator.source_retrieves_codename_from_hint()\n        assert source_codename\n\n        # And they are able to close the codename hint UI\n        content = navigator.driver.find_element(By.ID, \"codename-show-checkbox\")\n        assert content.get_attribute(\"checked\") is not None\n        navigator.nav_helper.safe_click_by_id(\"codename-show\")\n        assert content.get_attribute(\"checked\") is None\n\n        # And on their second login\n        navigator.source_logs_out()\n        navigator.source_visits_source_homepage()\n        navigator.source_chooses_to_login()\n        navigator.source_proceeds_to_login(codename=source_codename)\n\n        # The codename hint UI is no longer present\n        codename = navigator.driver.find_elements(By.CSS_SELECTOR, \"#codename-reminder\")\n        assert len(codename) == 0",
            "file": "test_source.py"
          },
          {
            "type": "function",
            "name": "test_submission_notifications_on_first_login",
            "code": "def test_submission_notifications_on_first_login(self, sd_servers, tor_browser_web_driver):\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        # Given a source user who creates an account\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n        navigator.source_continues_to_submit_page()\n\n        # When they submit a message during their first login\n        # Then it succeeds\n        confirmation_text_first_submission = navigator.source_submits_a_message()\n\n        # And they see the expected confirmation messages for a first submission on first login\n        assert self.FIRST_SUBMISSION_TEXT in confirmation_text_first_submission\n\n        # And when they submit a second message\n        confirmation_text_second_submission = navigator.source_submits_a_message()\n\n        # Then they don't see the messages since it's not their first submission\n        assert self.FIRST_SUBMISSION_TEXT not in confirmation_text_second_submission",
            "file": "test_source.py"
          },
          {
            "type": "function",
            "name": "test_submission_notifications_on_second_login",
            "code": "def test_submission_notifications_on_second_login(self, sd_servers, tor_browser_web_driver):\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        # Given a source user who creates an account\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n        navigator.source_continues_to_submit_page()\n        source_codename = navigator.source_retrieves_codename_from_hint()\n        assert source_codename\n\n        # When they submit a message during their second login\n        navigator.source_logs_out()\n        navigator.source_visits_source_homepage()\n        navigator.source_chooses_to_login()\n        navigator.source_proceeds_to_login(codename=source_codename)\n\n        # Then it succeeds\n        confirmation_text_first_submission = navigator.source_submits_a_message()\n\n        # And they see the expected confirmation messages for a first submission on second login\n        assert self.FIRST_SUBMISSION_TEXT in confirmation_text_first_submission\n\n        # And when they submit a second message\n        confirmation_text_second_submission = navigator.source_submits_a_message()\n\n        # Then they don't see the messages since it's not their first submission\n        assert self.FIRST_SUBMISSION_TEXT not in confirmation_text_second_submission",
            "file": "test_source.py"
          },
          {
            "type": "class",
            "name": "TestSourceAppDownloadJournalistKey",
            "code": "class TestSourceAppDownloadJournalistKey:\n    def test(self, sd_servers):\n        # Given a source app, when fetching the instance's journalist public key\n        url = f\"{sd_servers.source_app_base_url}/public-key\"\n        response = requests.get(url=url, proxies=tor_utils.proxies_for_url(url))\n\n        # Then it succeeds and the right data is returned\n        assert redwood.is_valid_public_key(response.content.decode(\"utf-8\"))",
            "file": "test_source.py"
          },
          {
            "type": "function",
            "name": "test",
            "code": "def test(self, sd_servers):\n        # Given a source app, when fetching the instance's journalist public key\n        url = f\"{sd_servers.source_app_base_url}/public-key\"\n        response = requests.get(url=url, proxies=tor_utils.proxies_for_url(url))\n\n        # Then it succeeds and the right data is returned\n        assert redwood.is_valid_public_key(response.content.decode(\"utf-8\"))",
            "file": "test_source.py"
          },
          {
            "type": "class",
            "name": "TestSourceAppCodenamesInMultipleTabs",
            "code": "class TestSourceAppCodenamesInMultipleTabs:\n    \"\"\"Test generation of multiple codenames in different browser tabs, ref. issue 4458.\"\"\"\n\n    @staticmethod\n    def _assert_is_on_lookup_page(navigator: SourceAppNavigator) -> None:\n        navigator.nav_helper.wait_for(lambda: navigator.driver.find_element(By.ID, \"upload\"))\n\n    @staticmethod\n    def _extract_generated_codename(navigator: SourceAppNavigator) -> str:\n        codename = navigator.driver.find_element(By.CSS_SELECTOR, \"#codename span\").text\n        assert codename\n        return codename\n\n    @staticmethod\n    def _extract_flash_message_content(navigator: SourceAppNavigator) -> str:\n        notification = navigator.driver.find_element(By.CSS_SELECTOR, \".notification\").text\n        assert notification\n        return notification\n\n    def test_generate_codenames_in_multiple_tabs(self, sd_servers, tor_browser_web_driver):\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        # Given a user who generated a codename in Tab A\n        tab_a = navigator.driver.window_handles[0]\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n        codename_a = self._extract_generated_codename(navigator)\n\n        # And they then opened a new tab, Tab B\n        navigator.driver.execute_script(\"window.open('about:blank', '_blank')\")\n        tab_b = navigator.driver.window_handles[1]\n        navigator.driver.switch_to.window(tab_b)\n        assert tab_a != tab_b\n\n        # And they also generated another codename in Tab B\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n        codename_b = self._extract_generated_codename(navigator)\n        assert codename_a != codename_b\n\n        # And they ended up creating their account and submitting documents in Tab A\n        navigator.driver.switch_to.window(tab_a)\n        navigator.source_continues_to_submit_page()\n        self._assert_is_on_lookup_page(navigator)\n        assert navigator.source_retrieves_codename_from_hint() == codename_a\n        navigator.source_submits_a_message()\n\n        # When the user tries to create an account and submit documents in Tab B\n        navigator.driver.switch_to.window(tab_b)\n        navigator.source_continues_to_submit_page()\n\n        # Then the submission fails and the user sees the corresponding flash message in Tab B\n        self._assert_is_on_lookup_page(navigator)\n        notification = self._extract_flash_message_content(navigator)\n        if not navigator.accept_languages:\n            assert \"You are already logged in.\" in notification\n\n        # And the user's actual codename is the one initially generated in Tab A\n        assert navigator.source_retrieves_codename_from_hint() == codename_a\n\n    def test_generate_and_refresh_codenames_in_multiple_tabs(\n        self, sd_servers, tor_browser_web_driver\n    ):\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        # Given a user who generated a codename in Tab A\n        tab_a = navigator.driver.window_handles[0]\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n        codename_a1 = self._extract_generated_codename(navigator)\n\n        # And they then re-generated their codename in Tab\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n        codename_a2 = self._extract_generated_codename(navigator)\n        assert codename_a1 != codename_a2\n\n        # And they then opened a new tab, Tab B\n        navigator.driver.execute_script(\"window.open('about:blank', '_blank')\")\n        tab_b = navigator.driver.window_handles[1]\n        navigator.driver.switch_to.window(tab_b)\n        assert tab_a != tab_b\n\n        # And they also generated another codename in Tab B\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n        codename_b = self._extract_generated_codename(navigator)\n        assert codename_a2 != codename_b\n\n        # And they ended up creating their account and submitting documents in Tab A\n        navigator.driver.switch_to.window(tab_a)\n        navigator.source_continues_to_submit_page()\n        self._assert_is_on_lookup_page(navigator)\n        assert navigator.source_retrieves_codename_from_hint() == codename_a2\n        navigator.source_submits_a_message()\n\n        # When they try to re-generate a codename in Tab B\n        navigator.driver.switch_to.window(tab_b)\n        navigator.source_visits_source_homepage()\n        navigator.nav_helper.safe_click_by_css_selector(\"#started-form button\")\n\n        # Then they get redirected to /lookup with the corresponding flash message\n        self._assert_is_on_lookup_page(navigator)\n        notification = self._extract_flash_message_content(navigator)\n        if not navigator.accept_languages:\n            assert \"You were redirected because you are already logged in.\" in notification\n\n        # And the user's actual codename is the expected one\n        assert navigator.source_retrieves_codename_from_hint() == codename_a2\n\n    # TODO(AD): This test takes ~50s ; we could refactor it to speed it up\n    def test_codenames_exceed_max_cookie_size(self, sd_servers, tor_browser_web_driver):\n        \"\"\"Test generation of enough codenames that the resulting cookie exceeds the recommended\n        `werkzeug.Response.max_cookie_size` = 4093 bytes. (#6043)\n        \"\"\"\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        too_many = 2 * (werkzeug.Response.max_cookie_size // len(VALID_PASSWORD))\n        for _ in range(too_many):\n            navigator.source_visits_source_homepage()\n            navigator.source_clicks_submit_documents_on_homepage()\n\n        navigator.source_continues_to_submit_page()",
            "file": "test_source.py"
          },
          {
            "type": "function",
            "name": "_assert_is_on_lookup_page",
            "code": "def _assert_is_on_lookup_page(navigator: SourceAppNavigator) -> None:\n        navigator.nav_helper.wait_for(lambda: navigator.driver.find_element(By.ID, \"upload\"))",
            "file": "test_source.py"
          },
          {
            "type": "function",
            "name": "_extract_generated_codename",
            "code": "def _extract_generated_codename(navigator: SourceAppNavigator) -> str:\n        codename = navigator.driver.find_element(By.CSS_SELECTOR, \"#codename span\").text\n        assert codename\n        return codename",
            "file": "test_source.py"
          },
          {
            "type": "function",
            "name": "_extract_flash_message_content",
            "code": "def _extract_flash_message_content(navigator: SourceAppNavigator) -> str:\n        notification = navigator.driver.find_element(By.CSS_SELECTOR, \".notification\").text\n        assert notification\n        return notification",
            "file": "test_source.py"
          },
          {
            "type": "function",
            "name": "test_generate_codenames_in_multiple_tabs",
            "code": "def test_generate_codenames_in_multiple_tabs(self, sd_servers, tor_browser_web_driver):\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        # Given a user who generated a codename in Tab A\n        tab_a = navigator.driver.window_handles[0]\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n        codename_a = self._extract_generated_codename(navigator)\n\n        # And they then opened a new tab, Tab B\n        navigator.driver.execute_script(\"window.open('about:blank', '_blank')\")\n        tab_b = navigator.driver.window_handles[1]\n        navigator.driver.switch_to.window(tab_b)\n        assert tab_a != tab_b\n\n        # And they also generated another codename in Tab B\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n        codename_b = self._extract_generated_codename(navigator)\n        assert codename_a != codename_b\n\n        # And they ended up creating their account and submitting documents in Tab A\n        navigator.driver.switch_to.window(tab_a)\n        navigator.source_continues_to_submit_page()\n        self._assert_is_on_lookup_page(navigator)\n        assert navigator.source_retrieves_codename_from_hint() == codename_a\n        navigator.source_submits_a_message()\n\n        # When the user tries to create an account and submit documents in Tab B\n        navigator.driver.switch_to.window(tab_b)\n        navigator.source_continues_to_submit_page()\n\n        # Then the submission fails and the user sees the corresponding flash message in Tab B\n        self._assert_is_on_lookup_page(navigator)\n        notification = self._extract_flash_message_content(navigator)\n        if not navigator.accept_languages:\n            assert \"You are already logged in.\" in notification\n\n        # And the user's actual codename is the one initially generated in Tab A\n        assert navigator.source_retrieves_codename_from_hint() == codename_a",
            "file": "test_source.py"
          },
          {
            "type": "function",
            "name": "test_generate_and_refresh_codenames_in_multiple_tabs",
            "code": "def test_generate_and_refresh_codenames_in_multiple_tabs(\n        self, sd_servers, tor_browser_web_driver\n    ):\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        # Given a user who generated a codename in Tab A\n        tab_a = navigator.driver.window_handles[0]\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n        codename_a1 = self._extract_generated_codename(navigator)\n\n        # And they then re-generated their codename in Tab\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n        codename_a2 = self._extract_generated_codename(navigator)\n        assert codename_a1 != codename_a2\n\n        # And they then opened a new tab, Tab B\n        navigator.driver.execute_script(\"window.open('about:blank', '_blank')\")\n        tab_b = navigator.driver.window_handles[1]\n        navigator.driver.switch_to.window(tab_b)\n        assert tab_a != tab_b\n\n        # And they also generated another codename in Tab B\n        navigator.source_visits_source_homepage()\n        navigator.source_clicks_submit_documents_on_homepage()\n        codename_b = self._extract_generated_codename(navigator)\n        assert codename_a2 != codename_b\n\n        # And they ended up creating their account and submitting documents in Tab A\n        navigator.driver.switch_to.window(tab_a)\n        navigator.source_continues_to_submit_page()\n        self._assert_is_on_lookup_page(navigator)\n        assert navigator.source_retrieves_codename_from_hint() == codename_a2\n        navigator.source_submits_a_message()\n\n        # When they try to re-generate a codename in Tab B\n        navigator.driver.switch_to.window(tab_b)\n        navigator.source_visits_source_homepage()\n        navigator.nav_helper.safe_click_by_css_selector(\"#started-form button\")\n\n        # Then they get redirected to /lookup with the corresponding flash message\n        self._assert_is_on_lookup_page(navigator)\n        notification = self._extract_flash_message_content(navigator)\n        if not navigator.accept_languages:\n            assert \"You were redirected because you are already logged in.\" in notification\n\n        # And the user's actual codename is the expected one\n        assert navigator.source_retrieves_codename_from_hint() == codename_a2",
            "file": "test_source.py"
          },
          {
            "type": "function",
            "name": "test_codenames_exceed_max_cookie_size",
            "code": "def test_codenames_exceed_max_cookie_size(self, sd_servers, tor_browser_web_driver):\n        \"\"\"Test generation of enough codenames that the resulting cookie exceeds the recommended\n        `werkzeug.Response.max_cookie_size` = 4093 bytes. (#6043)\n        \"\"\"\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        too_many = 2 * (werkzeug.Response.max_cookie_size // len(VALID_PASSWORD))\n        for _ in range(too_many):\n            navigator.source_visits_source_homepage()\n            navigator.source_clicks_submit_documents_on_homepage()\n\n        navigator.source_continues_to_submit_page()",
            "file": "test_source.py"
          }
        ],
        "test_source_warnings.py": [
          {
            "type": "function",
            "name": "orbot_web_driver",
            "code": "def orbot_web_driver(sd_servers):\n    # Create new profile and driver with the orbot user agent\n    orbot_user_agent = \"Mozilla/5.0 (Android; Mobile; rv:52.0) Gecko/20100101 Firefox/52.0\"\n    f_profile_path2 = \"/tmp/testprofile2\"\n    if os.path.exists(f_profile_path2):\n        shutil.rmtree(f_profile_path2)\n    os.mkdir(f_profile_path2)\n    profile = webdriver.FirefoxProfile(f_profile_path2)\n    profile.set_preference(\"general.useragent.override\", orbot_user_agent)\n\n    orbot_options = webdriver.FirefoxOptions()\n    orbot_options.binary_location = _FIREFOX_PATH\n    orbot_options.profile = profile\n\n    if sd_servers.journalist_app_base_url.find(\".onion\") != -1:\n        # set FF preference to socks proxy in Tor Browser\n        profile.set_preference(\"network.proxy.type\", 1)\n        profile.set_preference(\"network.proxy.socks\", \"127.0.0.1\")\n        profile.set_preference(\"network.proxy.socks_port\", 9150)\n        profile.set_preference(\"network.proxy.socks_version\", 5)\n        profile.set_preference(\"network.proxy.socks_remote_dns\", True)\n        profile.set_preference(\"network.dns.blockDotOnion\", False)\n    profile.update_preferences()\n    orbot_web_driver = webdriver.Firefox(options=orbot_options)\n\n    try:\n        driver_user_agent = orbot_web_driver.execute_script(\"return navigator.userAgent\")\n        assert driver_user_agent == orbot_user_agent\n        yield orbot_web_driver\n    finally:\n        orbot_web_driver.quit()",
            "file": "test_source_warnings.py"
          },
          {
            "type": "class",
            "name": "TestSourceAppBrowserWarnings",
            "code": "class TestSourceAppBrowserWarnings:\n    def test_warning_appears_if_tor_browser_not_in_use(self, sd_servers, firefox_web_driver):\n        # Given a user\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            # Who is using Firefox instead of the tor browser\n            web_driver=firefox_web_driver,\n        )\n\n        # When they access the source app's home page\n        navigator.source_visits_source_homepage()\n\n        # Then they see a warning\n        warning_banner = navigator.driver.find_element(By.ID, \"browser-tb\")\n        assert warning_banner.is_displayed()\n        assert \"It is recommended to use Tor Browser\" in warning_banner.text\n\n        # And they are able to dismiss the warning\n        warning_dismiss_button = navigator.driver.find_element(By.ID, \"browser-tb-close\")\n        warning_dismiss_button.click()\n\n        def warning_banner_is_hidden():\n            assert warning_banner.is_displayed() is False\n\n        navigator.nav_helper.wait_for(warning_banner_is_hidden)\n\n    def test_warning_appears_if_orbot_is_used(self, sd_servers, orbot_web_driver):\n        # Given a user\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            # Who is using Orbot instead of the (desktop) Tor browser\n            web_driver=orbot_web_driver,\n        )\n\n        # When they access the source app's home page\n        navigator.source_visits_source_homepage()\n\n        # Then they see a warning\n        warning_banner = navigator.driver.find_element(By.ID, \"browser-android\")\n        assert warning_banner.is_displayed()\n        assert \"use the desktop version of Tor Browser\" in warning_banner.text\n\n        # And they are able to dismiss the warning\n        warning_dismiss_button = navigator.driver.find_element(By.ID, \"browser-android-close\")\n        warning_dismiss_button.click()\n\n        def warning_banner_is_hidden():\n            assert warning_banner.is_displayed() is False\n\n        navigator.nav_helper.wait_for(warning_banner_is_hidden)\n\n    def test_warning_high_security(self, sd_servers, tor_browser_web_driver):\n        # Given a user\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            # Who is using the Tor browser\n            web_driver=tor_browser_web_driver,\n        )\n\n        # When they access the source app's home page\n        navigator.source_visits_source_homepage()\n\n        # Then they see a warning\n        banner = navigator.driver.find_element(By.ID, \"browser-security-level\")\n        assert banner.is_displayed()\n        assert \"Security Level is too low\" in banner.text",
            "file": "test_source_warnings.py"
          },
          {
            "type": "function",
            "name": "test_warning_appears_if_tor_browser_not_in_use",
            "code": "def test_warning_appears_if_tor_browser_not_in_use(self, sd_servers, firefox_web_driver):\n        # Given a user\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            # Who is using Firefox instead of the tor browser\n            web_driver=firefox_web_driver,\n        )\n\n        # When they access the source app's home page\n        navigator.source_visits_source_homepage()\n\n        # Then they see a warning\n        warning_banner = navigator.driver.find_element(By.ID, \"browser-tb\")\n        assert warning_banner.is_displayed()\n        assert \"It is recommended to use Tor Browser\" in warning_banner.text\n\n        # And they are able to dismiss the warning\n        warning_dismiss_button = navigator.driver.find_element(By.ID, \"browser-tb-close\")\n        warning_dismiss_button.click()\n\n        def warning_banner_is_hidden():\n            assert warning_banner.is_displayed() is False\n\n        navigator.nav_helper.wait_for(warning_banner_is_hidden)",
            "file": "test_source_warnings.py"
          },
          {
            "type": "function",
            "name": "warning_banner_is_hidden",
            "code": "def warning_banner_is_hidden():\n            assert warning_banner.is_displayed() is False",
            "file": "test_source_warnings.py"
          },
          {
            "type": "function",
            "name": "test_warning_appears_if_orbot_is_used",
            "code": "def test_warning_appears_if_orbot_is_used(self, sd_servers, orbot_web_driver):\n        # Given a user\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            # Who is using Orbot instead of the (desktop) Tor browser\n            web_driver=orbot_web_driver,\n        )\n\n        # When they access the source app's home page\n        navigator.source_visits_source_homepage()\n\n        # Then they see a warning\n        warning_banner = navigator.driver.find_element(By.ID, \"browser-android\")\n        assert warning_banner.is_displayed()\n        assert \"use the desktop version of Tor Browser\" in warning_banner.text\n\n        # And they are able to dismiss the warning\n        warning_dismiss_button = navigator.driver.find_element(By.ID, \"browser-android-close\")\n        warning_dismiss_button.click()\n\n        def warning_banner_is_hidden():\n            assert warning_banner.is_displayed() is False\n\n        navigator.nav_helper.wait_for(warning_banner_is_hidden)",
            "file": "test_source_warnings.py"
          },
          {
            "type": "function",
            "name": "warning_banner_is_hidden",
            "code": "def warning_banner_is_hidden():\n            assert warning_banner.is_displayed() is False",
            "file": "test_source_warnings.py"
          },
          {
            "type": "function",
            "name": "test_warning_high_security",
            "code": "def test_warning_high_security(self, sd_servers, tor_browser_web_driver):\n        # Given a user\n        navigator = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            # Who is using the Tor browser\n            web_driver=tor_browser_web_driver,\n        )\n\n        # When they access the source app's home page\n        navigator.source_visits_source_homepage()\n\n        # Then they see a warning\n        banner = navigator.driver.find_element(By.ID, \"browser-security-level\")\n        assert banner.is_displayed()\n        assert \"Security Level is too low\" in banner.text",
            "file": "test_source_warnings.py"
          }
        ],
        "web_drivers.py": [
          {
            "type": "class",
            "name": "WebDriverTypeEnum",
            "code": "class WebDriverTypeEnum(Enum):\n    TOR_BROWSER = 1\n    FIREFOX = 2",
            "file": "web_drivers.py"
          },
          {
            "type": "function",
            "name": "_create_torbrowser_driver",
            "code": "def _create_torbrowser_driver(\n    accept_languages: Optional[str] = None,\n) -> TorBrowserDriver:\n    logging.info(\"Creating TorBrowserDriver\")\n    log_file = open(_LOGFILE_PATH, \"a\")\n    log_file.write(f\"\\n\\n[{datetime.now()}] Running Functional Tests\\n\")\n    log_file.flush()\n\n    # Don't use Tor when reading from localhost, and turn off private\n    # browsing. We need to turn off private browsing because we won't be\n    # able to access the browser's cookies in private browsing mode. Since\n    # we use session cookies in SD anyway (in private browsing mode all\n    # cookies are set as session cookies), this should not affect session\n    # lifetime.\n    pref_dict = {\n        \"network.proxy.no_proxies_on\": \"127.0.0.1\",\n        \"browser.privatebrowsing.autostart\": False,\n    }\n\n    Path(_TBB_PATH).mkdir(parents=True, exist_ok=True)\n    torbrowser_driver = None\n    for i in range(_DRIVER_RETRY_COUNT):\n        try:\n            torbrowser_driver = TorBrowserDriver(\n                _TBB_PATH,\n                tor_cfg=cm.USE_RUNNING_TOR,\n                pref_dict=pref_dict,\n                tbb_logfile_path=_LOGFILE_PATH,\n            )\n            if accept_languages is not None:\n                # privacy.spoof_english per\n                # <https://github.com/arkenfox/user.js/issues/1827#issuecomment-2075819482>.\n                set_tbb_pref(torbrowser_driver, \"privacy.spoof_english\", 1)\n                set_tbb_pref(torbrowser_driver, \"intl.locale.requested\", accept_languages)\n\n            logging.info(\"Created Tor Browser web driver\")\n            torbrowser_driver.set_window_position(0, 0)\n            torbrowser_driver.set_window_size(*_BROWSER_SIZE)\n            break\n        except Exception as e:\n            logging.error(\"Error creating Tor Browser web driver: %s\", e)\n            if i < _DRIVER_RETRY_COUNT:\n                time.sleep(_DRIVER_RETRY_INTERVAL)\n\n    if not torbrowser_driver:\n        raise Exception(\"Could not create Tor Browser web driver\")\n\n    # Add this attribute to the returned driver object so that tests using this\n    # fixture can know what locale it's parameterized with.\n    torbrowser_driver.locale = accept_languages  # type: ignore[attr-defined]\n\n    return torbrowser_driver",
            "file": "web_drivers.py"
          },
          {
            "type": "function",
            "name": "_create_firefox_driver",
            "code": "def _create_firefox_driver(\n    accept_languages: Optional[str] = None,\n) -> webdriver.Firefox:\n    logging.info(\"Creating Firefox web driver\")\n\n    firefox_options = webdriver.FirefoxOptions()\n    firefox_options.binary_location = _FIREFOX_PATH\n    if accept_languages is not None:\n        firefox_options.set_preference(\"intl.accept_languages\", accept_languages)\n\n    firefox_driver = None\n    for i in range(_DRIVER_RETRY_COUNT):\n        try:\n            firefox_driver = webdriver.Firefox(options=firefox_options)\n            firefox_driver.set_window_position(0, 0)\n            firefox_driver.set_window_size(*_BROWSER_SIZE)\n            logging.info(\"Created Firefox web driver\")\n            break\n        except Exception as e:\n            logging.error(\"Error creating Firefox web driver: %s\", e)\n            if i < _DRIVER_RETRY_COUNT:\n                time.sleep(_DRIVER_RETRY_INTERVAL)\n\n    if not firefox_driver:\n        raise Exception(\"Could not create Firefox web driver\")\n\n    # Add this attribute to the returned driver object so that tests using this\n    # fixture can know what locale it's parameterized with.\n    firefox_driver.locale = accept_languages  # type: ignore[attr-defined]\n\n    return firefox_driver",
            "file": "web_drivers.py"
          },
          {
            "type": "function",
            "name": "get_web_driver",
            "code": "def get_web_driver(\n    web_driver_type: WebDriverTypeEnum = WebDriverTypeEnum.TOR_BROWSER,\n    accept_languages: Optional[str] = None,\n) -> Generator[WebDriver, None, None]:\n    if web_driver_type == WebDriverTypeEnum.TOR_BROWSER:\n        web_driver = _create_torbrowser_driver(accept_languages=accept_languages)\n    elif web_driver_type == WebDriverTypeEnum.FIREFOX:\n        web_driver = _create_firefox_driver(accept_languages=accept_languages)\n    else:\n        raise ValueError(f\"Unexpected value {web_driver_type}\")\n\n    try:\n        yield web_driver\n    finally:\n        try:\n            web_driver.quit()\n        except Exception:\n            logging.exception(\"Error stopping driver\")",
            "file": "web_drivers.py"
          }
        ],
        "test_journalist.py": [
          {
            "type": "class",
            "name": "TestJournalist",
            "code": "class TestJournalist:\n    def test_journalist_verifies_deletion_of_one_submission_modal(\n        self, sd_servers_with_submitted_file, firefox_web_driver\n    ):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # And the journalist went to the individual source's page\n        journ_app_nav.journalist_visits_col()\n\n        # And the source has at least one submission\n        initial_submissions_count = journ_app_nav.count_submissions_on_current_page()\n        assert initial_submissions_count > 0\n\n        # And the journalist selected the first submission\n        journ_app_nav.journalist_selects_first_doc()\n\n        # When the journalist clicks the delete button...\n        journ_app_nav.journalist_clicks_delete_selected_link()\n        # ...but then cancels the deletion\n        journ_app_nav.nav_helper.safe_click_by_id(\"cancel-selected-deletions\")\n\n        # Then they see the same number of submissions as before\n        submissions_after_canceling_count = journ_app_nav.count_submissions_on_current_page()\n        assert submissions_after_canceling_count == initial_submissions_count\n\n        # And when the journalist clicks the delete button...\n        journ_app_nav.journalist_clicks_delete_selected_link()\n        # ... and then confirms the deletion\n        journ_app_nav.journalist_confirms_delete_selected()\n\n        # Then they see less submissions than before because one was deleted\n        def submission_deleted():\n            submissions_after_confirming_count = journ_app_nav.count_submissions_on_current_page()\n            assert submissions_after_confirming_count < initial_submissions_count\n\n        journ_app_nav.nav_helper.wait_for(submission_deleted)\n\n    def test_journalist_uses_col_delete_collection_button_modal(\n        self, sd_servers_with_submitted_file, firefox_web_driver\n    ):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # And the journalist went to the individual source's page\n        journ_app_nav.journalist_visits_col()\n\n        # And the source has at least one submission\n        initial_submissions_count = journ_app_nav.count_submissions_on_current_page()\n        assert initial_submissions_count > 0\n\n        # When the journalist clicks the delete collection button...\n        self._journalist_clicks_delete_collection_link(journ_app_nav)\n        # ...but then cancels the deletion\n        journ_app_nav.nav_helper.safe_click_by_id(\"cancel-collection-deletions\")\n\n        # Then they see the same number of submissions as before\n        submissions_after_canceling_count = journ_app_nav.count_submissions_on_current_page()\n        assert submissions_after_canceling_count == initial_submissions_count\n\n        # When the journalist clicks the delete collection button...\n        self._journalist_clicks_delete_collection_link(journ_app_nav)\n        # ... and then confirms the deletion\n        journ_app_nav.nav_helper.safe_click_by_id(\"delete-collection-button\")\n\n        # Then the journalist was redirected to the home page\n        assert journ_app_nav.is_on_journalist_homepage()\n\n    @staticmethod\n    def _journalist_clicks_delete_collection_link(journ_app_nav: JournalistAppNavigator) -> None:\n        journ_app_nav.nav_helper.safe_click_by_id(\"delete-collection-link\")\n        journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.ID, \"delete-collection-confirmation-modal\")\n        )\n\n    def test_journalist_uses_index_delete_collections_button_modal(\n        self, sd_servers_with_submitted_file, firefox_web_driver\n    ):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # And at least one source previously used the app\n        initial_sources_count = journ_app_nav.count_sources_on_index_page()\n        assert initial_sources_count > 0\n\n        # And the journalist selected all sources on the index page\n        try:\n            # If JavaScript is enabled, use the select_all button.\n            journ_app_nav.driver.find_element(By.ID, \"select_all\")\n            journ_app_nav.nav_helper.safe_click_by_id(\"select_all\")\n        except NoSuchElementException:\n            journ_app_nav.nav_helper.safe_click_all_by_css_selector(\n                'input[type=\"checkbox\"][name=\"cols_selected\"]'\n            )\n\n        # When the journalist clicks the delete collection button...\n        self._journalist_clicks_delete_collections_link(journ_app_nav)\n        # ...but then cancels the deletion\n        self._journalist_clicks_delete_collections_cancel_on_first_modal(journ_app_nav)\n\n        # Then they see the same number of sources as before\n        assert initial_sources_count == journ_app_nav.count_sources_on_index_page()\n\n        # And when the journalist clicks the delete collection button again...\n        self._journalist_clicks_delete_collections_link(journ_app_nav)\n        # ...and then confirms the deletion...\n        self._journalist_clicks_delete_collections_on_first_modal(journ_app_nav)\n        # ... and cancels the deletion on the second modal/confirmation prompt\n        journ_app_nav.nav_helper.safe_click_by_id(\"cancel-collections-deletions\")\n\n        # Then they see the same number of sources as before\n        assert initial_sources_count == journ_app_nav.count_sources_on_index_page()\n\n        # And when the journalist clicks the delete collection button again and confirms it\n        self._journalist_clicks_delete_collections_link(journ_app_nav)\n        self._journalist_clicks_delete_collections_on_first_modal(journ_app_nav)\n        journ_app_nav.nav_helper.safe_click_by_id(\"delete-collections-confirm\")\n\n        # Then a message shows up to say that the collection was deleted\n        def collection_deleted():\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            assert \"The account and all data for the source have been deleted.\" in flash_msg.text\n\n        journ_app_nav.nav_helper.wait_for(collection_deleted)\n\n        # And the journalist gets redirected to the index with the source not present anymore\n        def no_sources():\n            assert journ_app_nav.count_sources_on_index_page() == 0\n\n        journ_app_nav.nav_helper.wait_for(no_sources)\n\n    @staticmethod\n    def _journalist_clicks_delete_collections_link(journ_app_nav: JournalistAppNavigator) -> None:\n        journ_app_nav.nav_helper.safe_click_by_id(\"delete-collections-link\")\n        journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.ID, \"delete-sources-modal\")\n        )\n\n    @staticmethod\n    def _journalist_clicks_delete_collections_cancel_on_first_modal(\n        journ_app_nav: JournalistAppNavigator,\n    ) -> None:\n        journ_app_nav.nav_helper.safe_click_by_id(\"delete-menu-dialog-cancel\")\n\n    @staticmethod\n    def _journalist_clicks_delete_collections_on_first_modal(\n        journ_app_nav: JournalistAppNavigator,\n    ) -> None:\n        journ_app_nav.nav_helper.safe_click_by_id(\"delete-collections\")\n        journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.ID, \"delete-collections-confirm\")\n        )\n\n    def test_journalist_interface_ui_with_modal(\n        self, sd_servers_with_submitted_file, firefox_web_driver\n    ):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # When the journalist uses the filter by sources to find text that doesn't match any source\n        filter_box = journ_app_nav.nav_helper.safe_send_keys_by_id(\n            \"filter\", \"thiswordisnotinthewordlist\"\n        )\n\n        # Then no sources are displayed on the page\n        sources = journ_app_nav.get_sources_on_index_page()\n        assert len(sources) > 0\n        for source in sources:\n            assert source.is_displayed() is False\n\n        # And when the journalist clears the filter\n        filter_box.clear()\n        filter_box.send_keys(Keys.RETURN)\n\n        # Then all sources are displayed\n        for source in sources:\n            assert source.is_displayed() is True\n\n        # And given the journalist designation of the first source\n        sources = journ_app_nav.get_sources_on_index_page()\n        assert len(sources) > 0\n        first_source_designation = sources[0].text\n\n        # When the journalist uses the filter find this source designation\n        filter_box.send_keys(first_source_designation)\n\n        # Then only the corresponding source is displayed\n        for source in sources:\n            assert source.text == first_source_designation or source.is_displayed() is False\n\n        # And when clicking \"select all\"\n        select_all = journ_app_nav.driver.find_element(By.ID, \"select_all\")\n        select_all.click()\n\n        # Then only the visible source gets selected\n        source_rows = journ_app_nav.driver.find_elements(By.CSS_SELECTOR, \"#cols li.source\")\n        for source_row in source_rows:\n            source_designation = source_row.get_attribute(\"data-source-designation\")\n            checkbox = source_row.find_element(By.CSS_SELECTOR, \"input[type=checkbox]\")\n            if source_designation == first_source_designation:\n                assert checkbox.is_selected()\n            else:\n                assert not checkbox.is_selected()\n\n        # And when the journalist clears the filter and then selects all sources\n        filter_box.clear()\n        filter_box.send_keys(Keys.RETURN)\n        select_all.click()\n        for source_row in source_rows:\n            checkbox = source_row.find_element(By.CSS_SELECTOR, \"input[type=checkbox]\")\n            assert checkbox.is_selected()\n\n        # And then they filter again and click \"select none\"\n        filter_box.send_keys(first_source_designation)\n        select_none = journ_app_nav.driver.find_element(By.ID, \"select_none\")\n        select_none.click()\n\n        # Then only the visible source gets de-selected\n        for source_row in source_rows:\n            source_designation = source_row.get_attribute(\"data-source-designation\")\n            checkbox = source_row.find_element(By.CSS_SELECTOR, \"input[type=checkbox]\")\n            if source_designation == first_source_designation:\n                assert not checkbox.is_selected()\n            else:\n                assert checkbox.is_selected()\n\n        # And when the journalist clears the filter and leaves none selected\n        filter_box.clear()\n        filter_box.send_keys(Keys.RETURN)\n        select_none.click()\n\n        for source_row in source_rows:\n            assert source_row.is_displayed()\n            checkbox = source_row.find_element(By.CSS_SELECTOR, \"input[type=checkbox]\")\n            assert not checkbox.is_selected()\n\n        # And the journalist clicks \"select all\" then all sources are selected\n        journ_app_nav.driver.find_element(By.ID, \"select_all\").click()\n        checkboxes = journ_app_nav.driver.find_elements(By.ID, \"checkbox\")\n        for checkbox in checkboxes:\n            assert checkbox.is_selected()\n\n        # And when the journalist clicks \"select none\" then no sources are selected\n        journ_app_nav.driver.find_element(By.ID, \"select_none\").click()\n        checkboxes = journ_app_nav.driver.find_elements(By.ID, \"checkbox\")\n        for checkbox in checkboxes:\n            assert checkbox.is_selected() is False\n\n        # And when the journalist clicks \"select unread\" then all unread sources are selected\n        journ_app_nav.journalist_selects_the_first_source()\n        journ_app_nav.driver.find_element(By.ID, \"select_unread\").click()\n        checkboxes = journ_app_nav.get_submission_checkboxes_on_current_page()\n        for checkbox in checkboxes:\n            classes = checkbox.get_attribute(\"class\")\n            assert \"unread-cb\" in classes\n\n        # And when the journalist clicks the delete button, it succeeds\n        journ_app_nav.journalist_clicks_delete_all_and_sees_confirmation()\n\n    def test_journalist_uses_index_delete_files_button_modal(\n        self, sd_servers_with_submitted_file, firefox_web_driver\n    ):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # And at least one source previously used the app\n        initial_sources_count = journ_app_nav.count_sources_on_index_page()\n        assert initial_sources_count > 0\n\n        # And the journalist selected all sources on the index page\n        try:\n            # If JavaScript is enabled, use the select_all button.\n            journ_app_nav.driver.find_element(By.ID, \"select_all\")\n            journ_app_nav.nav_helper.safe_click_by_id(\"select_all\")\n        except NoSuchElementException:\n            journ_app_nav.nav_helper.safe_click_all_by_css_selector(\n                'input[type=\"checkbox\"][name=\"cols_selected\"]'\n            )\n\n        # When the journalist clicks the delete collection button...\n        self._journalist_clicks_delete_collections_link(journ_app_nav)\n        # ...and then clicks the delete files button on the first modal\n        journ_app_nav.nav_helper.safe_click_by_id(\"delete-files-and-messages\")\n\n        # Then they are redirected to the index with the source present, files\n        # and messages zeroed, and a success flash message present\n        def one_source_no_files():\n            assert journ_app_nav.count_sources_on_index_page() == 1\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            assert \"The files and messages have been deleted\" in flash_msg.text\n            counts = journ_app_nav.driver.find_elements(By.CSS_SELECTOR, \".submission-count\")\n            assert \"0 docs\" in counts[0].text\n            assert \"0 messages\" in counts[1].text\n\n        journ_app_nav.nav_helper.wait_for(one_source_no_files)",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "test_journalist_verifies_deletion_of_one_submission_modal",
            "code": "def test_journalist_verifies_deletion_of_one_submission_modal(\n        self, sd_servers_with_submitted_file, firefox_web_driver\n    ):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # And the journalist went to the individual source's page\n        journ_app_nav.journalist_visits_col()\n\n        # And the source has at least one submission\n        initial_submissions_count = journ_app_nav.count_submissions_on_current_page()\n        assert initial_submissions_count > 0\n\n        # And the journalist selected the first submission\n        journ_app_nav.journalist_selects_first_doc()\n\n        # When the journalist clicks the delete button...\n        journ_app_nav.journalist_clicks_delete_selected_link()\n        # ...but then cancels the deletion\n        journ_app_nav.nav_helper.safe_click_by_id(\"cancel-selected-deletions\")\n\n        # Then they see the same number of submissions as before\n        submissions_after_canceling_count = journ_app_nav.count_submissions_on_current_page()\n        assert submissions_after_canceling_count == initial_submissions_count\n\n        # And when the journalist clicks the delete button...\n        journ_app_nav.journalist_clicks_delete_selected_link()\n        # ... and then confirms the deletion\n        journ_app_nav.journalist_confirms_delete_selected()\n\n        # Then they see less submissions than before because one was deleted\n        def submission_deleted():\n            submissions_after_confirming_count = journ_app_nav.count_submissions_on_current_page()\n            assert submissions_after_confirming_count < initial_submissions_count\n\n        journ_app_nav.nav_helper.wait_for(submission_deleted)",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "submission_deleted",
            "code": "def submission_deleted():\n            submissions_after_confirming_count = journ_app_nav.count_submissions_on_current_page()\n            assert submissions_after_confirming_count < initial_submissions_count",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "test_journalist_uses_col_delete_collection_button_modal",
            "code": "def test_journalist_uses_col_delete_collection_button_modal(\n        self, sd_servers_with_submitted_file, firefox_web_driver\n    ):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # And the journalist went to the individual source's page\n        journ_app_nav.journalist_visits_col()\n\n        # And the source has at least one submission\n        initial_submissions_count = journ_app_nav.count_submissions_on_current_page()\n        assert initial_submissions_count > 0\n\n        # When the journalist clicks the delete collection button...\n        self._journalist_clicks_delete_collection_link(journ_app_nav)\n        # ...but then cancels the deletion\n        journ_app_nav.nav_helper.safe_click_by_id(\"cancel-collection-deletions\")\n\n        # Then they see the same number of submissions as before\n        submissions_after_canceling_count = journ_app_nav.count_submissions_on_current_page()\n        assert submissions_after_canceling_count == initial_submissions_count\n\n        # When the journalist clicks the delete collection button...\n        self._journalist_clicks_delete_collection_link(journ_app_nav)\n        # ... and then confirms the deletion\n        journ_app_nav.nav_helper.safe_click_by_id(\"delete-collection-button\")\n\n        # Then the journalist was redirected to the home page\n        assert journ_app_nav.is_on_journalist_homepage()",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "_journalist_clicks_delete_collection_link",
            "code": "def _journalist_clicks_delete_collection_link(journ_app_nav: JournalistAppNavigator) -> None:\n        journ_app_nav.nav_helper.safe_click_by_id(\"delete-collection-link\")\n        journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.ID, \"delete-collection-confirmation-modal\")\n        )",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "test_journalist_uses_index_delete_collections_button_modal",
            "code": "def test_journalist_uses_index_delete_collections_button_modal(\n        self, sd_servers_with_submitted_file, firefox_web_driver\n    ):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # And at least one source previously used the app\n        initial_sources_count = journ_app_nav.count_sources_on_index_page()\n        assert initial_sources_count > 0\n\n        # And the journalist selected all sources on the index page\n        try:\n            # If JavaScript is enabled, use the select_all button.\n            journ_app_nav.driver.find_element(By.ID, \"select_all\")\n            journ_app_nav.nav_helper.safe_click_by_id(\"select_all\")\n        except NoSuchElementException:\n            journ_app_nav.nav_helper.safe_click_all_by_css_selector(\n                'input[type=\"checkbox\"][name=\"cols_selected\"]'\n            )\n\n        # When the journalist clicks the delete collection button...\n        self._journalist_clicks_delete_collections_link(journ_app_nav)\n        # ...but then cancels the deletion\n        self._journalist_clicks_delete_collections_cancel_on_first_modal(journ_app_nav)\n\n        # Then they see the same number of sources as before\n        assert initial_sources_count == journ_app_nav.count_sources_on_index_page()\n\n        # And when the journalist clicks the delete collection button again...\n        self._journalist_clicks_delete_collections_link(journ_app_nav)\n        # ...and then confirms the deletion...\n        self._journalist_clicks_delete_collections_on_first_modal(journ_app_nav)\n        # ... and cancels the deletion on the second modal/confirmation prompt\n        journ_app_nav.nav_helper.safe_click_by_id(\"cancel-collections-deletions\")\n\n        # Then they see the same number of sources as before\n        assert initial_sources_count == journ_app_nav.count_sources_on_index_page()\n\n        # And when the journalist clicks the delete collection button again and confirms it\n        self._journalist_clicks_delete_collections_link(journ_app_nav)\n        self._journalist_clicks_delete_collections_on_first_modal(journ_app_nav)\n        journ_app_nav.nav_helper.safe_click_by_id(\"delete-collections-confirm\")\n\n        # Then a message shows up to say that the collection was deleted\n        def collection_deleted():\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            assert \"The account and all data for the source have been deleted.\" in flash_msg.text\n\n        journ_app_nav.nav_helper.wait_for(collection_deleted)\n\n        # And the journalist gets redirected to the index with the source not present anymore\n        def no_sources():\n            assert journ_app_nav.count_sources_on_index_page() == 0\n\n        journ_app_nav.nav_helper.wait_for(no_sources)",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "collection_deleted",
            "code": "def collection_deleted():\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            assert \"The account and all data for the source have been deleted.\" in flash_msg.text",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "no_sources",
            "code": "def no_sources():\n            assert journ_app_nav.count_sources_on_index_page() == 0",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "_journalist_clicks_delete_collections_link",
            "code": "def _journalist_clicks_delete_collections_link(journ_app_nav: JournalistAppNavigator) -> None:\n        journ_app_nav.nav_helper.safe_click_by_id(\"delete-collections-link\")\n        journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.ID, \"delete-sources-modal\")\n        )",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "_journalist_clicks_delete_collections_cancel_on_first_modal",
            "code": "def _journalist_clicks_delete_collections_cancel_on_first_modal(\n        journ_app_nav: JournalistAppNavigator,\n    ) -> None:\n        journ_app_nav.nav_helper.safe_click_by_id(\"delete-menu-dialog-cancel\")",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "_journalist_clicks_delete_collections_on_first_modal",
            "code": "def _journalist_clicks_delete_collections_on_first_modal(\n        journ_app_nav: JournalistAppNavigator,\n    ) -> None:\n        journ_app_nav.nav_helper.safe_click_by_id(\"delete-collections\")\n        journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.ID, \"delete-collections-confirm\")\n        )",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "test_journalist_interface_ui_with_modal",
            "code": "def test_journalist_interface_ui_with_modal(\n        self, sd_servers_with_submitted_file, firefox_web_driver\n    ):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # When the journalist uses the filter by sources to find text that doesn't match any source\n        filter_box = journ_app_nav.nav_helper.safe_send_keys_by_id(\n            \"filter\", \"thiswordisnotinthewordlist\"\n        )\n\n        # Then no sources are displayed on the page\n        sources = journ_app_nav.get_sources_on_index_page()\n        assert len(sources) > 0\n        for source in sources:\n            assert source.is_displayed() is False\n\n        # And when the journalist clears the filter\n        filter_box.clear()\n        filter_box.send_keys(Keys.RETURN)\n\n        # Then all sources are displayed\n        for source in sources:\n            assert source.is_displayed() is True\n\n        # And given the journalist designation of the first source\n        sources = journ_app_nav.get_sources_on_index_page()\n        assert len(sources) > 0\n        first_source_designation = sources[0].text\n\n        # When the journalist uses the filter find this source designation\n        filter_box.send_keys(first_source_designation)\n\n        # Then only the corresponding source is displayed\n        for source in sources:\n            assert source.text == first_source_designation or source.is_displayed() is False\n\n        # And when clicking \"select all\"\n        select_all = journ_app_nav.driver.find_element(By.ID, \"select_all\")\n        select_all.click()\n\n        # Then only the visible source gets selected\n        source_rows = journ_app_nav.driver.find_elements(By.CSS_SELECTOR, \"#cols li.source\")\n        for source_row in source_rows:\n            source_designation = source_row.get_attribute(\"data-source-designation\")\n            checkbox = source_row.find_element(By.CSS_SELECTOR, \"input[type=checkbox]\")\n            if source_designation == first_source_designation:\n                assert checkbox.is_selected()\n            else:\n                assert not checkbox.is_selected()\n\n        # And when the journalist clears the filter and then selects all sources\n        filter_box.clear()\n        filter_box.send_keys(Keys.RETURN)\n        select_all.click()\n        for source_row in source_rows:\n            checkbox = source_row.find_element(By.CSS_SELECTOR, \"input[type=checkbox]\")\n            assert checkbox.is_selected()\n\n        # And then they filter again and click \"select none\"\n        filter_box.send_keys(first_source_designation)\n        select_none = journ_app_nav.driver.find_element(By.ID, \"select_none\")\n        select_none.click()\n\n        # Then only the visible source gets de-selected\n        for source_row in source_rows:\n            source_designation = source_row.get_attribute(\"data-source-designation\")\n            checkbox = source_row.find_element(By.CSS_SELECTOR, \"input[type=checkbox]\")\n            if source_designation == first_source_designation:\n                assert not checkbox.is_selected()\n            else:\n                assert checkbox.is_selected()\n\n        # And when the journalist clears the filter and leaves none selected\n        filter_box.clear()\n        filter_box.send_keys(Keys.RETURN)\n        select_none.click()\n\n        for source_row in source_rows:\n            assert source_row.is_displayed()\n            checkbox = source_row.find_element(By.CSS_SELECTOR, \"input[type=checkbox]\")\n            assert not checkbox.is_selected()\n\n        # And the journalist clicks \"select all\" then all sources are selected\n        journ_app_nav.driver.find_element(By.ID, \"select_all\").click()\n        checkboxes = journ_app_nav.driver.find_elements(By.ID, \"checkbox\")\n        for checkbox in checkboxes:\n            assert checkbox.is_selected()\n\n        # And when the journalist clicks \"select none\" then no sources are selected\n        journ_app_nav.driver.find_element(By.ID, \"select_none\").click()\n        checkboxes = journ_app_nav.driver.find_elements(By.ID, \"checkbox\")\n        for checkbox in checkboxes:\n            assert checkbox.is_selected() is False\n\n        # And when the journalist clicks \"select unread\" then all unread sources are selected\n        journ_app_nav.journalist_selects_the_first_source()\n        journ_app_nav.driver.find_element(By.ID, \"select_unread\").click()\n        checkboxes = journ_app_nav.get_submission_checkboxes_on_current_page()\n        for checkbox in checkboxes:\n            classes = checkbox.get_attribute(\"class\")\n            assert \"unread-cb\" in classes\n\n        # And when the journalist clicks the delete button, it succeeds\n        journ_app_nav.journalist_clicks_delete_all_and_sees_confirmation()",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "test_journalist_uses_index_delete_files_button_modal",
            "code": "def test_journalist_uses_index_delete_files_button_modal(\n        self, sd_servers_with_submitted_file, firefox_web_driver\n    ):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # And at least one source previously used the app\n        initial_sources_count = journ_app_nav.count_sources_on_index_page()\n        assert initial_sources_count > 0\n\n        # And the journalist selected all sources on the index page\n        try:\n            # If JavaScript is enabled, use the select_all button.\n            journ_app_nav.driver.find_element(By.ID, \"select_all\")\n            journ_app_nav.nav_helper.safe_click_by_id(\"select_all\")\n        except NoSuchElementException:\n            journ_app_nav.nav_helper.safe_click_all_by_css_selector(\n                'input[type=\"checkbox\"][name=\"cols_selected\"]'\n            )\n\n        # When the journalist clicks the delete collection button...\n        self._journalist_clicks_delete_collections_link(journ_app_nav)\n        # ...and then clicks the delete files button on the first modal\n        journ_app_nav.nav_helper.safe_click_by_id(\"delete-files-and-messages\")\n\n        # Then they are redirected to the index with the source present, files\n        # and messages zeroed, and a success flash message present\n        def one_source_no_files():\n            assert journ_app_nav.count_sources_on_index_page() == 1\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            assert \"The files and messages have been deleted\" in flash_msg.text\n            counts = journ_app_nav.driver.find_elements(By.CSS_SELECTOR, \".submission-count\")\n            assert \"0 docs\" in counts[0].text\n            assert \"0 messages\" in counts[1].text\n\n        journ_app_nav.nav_helper.wait_for(one_source_no_files)",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "one_source_no_files",
            "code": "def one_source_no_files():\n            assert journ_app_nav.count_sources_on_index_page() == 1\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            assert \"The files and messages have been deleted\" in flash_msg.text\n            counts = journ_app_nav.driver.find_elements(By.CSS_SELECTOR, \".submission-count\")\n            assert \"0 docs\" in counts[0].text\n            assert \"0 messages\" in counts[1].text",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "sd_servers_with_missing_file",
            "code": "def sd_servers_with_missing_file(\n    setup_journalist_key_and_gpg_folder: Tuple[str, Path],\n    setup_rqworker: Tuple[str, Path],\n) -> Generator[SdServersFixtureResult, None, None]:\n    \"\"\"Same as sd_servers but spawns the apps with a submission whose file has been deleted.\"\"\"\n    journalist_key_fingerprint, gpg_key_dir = setup_journalist_key_and_gpg_folder\n    worker_name, _ = setup_rqworker\n    default_config = SecureDropConfigFactory.create(\n        SECUREDROP_DATA_ROOT=Path(f\"/tmp/sd-tests/functional-with-missing-file-{uuid4()}\"),\n        GPG_KEY_DIR=gpg_key_dir,\n        JOURNALIST_KEY=journalist_key_fingerprint,\n        RQ_WORKER_NAME=worker_name,\n    )\n\n    # Spawn the apps in separate processes with a callback have a submission with a missing file\n    with spawn_sd_servers(\n        config_to_use=default_config,\n        journalist_app_setup_callback=_create_submission_with_missing_file,\n    ) as sd_servers_result:\n        yield sd_servers_result",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "_create_submission_with_missing_file",
            "code": "def _create_submission_with_missing_file(config_in_use: SecureDropConfig) -> None:\n    _, submission_file_path = create_source_and_submission(config_in_use)\n    submission_file_path.unlink()",
            "file": "test_journalist.py"
          },
          {
            "type": "class",
            "name": "TestJournalistMissingFile",
            "code": "class TestJournalistMissingFile:\n    \"\"\"Test error handling when a message file has been deleted from disk but remains in the\n    database. Ref #4787.\"\"\"\n\n    def test_download_source_unread(self, sd_servers_with_missing_file, firefox_web_driver):\n        # Given an SD server with a submission whose file was deleted from disk\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_missing_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_missing_file.journalist_username,\n            password=sd_servers_with_missing_file.journalist_password,\n            otp_secret=sd_servers_with_missing_file.journalist_otp_secret,\n        )\n\n        # When the journalist clicks on the source's \"n unread\" button\n        journ_app_nav.driver.find_element(\n            By.CSS_SELECTOR, \"table#collections tr.source > td.unread a\"\n        ).click()\n\n        # Then they see the expected error message\n        self._journalist_sees_missing_file_error_message(journ_app_nav)\n        journ_app_nav.is_on_journalist_homepage()\n\n    @staticmethod\n    def _journalist_sees_missing_file_error_message(journ_app_nav: JournalistAppNavigator) -> None:\n        notification = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".error\")\n\n        # We use a definite article (\"the\" instead of \"a\") if a single file\n        # is downloaded directly.\n        error_msg = (\n            \"Your download failed because the file could not be found. An admin can find \"\n            + \"more information in the system and monitoring logs.\"\n        )\n\n        assert notification.text in error_msg\n\n    def test_select_source_and_download_all(self, sd_servers_with_missing_file, firefox_web_driver):\n        # Given an SD server with a submission whose file was deleted from disk\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_missing_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_missing_file.journalist_username,\n            password=sd_servers_with_missing_file.journalist_password,\n            otp_secret=sd_servers_with_missing_file.journalist_otp_secret,\n        )\n\n        # When the journalist selects the source and then clicks the \"Download\" button\n        checkboxes = journ_app_nav.driver.find_elements(By.NAME, \"cols_selected\")\n        assert len(checkboxes) == 1\n        checkboxes[0].click()\n        journ_app_nav.driver.find_element(By.XPATH, \"//button[@value='download-all']\").click()\n\n        # Then they see the expected error message\n        self._journalist_sees_missing_file_error_message(journ_app_nav)\n        journ_app_nav.is_on_journalist_homepage()\n\n    def test_select_source_and_download_unread(\n        self, sd_servers_with_missing_file, firefox_web_driver\n    ):\n        # Given an SD server with a submission whose file was deleted from disk\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_missing_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_missing_file.journalist_username,\n            password=sd_servers_with_missing_file.journalist_password,\n            otp_secret=sd_servers_with_missing_file.journalist_otp_secret,\n        )\n\n        # When the journalist selects the source then clicks the \"Download Unread\" button\n        checkboxes = journ_app_nav.driver.find_elements(By.NAME, \"cols_selected\")\n        assert len(checkboxes) == 1\n        checkboxes[0].click()\n        journ_app_nav.driver.find_element(By.XPATH, \"//button[@value='download-unread']\").click()\n\n        # Then they see the expected error message\n        self._journalist_sees_missing_file_error_message(journ_app_nav)\n        journ_app_nav.is_on_journalist_homepage()\n\n    def test_download_message(self, sd_servers_with_missing_file, firefox_web_driver):\n        # Given an SD server with a submission whose file was deleted from disk\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_missing_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_missing_file.journalist_username,\n            password=sd_servers_with_missing_file.journalist_password,\n            otp_secret=sd_servers_with_missing_file.journalist_otp_secret,\n        )\n\n        # When the journalist clicks on the individual message from the source page\n        journ_app_nav.journalist_checks_messages()\n        journ_app_nav.journalist_selects_the_first_source()\n\n        journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.CSS_SELECTOR, \"table#submissions\")\n        )\n        submissions = journ_app_nav.driver.find_elements(By.CSS_SELECTOR, \"#submissions a\")\n        assert len(submissions) == 1\n\n        file_link = submissions[0]\n        file_link.click()\n\n        # Then they see the expected error message\n        self._journalist_sees_missing_file_error_message(journ_app_nav)\n        self._journalist_is_on_collection_page(journ_app_nav)\n\n    @staticmethod\n    def _journalist_is_on_collection_page(journ_app_nav: JournalistAppNavigator) -> None:\n        return journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.CSS_SELECTOR, \"div.journalist-view-single\")\n        )\n\n    def test_select_message_and_download_selected(\n        self, sd_servers_with_missing_file, firefox_web_driver\n    ):\n        # Given an SD server with a submission whose file was deleted from disk\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_missing_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_missing_file.journalist_username,\n            password=sd_servers_with_missing_file.journalist_password,\n            otp_secret=sd_servers_with_missing_file.journalist_otp_secret,\n        )\n        # When the journalist selects the individual message from the source page\n        # and clicks \"Download Selected\"\n        journ_app_nav.journalist_selects_the_first_source()\n        checkboxes = journ_app_nav.driver.find_elements(By.NAME, \"doc_names_selected\")\n        assert len(checkboxes) == 1\n        checkboxes[0].click()\n        journ_app_nav.driver.find_element(By.XPATH, \"//button[@value='download']\").click()\n\n        # Then they see the expected error message\n        self._journalist_sees_missing_file_error_message(journ_app_nav)\n        self._journalist_is_on_collection_page(journ_app_nav)",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "test_download_source_unread",
            "code": "def test_download_source_unread(self, sd_servers_with_missing_file, firefox_web_driver):\n        # Given an SD server with a submission whose file was deleted from disk\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_missing_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_missing_file.journalist_username,\n            password=sd_servers_with_missing_file.journalist_password,\n            otp_secret=sd_servers_with_missing_file.journalist_otp_secret,\n        )\n\n        # When the journalist clicks on the source's \"n unread\" button\n        journ_app_nav.driver.find_element(\n            By.CSS_SELECTOR, \"table#collections tr.source > td.unread a\"\n        ).click()\n\n        # Then they see the expected error message\n        self._journalist_sees_missing_file_error_message(journ_app_nav)\n        journ_app_nav.is_on_journalist_homepage()",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "_journalist_sees_missing_file_error_message",
            "code": "def _journalist_sees_missing_file_error_message(journ_app_nav: JournalistAppNavigator) -> None:\n        notification = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".error\")\n\n        # We use a definite article (\"the\" instead of \"a\") if a single file\n        # is downloaded directly.\n        error_msg = (\n            \"Your download failed because the file could not be found. An admin can find \"\n            + \"more information in the system and monitoring logs.\"\n        )\n\n        assert notification.text in error_msg",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "test_select_source_and_download_all",
            "code": "def test_select_source_and_download_all(self, sd_servers_with_missing_file, firefox_web_driver):\n        # Given an SD server with a submission whose file was deleted from disk\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_missing_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_missing_file.journalist_username,\n            password=sd_servers_with_missing_file.journalist_password,\n            otp_secret=sd_servers_with_missing_file.journalist_otp_secret,\n        )\n\n        # When the journalist selects the source and then clicks the \"Download\" button\n        checkboxes = journ_app_nav.driver.find_elements(By.NAME, \"cols_selected\")\n        assert len(checkboxes) == 1\n        checkboxes[0].click()\n        journ_app_nav.driver.find_element(By.XPATH, \"//button[@value='download-all']\").click()\n\n        # Then they see the expected error message\n        self._journalist_sees_missing_file_error_message(journ_app_nav)\n        journ_app_nav.is_on_journalist_homepage()",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "test_select_source_and_download_unread",
            "code": "def test_select_source_and_download_unread(\n        self, sd_servers_with_missing_file, firefox_web_driver\n    ):\n        # Given an SD server with a submission whose file was deleted from disk\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_missing_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_missing_file.journalist_username,\n            password=sd_servers_with_missing_file.journalist_password,\n            otp_secret=sd_servers_with_missing_file.journalist_otp_secret,\n        )\n\n        # When the journalist selects the source then clicks the \"Download Unread\" button\n        checkboxes = journ_app_nav.driver.find_elements(By.NAME, \"cols_selected\")\n        assert len(checkboxes) == 1\n        checkboxes[0].click()\n        journ_app_nav.driver.find_element(By.XPATH, \"//button[@value='download-unread']\").click()\n\n        # Then they see the expected error message\n        self._journalist_sees_missing_file_error_message(journ_app_nav)\n        journ_app_nav.is_on_journalist_homepage()",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "test_download_message",
            "code": "def test_download_message(self, sd_servers_with_missing_file, firefox_web_driver):\n        # Given an SD server with a submission whose file was deleted from disk\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_missing_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_missing_file.journalist_username,\n            password=sd_servers_with_missing_file.journalist_password,\n            otp_secret=sd_servers_with_missing_file.journalist_otp_secret,\n        )\n\n        # When the journalist clicks on the individual message from the source page\n        journ_app_nav.journalist_checks_messages()\n        journ_app_nav.journalist_selects_the_first_source()\n\n        journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.CSS_SELECTOR, \"table#submissions\")\n        )\n        submissions = journ_app_nav.driver.find_elements(By.CSS_SELECTOR, \"#submissions a\")\n        assert len(submissions) == 1\n\n        file_link = submissions[0]\n        file_link.click()\n\n        # Then they see the expected error message\n        self._journalist_sees_missing_file_error_message(journ_app_nav)\n        self._journalist_is_on_collection_page(journ_app_nav)",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "_journalist_is_on_collection_page",
            "code": "def _journalist_is_on_collection_page(journ_app_nav: JournalistAppNavigator) -> None:\n        return journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.CSS_SELECTOR, \"div.journalist-view-single\")\n        )",
            "file": "test_journalist.py"
          },
          {
            "type": "function",
            "name": "test_select_message_and_download_selected",
            "code": "def test_select_message_and_download_selected(\n        self, sd_servers_with_missing_file, firefox_web_driver\n    ):\n        # Given an SD server with a submission whose file was deleted from disk\n        # And a journalist logged into the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_missing_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_missing_file.journalist_username,\n            password=sd_servers_with_missing_file.journalist_password,\n            otp_secret=sd_servers_with_missing_file.journalist_otp_secret,\n        )\n        # When the journalist selects the individual message from the source page\n        # and clicks \"Download Selected\"\n        journ_app_nav.journalist_selects_the_first_source()\n        checkboxes = journ_app_nav.driver.find_elements(By.NAME, \"doc_names_selected\")\n        assert len(checkboxes) == 1\n        checkboxes[0].click()\n        journ_app_nav.driver.find_element(By.XPATH, \"//button[@value='download']\").click()\n\n        # Then they see the expected error message\n        self._journalist_sees_missing_file_error_message(journ_app_nav)\n        self._journalist_is_on_collection_page(journ_app_nav)",
            "file": "test_journalist.py"
          }
        ],
        "test_source_cancels.py": [
          {
            "type": "class",
            "name": "TestSourceUserCancels",
            "code": "class TestSourceUserCancels:\n    def test_source_cancels_at_login_page(self, sd_servers, tor_browser_web_driver):\n        # Given a source user who goes to the login page\n        source_app_nav = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n        source_app_nav.source_visits_source_homepage()\n        source_app_nav.source_chooses_to_login()\n\n        # When they click on the cancel button on the login page, it succeeds\n        source_app_nav.nav_helper.safe_click_by_css_selector(\".form-controls a\")\n        source_app_nav.driver.get(sd_servers.source_app_base_url)\n        assert source_app_nav._is_on_source_homepage()\n\n    def test_source_cancels_at_submit_page(self, sd_servers, tor_browser_web_driver):\n        # Given a source user who created an account\n        source_app_nav = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n        source_app_nav.source_visits_source_homepage()\n        source_app_nav.source_clicks_submit_documents_on_homepage()\n        source_app_nav.source_continues_to_submit_page()\n\n        # When they click on the cancel button on the submit page, it succeeds\n        source_app_nav.nav_helper.safe_click_by_css_selector(\".form-controls a\")\n\n        # And the right message is displayed\n        heading = source_app_nav.driver.find_element(By.ID, \"submit-heading\")\n        assert heading.text == \"Submit Files or Messages\"",
            "file": "test_source_cancels.py"
          },
          {
            "type": "function",
            "name": "test_source_cancels_at_login_page",
            "code": "def test_source_cancels_at_login_page(self, sd_servers, tor_browser_web_driver):\n        # Given a source user who goes to the login page\n        source_app_nav = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n        source_app_nav.source_visits_source_homepage()\n        source_app_nav.source_chooses_to_login()\n\n        # When they click on the cancel button on the login page, it succeeds\n        source_app_nav.nav_helper.safe_click_by_css_selector(\".form-controls a\")\n        source_app_nav.driver.get(sd_servers.source_app_base_url)\n        assert source_app_nav._is_on_source_homepage()",
            "file": "test_source_cancels.py"
          },
          {
            "type": "function",
            "name": "test_source_cancels_at_submit_page",
            "code": "def test_source_cancels_at_submit_page(self, sd_servers, tor_browser_web_driver):\n        # Given a source user who created an account\n        source_app_nav = SourceAppNavigator(\n            source_app_base_url=sd_servers.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n        source_app_nav.source_visits_source_homepage()\n        source_app_nav.source_clicks_submit_documents_on_homepage()\n        source_app_nav.source_continues_to_submit_page()\n\n        # When they click on the cancel button on the submit page, it succeeds\n        source_app_nav.nav_helper.safe_click_by_css_selector(\".form-controls a\")\n\n        # And the right message is displayed\n        heading = source_app_nav.driver.find_element(By.ID, \"submit-heading\")\n        assert heading.text == \"Submit Files or Messages\"",
            "file": "test_source_cancels.py"
          }
        ],
        "db_session.py": [
          {
            "type": "function",
            "name": "_get_fake_db_module",
            "code": "def _get_fake_db_module(\n    database_uri: str,\n) -> Generator[SQLAlchemy, None, None]:\n    # flask-sqlalchemy's API only allows DB access via a Flask app\n    # So we create a fake Flask app just so we can get a connection to the DB\n    app_for_db_connection = Flask(\"FakeAppForDbConnection\")\n    app_for_db_connection.config[\"SQLALCHEMY_TRACK_MODIFICATIONS\"] = False\n    app_for_db_connection.config[\"SQLALCHEMY_DATABASE_URI\"] = database_uri\n    db.init_app(app_for_db_connection)\n\n    with app_for_db_connection.app_context():\n        yield db",
            "file": "db_session.py"
          },
          {
            "type": "function",
            "name": "get_database_session",
            "code": "def get_database_session(database_uri: str) -> Generator[Session, None, None]:\n    \"\"\"Easily get a session to the DB without having to deal with Flask apps.\n\n    Can be used for tests and utility scripts that need to add data to the DB outside of the\n    context of the source & journalist Flask applications.\n    \"\"\"\n    # If there is already an app context, directly return the existing DB session\n    if flask.current_app:\n        assert flask.current_app.config[\"SQLALCHEMY_DATABASE_URI\"] == database_uri\n        yield db.session\n\n    # If there is no app context, create a fake one just to get a DB session\n    else:\n        with _get_fake_db_module(database_uri) as initialized_db_module:\n            db_session = initialized_db_module.session\n            try:\n                yield db_session\n            finally:\n                db_session.close()",
            "file": "db_session.py"
          }
        ],
        "test_admin_interface.py": [
          {
            "type": "class",
            "name": "TestAdminInterfaceAddUser",
            "code": "class TestAdminInterfaceAddUser:\n    def test_admin_adds_non_admin_user(self, sd_servers_with_clean_state, firefox_web_driver):\n        # Given an SD server\n        # And a journalist logged into the journalist interface as an admin\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        assert sd_servers_with_clean_state.journalist_is_admin\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # Then they see the same interface as a normal user, since there may be users who wish to\n        # be both journalists and admins\n        assert journ_app_nav.is_on_journalist_homepage()\n\n        # And they see a link that take them to the admin page\n        assert journ_app_nav.journalist_sees_link_to_admin_page()\n\n        # And when they go to the admin page to add a new non-admin user\n        journ_app_nav.admin_visits_admin_interface()\n        result = journ_app_nav.admin_creates_a_user(is_admin=False)\n        new_user_username, new_user_pw, new_user_otp_secret = result\n\n        # Then it succeeds\n\n        # Log the admin user out\n        journ_app_nav.journalist_logs_out()\n        journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".login-form\")\n        )\n\n        # And when the new user tries to login\n        journ_app_nav.journalist_logs_in(\n            username=new_user_username,\n            password=new_user_pw,\n            otp_secret=new_user_otp_secret,\n        )\n\n        # It succeeds\n        # And since the new user is not an admin, they don't see a link to the admin page\n        assert not journ_app_nav.journalist_sees_link_to_admin_page()\n\n    def test_admin_adds_admin_user(self, sd_servers_with_clean_state, firefox_web_driver):\n        # Given an SD server\n        # And the first journalist logged into the journalist interface as an admin\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        assert sd_servers_with_clean_state.journalist_is_admin\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # When they go to the admin page to add a new admin user\n        journ_app_nav.admin_visits_admin_interface()\n        result = journ_app_nav.admin_creates_a_user(is_admin=True)\n        new_user_username, new_user_pw, new_user_otp_secret = result\n\n        # Then it succeeds\n\n        # Log the admin user out\n        journ_app_nav.journalist_logs_out()\n        journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".login-form\")\n        )\n\n        # And when the new user tries to login\n        journ_app_nav.journalist_logs_in(\n            username=new_user_username,\n            password=new_user_pw,\n            otp_secret=new_user_otp_secret,\n        )\n\n        # It succeeds\n        # And since the new user is an admin, they see a link to the admin page\n        assert journ_app_nav.journalist_sees_link_to_admin_page()\n\n    def test_admin_adds_user_with_invalid_username(\n        self, sd_servers_with_clean_state, firefox_web_driver\n    ):\n        # Given an SD server\n        # And the first journalist logged into the journalist interface as an admin\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        assert sd_servers_with_clean_state.journalist_is_admin\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # When they go to the admin page to add a new admin user with an invalid name\n        journ_app_nav.admin_visits_admin_interface()\n\n        # We use a callback and an exception to stop after the form was submitted\n        class StopAfterFormSubmitted(Exception):\n            pass\n\n        def submit_form_and_stop():\n            journ_app_nav.nav_helper.safe_click_by_css_selector(\n                \"form#admin-add-user button[type=submit]\"\n            )\n            raise StopAfterFormSubmitted()\n\n        try:\n            journ_app_nav.admin_creates_a_user(\n                username=\"deleted\", callback_before_submitting_add_user_step=submit_form_and_stop\n            )\n        except StopAfterFormSubmitted:\n            pass\n\n        # Then it fails with an error\n        error_msg = journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".form-validation-error\")\n        )\n\n        # And they see the corresponding error message\n        assert (\n            \"This username is invalid because it is reserved for internal use \"\n            \"by the software.\" in error_msg.text\n        )",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "test_admin_adds_non_admin_user",
            "code": "def test_admin_adds_non_admin_user(self, sd_servers_with_clean_state, firefox_web_driver):\n        # Given an SD server\n        # And a journalist logged into the journalist interface as an admin\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        assert sd_servers_with_clean_state.journalist_is_admin\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # Then they see the same interface as a normal user, since there may be users who wish to\n        # be both journalists and admins\n        assert journ_app_nav.is_on_journalist_homepage()\n\n        # And they see a link that take them to the admin page\n        assert journ_app_nav.journalist_sees_link_to_admin_page()\n\n        # And when they go to the admin page to add a new non-admin user\n        journ_app_nav.admin_visits_admin_interface()\n        result = journ_app_nav.admin_creates_a_user(is_admin=False)\n        new_user_username, new_user_pw, new_user_otp_secret = result\n\n        # Then it succeeds\n\n        # Log the admin user out\n        journ_app_nav.journalist_logs_out()\n        journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".login-form\")\n        )\n\n        # And when the new user tries to login\n        journ_app_nav.journalist_logs_in(\n            username=new_user_username,\n            password=new_user_pw,\n            otp_secret=new_user_otp_secret,\n        )\n\n        # It succeeds\n        # And since the new user is not an admin, they don't see a link to the admin page\n        assert not journ_app_nav.journalist_sees_link_to_admin_page()",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "test_admin_adds_admin_user",
            "code": "def test_admin_adds_admin_user(self, sd_servers_with_clean_state, firefox_web_driver):\n        # Given an SD server\n        # And the first journalist logged into the journalist interface as an admin\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        assert sd_servers_with_clean_state.journalist_is_admin\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # When they go to the admin page to add a new admin user\n        journ_app_nav.admin_visits_admin_interface()\n        result = journ_app_nav.admin_creates_a_user(is_admin=True)\n        new_user_username, new_user_pw, new_user_otp_secret = result\n\n        # Then it succeeds\n\n        # Log the admin user out\n        journ_app_nav.journalist_logs_out()\n        journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".login-form\")\n        )\n\n        # And when the new user tries to login\n        journ_app_nav.journalist_logs_in(\n            username=new_user_username,\n            password=new_user_pw,\n            otp_secret=new_user_otp_secret,\n        )\n\n        # It succeeds\n        # And since the new user is an admin, they see a link to the admin page\n        assert journ_app_nav.journalist_sees_link_to_admin_page()",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "test_admin_adds_user_with_invalid_username",
            "code": "def test_admin_adds_user_with_invalid_username(\n        self, sd_servers_with_clean_state, firefox_web_driver\n    ):\n        # Given an SD server\n        # And the first journalist logged into the journalist interface as an admin\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        assert sd_servers_with_clean_state.journalist_is_admin\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # When they go to the admin page to add a new admin user with an invalid name\n        journ_app_nav.admin_visits_admin_interface()\n\n        # We use a callback and an exception to stop after the form was submitted\n        class StopAfterFormSubmitted(Exception):\n            pass\n\n        def submit_form_and_stop():\n            journ_app_nav.nav_helper.safe_click_by_css_selector(\n                \"form#admin-add-user button[type=submit]\"\n            )\n            raise StopAfterFormSubmitted()\n\n        try:\n            journ_app_nav.admin_creates_a_user(\n                username=\"deleted\", callback_before_submitting_add_user_step=submit_form_and_stop\n            )\n        except StopAfterFormSubmitted:\n            pass\n\n        # Then it fails with an error\n        error_msg = journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".form-validation-error\")\n        )\n\n        # And they see the corresponding error message\n        assert (\n            \"This username is invalid because it is reserved for internal use \"\n            \"by the software.\" in error_msg.text\n        )",
            "file": "test_admin_interface.py"
          },
          {
            "type": "class",
            "name": "StopAfterFormSubmitted",
            "code": "class StopAfterFormSubmitted(Exception):\n            pass",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "submit_form_and_stop",
            "code": "def submit_form_and_stop():\n            journ_app_nav.nav_helper.safe_click_by_css_selector(\n                \"form#admin-add-user button[type=submit]\"\n            )\n            raise StopAfterFormSubmitted()",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "_create_second_journalist",
            "code": "def _create_second_journalist(config_in_use: SecureDropConfig) -> None:\n    # Add a test journalist\n    with get_database_session(database_uri=config_in_use.DATABASE_URI) as db_session_for_sd_servers:\n        journalist = Journalist(\n            username=_SECOND_JOURNALIST_USERNAME,\n            password=_SECOND_JOURNALIST_PASSWORD,\n            is_admin=False,\n        )\n        journalist.otp_secret = _SECOND_JOURNALIST_OTP_SECRET\n        db_session_for_sd_servers.add(journalist)\n        db_session_for_sd_servers.commit()",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "sd_servers_with_second_journalist",
            "code": "def sd_servers_with_second_journalist(\n    setup_journalist_key_and_gpg_folder: Tuple[str, Path],\n    setup_rqworker: Tuple[str, Path],\n) -> Generator[SdServersFixtureResult, None, None]:\n    \"\"\"Sams as sd_servers but spawns the apps with an already-created second journalist.\n\n    Slower than sd_servers as it is function-scoped.\n    \"\"\"\n    journalist_key_fingerprint, gpg_key_dir = setup_journalist_key_and_gpg_folder\n    worker_name, _ = setup_rqworker\n    default_config = SecureDropConfigFactory.create(\n        SECUREDROP_DATA_ROOT=Path(f\"/tmp/sd-tests/functional-with-second-journalist-{uuid4()}\"),\n        GPG_KEY_DIR=gpg_key_dir,\n        JOURNALIST_KEY=journalist_key_fingerprint,\n        RQ_WORKER_NAME=worker_name,\n    )\n\n    # Spawn the apps in separate processes with a callback to create a submission\n    with spawn_sd_servers(\n        config_to_use=default_config, journalist_app_setup_callback=_create_second_journalist\n    ) as sd_servers_result:\n        yield sd_servers_result",
            "file": "test_admin_interface.py"
          },
          {
            "type": "class",
            "name": "TestAdminInterfaceEditAndDeleteUser",
            "code": "class TestAdminInterfaceEditAndDeleteUser:\n    \"\"\"Test editing another journalist/user.\n\n    Note: Tests to edit a user's 2fa secret are implemented in TestJournalistLayoutAdmin.\n    \"\"\"\n\n    @staticmethod\n    def _admin_logs_in_and_goes_to_edit_page_for_second_journalist(\n        sd_servers_result: SdServersFixtureResult,\n        firefox_web_driver: WebDriver,\n    ) -> JournalistAppNavigator:\n        # Log in as the admin\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_result.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        assert sd_servers_result.journalist_is_admin\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_result.journalist_username,\n            password=sd_servers_result.journalist_password,\n            otp_secret=sd_servers_result.journalist_otp_secret,\n        )\n\n        journ_app_nav.admin_visits_admin_interface()\n\n        # Go to the \"edit user\" page for the second journalist\n        journ_app_nav.admin_visits_user_edit_page(\n            username_of_journalist_to_edit=_SECOND_JOURNALIST_USERNAME\n        )\n        return journ_app_nav\n\n    def test_admin_edits_username(self, sd_servers_with_second_journalist, firefox_web_driver):\n        # Given an SD server with a second journalist created\n        # And the first journalist logged into the journalist interface as an admin\n        # And they went to the \"edit user\" page for the second journalist\n        journ_app_nav = self._admin_logs_in_and_goes_to_edit_page_for_second_journalist(\n            sd_servers_result=sd_servers_with_second_journalist,\n            firefox_web_driver=firefox_web_driver,\n        )\n\n        # When they change the second journalist's username\n        self._admin_edits_username_and_submits_form(journ_app_nav, new_username=\"new_name\")\n\n        # Then it succeeds\n        def user_edited():\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            assert \"Account updated.\" in flash_msg.text\n\n        journ_app_nav.nav_helper.wait_for(user_edited)\n\n    def test_admin_edits_invalid_username(\n        self, sd_servers_with_second_journalist, firefox_web_driver\n    ):\n        # Given an SD server with a second journalist created\n        # And the first journalist logged into the journalist interface as an admin\n        # And they went to the \"edit user\" page for the second journalist\n        journ_app_nav = self._admin_logs_in_and_goes_to_edit_page_for_second_journalist(\n            sd_servers_result=sd_servers_with_second_journalist,\n            firefox_web_driver=firefox_web_driver,\n        )\n\n        # When they change the second journalist's username to an invalid username\n        self._admin_edits_username_and_submits_form(journ_app_nav, new_username=\"deleted\")\n\n        # Then it fails\n        def user_edited():\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            assert \"Invalid username\" in flash_msg.text\n\n        journ_app_nav.nav_helper.wait_for(user_edited)\n\n    @staticmethod\n    def _admin_edits_username_and_submits_form(\n        journ_app_nav: JournalistAppNavigator,\n        new_username: str,\n    ) -> None:\n        journ_app_nav.nav_helper.safe_send_keys_by_css_selector(\n            'input[name=\"username\"]', Keys.CONTROL + \"a\"\n        )\n        journ_app_nav.nav_helper.safe_send_keys_by_css_selector(\n            'input[name=\"username\"]', Keys.DELETE\n        )\n        journ_app_nav.nav_helper.safe_send_keys_by_css_selector(\n            'input[name=\"username\"]', new_username\n        )\n        journ_app_nav.nav_helper.safe_click_by_css_selector(\"form#edit-account button[type=submit]\")\n\n    def test_admin_resets_password(self, sd_servers_with_second_journalist, firefox_web_driver):\n        # Given an SD server with a second journalist created\n        # And the first journalist logged into the journalist interface as an admin\n        # And they went to the \"edit user\" page for the second journalist\n        journ_app_nav = self._admin_logs_in_and_goes_to_edit_page_for_second_journalist(\n            sd_servers_result=sd_servers_with_second_journalist,\n            firefox_web_driver=firefox_web_driver,\n        )\n\n        # When they reset the second journalist's password\n        new_password = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \"#password\").text.strip()\n        assert new_password\n        reset_pw_btn = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \"#reset-password\")\n        reset_pw_btn.click()\n\n        # Then it succeeds\n        # Wait until page refreshes to avoid causing a broken pipe error (#623)\n        def update_password_success():\n            assert \"Password updated.\" in journ_app_nav.driver.page_source\n\n        journ_app_nav.nav_helper.wait_for(update_password_success)\n\n        # And the second journalist is able to login using the new password\n        journ_app_nav.journalist_logs_out()\n        journ_app_nav.journalist_logs_in(\n            username=_SECOND_JOURNALIST_USERNAME,\n            password=new_password,\n            otp_secret=_SECOND_JOURNALIST_OTP_SECRET,\n        )\n        assert journ_app_nav.is_on_journalist_homepage()\n\n    def test_admin_deletes_user(self, sd_servers_with_second_journalist, firefox_web_driver):\n        # Given an SD server with a second journalist created\n        # And the first journalist logged into the journalist interface as an admin\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_second_journalist.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        assert sd_servers_with_second_journalist.journalist_is_admin\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_second_journalist.journalist_username,\n            password=sd_servers_with_second_journalist.journalist_password,\n            otp_secret=sd_servers_with_second_journalist.journalist_otp_secret,\n        )\n\n        # When the admin deletes the second journalist\n        journ_app_nav.admin_visits_admin_interface()\n        for _i in range(15):\n            try:\n                journ_app_nav.nav_helper.safe_click_by_css_selector(\".delete-user a\")\n                journ_app_nav.nav_helper.wait_for(\n                    lambda: expected_conditions.element_to_be_clickable((By.ID, \"delete-selected\"))\n                )\n                journ_app_nav.nav_helper.safe_click_by_id(\"delete-selected\")\n                journ_app_nav.nav_helper.alert_wait()\n                journ_app_nav.nav_helper.alert_accept()\n                break\n            except TimeoutException:\n                # Selenium has failed to click, and the confirmation\n                # alert didn't happen. Try once more.\n                logging.info(\"Selenium has failed to click yet again; retrying.\")\n\n        # Then it succeeds\n        def user_deleted():\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            assert \"Deleted user\" in flash_msg.text\n\n        journ_app_nav.nav_helper.wait_for(user_deleted)",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "_admin_logs_in_and_goes_to_edit_page_for_second_journalist",
            "code": "def _admin_logs_in_and_goes_to_edit_page_for_second_journalist(\n        sd_servers_result: SdServersFixtureResult,\n        firefox_web_driver: WebDriver,\n    ) -> JournalistAppNavigator:\n        # Log in as the admin\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_result.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        assert sd_servers_result.journalist_is_admin\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_result.journalist_username,\n            password=sd_servers_result.journalist_password,\n            otp_secret=sd_servers_result.journalist_otp_secret,\n        )\n\n        journ_app_nav.admin_visits_admin_interface()\n\n        # Go to the \"edit user\" page for the second journalist\n        journ_app_nav.admin_visits_user_edit_page(\n            username_of_journalist_to_edit=_SECOND_JOURNALIST_USERNAME\n        )\n        return journ_app_nav",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "test_admin_edits_username",
            "code": "def test_admin_edits_username(self, sd_servers_with_second_journalist, firefox_web_driver):\n        # Given an SD server with a second journalist created\n        # And the first journalist logged into the journalist interface as an admin\n        # And they went to the \"edit user\" page for the second journalist\n        journ_app_nav = self._admin_logs_in_and_goes_to_edit_page_for_second_journalist(\n            sd_servers_result=sd_servers_with_second_journalist,\n            firefox_web_driver=firefox_web_driver,\n        )\n\n        # When they change the second journalist's username\n        self._admin_edits_username_and_submits_form(journ_app_nav, new_username=\"new_name\")\n\n        # Then it succeeds\n        def user_edited():\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            assert \"Account updated.\" in flash_msg.text\n\n        journ_app_nav.nav_helper.wait_for(user_edited)",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "user_edited",
            "code": "def user_edited():\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            assert \"Account updated.\" in flash_msg.text",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "test_admin_edits_invalid_username",
            "code": "def test_admin_edits_invalid_username(\n        self, sd_servers_with_second_journalist, firefox_web_driver\n    ):\n        # Given an SD server with a second journalist created\n        # And the first journalist logged into the journalist interface as an admin\n        # And they went to the \"edit user\" page for the second journalist\n        journ_app_nav = self._admin_logs_in_and_goes_to_edit_page_for_second_journalist(\n            sd_servers_result=sd_servers_with_second_journalist,\n            firefox_web_driver=firefox_web_driver,\n        )\n\n        # When they change the second journalist's username to an invalid username\n        self._admin_edits_username_and_submits_form(journ_app_nav, new_username=\"deleted\")\n\n        # Then it fails\n        def user_edited():\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            assert \"Invalid username\" in flash_msg.text\n\n        journ_app_nav.nav_helper.wait_for(user_edited)",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "user_edited",
            "code": "def user_edited():\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            assert \"Invalid username\" in flash_msg.text",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "_admin_edits_username_and_submits_form",
            "code": "def _admin_edits_username_and_submits_form(\n        journ_app_nav: JournalistAppNavigator,\n        new_username: str,\n    ) -> None:\n        journ_app_nav.nav_helper.safe_send_keys_by_css_selector(\n            'input[name=\"username\"]', Keys.CONTROL + \"a\"\n        )\n        journ_app_nav.nav_helper.safe_send_keys_by_css_selector(\n            'input[name=\"username\"]', Keys.DELETE\n        )\n        journ_app_nav.nav_helper.safe_send_keys_by_css_selector(\n            'input[name=\"username\"]', new_username\n        )\n        journ_app_nav.nav_helper.safe_click_by_css_selector(\"form#edit-account button[type=submit]\")",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "test_admin_resets_password",
            "code": "def test_admin_resets_password(self, sd_servers_with_second_journalist, firefox_web_driver):\n        # Given an SD server with a second journalist created\n        # And the first journalist logged into the journalist interface as an admin\n        # And they went to the \"edit user\" page for the second journalist\n        journ_app_nav = self._admin_logs_in_and_goes_to_edit_page_for_second_journalist(\n            sd_servers_result=sd_servers_with_second_journalist,\n            firefox_web_driver=firefox_web_driver,\n        )\n\n        # When they reset the second journalist's password\n        new_password = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \"#password\").text.strip()\n        assert new_password\n        reset_pw_btn = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \"#reset-password\")\n        reset_pw_btn.click()\n\n        # Then it succeeds\n        # Wait until page refreshes to avoid causing a broken pipe error (#623)\n        def update_password_success():\n            assert \"Password updated.\" in journ_app_nav.driver.page_source\n\n        journ_app_nav.nav_helper.wait_for(update_password_success)\n\n        # And the second journalist is able to login using the new password\n        journ_app_nav.journalist_logs_out()\n        journ_app_nav.journalist_logs_in(\n            username=_SECOND_JOURNALIST_USERNAME,\n            password=new_password,\n            otp_secret=_SECOND_JOURNALIST_OTP_SECRET,\n        )\n        assert journ_app_nav.is_on_journalist_homepage()",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "update_password_success",
            "code": "def update_password_success():\n            assert \"Password updated.\" in journ_app_nav.driver.page_source",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "test_admin_deletes_user",
            "code": "def test_admin_deletes_user(self, sd_servers_with_second_journalist, firefox_web_driver):\n        # Given an SD server with a second journalist created\n        # And the first journalist logged into the journalist interface as an admin\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_second_journalist.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        assert sd_servers_with_second_journalist.journalist_is_admin\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_second_journalist.journalist_username,\n            password=sd_servers_with_second_journalist.journalist_password,\n            otp_secret=sd_servers_with_second_journalist.journalist_otp_secret,\n        )\n\n        # When the admin deletes the second journalist\n        journ_app_nav.admin_visits_admin_interface()\n        for _i in range(15):\n            try:\n                journ_app_nav.nav_helper.safe_click_by_css_selector(\".delete-user a\")\n                journ_app_nav.nav_helper.wait_for(\n                    lambda: expected_conditions.element_to_be_clickable((By.ID, \"delete-selected\"))\n                )\n                journ_app_nav.nav_helper.safe_click_by_id(\"delete-selected\")\n                journ_app_nav.nav_helper.alert_wait()\n                journ_app_nav.nav_helper.alert_accept()\n                break\n            except TimeoutException:\n                # Selenium has failed to click, and the confirmation\n                # alert didn't happen. Try once more.\n                logging.info(\"Selenium has failed to click yet again; retrying.\")\n\n        # Then it succeeds\n        def user_deleted():\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            assert \"Deleted user\" in flash_msg.text\n\n        journ_app_nav.nav_helper.wait_for(user_deleted)",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "user_deleted",
            "code": "def user_deleted():\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            assert \"Deleted user\" in flash_msg.text",
            "file": "test_admin_interface.py"
          },
          {
            "type": "class",
            "name": "TestAdminInterfaceEditConfig",
            "code": "class TestAdminInterfaceEditConfig:\n    \"\"\"Test the instance's system settings.\n\n    Note: Additional/related tests are also implemented in TestAdminLayoutEditConfig.\n    \"\"\"\n\n    def test_disallow_file_submission(\n        self, sd_servers_with_clean_state, firefox_web_driver, tor_browser_web_driver\n    ):\n        # Given an SD server\n        # And a journalist logged into the journalist interface as an admin\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        assert sd_servers_with_clean_state.journalist_is_admin\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # And they go to the admin config page\n        journ_app_nav.admin_visits_admin_interface()\n        journ_app_nav.admin_visits_system_config_page()\n\n        # When they disallow file uploads, then it succeeds\n        self._admin_updates_document_upload_instance_setting(\n            instance_should_allow_file_uploads=False,\n            journ_app_nav=journ_app_nav,\n        )\n\n        # And then, when a source user tries to upload a file\n        source_app_nav = SourceAppNavigator(\n            source_app_base_url=sd_servers_with_clean_state.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n        source_app_nav.source_visits_source_homepage()\n        source_app_nav.source_clicks_submit_documents_on_homepage()\n        source_app_nav.source_continues_to_submit_page()\n\n        # Then they don't see the option to upload a file because uploads were disallowed\n        with pytest.raises(NoSuchElementException):\n            source_app_nav.driver.find_element(By.CLASS_NAME, \"attachment\")\n\n    @classmethod\n    def _admin_updates_document_upload_instance_setting(\n        cls,\n        instance_should_allow_file_uploads: bool,\n        journ_app_nav: JournalistAppNavigator,\n    ) -> None:\n        # Retrieve the instance's current upload setting\n        upload_element_id = \"prevent_document_uploads\"\n        instance_currently_allows_file_uploads = not journ_app_nav.driver.find_element(\n            By.ID, upload_element_id\n        ).is_selected()\n\n        # Ensure the new setting is different from the existing setting\n        assert instance_currently_allows_file_uploads != instance_should_allow_file_uploads\n\n        # Update the instance's upload setting\n        journ_app_nav.nav_helper.safe_click_by_id(upload_element_id)\n        journ_app_nav.nav_helper.safe_click_by_id(\"submit-submission-preferences\")\n        cls._admin_submits_instance_settings_form(journ_app_nav)\n\n    @staticmethod\n    def _admin_submits_instance_settings_form(journ_app_nav: JournalistAppNavigator) -> None:\n        def preferences_saved():\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            assert \"Preferences saved.\" in flash_msg.text\n\n        journ_app_nav.nav_helper.wait_for(preferences_saved, timeout=20)\n\n    def test_allow_file_submission(\n        self, sd_servers_with_clean_state, firefox_web_driver, tor_browser_web_driver\n    ):\n        # Given an SD server\n        # And a journalist logged into the journalist interface as an admin\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        assert sd_servers_with_clean_state.journalist_is_admin\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # And they go to the admin config page\n        journ_app_nav.admin_visits_admin_interface()\n        journ_app_nav.admin_visits_system_config_page()\n\n        # And the instance is configured to disallow file uploads\n        self._admin_updates_document_upload_instance_setting(\n            instance_should_allow_file_uploads=False,\n            journ_app_nav=journ_app_nav,\n        )\n\n        # When they re-allow file uploads, then it succeeds\n        self._admin_updates_document_upload_instance_setting(\n            instance_should_allow_file_uploads=True,\n            journ_app_nav=journ_app_nav,\n        )\n\n        # And then, when a source user tries to upload a file\n        source_app_nav = SourceAppNavigator(\n            source_app_base_url=sd_servers_with_clean_state.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n        source_app_nav.source_visits_source_homepage()\n        source_app_nav.source_clicks_submit_documents_on_homepage()\n        source_app_nav.source_continues_to_submit_page()\n\n        # Then they see the option to upload a file because uploads were allowed\n        assert source_app_nav.driver.find_element(By.CLASS_NAME, \"attachment\")\n\n    def test_orgname_is_changed(\n        self, sd_servers_with_clean_state, firefox_web_driver, tor_browser_web_driver\n    ):\n        # Given an SD server\n        # And a journalist logged into the journalist interface as an admin\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        assert sd_servers_with_clean_state.journalist_is_admin\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # And they go to the admin config page\n        journ_app_nav.admin_visits_admin_interface()\n        journ_app_nav.admin_visits_system_config_page()\n\n        # When they update the organization's name\n        assert \"SecureDrop\" in journ_app_nav.driver.title\n        journ_app_nav.driver.find_element(By.ID, \"organization_name\").clear()\n        new_org_name = \"Walden Inquirer\"\n        journ_app_nav.nav_helper.safe_send_keys_by_id(\"organization_name\", new_org_name)\n        journ_app_nav.nav_helper.safe_click_by_id(\"submit-update-org-name\")\n        self._admin_submits_instance_settings_form(journ_app_nav)\n\n        # Then it succeeds\n        assert new_org_name in journ_app_nav.driver.title\n\n        # And then, when a source user logs into the source app\n        source_app_nav = SourceAppNavigator(\n            source_app_base_url=sd_servers_with_clean_state.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        # THey see the new organization name in the homepage\n        source_app_nav.source_visits_source_homepage()\n        assert new_org_name in source_app_nav.driver.title\n\n        # And in the submission page\n        source_app_nav.source_clicks_submit_documents_on_homepage()\n        source_app_nav.source_continues_to_submit_page()\n        assert new_org_name in source_app_nav.driver.title",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "test_disallow_file_submission",
            "code": "def test_disallow_file_submission(\n        self, sd_servers_with_clean_state, firefox_web_driver, tor_browser_web_driver\n    ):\n        # Given an SD server\n        # And a journalist logged into the journalist interface as an admin\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        assert sd_servers_with_clean_state.journalist_is_admin\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # And they go to the admin config page\n        journ_app_nav.admin_visits_admin_interface()\n        journ_app_nav.admin_visits_system_config_page()\n\n        # When they disallow file uploads, then it succeeds\n        self._admin_updates_document_upload_instance_setting(\n            instance_should_allow_file_uploads=False,\n            journ_app_nav=journ_app_nav,\n        )\n\n        # And then, when a source user tries to upload a file\n        source_app_nav = SourceAppNavigator(\n            source_app_base_url=sd_servers_with_clean_state.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n        source_app_nav.source_visits_source_homepage()\n        source_app_nav.source_clicks_submit_documents_on_homepage()\n        source_app_nav.source_continues_to_submit_page()\n\n        # Then they don't see the option to upload a file because uploads were disallowed\n        with pytest.raises(NoSuchElementException):\n            source_app_nav.driver.find_element(By.CLASS_NAME, \"attachment\")",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "_admin_updates_document_upload_instance_setting",
            "code": "def _admin_updates_document_upload_instance_setting(\n        cls,\n        instance_should_allow_file_uploads: bool,\n        journ_app_nav: JournalistAppNavigator,\n    ) -> None:\n        # Retrieve the instance's current upload setting\n        upload_element_id = \"prevent_document_uploads\"\n        instance_currently_allows_file_uploads = not journ_app_nav.driver.find_element(\n            By.ID, upload_element_id\n        ).is_selected()\n\n        # Ensure the new setting is different from the existing setting\n        assert instance_currently_allows_file_uploads != instance_should_allow_file_uploads\n\n        # Update the instance's upload setting\n        journ_app_nav.nav_helper.safe_click_by_id(upload_element_id)\n        journ_app_nav.nav_helper.safe_click_by_id(\"submit-submission-preferences\")\n        cls._admin_submits_instance_settings_form(journ_app_nav)",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "_admin_submits_instance_settings_form",
            "code": "def _admin_submits_instance_settings_form(journ_app_nav: JournalistAppNavigator) -> None:\n        def preferences_saved():\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            assert \"Preferences saved.\" in flash_msg.text\n\n        journ_app_nav.nav_helper.wait_for(preferences_saved, timeout=20)",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "preferences_saved",
            "code": "def preferences_saved():\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            assert \"Preferences saved.\" in flash_msg.text",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "test_allow_file_submission",
            "code": "def test_allow_file_submission(\n        self, sd_servers_with_clean_state, firefox_web_driver, tor_browser_web_driver\n    ):\n        # Given an SD server\n        # And a journalist logged into the journalist interface as an admin\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        assert sd_servers_with_clean_state.journalist_is_admin\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # And they go to the admin config page\n        journ_app_nav.admin_visits_admin_interface()\n        journ_app_nav.admin_visits_system_config_page()\n\n        # And the instance is configured to disallow file uploads\n        self._admin_updates_document_upload_instance_setting(\n            instance_should_allow_file_uploads=False,\n            journ_app_nav=journ_app_nav,\n        )\n\n        # When they re-allow file uploads, then it succeeds\n        self._admin_updates_document_upload_instance_setting(\n            instance_should_allow_file_uploads=True,\n            journ_app_nav=journ_app_nav,\n        )\n\n        # And then, when a source user tries to upload a file\n        source_app_nav = SourceAppNavigator(\n            source_app_base_url=sd_servers_with_clean_state.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n        source_app_nav.source_visits_source_homepage()\n        source_app_nav.source_clicks_submit_documents_on_homepage()\n        source_app_nav.source_continues_to_submit_page()\n\n        # Then they see the option to upload a file because uploads were allowed\n        assert source_app_nav.driver.find_element(By.CLASS_NAME, \"attachment\")",
            "file": "test_admin_interface.py"
          },
          {
            "type": "function",
            "name": "test_orgname_is_changed",
            "code": "def test_orgname_is_changed(\n        self, sd_servers_with_clean_state, firefox_web_driver, tor_browser_web_driver\n    ):\n        # Given an SD server\n        # And a journalist logged into the journalist interface as an admin\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n        )\n        assert sd_servers_with_clean_state.journalist_is_admin\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # And they go to the admin config page\n        journ_app_nav.admin_visits_admin_interface()\n        journ_app_nav.admin_visits_system_config_page()\n\n        # When they update the organization's name\n        assert \"SecureDrop\" in journ_app_nav.driver.title\n        journ_app_nav.driver.find_element(By.ID, \"organization_name\").clear()\n        new_org_name = \"Walden Inquirer\"\n        journ_app_nav.nav_helper.safe_send_keys_by_id(\"organization_name\", new_org_name)\n        journ_app_nav.nav_helper.safe_click_by_id(\"submit-update-org-name\")\n        self._admin_submits_instance_settings_form(journ_app_nav)\n\n        # Then it succeeds\n        assert new_org_name in journ_app_nav.driver.title\n\n        # And then, when a source user logs into the source app\n        source_app_nav = SourceAppNavigator(\n            source_app_base_url=sd_servers_with_clean_state.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        # THey see the new organization name in the homepage\n        source_app_nav.source_visits_source_homepage()\n        assert new_org_name in source_app_nav.driver.title\n\n        # And in the submission page\n        source_app_nav.source_clicks_submit_documents_on_homepage()\n        source_app_nav.source_continues_to_submit_page()\n        assert new_org_name in source_app_nav.driver.title",
            "file": "test_admin_interface.py"
          }
        ],
        "pageslayout": {
          "test_journalist_col.py": [
            {
              "type": "function",
              "name": "_create_source_and_submission_and_delete_source_key",
              "code": "def _create_source_and_submission_and_delete_source_key(config_in_use: SecureDropConfig) -> None:\n    # This function will be called in a separate Process that runs the app\n    # Hence the late imports\n    from encryption import EncryptionManager\n    from tests.functional.conftest import create_source_and_submission\n\n    source_user, _ = create_source_and_submission(config_in_use)\n    EncryptionManager.get_default().delete_source_key_pair(source_user.filesystem_id)",
              "file": "test_journalist_col.py"
            },
            {
              "type": "function",
              "name": "sd_servers_with_deleted_source_key",
              "code": "def sd_servers_with_deleted_source_key(\n    setup_journalist_key_and_gpg_folder: Tuple[str, Path],\n    setup_rqworker: Tuple[str, Path],\n) -> Generator[SdServersFixtureResult, None, None]:\n    \"\"\"Same as sd_servers but spawns the apps with a source whose key was deleted.\n\n    Slower than sd_servers as it is function-scoped.\n    \"\"\"\n    journalist_key_fingerprint, gpg_key_dir = setup_journalist_key_and_gpg_folder\n    worker_name, _ = setup_rqworker\n    default_config = SecureDropConfigFactory.create(\n        SECUREDROP_DATA_ROOT=Path(f\"/tmp/sd-tests/functional-with-deleted-source-key-{uuid4()}\"),\n        GPG_KEY_DIR=gpg_key_dir,\n        JOURNALIST_KEY=journalist_key_fingerprint,\n        RQ_WORKER_NAME=worker_name,\n    )\n\n    # Spawn the apps in separate processes with a callback to create a submission\n    with spawn_sd_servers(\n        config_to_use=default_config,\n        journalist_app_setup_callback=_create_source_and_submission_and_delete_source_key,\n    ) as sd_servers_result:\n        yield sd_servers_result",
              "file": "test_journalist_col.py"
            },
            {
              "type": "class",
              "name": "TestJournalistLayoutCol",
              "code": "class TestJournalistLayoutCol:\n    def test_col_with_and_without_documents(\n        self, sd_servers_with_submitted_file, firefox_web_driver\n    ):\n        # Given an SD server with an already-submitted file\n        # And a journalist logging into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # Take a screenshot of the individual source's page when there is a document\n        journ_app_nav.journalist_visits_col()\n        save_static_data(journ_app_nav.driver, locale, \"journalist-col\")\n        # The documentation uses an identical screenshot with a different name:\n        # https://github.com/freedomofpress/securedrop-docs/blob/main/docs/images/manual\n        # /screenshots/journalist-col_javascript.png\n        # So we take the same screenshot again here\n        # TODO(AD): Update the documentation to use a single screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-col_javascript\")\n\n        # Take a screenshot of the individual source's page when there are no documents\n        journ_app_nav.journalist_clicks_delete_all_and_sees_confirmation()\n        journ_app_nav.journalist_confirms_delete_selected()\n\n        def submission_deleted() -> None:\n            submissions_after_confirming_count = journ_app_nav.count_submissions_on_current_page()\n            # There will be 0 submissions left after deleting the source's submission\n            assert submissions_after_confirming_count == 0\n\n        journ_app_nav.nav_helper.wait_for(submission_deleted)\n        save_static_data(journ_app_nav.driver, locale, \"journalist-col_no_document\")\n\n    def test_col_has_no_key(self, sd_servers_with_deleted_source_key, firefox_web_driver):\n        # Given an SD server with an already-submitted file, but the source's key was deleted\n        # And a journalist logging into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_deleted_source_key.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_deleted_source_key.journalist_username,\n            password=sd_servers_with_deleted_source_key.journalist_password,\n            otp_secret=sd_servers_with_deleted_source_key.journalist_otp_secret,\n        )\n\n        # Take a screenshot of the source's page after their key was deleted\n        journ_app_nav.journalist_visits_col()\n        save_static_data(journ_app_nav.driver, locale, \"journalist-col_has_no_key\")",
              "file": "test_journalist_col.py"
            },
            {
              "type": "function",
              "name": "test_col_with_and_without_documents",
              "code": "def test_col_with_and_without_documents(\n        self, sd_servers_with_submitted_file, firefox_web_driver\n    ):\n        # Given an SD server with an already-submitted file\n        # And a journalist logging into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # Take a screenshot of the individual source's page when there is a document\n        journ_app_nav.journalist_visits_col()\n        save_static_data(journ_app_nav.driver, locale, \"journalist-col\")\n        # The documentation uses an identical screenshot with a different name:\n        # https://github.com/freedomofpress/securedrop-docs/blob/main/docs/images/manual\n        # /screenshots/journalist-col_javascript.png\n        # So we take the same screenshot again here\n        # TODO(AD): Update the documentation to use a single screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-col_javascript\")\n\n        # Take a screenshot of the individual source's page when there are no documents\n        journ_app_nav.journalist_clicks_delete_all_and_sees_confirmation()\n        journ_app_nav.journalist_confirms_delete_selected()\n\n        def submission_deleted() -> None:\n            submissions_after_confirming_count = journ_app_nav.count_submissions_on_current_page()\n            # There will be 0 submissions left after deleting the source's submission\n            assert submissions_after_confirming_count == 0\n\n        journ_app_nav.nav_helper.wait_for(submission_deleted)\n        save_static_data(journ_app_nav.driver, locale, \"journalist-col_no_document\")",
              "file": "test_journalist_col.py"
            },
            {
              "type": "function",
              "name": "submission_deleted",
              "code": "def submission_deleted() -> None:\n            submissions_after_confirming_count = journ_app_nav.count_submissions_on_current_page()\n            # There will be 0 submissions left after deleting the source's submission\n            assert submissions_after_confirming_count == 0",
              "file": "test_journalist_col.py"
            },
            {
              "type": "function",
              "name": "test_col_has_no_key",
              "code": "def test_col_has_no_key(self, sd_servers_with_deleted_source_key, firefox_web_driver):\n        # Given an SD server with an already-submitted file, but the source's key was deleted\n        # And a journalist logging into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_deleted_source_key.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_deleted_source_key.journalist_username,\n            password=sd_servers_with_deleted_source_key.journalist_password,\n            otp_secret=sd_servers_with_deleted_source_key.journalist_otp_secret,\n        )\n\n        # Take a screenshot of the source's page after their key was deleted\n        journ_app_nav.journalist_visits_col()\n        save_static_data(journ_app_nav.driver, locale, \"journalist-col_has_no_key\")",
              "file": "test_journalist_col.py"
            }
          ],
          "utils.py": [
            {
              "type": "function",
              "name": "save_static_data",
              "code": "def save_static_data(driver: WebDriver, locale: str, test_name: str) -> None:\n    \"\"\"Save a screenshot, a copy of the static HTML, and the output of accessibility sniffs.\"\"\"\n    # Save a screenshot\n    locale_screenshot_dir = _SCREENSHOTS_DIR / locale\n    locale_screenshot_dir.mkdir(parents=True, exist_ok=True)\n\n    img = Image.open(BytesIO(driver.get_screenshot_as_png()))\n    cropped = _autocrop_btm(img)\n    cropped.save(str(locale_screenshot_dir / f\"{test_name}.png\"))\n\n    # Save the HTML content\n    locale_html_dir = _HTML_DIR / locale\n    locale_html_dir.mkdir(parents=True, exist_ok=True)\n\n    html = driver.page_source\n    (locale_html_dir / f\"{test_name}.html\").write_text(html)\n\n    sniff_accessibility_issues(driver, locale, test_name)",
              "file": "utils.py"
            },
            {
              "type": "function",
              "name": "_autocrop_btm",
              "code": "def _autocrop_btm(img, bottom_padding=12):\n    \"\"\"Automatically crop the bottom of a screenshot.\"\"\"\n    # Get the grayscale of img\n    gray = img.convert(\"L\")\n    # We start one row above the bottom since the \"modal\" windows screenshots\n    # have a bottom line color different than the background\n    btm = img.height - 2\n    # Get the background luminance value from the bottom-leftmost pixel\n    bg = gray.getpixel((0, btm))\n\n    # Move up until the full row is not of the background luminance\n    while btm > 0 and all([gray.getpixel((col, btm)) == bg for col in range(gray.width)]):\n        btm -= 1\n\n    btm = min(btm + bottom_padding, img.height)\n\n    return img.crop((0, 0, img.width, btm))",
              "file": "utils.py"
            },
            {
              "type": "function",
              "name": "list_locales",
              "code": "def list_locales() -> List[str]:\n    if \"TEST_LOCALES\" in os.environ:\n        locales = os.environ[\"TEST_LOCALES\"].split()\n    else:\n        locales = [\"en_US\"]\n    return locales",
              "file": "utils.py"
            }
          ],
          "test_submit_and_retrieve_file.py": [
            {
              "type": "class",
              "name": "TestSubmitAndRetrieveFile",
              "code": "class TestSubmitAndRetrieveFile:\n    def test_submit_and_retrieve_happy_path(self, sd_servers_with_clean_state, firefox_web_driver):\n        # Given a source user accessing the app from their browser\n        locale = firefox_web_driver.locale\n        source_app_nav = SourceAppNavigator(\n            source_app_base_url=sd_servers_with_clean_state.source_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n\n        # And they created an account\n        source_app_nav.source_visits_source_homepage()\n        source_app_nav.source_clicks_submit_documents_on_homepage()\n        source_app_nav.source_continues_to_submit_page()\n        source_codename = source_app_nav.source_retrieves_codename_from_hint()\n\n        # And the source user submitted a file\n        submitted_content = \"Confidential file with some international characters: éèö\"\n        source_app_nav.source_submits_a_file(file_content=submitted_content)\n        source_app_nav.source_logs_out()\n\n        # And a journalist logs in\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n        journ_app_nav.journalist_checks_messages()\n\n        # When they star and unstar the submission, then it succeeds\n        self._journalist_stars_and_unstars_single_message(journ_app_nav)\n\n        # And when they try to download the file\n        # Then it succeeds and the journalist sees the correct content\n        retrieved_message = journ_app_nav.journalist_downloads_first_message()\n        assert retrieved_message == submitted_content\n\n        # And when they reply to the source, it succeeds\n        journ_app_nav.journalist_sends_reply_to_source()\n\n        # And when the source user comes back\n        source_app_nav.source_visits_source_homepage()\n        source_app_nav.source_chooses_to_login()\n        source_app_nav.source_proceeds_to_login(codename=source_codename)\n        save_static_data(source_app_nav.driver, locale, \"source-checks_for_reply\")\n\n        # When they delete the journalist's reply, it succeeds\n        self._source_deletes_journalist_reply(source_app_nav)\n        save_static_data(source_app_nav.driver, locale, \"source-deletes_reply\")\n\n    @staticmethod\n    def _source_deletes_journalist_reply(navigator: SourceAppNavigator) -> None:\n        # Get the reply filename so we can use IDs to select the delete buttons\n        reply_filename_element = navigator.driver.find_element(By.NAME, \"reply_filename\")\n        reply_filename = reply_filename_element.get_attribute(\"value\")\n\n        confirm_dialog_id = f\"confirm-delete-{reply_filename}\"\n        navigator.nav_helper.safe_click_by_css_selector(f\"a[href='#{confirm_dialog_id}']\")\n\n        def confirm_displayed():\n            confirm_dialog = navigator.driver.find_element(By.ID, confirm_dialog_id)\n            assert confirm_dialog.location_once_scrolled_into_view\n            assert confirm_dialog.is_displayed()\n\n        navigator.nav_helper.wait_for(confirm_displayed)\n        # Due to the . in the filename (which is used as ID), we need to escape it because otherwise\n        # we'd select the class gpg\n        navigator.nav_helper.safe_click_by_css_selector(\n            \"#{} button\".format(confirm_dialog_id.replace(\".\", \"\\\\.\"))\n        )\n\n        def reply_deleted():\n            if not navigator.accept_languages:\n                notification = navigator.driver.find_element(By.CLASS_NAME, \"success\")\n                assert \"Reply deleted\" in notification.text\n\n        navigator.nav_helper.wait_for(reply_deleted)\n\n    @staticmethod\n    def _journalist_stars_and_unstars_single_message(journ_app_nav: JournalistAppNavigator) -> None:\n        # Message begins unstarred\n        with pytest.raises(NoSuchElementException):\n            journ_app_nav.driver.find_element(By.ID, \"starred-source-link-1\")\n\n        # Journalist stars the message\n        journ_app_nav.driver.find_element(By.CLASS_NAME, \"button-star\").click()\n\n        def message_starred():\n            starred = journ_app_nav.driver.find_elements(By.ID, \"starred-source-link-1\")\n            assert len(starred) == 1\n\n        journ_app_nav.nav_helper.wait_for(message_starred)\n\n        # Journalist unstars the message\n        journ_app_nav.driver.find_element(By.CLASS_NAME, \"button-star\").click()\n\n        def message_unstarred():\n            with pytest.raises(NoSuchElementException):\n                journ_app_nav.driver.find_element(By.ID, \"starred-source-link-1\")\n\n        journ_app_nav.nav_helper.wait_for(message_unstarred)",
              "file": "test_submit_and_retrieve_file.py"
            },
            {
              "type": "function",
              "name": "test_submit_and_retrieve_happy_path",
              "code": "def test_submit_and_retrieve_happy_path(self, sd_servers_with_clean_state, firefox_web_driver):\n        # Given a source user accessing the app from their browser\n        locale = firefox_web_driver.locale\n        source_app_nav = SourceAppNavigator(\n            source_app_base_url=sd_servers_with_clean_state.source_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n\n        # And they created an account\n        source_app_nav.source_visits_source_homepage()\n        source_app_nav.source_clicks_submit_documents_on_homepage()\n        source_app_nav.source_continues_to_submit_page()\n        source_codename = source_app_nav.source_retrieves_codename_from_hint()\n\n        # And the source user submitted a file\n        submitted_content = \"Confidential file with some international characters: éèö\"\n        source_app_nav.source_submits_a_file(file_content=submitted_content)\n        source_app_nav.source_logs_out()\n\n        # And a journalist logs in\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n        journ_app_nav.journalist_checks_messages()\n\n        # When they star and unstar the submission, then it succeeds\n        self._journalist_stars_and_unstars_single_message(journ_app_nav)\n\n        # And when they try to download the file\n        # Then it succeeds and the journalist sees the correct content\n        retrieved_message = journ_app_nav.journalist_downloads_first_message()\n        assert retrieved_message == submitted_content\n\n        # And when they reply to the source, it succeeds\n        journ_app_nav.journalist_sends_reply_to_source()\n\n        # And when the source user comes back\n        source_app_nav.source_visits_source_homepage()\n        source_app_nav.source_chooses_to_login()\n        source_app_nav.source_proceeds_to_login(codename=source_codename)\n        save_static_data(source_app_nav.driver, locale, \"source-checks_for_reply\")\n\n        # When they delete the journalist's reply, it succeeds\n        self._source_deletes_journalist_reply(source_app_nav)\n        save_static_data(source_app_nav.driver, locale, \"source-deletes_reply\")",
              "file": "test_submit_and_retrieve_file.py"
            },
            {
              "type": "function",
              "name": "_source_deletes_journalist_reply",
              "code": "def _source_deletes_journalist_reply(navigator: SourceAppNavigator) -> None:\n        # Get the reply filename so we can use IDs to select the delete buttons\n        reply_filename_element = navigator.driver.find_element(By.NAME, \"reply_filename\")\n        reply_filename = reply_filename_element.get_attribute(\"value\")\n\n        confirm_dialog_id = f\"confirm-delete-{reply_filename}\"\n        navigator.nav_helper.safe_click_by_css_selector(f\"a[href='#{confirm_dialog_id}']\")\n\n        def confirm_displayed():\n            confirm_dialog = navigator.driver.find_element(By.ID, confirm_dialog_id)\n            assert confirm_dialog.location_once_scrolled_into_view\n            assert confirm_dialog.is_displayed()\n\n        navigator.nav_helper.wait_for(confirm_displayed)\n        # Due to the . in the filename (which is used as ID), we need to escape it because otherwise\n        # we'd select the class gpg\n        navigator.nav_helper.safe_click_by_css_selector(\n            \"#{} button\".format(confirm_dialog_id.replace(\".\", \"\\\\.\"))\n        )\n\n        def reply_deleted():\n            if not navigator.accept_languages:\n                notification = navigator.driver.find_element(By.CLASS_NAME, \"success\")\n                assert \"Reply deleted\" in notification.text\n\n        navigator.nav_helper.wait_for(reply_deleted)",
              "file": "test_submit_and_retrieve_file.py"
            },
            {
              "type": "function",
              "name": "confirm_displayed",
              "code": "def confirm_displayed():\n            confirm_dialog = navigator.driver.find_element(By.ID, confirm_dialog_id)\n            assert confirm_dialog.location_once_scrolled_into_view\n            assert confirm_dialog.is_displayed()",
              "file": "test_submit_and_retrieve_file.py"
            },
            {
              "type": "function",
              "name": "reply_deleted",
              "code": "def reply_deleted():\n            if not navigator.accept_languages:\n                notification = navigator.driver.find_element(By.CLASS_NAME, \"success\")\n                assert \"Reply deleted\" in notification.text",
              "file": "test_submit_and_retrieve_file.py"
            },
            {
              "type": "function",
              "name": "_journalist_stars_and_unstars_single_message",
              "code": "def _journalist_stars_and_unstars_single_message(journ_app_nav: JournalistAppNavigator) -> None:\n        # Message begins unstarred\n        with pytest.raises(NoSuchElementException):\n            journ_app_nav.driver.find_element(By.ID, \"starred-source-link-1\")\n\n        # Journalist stars the message\n        journ_app_nav.driver.find_element(By.CLASS_NAME, \"button-star\").click()\n\n        def message_starred():\n            starred = journ_app_nav.driver.find_elements(By.ID, \"starred-source-link-1\")\n            assert len(starred) == 1\n\n        journ_app_nav.nav_helper.wait_for(message_starred)\n\n        # Journalist unstars the message\n        journ_app_nav.driver.find_element(By.CLASS_NAME, \"button-star\").click()\n\n        def message_unstarred():\n            with pytest.raises(NoSuchElementException):\n                journ_app_nav.driver.find_element(By.ID, \"starred-source-link-1\")\n\n        journ_app_nav.nav_helper.wait_for(message_unstarred)",
              "file": "test_submit_and_retrieve_file.py"
            },
            {
              "type": "function",
              "name": "message_starred",
              "code": "def message_starred():\n            starred = journ_app_nav.driver.find_elements(By.ID, \"starred-source-link-1\")\n            assert len(starred) == 1",
              "file": "test_submit_and_retrieve_file.py"
            },
            {
              "type": "function",
              "name": "message_unstarred",
              "code": "def message_unstarred():\n            with pytest.raises(NoSuchElementException):\n                journ_app_nav.driver.find_element(By.ID, \"starred-source-link-1\")",
              "file": "test_submit_and_retrieve_file.py"
            }
          ],
          "test_journalist_admin.py": [
            {
              "type": "class",
              "name": "TestAdminLayoutAddAndEditUser",
              "code": "class TestAdminLayoutAddAndEditUser:\n    def test_admin_adds_user_hotp_and_edits_hotp(\n        self, sd_servers_with_clean_state, firefox_web_driver\n    ):\n        # Given an SD server\n        # And a journalist logging into the journalist interface as an admin\n        assert sd_servers_with_clean_state.journalist_is_admin\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # Take a screenshot of the admin interface\n        journ_app_nav.admin_visits_admin_interface()\n        save_static_data(journ_app_nav.driver, locale, \"journalist-admin_interface_index\")\n\n        # Take screenshots of the steps for creating an hotp journalist\n        def screenshot_of_add_user_hotp_form() -> None:\n            save_static_data(journ_app_nav.driver, locale, \"journalist-admin_add_user_hotp\")\n\n        def screenshot_of_journalist_new_user_two_factor_hotp() -> None:\n            save_static_data(\n                journ_app_nav.driver, locale, \"journalist-admin_new_user_two_factor_hotp\"\n            )\n\n        result = journ_app_nav.admin_creates_a_user(\n            hotp_secret=\"c4 26 43 52 69 13 02 49 9f 6a a5 33 96 46 d9 05 42 a3 4f ae\",\n            callback_before_submitting_add_user_step=screenshot_of_add_user_hotp_form,\n            callback_before_submitting_2fa_step=screenshot_of_journalist_new_user_two_factor_hotp,\n        )\n        new_user_username, new_user_pw, new_user_otp_secret = result\n        save_static_data(journ_app_nav.driver, locale, \"journalist-admin\")\n\n        # Take a screenshot of the new journalist's edit page\n        journ_app_nav.admin_visits_user_edit_page(username_of_journalist_to_edit=new_user_username)\n        save_static_data(journ_app_nav.driver, locale, \"journalist-edit_account_admin\")\n        # The documentation uses an identical screenshot with a different name:\n        # https://github.com/freedomofpress/securedrop-docs/blob/main/docs/images/manual\n        # /screenshots/journalist-admin_edit_hotp_secret.png\n        # So we take the same screenshot again here\n        # TODO(AD): Update the documentation to use a single screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-admin_edit_hotp_secret\")\n\n        # Then the admin resets the new journalist's hotp\n        def _admin_visits_reset_2fa_hotp_step() -> None:\n            # 2FA reset buttons show a tooltip with explanatory text on hover.\n            # Also, confirm the text on the tooltip is the correct one.\n            hotp_reset_button = journ_app_nav.driver.find_elements(By.ID, \"reset-two-factor-hotp\")[\n                0\n            ]\n            assert hotp_reset_button.location_once_scrolled_into_view\n            ActionChains(journ_app_nav.driver).move_to_element(hotp_reset_button).perform()\n\n            time.sleep(1)\n\n            tip_opacity = journ_app_nav.driver.find_elements(\n                By.CSS_SELECTOR, \"#button-reset-two-factor-hotp span.tooltip\"\n            )[0].value_of_css_property(\"opacity\")\n            tip_text = journ_app_nav.driver.find_elements(\n                By.CSS_SELECTOR, \"#button-reset-two-factor-hotp span.tooltip\"\n            )[0].text\n            assert tip_opacity == \"1\"\n\n            if not journ_app_nav.accept_languages:\n                assert (\n                    tip_text == \"Reset two-factor authentication for security keys, like a YubiKey\"\n                )\n\n            journ_app_nav.nav_helper.safe_click_by_id(\"button-reset-two-factor-hotp\")\n\n        # Run the above step in a retry loop\n        self._retry_2fa_pop_ups(\n            journ_app_nav, _admin_visits_reset_2fa_hotp_step, \"reset-two-factor-hotp\"\n        )\n\n        # Wait for it to succeed\n        journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.CSS_SELECTOR, 'input[name=\"otp_secret\"]')\n        )\n\n    def test_admin_adds_user_totp_and_edits_totp(\n        self, sd_servers_with_clean_state, firefox_web_driver\n    ):\n        # Given an SD server\n        # And a journalist logging into the journalist interface as an admin\n        assert sd_servers_with_clean_state.journalist_is_admin\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n        journ_app_nav.admin_visits_admin_interface()\n\n        # Take screenshots of the steps for creating a totp journalist\n        def screenshot_of_add_user_totp_form() -> None:\n            save_static_data(journ_app_nav.driver, locale, \"journalist-admin_add_user_totp\")\n\n        def screenshot_of_journalist_new_user_two_factor_totp() -> None:\n            save_static_data(\n                journ_app_nav.driver, locale, \"journalist-admin_new_user_two_factor_totp\"\n            )\n\n        result = journ_app_nav.admin_creates_a_user(\n            callback_before_submitting_add_user_step=screenshot_of_add_user_totp_form,\n            callback_before_submitting_2fa_step=screenshot_of_journalist_new_user_two_factor_totp,\n        )\n        new_user_username, new_user_pw, new_user_otp_secret = result\n\n        # Then the admin resets the second journalist's totp\n        journ_app_nav.admin_visits_user_edit_page(username_of_journalist_to_edit=new_user_username)\n\n        def _admin_visits_reset_2fa_totp_step() -> None:\n            totp_reset_button = journ_app_nav.driver.find_elements(By.ID, \"reset-two-factor-totp\")[\n                0\n            ]\n            assert \"/admin/reset-2fa-totp\" in totp_reset_button.get_attribute(\"action\")\n            # 2FA reset buttons show a tooltip with explanatory text on hover.\n            # Also, confirm the text on the tooltip is the correct one.\n            totp_reset_button = journ_app_nav.driver.find_elements(\n                By.CSS_SELECTOR, \"#button-reset-two-factor-totp\"\n            )[0]\n            assert totp_reset_button.location_once_scrolled_into_view\n            ActionChains(journ_app_nav.driver).move_to_element(totp_reset_button).perform()\n\n            time.sleep(1)\n\n            tip_opacity = journ_app_nav.driver.find_elements(\n                By.CSS_SELECTOR, \"#button-reset-two-factor-totp span.tooltip\"\n            )[0].value_of_css_property(\"opacity\")\n            tip_text = journ_app_nav.driver.find_elements(\n                By.CSS_SELECTOR, \"#button-reset-two-factor-totp span.tooltip\"\n            )[0].text\n\n            assert tip_opacity == \"1\"\n            if not journ_app_nav.accept_languages:\n                expected_text = \"Reset two-factor authentication for mobile apps, such as FreeOTP\"\n                assert tip_text == expected_text\n\n            journ_app_nav.nav_helper.safe_click_by_id(\"button-reset-two-factor-totp\")\n\n        # Run the above step in a retry loop\n        self._retry_2fa_pop_ups(\n            journ_app_nav, _admin_visits_reset_2fa_totp_step, \"reset-two-factor-totp\"\n        )\n\n        # Then it succeeds\n        # Take a screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-admin_edit_totp_secret\")\n\n    @staticmethod\n    def _retry_2fa_pop_ups(\n        journ_app_nav: JournalistAppNavigator, navigation_step: Callable, button_to_click: str\n    ) -> None:\n        \"\"\"Clicking on Selenium alerts can be flaky. We need to retry them if they timeout.\"\"\"\n        for _i in range(15):\n            try:\n                try:\n                    # This is the button we click to trigger the alert.\n                    journ_app_nav.nav_helper.wait_for(\n                        lambda: journ_app_nav.driver.find_elements(By.ID, button_to_click)[0]\n                    )\n                except IndexError:\n                    # If the button isn't there, then the alert is up from the last\n                    # time we attempted to run this test. Switch to it and accept it.\n                    journ_app_nav.nav_helper.alert_wait()\n                    journ_app_nav.nav_helper.alert_accept()\n                    break\n\n                # The alert isn't up. Run the rest of the logic.\n                navigation_step()\n\n                journ_app_nav.nav_helper.alert_wait()\n                journ_app_nav.nav_helper.alert_accept()\n                break\n            except TimeoutException:\n                # Selenium has failed to click, and the confirmation\n                # alert didn't happen. We'll try again.\n                logging.info(\"Selenium has failed to click; retrying.\")",
              "file": "test_journalist_admin.py"
            },
            {
              "type": "function",
              "name": "test_admin_adds_user_hotp_and_edits_hotp",
              "code": "def test_admin_adds_user_hotp_and_edits_hotp(\n        self, sd_servers_with_clean_state, firefox_web_driver\n    ):\n        # Given an SD server\n        # And a journalist logging into the journalist interface as an admin\n        assert sd_servers_with_clean_state.journalist_is_admin\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # Take a screenshot of the admin interface\n        journ_app_nav.admin_visits_admin_interface()\n        save_static_data(journ_app_nav.driver, locale, \"journalist-admin_interface_index\")\n\n        # Take screenshots of the steps for creating an hotp journalist\n        def screenshot_of_add_user_hotp_form() -> None:\n            save_static_data(journ_app_nav.driver, locale, \"journalist-admin_add_user_hotp\")\n\n        def screenshot_of_journalist_new_user_two_factor_hotp() -> None:\n            save_static_data(\n                journ_app_nav.driver, locale, \"journalist-admin_new_user_two_factor_hotp\"\n            )\n\n        result = journ_app_nav.admin_creates_a_user(\n            hotp_secret=\"c4 26 43 52 69 13 02 49 9f 6a a5 33 96 46 d9 05 42 a3 4f ae\",\n            callback_before_submitting_add_user_step=screenshot_of_add_user_hotp_form,\n            callback_before_submitting_2fa_step=screenshot_of_journalist_new_user_two_factor_hotp,\n        )\n        new_user_username, new_user_pw, new_user_otp_secret = result\n        save_static_data(journ_app_nav.driver, locale, \"journalist-admin\")\n\n        # Take a screenshot of the new journalist's edit page\n        journ_app_nav.admin_visits_user_edit_page(username_of_journalist_to_edit=new_user_username)\n        save_static_data(journ_app_nav.driver, locale, \"journalist-edit_account_admin\")\n        # The documentation uses an identical screenshot with a different name:\n        # https://github.com/freedomofpress/securedrop-docs/blob/main/docs/images/manual\n        # /screenshots/journalist-admin_edit_hotp_secret.png\n        # So we take the same screenshot again here\n        # TODO(AD): Update the documentation to use a single screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-admin_edit_hotp_secret\")\n\n        # Then the admin resets the new journalist's hotp\n        def _admin_visits_reset_2fa_hotp_step() -> None:\n            # 2FA reset buttons show a tooltip with explanatory text on hover.\n            # Also, confirm the text on the tooltip is the correct one.\n            hotp_reset_button = journ_app_nav.driver.find_elements(By.ID, \"reset-two-factor-hotp\")[\n                0\n            ]\n            assert hotp_reset_button.location_once_scrolled_into_view\n            ActionChains(journ_app_nav.driver).move_to_element(hotp_reset_button).perform()\n\n            time.sleep(1)\n\n            tip_opacity = journ_app_nav.driver.find_elements(\n                By.CSS_SELECTOR, \"#button-reset-two-factor-hotp span.tooltip\"\n            )[0].value_of_css_property(\"opacity\")\n            tip_text = journ_app_nav.driver.find_elements(\n                By.CSS_SELECTOR, \"#button-reset-two-factor-hotp span.tooltip\"\n            )[0].text\n            assert tip_opacity == \"1\"\n\n            if not journ_app_nav.accept_languages:\n                assert (\n                    tip_text == \"Reset two-factor authentication for security keys, like a YubiKey\"\n                )\n\n            journ_app_nav.nav_helper.safe_click_by_id(\"button-reset-two-factor-hotp\")\n\n        # Run the above step in a retry loop\n        self._retry_2fa_pop_ups(\n            journ_app_nav, _admin_visits_reset_2fa_hotp_step, \"reset-two-factor-hotp\"\n        )\n\n        # Wait for it to succeed\n        journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.CSS_SELECTOR, 'input[name=\"otp_secret\"]')\n        )",
              "file": "test_journalist_admin.py"
            },
            {
              "type": "function",
              "name": "screenshot_of_add_user_hotp_form",
              "code": "def screenshot_of_add_user_hotp_form() -> None:\n            save_static_data(journ_app_nav.driver, locale, \"journalist-admin_add_user_hotp\")",
              "file": "test_journalist_admin.py"
            },
            {
              "type": "function",
              "name": "screenshot_of_journalist_new_user_two_factor_hotp",
              "code": "def screenshot_of_journalist_new_user_two_factor_hotp() -> None:\n            save_static_data(\n                journ_app_nav.driver, locale, \"journalist-admin_new_user_two_factor_hotp\"\n            )",
              "file": "test_journalist_admin.py"
            },
            {
              "type": "function",
              "name": "_admin_visits_reset_2fa_hotp_step",
              "code": "def _admin_visits_reset_2fa_hotp_step() -> None:\n            # 2FA reset buttons show a tooltip with explanatory text on hover.\n            # Also, confirm the text on the tooltip is the correct one.\n            hotp_reset_button = journ_app_nav.driver.find_elements(By.ID, \"reset-two-factor-hotp\")[\n                0\n            ]\n            assert hotp_reset_button.location_once_scrolled_into_view\n            ActionChains(journ_app_nav.driver).move_to_element(hotp_reset_button).perform()\n\n            time.sleep(1)\n\n            tip_opacity = journ_app_nav.driver.find_elements(\n                By.CSS_SELECTOR, \"#button-reset-two-factor-hotp span.tooltip\"\n            )[0].value_of_css_property(\"opacity\")\n            tip_text = journ_app_nav.driver.find_elements(\n                By.CSS_SELECTOR, \"#button-reset-two-factor-hotp span.tooltip\"\n            )[0].text\n            assert tip_opacity == \"1\"\n\n            if not journ_app_nav.accept_languages:\n                assert (\n                    tip_text == \"Reset two-factor authentication for security keys, like a YubiKey\"\n                )\n\n            journ_app_nav.nav_helper.safe_click_by_id(\"button-reset-two-factor-hotp\")",
              "file": "test_journalist_admin.py"
            },
            {
              "type": "function",
              "name": "test_admin_adds_user_totp_and_edits_totp",
              "code": "def test_admin_adds_user_totp_and_edits_totp(\n        self, sd_servers_with_clean_state, firefox_web_driver\n    ):\n        # Given an SD server\n        # And a journalist logging into the journalist interface as an admin\n        assert sd_servers_with_clean_state.journalist_is_admin\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n        journ_app_nav.admin_visits_admin_interface()\n\n        # Take screenshots of the steps for creating a totp journalist\n        def screenshot_of_add_user_totp_form() -> None:\n            save_static_data(journ_app_nav.driver, locale, \"journalist-admin_add_user_totp\")\n\n        def screenshot_of_journalist_new_user_two_factor_totp() -> None:\n            save_static_data(\n                journ_app_nav.driver, locale, \"journalist-admin_new_user_two_factor_totp\"\n            )\n\n        result = journ_app_nav.admin_creates_a_user(\n            callback_before_submitting_add_user_step=screenshot_of_add_user_totp_form,\n            callback_before_submitting_2fa_step=screenshot_of_journalist_new_user_two_factor_totp,\n        )\n        new_user_username, new_user_pw, new_user_otp_secret = result\n\n        # Then the admin resets the second journalist's totp\n        journ_app_nav.admin_visits_user_edit_page(username_of_journalist_to_edit=new_user_username)\n\n        def _admin_visits_reset_2fa_totp_step() -> None:\n            totp_reset_button = journ_app_nav.driver.find_elements(By.ID, \"reset-two-factor-totp\")[\n                0\n            ]\n            assert \"/admin/reset-2fa-totp\" in totp_reset_button.get_attribute(\"action\")\n            # 2FA reset buttons show a tooltip with explanatory text on hover.\n            # Also, confirm the text on the tooltip is the correct one.\n            totp_reset_button = journ_app_nav.driver.find_elements(\n                By.CSS_SELECTOR, \"#button-reset-two-factor-totp\"\n            )[0]\n            assert totp_reset_button.location_once_scrolled_into_view\n            ActionChains(journ_app_nav.driver).move_to_element(totp_reset_button).perform()\n\n            time.sleep(1)\n\n            tip_opacity = journ_app_nav.driver.find_elements(\n                By.CSS_SELECTOR, \"#button-reset-two-factor-totp span.tooltip\"\n            )[0].value_of_css_property(\"opacity\")\n            tip_text = journ_app_nav.driver.find_elements(\n                By.CSS_SELECTOR, \"#button-reset-two-factor-totp span.tooltip\"\n            )[0].text\n\n            assert tip_opacity == \"1\"\n            if not journ_app_nav.accept_languages:\n                expected_text = \"Reset two-factor authentication for mobile apps, such as FreeOTP\"\n                assert tip_text == expected_text\n\n            journ_app_nav.nav_helper.safe_click_by_id(\"button-reset-two-factor-totp\")\n\n        # Run the above step in a retry loop\n        self._retry_2fa_pop_ups(\n            journ_app_nav, _admin_visits_reset_2fa_totp_step, \"reset-two-factor-totp\"\n        )\n\n        # Then it succeeds\n        # Take a screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-admin_edit_totp_secret\")",
              "file": "test_journalist_admin.py"
            },
            {
              "type": "function",
              "name": "screenshot_of_add_user_totp_form",
              "code": "def screenshot_of_add_user_totp_form() -> None:\n            save_static_data(journ_app_nav.driver, locale, \"journalist-admin_add_user_totp\")",
              "file": "test_journalist_admin.py"
            },
            {
              "type": "function",
              "name": "screenshot_of_journalist_new_user_two_factor_totp",
              "code": "def screenshot_of_journalist_new_user_two_factor_totp() -> None:\n            save_static_data(\n                journ_app_nav.driver, locale, \"journalist-admin_new_user_two_factor_totp\"\n            )",
              "file": "test_journalist_admin.py"
            },
            {
              "type": "function",
              "name": "_admin_visits_reset_2fa_totp_step",
              "code": "def _admin_visits_reset_2fa_totp_step() -> None:\n            totp_reset_button = journ_app_nav.driver.find_elements(By.ID, \"reset-two-factor-totp\")[\n                0\n            ]\n            assert \"/admin/reset-2fa-totp\" in totp_reset_button.get_attribute(\"action\")\n            # 2FA reset buttons show a tooltip with explanatory text on hover.\n            # Also, confirm the text on the tooltip is the correct one.\n            totp_reset_button = journ_app_nav.driver.find_elements(\n                By.CSS_SELECTOR, \"#button-reset-two-factor-totp\"\n            )[0]\n            assert totp_reset_button.location_once_scrolled_into_view\n            ActionChains(journ_app_nav.driver).move_to_element(totp_reset_button).perform()\n\n            time.sleep(1)\n\n            tip_opacity = journ_app_nav.driver.find_elements(\n                By.CSS_SELECTOR, \"#button-reset-two-factor-totp span.tooltip\"\n            )[0].value_of_css_property(\"opacity\")\n            tip_text = journ_app_nav.driver.find_elements(\n                By.CSS_SELECTOR, \"#button-reset-two-factor-totp span.tooltip\"\n            )[0].text\n\n            assert tip_opacity == \"1\"\n            if not journ_app_nav.accept_languages:\n                expected_text = \"Reset two-factor authentication for mobile apps, such as FreeOTP\"\n                assert tip_text == expected_text\n\n            journ_app_nav.nav_helper.safe_click_by_id(\"button-reset-two-factor-totp\")",
              "file": "test_journalist_admin.py"
            },
            {
              "type": "function",
              "name": "_retry_2fa_pop_ups",
              "code": "def _retry_2fa_pop_ups(\n        journ_app_nav: JournalistAppNavigator, navigation_step: Callable, button_to_click: str\n    ) -> None:\n        \"\"\"Clicking on Selenium alerts can be flaky. We need to retry them if they timeout.\"\"\"\n        for _i in range(15):\n            try:\n                try:\n                    # This is the button we click to trigger the alert.\n                    journ_app_nav.nav_helper.wait_for(\n                        lambda: journ_app_nav.driver.find_elements(By.ID, button_to_click)[0]\n                    )\n                except IndexError:\n                    # If the button isn't there, then the alert is up from the last\n                    # time we attempted to run this test. Switch to it and accept it.\n                    journ_app_nav.nav_helper.alert_wait()\n                    journ_app_nav.nav_helper.alert_accept()\n                    break\n\n                # The alert isn't up. Run the rest of the logic.\n                navigation_step()\n\n                journ_app_nav.nav_helper.alert_wait()\n                journ_app_nav.nav_helper.alert_accept()\n                break\n            except TimeoutException:\n                # Selenium has failed to click, and the confirmation\n                # alert didn't happen. We'll try again.\n                logging.info(\"Selenium has failed to click; retrying.\")",
              "file": "test_journalist_admin.py"
            },
            {
              "type": "class",
              "name": "TestAdminLayoutEditConfig",
              "code": "class TestAdminLayoutEditConfig:\n    def test_admin_changes_logo(self, sd_servers_with_clean_state, firefox_web_driver):\n        # Given an SD server\n        # And a journalist logging into the journalist interface as an admin\n        assert sd_servers_with_clean_state.journalist_is_admin\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # Take a screenshot of the system config page\n        journ_app_nav.admin_visits_admin_interface()\n        journ_app_nav.admin_visits_system_config_page()\n        save_static_data(journ_app_nav.driver, locale, \"journalist-admin_system_config_page\")\n\n        # When the admin tries to upload a new logo\n        current_file_path = Path(__file__).absolute().parent\n        logo_path = current_file_path / \"..\" / \"..\" / \"..\" / \"static\" / \"i\" / \"logo.png\"\n        assert logo_path.is_file()\n        journ_app_nav.nav_helper.safe_send_keys_by_id(\"logo-upload\", str(logo_path))\n        journ_app_nav.nav_helper.safe_click_by_id(\"submit-logo-update\")\n\n        # Then it succeeds\n        def updated_image() -> None:\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            if not journ_app_nav.accept_languages:\n                assert \"Image updated.\" in flash_msg.text\n\n        journ_app_nav.nav_helper.wait_for(updated_image, timeout=20)\n\n        # Take a screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-admin_changes_logo_image\")\n\n    def test_ossec_alert_button(self, sd_servers, firefox_web_driver):\n        # Given an SD server\n        # And a journalist logging into the journalist interface as an admin\n        assert sd_servers.journalist_is_admin\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers.journalist_username,\n            password=sd_servers.journalist_password,\n            otp_secret=sd_servers.journalist_otp_secret,\n        )\n        # And they go to the admin config page\n        journ_app_nav.admin_visits_admin_interface()\n        journ_app_nav.admin_visits_system_config_page()\n\n        # When they try to send an OSSEC alert\n        alert_button = journ_app_nav.driver.find_element(By.ID, \"test-ossec-alert\")\n        alert_button.click()\n\n        # Then it succeeds\n        def test_alert_sent():\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            if not journ_app_nav.accept_languages:\n                assert \"Test alert sent. Please check your email.\" in flash_msg.text\n\n        journ_app_nav.nav_helper.wait_for(test_alert_sent)\n\n        # Take a screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-admin_ossec_alert_button\")",
              "file": "test_journalist_admin.py"
            },
            {
              "type": "function",
              "name": "test_admin_changes_logo",
              "code": "def test_admin_changes_logo(self, sd_servers_with_clean_state, firefox_web_driver):\n        # Given an SD server\n        # And a journalist logging into the journalist interface as an admin\n        assert sd_servers_with_clean_state.journalist_is_admin\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # Take a screenshot of the system config page\n        journ_app_nav.admin_visits_admin_interface()\n        journ_app_nav.admin_visits_system_config_page()\n        save_static_data(journ_app_nav.driver, locale, \"journalist-admin_system_config_page\")\n\n        # When the admin tries to upload a new logo\n        current_file_path = Path(__file__).absolute().parent\n        logo_path = current_file_path / \"..\" / \"..\" / \"..\" / \"static\" / \"i\" / \"logo.png\"\n        assert logo_path.is_file()\n        journ_app_nav.nav_helper.safe_send_keys_by_id(\"logo-upload\", str(logo_path))\n        journ_app_nav.nav_helper.safe_click_by_id(\"submit-logo-update\")\n\n        # Then it succeeds\n        def updated_image() -> None:\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            if not journ_app_nav.accept_languages:\n                assert \"Image updated.\" in flash_msg.text\n\n        journ_app_nav.nav_helper.wait_for(updated_image, timeout=20)\n\n        # Take a screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-admin_changes_logo_image\")",
              "file": "test_journalist_admin.py"
            },
            {
              "type": "function",
              "name": "updated_image",
              "code": "def updated_image() -> None:\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            if not journ_app_nav.accept_languages:\n                assert \"Image updated.\" in flash_msg.text",
              "file": "test_journalist_admin.py"
            },
            {
              "type": "function",
              "name": "test_ossec_alert_button",
              "code": "def test_ossec_alert_button(self, sd_servers, firefox_web_driver):\n        # Given an SD server\n        # And a journalist logging into the journalist interface as an admin\n        assert sd_servers.journalist_is_admin\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers.journalist_username,\n            password=sd_servers.journalist_password,\n            otp_secret=sd_servers.journalist_otp_secret,\n        )\n        # And they go to the admin config page\n        journ_app_nav.admin_visits_admin_interface()\n        journ_app_nav.admin_visits_system_config_page()\n\n        # When they try to send an OSSEC alert\n        alert_button = journ_app_nav.driver.find_element(By.ID, \"test-ossec-alert\")\n        alert_button.click()\n\n        # Then it succeeds\n        def test_alert_sent():\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            if not journ_app_nav.accept_languages:\n                assert \"Test alert sent. Please check your email.\" in flash_msg.text\n\n        journ_app_nav.nav_helper.wait_for(test_alert_sent)\n\n        # Take a screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-admin_ossec_alert_button\")",
              "file": "test_journalist_admin.py"
            },
            {
              "type": "function",
              "name": "test_alert_sent",
              "code": "def test_alert_sent():\n            flash_msg = journ_app_nav.driver.find_element(By.CSS_SELECTOR, \".flash\")\n            if not journ_app_nav.accept_languages:\n                assert \"Test alert sent. Please check your email.\" in flash_msg.text",
              "file": "test_journalist_admin.py"
            }
          ],
          "test_source.py": [
            {
              "type": "class",
              "name": "TestSourceLayout",
              "code": "class TestSourceLayout:\n    def test(self, locale, sd_servers_with_clean_state, tor_browser_web_driver):\n        # Given a source user accessing the app from their browser\n        source_app_nav = SourceAppNavigator(\n            source_app_base_url=sd_servers_with_clean_state.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n            accept_languages=locale,\n        )\n\n        # And they created an account\n        source_app_nav.source_visits_source_homepage()\n\n        # Take a screenshot of the \"account created\" page\n        source_app_nav.source_clicks_submit_documents_on_homepage()\n        save_static_data(source_app_nav.driver, locale, \"source-generate\")\n\n        # Take a screenshot of showing the codename hint\n        source_app_nav.source_continues_to_submit_page()\n        source_app_nav.source_retrieves_codename_from_hint()\n        save_static_data(source_app_nav.driver, locale, \"source-lookup-shows-codename\")\n\n        # Take a screenshot of entering text in the message field\n        source_app_nav.nav_helper.safe_send_keys_by_id(\"msg\", \"Secret message éè\")\n        save_static_data(source_app_nav.driver, locale, \"source-submission_entered_text\")\n\n        # Take a screenshot of submitting a file\n        source_app_nav.source_submits_a_file()\n        save_static_data(source_app_nav.driver, locale, \"source-lookup\")\n\n        # Take a screenshot of doing a second submission\n        source_app_nav.source_submits_a_message()\n        save_static_data(source_app_nav.driver, locale, \"source-next_submission_flashed_message\")\n\n    def test_login(self, locale, sd_servers_with_clean_state, tor_browser_web_driver):\n        # Given a source user accessing the app from their browser\n        source_app_nav = SourceAppNavigator(\n            source_app_base_url=sd_servers_with_clean_state.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        # And they created an account\n        source_app_nav.source_visits_source_homepage()\n\n        # Take a screenshot of the login page\n        source_app_nav.source_chooses_to_login()\n        save_static_data(source_app_nav.driver, locale, \"source-login\")\n\n        # Take a screenshot of entering text in the login form\n        source_app_nav.nav_helper.safe_send_keys_by_id(\n            \"codename\", \"ascension hypertext concert synopses\"\n        )\n        save_static_data(source_app_nav.driver, locale, \"source-enter-codename-in-login\")",
              "file": "test_source.py"
            },
            {
              "type": "function",
              "name": "test",
              "code": "def test(self, locale, sd_servers_with_clean_state, tor_browser_web_driver):\n        # Given a source user accessing the app from their browser\n        source_app_nav = SourceAppNavigator(\n            source_app_base_url=sd_servers_with_clean_state.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n            accept_languages=locale,\n        )\n\n        # And they created an account\n        source_app_nav.source_visits_source_homepage()\n\n        # Take a screenshot of the \"account created\" page\n        source_app_nav.source_clicks_submit_documents_on_homepage()\n        save_static_data(source_app_nav.driver, locale, \"source-generate\")\n\n        # Take a screenshot of showing the codename hint\n        source_app_nav.source_continues_to_submit_page()\n        source_app_nav.source_retrieves_codename_from_hint()\n        save_static_data(source_app_nav.driver, locale, \"source-lookup-shows-codename\")\n\n        # Take a screenshot of entering text in the message field\n        source_app_nav.nav_helper.safe_send_keys_by_id(\"msg\", \"Secret message éè\")\n        save_static_data(source_app_nav.driver, locale, \"source-submission_entered_text\")\n\n        # Take a screenshot of submitting a file\n        source_app_nav.source_submits_a_file()\n        save_static_data(source_app_nav.driver, locale, \"source-lookup\")\n\n        # Take a screenshot of doing a second submission\n        source_app_nav.source_submits_a_message()\n        save_static_data(source_app_nav.driver, locale, \"source-next_submission_flashed_message\")",
              "file": "test_source.py"
            },
            {
              "type": "function",
              "name": "test_login",
              "code": "def test_login(self, locale, sd_servers_with_clean_state, tor_browser_web_driver):\n        # Given a source user accessing the app from their browser\n        source_app_nav = SourceAppNavigator(\n            source_app_base_url=sd_servers_with_clean_state.source_app_base_url,\n            web_driver=tor_browser_web_driver,\n        )\n\n        # And they created an account\n        source_app_nav.source_visits_source_homepage()\n\n        # Take a screenshot of the login page\n        source_app_nav.source_chooses_to_login()\n        save_static_data(source_app_nav.driver, locale, \"source-login\")\n\n        # Take a screenshot of entering text in the login form\n        source_app_nav.nav_helper.safe_send_keys_by_id(\n            \"codename\", \"ascension hypertext concert synopses\"\n        )\n        save_static_data(source_app_nav.driver, locale, \"source-enter-codename-in-login\")",
              "file": "test_source.py"
            }
          ],
          "test_journalist_delete.py": [
            {
              "type": "class",
              "name": "TestJournalistLayoutDelete",
              "code": "class TestJournalistLayoutDelete:\n    def test_delete_none(self, sd_servers_with_submitted_file, firefox_web_driver):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # And the journalist went to the individual source's page\n        journ_app_nav.journalist_visits_col()\n\n        journ_app_nav.journalist_clicks_delete_selected_link()\n\n        journ_app_nav.journalist_confirm_delete_selected()\n        save_static_data(journ_app_nav.driver, locale, \"journalist-delete_none\")\n\n    def test_delete_one_confirmation(self, sd_servers_with_submitted_file, firefox_web_driver):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # And the journalist went to the individual source's page\n        journ_app_nav.journalist_visits_col()\n\n        # And the journalist selected the first submission\n        journ_app_nav.journalist_selects_first_doc()\n\n        # Take a screenshot of the confirmation prompt when the journalist clicks the delete button\n        journ_app_nav.journalist_clicks_delete_selected_link()\n        save_static_data(journ_app_nav.driver, locale, \"journalist-delete_one_confirmation\")\n\n    def test_delete_all_confirmation(self, sd_servers_with_submitted_file, firefox_web_driver):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # And the journalist went to the individual source's page\n        journ_app_nav.journalist_visits_col()\n\n        # Take a screenshot of the prompt when the journalist clicks the delete all button\n        journ_app_nav.journalist_clicks_delete_all_and_sees_confirmation()\n        save_static_data(journ_app_nav.driver, locale, \"journalist-delete_all_confirmation\")\n\n    def test_delete_one(self, sd_servers_with_submitted_file, firefox_web_driver):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # And the journalist went to the individual source's page\n        journ_app_nav.journalist_visits_col()\n\n        journ_app_nav.journalist_selects_first_doc()\n\n        # And the journalist clicked the delete button and confirmed\n        journ_app_nav.journalist_clicks_delete_selected_link()\n        journ_app_nav.nav_helper.safe_click_by_id(\"delete-selected\")\n\n        # Take a screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-delete_one\")\n\n    def test_delete_all(self, sd_servers_with_submitted_file, firefox_web_driver):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # And the journalist went to the individual source's page\n        journ_app_nav.journalist_visits_col()\n\n        journ_app_nav.journalist_clicks_delete_all_and_sees_confirmation()\n        journ_app_nav.journalist_confirms_delete_selected()\n\n        # Take a screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-delete_all\")",
              "file": "test_journalist_delete.py"
            },
            {
              "type": "function",
              "name": "test_delete_none",
              "code": "def test_delete_none(self, sd_servers_with_submitted_file, firefox_web_driver):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # And the journalist went to the individual source's page\n        journ_app_nav.journalist_visits_col()\n\n        journ_app_nav.journalist_clicks_delete_selected_link()\n\n        journ_app_nav.journalist_confirm_delete_selected()\n        save_static_data(journ_app_nav.driver, locale, \"journalist-delete_none\")",
              "file": "test_journalist_delete.py"
            },
            {
              "type": "function",
              "name": "test_delete_one_confirmation",
              "code": "def test_delete_one_confirmation(self, sd_servers_with_submitted_file, firefox_web_driver):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # And the journalist went to the individual source's page\n        journ_app_nav.journalist_visits_col()\n\n        # And the journalist selected the first submission\n        journ_app_nav.journalist_selects_first_doc()\n\n        # Take a screenshot of the confirmation prompt when the journalist clicks the delete button\n        journ_app_nav.journalist_clicks_delete_selected_link()\n        save_static_data(journ_app_nav.driver, locale, \"journalist-delete_one_confirmation\")",
              "file": "test_journalist_delete.py"
            },
            {
              "type": "function",
              "name": "test_delete_all_confirmation",
              "code": "def test_delete_all_confirmation(self, sd_servers_with_submitted_file, firefox_web_driver):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # And the journalist went to the individual source's page\n        journ_app_nav.journalist_visits_col()\n\n        # Take a screenshot of the prompt when the journalist clicks the delete all button\n        journ_app_nav.journalist_clicks_delete_all_and_sees_confirmation()\n        save_static_data(journ_app_nav.driver, locale, \"journalist-delete_all_confirmation\")",
              "file": "test_journalist_delete.py"
            },
            {
              "type": "function",
              "name": "test_delete_one",
              "code": "def test_delete_one(self, sd_servers_with_submitted_file, firefox_web_driver):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # And the journalist went to the individual source's page\n        journ_app_nav.journalist_visits_col()\n\n        journ_app_nav.journalist_selects_first_doc()\n\n        # And the journalist clicked the delete button and confirmed\n        journ_app_nav.journalist_clicks_delete_selected_link()\n        journ_app_nav.nav_helper.safe_click_by_id(\"delete-selected\")\n\n        # Take a screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-delete_one\")",
              "file": "test_journalist_delete.py"
            },
            {
              "type": "function",
              "name": "test_delete_all",
              "code": "def test_delete_all(self, sd_servers_with_submitted_file, firefox_web_driver):\n        # Given an SD server with a file submitted by a source\n        # And a journalist logged into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n\n        # And the journalist went to the individual source's page\n        journ_app_nav.journalist_visits_col()\n\n        journ_app_nav.journalist_clicks_delete_all_and_sees_confirmation()\n        journ_app_nav.journalist_confirms_delete_selected()\n\n        # Take a screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-delete_all\")",
              "file": "test_journalist_delete.py"
            }
          ],
          "accessibility.py": [
            {
              "type": "class",
              "name": "MessageType",
              "code": "class MessageType(Enum):\n    ERROR = 1\n    WARNING = 2\n    NOTICE = 3",
              "file": "accessibility.py"
            },
            {
              "type": "class",
              "name": "Message",
              "code": "class Message:\n    \"\"\"Contains all of the information in a message emitted by HTML CodeSniffer.\"\"\"\n\n    principle_id: str\n    message: str\n    message_type: MessageType\n    responsible_html: str\n    selector: str\n    element_type: str\n\n    @staticmethod\n    def from_output(output: str) -> \"Message\":\n        \"\"\"Parses the output of htmlcs and returns an instance containing all data.\n\n        No processing is performed for flexibility.\n\n        Example message, post-split (note: contents of index 4 contains no newlines, but I had to\n        split it to keep the linter happy):\n\n        0: [HTMLCS] Error\n        1: WCAG2AAA.Principle1.Guideline1_3.1_3_1_AAA.G141\n        2: h2\n        3: #security-level-heading\n        4: The heading structure is not logically nested. This h2 element appears to be the\n           primary document heading, so should be an h1 element.\n        5: <h2 id=\"security-level-heading\" hidden=\"\">...</h2>\n        \"\"\"\n\n        fields = output.split(\"|\")\n\n        if \"Error\" in fields[0]:\n            message_type = MessageType.ERROR\n        elif \"Warning\" in fields[0]:\n            message_type = MessageType.WARNING\n        elif \"Notice\" in fields[0]:\n            message_type = MessageType.NOTICE\n        else:\n            raise ValueError(f\"Unexpected message type: {fields[0]}\")\n\n        return Message(\n            message_type=message_type,\n            principle_id=fields[1],\n            element_type=fields[2],\n            selector=fields[3],\n            message=fields[4],\n            responsible_html=fields[5],\n        )\n\n    def __format__(self, _spec: str) -> str:\n        newline = \"\\n\"\n        return f\"\"\"\n{self.message_type}: {self.principle_id}\n    {self.message}\n\n    html:\n        {self.responsible_html.replace(newline, f\"{newline}        \")}\n        \"\"\"",
              "file": "accessibility.py"
            },
            {
              "type": "function",
              "name": "from_output",
              "code": "def from_output(output: str) -> \"Message\":\n        \"\"\"Parses the output of htmlcs and returns an instance containing all data.\n\n        No processing is performed for flexibility.\n\n        Example message, post-split (note: contents of index 4 contains no newlines, but I had to\n        split it to keep the linter happy):\n\n        0: [HTMLCS] Error\n        1: WCAG2AAA.Principle1.Guideline1_3.1_3_1_AAA.G141\n        2: h2\n        3: #security-level-heading\n        4: The heading structure is not logically nested. This h2 element appears to be the\n           primary document heading, so should be an h1 element.\n        5: <h2 id=\"security-level-heading\" hidden=\"\">...</h2>\n        \"\"\"\n\n        fields = output.split(\"|\")\n\n        if \"Error\" in fields[0]:\n            message_type = MessageType.ERROR\n        elif \"Warning\" in fields[0]:\n            message_type = MessageType.WARNING\n        elif \"Notice\" in fields[0]:\n            message_type = MessageType.NOTICE\n        else:\n            raise ValueError(f\"Unexpected message type: {fields[0]}\")\n\n        return Message(\n            message_type=message_type,\n            principle_id=fields[1],\n            element_type=fields[2],\n            selector=fields[3],\n            message=fields[4],\n            responsible_html=fields[5],\n        )",
              "file": "accessibility.py"
            },
            {
              "type": "function",
              "name": "__format__",
              "code": "def __format__(self, _spec: str) -> str:\n        newline = \"\\n\"\n        return f\"\"\"\n{self.message_type}: {self.principle_id}\n    {self.message}\n\n    html:\n        {self.responsible_html.replace(newline, f\"{newline}        \")}\n        \"\"\"",
              "file": "accessibility.py"
            },
            {
              "type": "function",
              "name": "sniff_accessibility_issues",
              "code": "def sniff_accessibility_issues(driver: WebDriver, locale: str, test_name: str) -> None:\n    \"\"\"Runs accessibility sniffs on the driver's current page.\n\n    This function is responsible for injecting HTML CodeSniffer into the current page and writing\n    the results to a file. This way, test functions can focus on the setup required to navigate to\n    a particular URL (for example, logging in to get to the messages page).\n    \"\"\"\n\n    # 1. Retrieve/compute required data\n    with open(\"/usr/local/lib/node_modules/html_codesniffer/build/HTMLCS.js\") as htmlcs:\n        html_codesniffer = htmlcs.read()\n\n    errors_dir = _ACCESSIBILITY_DIR / locale / \"errors\"\n    errors_dir.mkdir(parents=True, exist_ok=True)\n\n    reviews_dir = _ACCESSIBILITY_DIR / locale / \"reviews\"\n    reviews_dir.mkdir(parents=True, exist_ok=True)\n\n    # 2. Do the thing\n    raw_messages = driver.execute_script(html_codesniffer + _HTMLCS_RUNNER_CODE)\n\n    # 3. Organize the data\n    messages: Dict[str, List[Message]] = {\n        \"machine-verified\": [],\n        \"human-reviewed\": [],\n    }\n\n    for message in map(Message.from_output, raw_messages[:-1]):  # last message is effectievly EOF\n        if message.message_type == MessageType.ERROR:\n            messages[\"machine-verified\"].append(message)\n        else:\n            messages[\"human-reviewed\"].append(message)\n\n    # 4. Report the data\n    # Note: it is useful to create empty files when there are no results to simplify the logic for\n    #       summarizing the results, implemented in `summarize_accessibility_results`.\n    with open(errors_dir / f\"{test_name}.txt\", \"w\") as error_file:\n        for message in messages[\"machine-verified\"]:\n            error_file.write(f\"{message}\")\n\n    with open(reviews_dir / f\"{test_name}.txt\", \"w\") as review_file:\n        for message in messages[\"human-reviewed\"]:\n            review_file.write(f\"{message}\")",
              "file": "accessibility.py"
            },
            {
              "type": "function",
              "name": "summarize_accessibility_results",
              "code": "def summarize_accessibility_results() -> None:\n    \"\"\"Creates a file containing summary information about the result of accessiblity sniffing\n\n    Note: This does not automatically run as part of the test suite, use\n          `make accessibility-summary` instead.\n    \"\"\"\n\n    try:\n        summary: Dict[str, Dict[str, Dict[str, Union[int, bool]]]] = {}\n\n        # since `sniff_accessibility_issues` creates empty files, all locale/type combinations will\n        # contain the same set of files; getting filenames from en_US/reviews instead of, say,\n        # ar/errors is arbitrary and sufficient\n        for out_filename in os.listdir(_ACCESSIBILITY_DIR / \"en_US\" / \"reviews\"):\n            summary[out_filename] = {\n                \"reviews\": {\"count\": 0, \"locale_differs\": False},\n                \"errors\": {\"count\": 0, \"locale_differs\": False},\n            }\n\n            # collect all of the relevant data\n            for message_type in [\"reviews\", \"errors\"]:\n                outputs: Dict[str, Dict[str, List[str]]] = {}\n\n                for locale in [\"en_US\", \"ar\"]:\n                    outputs[locale] = {}\n                    with open(\n                        _ACCESSIBILITY_DIR / locale / message_type / out_filename\n                    ) as out_file:\n                        # Only look at lines specifying the message (including the exact WCAG error\n                        # code); this is exactly correct for the count, and approximately correct\n                        # for comparing locales. If the order of the errors differs, or if there\n                        # are a different number of any kind of error, this approximation will catch\n                        # it.\n                        outputs[locale][message_type] = [\n                            line for line in out_file.readlines() if \"MessageType.\" in line\n                        ]\n\n                summary[out_filename][message_type][\"count\"] = len(outputs[\"en_US\"][message_type])\n                summary[out_filename][message_type][\"locale_differs\"] = (\n                    outputs[\"en_US\"][message_type] != outputs[\"ar\"][message_type]\n                )\n\n        # save the data to a convenient file\n        with open(_ACCESSIBILITY_DIR / \"summary.txt\", \"w\") as summary_file:\n            for name in sorted(summary.keys()):\n                summary_file.write(name + \":\\n\")\n                for message_type in [\"errors\", \"reviews\"]:\n                    summary_file.write(\n                        f\"\\t{message_type}: {summary[name][message_type]['count']}\\n\"\n                    )\n                    if summary[name][message_type][\"locale_differs\"]:\n                        summary_file.write(f\"\\t        NOTE: {message_type} differ by locale\\n\")\n\n                summary_file.write(\"\\n\")\n\n    # this should only happen if the pageslayout tests have not created the raw output files\n    except FileNotFoundError:\n        print(\n            f\"ERROR: Run `make test TESTFILES={os.path.dirname(_ACCESSIBILITY_DIR)}` before \"\n            \"running `make accessibility-summary`\"\n        )",
              "file": "accessibility.py"
            }
          ],
          "test_source_session_layout.py": [
            {
              "type": "function",
              "name": "sd_servers_with_short_timeout",
              "code": "def sd_servers_with_short_timeout(\n    setup_journalist_key_and_gpg_folder: Tuple[str, Path],\n    setup_rqworker: Tuple[str, str],\n) -> Generator[SdServersFixtureResult, None, None]:\n    \"\"\"Spawn the source and journalist apps as separate processes with a short session timeout.\"\"\"\n    # Generate a securedrop config with a very short session timeout\n    journalist_key_fingerprint, gpg_key_dir = setup_journalist_key_and_gpg_folder\n    worker_name, _ = setup_rqworker\n    config_with_short_timeout = SecureDropConfigFactory.create(\n        SESSION_EXPIRATION_MINUTES=SESSION_EXPIRATION_SECONDS / 60,\n        SECUREDROP_DATA_ROOT=Path(\"/tmp/sd-tests/functional-session-timeout\"),\n        GPG_KEY_DIR=gpg_key_dir,\n        JOURNALIST_KEY=journalist_key_fingerprint,\n        RQ_WORKER_NAME=worker_name,\n    )\n\n    # Spawn the apps in separate processes\n    with spawn_sd_servers(config_to_use=config_with_short_timeout) as sd_servers_result:\n        yield sd_servers_result",
              "file": "test_source_session_layout.py"
            },
            {
              "type": "class",
              "name": "TestSourceAppSessionTimeout",
              "code": "class TestSourceAppSessionTimeout:\n    def test_source_session_timeout(self, locale, sd_servers_with_short_timeout):\n        # Given an SD server with a very short session timeout\n        # And a source user accessing the source app from their browser\n        with SourceAppNavigator.using_tor_browser_web_driver(\n            source_app_base_url=sd_servers_with_short_timeout.source_app_base_url,\n            accept_languages=locale,\n        ) as navigator:\n            # And they're logged in and are using the app\n            navigator.source_visits_source_homepage()\n            navigator.source_clicks_submit_documents_on_homepage()\n            navigator.source_continues_to_submit_page()\n\n            # And their session just expired\n            time.sleep(SESSION_EXPIRATION_SECONDS + 1)\n\n            # When the source user reloads the page\n            navigator.driver.refresh()\n\n            # Then the source user sees the \"session expired\" message\n            notification = navigator.driver.find_element(By.CLASS_NAME, \"error\")\n            assert notification.text\n            if locale == \"en_US\":\n                expected_text = (\n                    \"You have been logged out due to inactivity or a problem with your session.\"\n                )\n                assert expected_text in notification.text\n\n            save_static_data(navigator.driver, locale, \"source-session_timeout\")",
              "file": "test_source_session_layout.py"
            },
            {
              "type": "function",
              "name": "test_source_session_timeout",
              "code": "def test_source_session_timeout(self, locale, sd_servers_with_short_timeout):\n        # Given an SD server with a very short session timeout\n        # And a source user accessing the source app from their browser\n        with SourceAppNavigator.using_tor_browser_web_driver(\n            source_app_base_url=sd_servers_with_short_timeout.source_app_base_url,\n            accept_languages=locale,\n        ) as navigator:\n            # And they're logged in and are using the app\n            navigator.source_visits_source_homepage()\n            navigator.source_clicks_submit_documents_on_homepage()\n            navigator.source_continues_to_submit_page()\n\n            # And their session just expired\n            time.sleep(SESSION_EXPIRATION_SECONDS + 1)\n\n            # When the source user reloads the page\n            navigator.driver.refresh()\n\n            # Then the source user sees the \"session expired\" message\n            notification = navigator.driver.find_element(By.CLASS_NAME, \"error\")\n            assert notification.text\n            if locale == \"en_US\":\n                expected_text = (\n                    \"You have been logged out due to inactivity or a problem with your session.\"\n                )\n                assert expected_text in notification.text\n\n            save_static_data(navigator.driver, locale, \"source-session_timeout\")",
              "file": "test_source_session_layout.py"
            }
          ],
          "test_source_layout_torbrowser.py": [
            {
              "type": "class",
              "name": "TestSourceLayoutTorBrowser",
              "code": "class TestSourceLayoutTorBrowser:\n    def test_index_and_logout(self, locale, sd_servers):\n        # Given a source user accessing the app from their browser\n        with SourceAppNavigator.using_tor_browser_web_driver(\n            source_app_base_url=sd_servers.source_app_base_url,\n            accept_languages=locale,\n        ) as navigator:\n            # And they have disabled JS in their browser\n            disable_js(navigator.driver)\n\n            # When they first login, it succeeds\n            navigator.source_visits_source_homepage()\n            save_static_data(navigator.driver, locale, \"source-index\")\n\n            navigator.source_clicks_submit_documents_on_homepage()\n            navigator.source_continues_to_submit_page()\n\n            # And when they logout, it succeeds\n            navigator.source_logs_out()\n            save_static_data(navigator.driver, locale, \"source-logout_page\")",
              "file": "test_source_layout_torbrowser.py"
            },
            {
              "type": "function",
              "name": "test_index_and_logout",
              "code": "def test_index_and_logout(self, locale, sd_servers):\n        # Given a source user accessing the app from their browser\n        with SourceAppNavigator.using_tor_browser_web_driver(\n            source_app_base_url=sd_servers.source_app_base_url,\n            accept_languages=locale,\n        ) as navigator:\n            # And they have disabled JS in their browser\n            disable_js(navigator.driver)\n\n            # When they first login, it succeeds\n            navigator.source_visits_source_homepage()\n            save_static_data(navigator.driver, locale, \"source-index\")\n\n            navigator.source_clicks_submit_documents_on_homepage()\n            navigator.source_continues_to_submit_page()\n\n            # And when they logout, it succeeds\n            navigator.source_logs_out()\n            save_static_data(navigator.driver, locale, \"source-logout_page\")",
              "file": "test_source_layout_torbrowser.py"
            }
          ],
          "test_journalist_account.py": [
            {
              "type": "class",
              "name": "TestJournalistLayoutAccount",
              "code": "class TestJournalistLayoutAccount:\n    def test_account_edit_and_set_hotp_secret(\n        self, sd_servers_with_clean_state, firefox_web_driver\n    ):\n        # Given an SD server\n        # And a journalist logging into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # And the journalist went to the edit account page\n        journ_app_nav.journalist_visits_edit_account()\n\n        # Take a screenshot of the edit hotp page\n        self._clicks_reset_secret(\n            journ_app_nav, \"hotp\", assert_tooltip_text_is=\"RESET SECURITY KEY CREDENTIALS\"\n        )\n        save_static_data(journ_app_nav.driver, locale, \"journalist-account_edit_hotp_secret\")\n\n        # Update the hotp secret and take a screenshot\n        journ_app_nav.nav_helper.safe_send_keys_by_css_selector(\n            'input[name=\"otp_secret\"]', \"123456\"\n        )\n        journ_app_nav.nav_helper.safe_click_by_css_selector(\n            \"form#account-edit-hotp-secret button[type=submit]\"\n        )\n        save_static_data(journ_app_nav.driver, locale, \"journalist-account_new_two_factor_hotp\")\n\n    @staticmethod\n    def _clicks_reset_secret(\n        journ_app_nav: JournalistAppNavigator, otp_type: str, assert_tooltip_text_is: Optional[str]\n    ) -> None:\n        reset_form = journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.ID, f\"reset-two-factor-{otp_type}\")\n        )\n        assert f\"/account/reset-2fa-{otp_type}\" in reset_form.get_attribute(\"action\")\n        reset_button = journ_app_nav.driver.find_elements(\n            By.CSS_SELECTOR, f\"#button-reset-two-factor-{otp_type}\"\n        )[0]\n\n        # 2FA reset buttons show a tooltip with explanatory text on hover.\n        # Also, confirm the text on the tooltip is the correct one.\n        assert reset_button.location_once_scrolled_into_view\n        ActionChains(journ_app_nav.driver).move_to_element(reset_button).perform()\n\n        def explanatory_tooltip_is_correct() -> None:\n            explanatory_tooltip = journ_app_nav.driver.find_element(\n                By.CSS_SELECTOR, f\"#button-reset-two-factor-{otp_type} span\"\n            )\n\n            explanatory_tooltip_opacity = explanatory_tooltip.value_of_css_property(\"opacity\")\n            assert explanatory_tooltip_opacity == \"1\"\n\n            if assert_tooltip_text_is and not journ_app_nav.accept_languages:\n                assert explanatory_tooltip.text == assert_tooltip_text_is\n\n        journ_app_nav.nav_helper.wait_for(explanatory_tooltip_is_correct)\n\n        reset_form.submit()\n\n        alert = journ_app_nav.driver.switch_to.alert\n        alert.accept()\n\n    def test_account_new_two_factor_totp(self, sd_servers_with_clean_state, firefox_web_driver):\n        # Given an SD server\n        # And a journalist logging into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # And the journalist went to the edit account page\n        journ_app_nav.journalist_visits_edit_account()\n\n        # Take a screenshot of the edit totp page\n        self._clicks_reset_secret(\n            journ_app_nav, \"totp\", assert_tooltip_text_is=\"RESET MOBILE APP CREDENTIALS\"\n        )\n        save_static_data(journ_app_nav.driver, locale, \"journalist-account_new_two_factor_totp\")",
              "file": "test_journalist_account.py"
            },
            {
              "type": "function",
              "name": "test_account_edit_and_set_hotp_secret",
              "code": "def test_account_edit_and_set_hotp_secret(\n        self, sd_servers_with_clean_state, firefox_web_driver\n    ):\n        # Given an SD server\n        # And a journalist logging into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # And the journalist went to the edit account page\n        journ_app_nav.journalist_visits_edit_account()\n\n        # Take a screenshot of the edit hotp page\n        self._clicks_reset_secret(\n            journ_app_nav, \"hotp\", assert_tooltip_text_is=\"RESET SECURITY KEY CREDENTIALS\"\n        )\n        save_static_data(journ_app_nav.driver, locale, \"journalist-account_edit_hotp_secret\")\n\n        # Update the hotp secret and take a screenshot\n        journ_app_nav.nav_helper.safe_send_keys_by_css_selector(\n            'input[name=\"otp_secret\"]', \"123456\"\n        )\n        journ_app_nav.nav_helper.safe_click_by_css_selector(\n            \"form#account-edit-hotp-secret button[type=submit]\"\n        )\n        save_static_data(journ_app_nav.driver, locale, \"journalist-account_new_two_factor_hotp\")",
              "file": "test_journalist_account.py"
            },
            {
              "type": "function",
              "name": "_clicks_reset_secret",
              "code": "def _clicks_reset_secret(\n        journ_app_nav: JournalistAppNavigator, otp_type: str, assert_tooltip_text_is: Optional[str]\n    ) -> None:\n        reset_form = journ_app_nav.nav_helper.wait_for(\n            lambda: journ_app_nav.driver.find_element(By.ID, f\"reset-two-factor-{otp_type}\")\n        )\n        assert f\"/account/reset-2fa-{otp_type}\" in reset_form.get_attribute(\"action\")\n        reset_button = journ_app_nav.driver.find_elements(\n            By.CSS_SELECTOR, f\"#button-reset-two-factor-{otp_type}\"\n        )[0]\n\n        # 2FA reset buttons show a tooltip with explanatory text on hover.\n        # Also, confirm the text on the tooltip is the correct one.\n        assert reset_button.location_once_scrolled_into_view\n        ActionChains(journ_app_nav.driver).move_to_element(reset_button).perform()\n\n        def explanatory_tooltip_is_correct() -> None:\n            explanatory_tooltip = journ_app_nav.driver.find_element(\n                By.CSS_SELECTOR, f\"#button-reset-two-factor-{otp_type} span\"\n            )\n\n            explanatory_tooltip_opacity = explanatory_tooltip.value_of_css_property(\"opacity\")\n            assert explanatory_tooltip_opacity == \"1\"\n\n            if assert_tooltip_text_is and not journ_app_nav.accept_languages:\n                assert explanatory_tooltip.text == assert_tooltip_text_is\n\n        journ_app_nav.nav_helper.wait_for(explanatory_tooltip_is_correct)\n\n        reset_form.submit()\n\n        alert = journ_app_nav.driver.switch_to.alert\n        alert.accept()",
              "file": "test_journalist_account.py"
            },
            {
              "type": "function",
              "name": "explanatory_tooltip_is_correct",
              "code": "def explanatory_tooltip_is_correct() -> None:\n            explanatory_tooltip = journ_app_nav.driver.find_element(\n                By.CSS_SELECTOR, f\"#button-reset-two-factor-{otp_type} span\"\n            )\n\n            explanatory_tooltip_opacity = explanatory_tooltip.value_of_css_property(\"opacity\")\n            assert explanatory_tooltip_opacity == \"1\"\n\n            if assert_tooltip_text_is and not journ_app_nav.accept_languages:\n                assert explanatory_tooltip.text == assert_tooltip_text_is",
              "file": "test_journalist_account.py"
            },
            {
              "type": "function",
              "name": "test_account_new_two_factor_totp",
              "code": "def test_account_new_two_factor_totp(self, sd_servers_with_clean_state, firefox_web_driver):\n        # Given an SD server\n        # And a journalist logging into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_clean_state.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_clean_state.journalist_username,\n            password=sd_servers_with_clean_state.journalist_password,\n            otp_secret=sd_servers_with_clean_state.journalist_otp_secret,\n        )\n\n        # And the journalist went to the edit account page\n        journ_app_nav.journalist_visits_edit_account()\n\n        # Take a screenshot of the edit totp page\n        self._clicks_reset_secret(\n            journ_app_nav, \"totp\", assert_tooltip_text_is=\"RESET MOBILE APP CREDENTIALS\"\n        )\n        save_static_data(journ_app_nav.driver, locale, \"journalist-account_new_two_factor_totp\")",
              "file": "test_journalist_account.py"
            }
          ],
          "test_journalist.py": [
            {
              "type": "class",
              "name": "TestJournalistLayout",
              "code": "class TestJournalistLayout:\n    def test_login_index_and_edit(self, sd_servers, firefox_web_driver, request):\n        locale = firefox_web_driver.locale\n        # Given an SD server\n        # And a journalist accessing the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.driver.get(f\"{sd_servers.journalist_app_base_url}/login\")\n        journ_app_nav.got_expected_language(locale)\n        save_static_data(journ_app_nav.driver, locale, \"journalist-login\")\n\n        # And they log into the app and are an admin\n        assert sd_servers.journalist_is_admin\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers.journalist_username,\n            password=sd_servers.journalist_password,\n            otp_secret=sd_servers.journalist_otp_secret,\n        )\n        save_static_data(journ_app_nav.driver, locale, \"journalist-index_no_documents\")\n        # The documentation uses an identical screenshot with a different name:\n        # https://github.com/freedomofpress/securedrop-docs/blob/main/docs/images/manual\n        # /screenshots/journalist-admin_index_no_documents.png\n        # So we take the same screenshot again here\n        # TODO(AD): Update the documentation to use a single screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-admin_index_no_documents\")\n\n        # Take a screenshot of the edit account page\n        journ_app_nav.journalist_visits_edit_account()\n        save_static_data(journ_app_nav.driver, locale, \"journalist-edit_account_user\")\n\n    def test_index_entered_text(self, sd_servers, firefox_web_driver):\n        # Given an SD server\n        # And a journalist accessing the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n\n        # Take a screenshot of the login page with the form completed\n        journ_app_nav.journalist_goes_to_login_page_and_enters_credentials(\n            username=\"jane_doe\",\n            password=\"my password is long\",\n            otp_secret=\"2HGGVF5VPHWMCAYQ\",\n            should_submit_login_form=False,\n        )\n        save_static_data(journ_app_nav.driver, locale, \"journalist-index_with_text\")\n\n    def test_index_with_submission_and_select_documents(\n        self, sd_servers_with_submitted_file, firefox_web_driver\n    ):\n        # Given an SD server with an already-submitted file\n        # And a journalist logging into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n\n        # Take a screenshot of the index page when there is a source and submission\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n        save_static_data(journ_app_nav.driver, locale, \"journalist-index\")\n        # The documentation uses an identical screenshot with a different name:\n        # https://github.com/freedomofpress/securedrop-docs/blob/main/docs/images/manual\n        # /screenshots/journalist-index_javascript.png\n        # So we take the same screenshot again here\n        # TODO(AD): Update the documentation to use a single screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-index_javascript\")\n\n        # Take a screenshot of the source's page\n        journ_app_nav.journalist_selects_the_first_source()\n        checkboxes = journ_app_nav.get_submission_checkboxes_on_current_page()\n        for checkbox in checkboxes:\n            checkbox.click()\n        save_static_data(\n            journ_app_nav.driver, locale, \"journalist-clicks_on_source_and_selects_documents\"\n        )\n\n        # Take a screenshot of the reply page\n        journ_app_nav.journalist_composes_reply_to_source(\n            reply_content=\"Thanks for the documents.\"\n            \" Can you submit more information about the main program?\"\n        )\n        save_static_data(journ_app_nav.driver, locale, \"journalist-composes_reply\")\n\n    def test_fail_to_visit_admin(self, sd_servers, firefox_web_driver):\n        # Given an SD server\n        # And someone accessing the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        # Take a screenshot of them trying to force-browse to the admin interface\n        journ_app_nav.driver.get(f\"{sd_servers.journalist_app_base_url}/admin\")\n        save_static_data(journ_app_nav.driver, locale, \"journalist-code-fail_to_visit_admin\")\n\n    def test_fail_login(self, sd_servers, firefox_web_driver):\n        # Given an SD server\n        # And someone accessing the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n\n        # Take a screenshot of trying to log in using invalid credentials\n        journ_app_nav.journalist_goes_to_login_page_and_enters_credentials(\n            username=\"root\",\n            password=\"worse\",\n            otp_secret=\"2HGGVF5VPHWMCAYQ\",\n            should_submit_login_form=True,\n        )\n        save_static_data(journ_app_nav.driver, locale, \"journalist-code-fail_login\")\n        # The documentation uses an identical screenshot with a different name:\n        # https://github.com/freedomofpress/securedrop-docs/blob/main/docs/images/manual\n        # /screenshots/journalist-code-fail_login_many.png\n        # So we take the same screenshot again here\n        # TODO(AD): Update the documentation to use a single screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-code-fail_login_many\")",
              "file": "test_journalist.py"
            },
            {
              "type": "function",
              "name": "test_login_index_and_edit",
              "code": "def test_login_index_and_edit(self, sd_servers, firefox_web_driver, request):\n        locale = firefox_web_driver.locale\n        # Given an SD server\n        # And a journalist accessing the journalist interface\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        journ_app_nav.driver.get(f\"{sd_servers.journalist_app_base_url}/login\")\n        journ_app_nav.got_expected_language(locale)\n        save_static_data(journ_app_nav.driver, locale, \"journalist-login\")\n\n        # And they log into the app and are an admin\n        assert sd_servers.journalist_is_admin\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers.journalist_username,\n            password=sd_servers.journalist_password,\n            otp_secret=sd_servers.journalist_otp_secret,\n        )\n        save_static_data(journ_app_nav.driver, locale, \"journalist-index_no_documents\")\n        # The documentation uses an identical screenshot with a different name:\n        # https://github.com/freedomofpress/securedrop-docs/blob/main/docs/images/manual\n        # /screenshots/journalist-admin_index_no_documents.png\n        # So we take the same screenshot again here\n        # TODO(AD): Update the documentation to use a single screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-admin_index_no_documents\")\n\n        # Take a screenshot of the edit account page\n        journ_app_nav.journalist_visits_edit_account()\n        save_static_data(journ_app_nav.driver, locale, \"journalist-edit_account_user\")",
              "file": "test_journalist.py"
            },
            {
              "type": "function",
              "name": "test_index_entered_text",
              "code": "def test_index_entered_text(self, sd_servers, firefox_web_driver):\n        # Given an SD server\n        # And a journalist accessing the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n\n        # Take a screenshot of the login page with the form completed\n        journ_app_nav.journalist_goes_to_login_page_and_enters_credentials(\n            username=\"jane_doe\",\n            password=\"my password is long\",\n            otp_secret=\"2HGGVF5VPHWMCAYQ\",\n            should_submit_login_form=False,\n        )\n        save_static_data(journ_app_nav.driver, locale, \"journalist-index_with_text\")",
              "file": "test_journalist.py"
            },
            {
              "type": "function",
              "name": "test_index_with_submission_and_select_documents",
              "code": "def test_index_with_submission_and_select_documents(\n        self, sd_servers_with_submitted_file, firefox_web_driver\n    ):\n        # Given an SD server with an already-submitted file\n        # And a journalist logging into the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers_with_submitted_file.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n\n        # Take a screenshot of the index page when there is a source and submission\n        journ_app_nav.journalist_logs_in(\n            username=sd_servers_with_submitted_file.journalist_username,\n            password=sd_servers_with_submitted_file.journalist_password,\n            otp_secret=sd_servers_with_submitted_file.journalist_otp_secret,\n        )\n        save_static_data(journ_app_nav.driver, locale, \"journalist-index\")\n        # The documentation uses an identical screenshot with a different name:\n        # https://github.com/freedomofpress/securedrop-docs/blob/main/docs/images/manual\n        # /screenshots/journalist-index_javascript.png\n        # So we take the same screenshot again here\n        # TODO(AD): Update the documentation to use a single screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-index_javascript\")\n\n        # Take a screenshot of the source's page\n        journ_app_nav.journalist_selects_the_first_source()\n        checkboxes = journ_app_nav.get_submission_checkboxes_on_current_page()\n        for checkbox in checkboxes:\n            checkbox.click()\n        save_static_data(\n            journ_app_nav.driver, locale, \"journalist-clicks_on_source_and_selects_documents\"\n        )\n\n        # Take a screenshot of the reply page\n        journ_app_nav.journalist_composes_reply_to_source(\n            reply_content=\"Thanks for the documents.\"\n            \" Can you submit more information about the main program?\"\n        )\n        save_static_data(journ_app_nav.driver, locale, \"journalist-composes_reply\")",
              "file": "test_journalist.py"
            },
            {
              "type": "function",
              "name": "test_fail_to_visit_admin",
              "code": "def test_fail_to_visit_admin(self, sd_servers, firefox_web_driver):\n        # Given an SD server\n        # And someone accessing the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n        # Take a screenshot of them trying to force-browse to the admin interface\n        journ_app_nav.driver.get(f\"{sd_servers.journalist_app_base_url}/admin\")\n        save_static_data(journ_app_nav.driver, locale, \"journalist-code-fail_to_visit_admin\")",
              "file": "test_journalist.py"
            },
            {
              "type": "function",
              "name": "test_fail_login",
              "code": "def test_fail_login(self, sd_servers, firefox_web_driver):\n        # Given an SD server\n        # And someone accessing the journalist interface\n        locale = firefox_web_driver.locale\n        journ_app_nav = JournalistAppNavigator(\n            journalist_app_base_url=sd_servers.journalist_app_base_url,\n            web_driver=firefox_web_driver,\n            accept_languages=locale,\n        )\n\n        # Take a screenshot of trying to log in using invalid credentials\n        journ_app_nav.journalist_goes_to_login_page_and_enters_credentials(\n            username=\"root\",\n            password=\"worse\",\n            otp_secret=\"2HGGVF5VPHWMCAYQ\",\n            should_submit_login_form=True,\n        )\n        save_static_data(journ_app_nav.driver, locale, \"journalist-code-fail_login\")\n        # The documentation uses an identical screenshot with a different name:\n        # https://github.com/freedomofpress/securedrop-docs/blob/main/docs/images/manual\n        # /screenshots/journalist-code-fail_login_many.png\n        # So we take the same screenshot again here\n        # TODO(AD): Update the documentation to use a single screenshot\n        save_static_data(journ_app_nav.driver, locale, \"journalist-code-fail_login_many\")",
              "file": "test_journalist.py"
            }
          ],
          "test_source_static_pages.py": [
            {
              "type": "class",
              "name": "TestSourceAppStaticPages",
              "code": "class TestSourceAppStaticPages:\n    @pytest.mark.parametrize(\"locale\", list_locales())\n    def test_notfound(self, locale, sd_servers):\n        # Given a source user accessing the app from their browser\n        with get_web_driver(\n            web_driver_type=WebDriverTypeEnum.TOR_BROWSER,\n            accept_languages=locale,\n        ) as tor_browser_web_driver:\n            # When they try to access a page that does not exist\n            tor_browser_web_driver.get(f\"{sd_servers.source_app_base_url}/does_not_exist\")\n\n            # Then the right error is displayed\n            message = tor_browser_web_driver.find_element(By.ID, \"page-not-found\")\n            assert message.is_displayed()\n\n            save_static_data(tor_browser_web_driver, locale, \"source-notfound\")\n\n    @pytest.mark.parametrize(\"locale\", list_locales())\n    def test_static_pages(self, locale, sd_servers):\n        # Given a source user accessing the app from their browser\n        with get_web_driver(\n            web_driver_type=WebDriverTypeEnum.TOR_BROWSER,\n            accept_languages=locale,\n        ) as tor_browser_web_driver:\n            # The user can browse to some of the app's static pages\n            tor_browser_web_driver.get(f\"{sd_servers.source_app_base_url}/use-tor\")\n            save_static_data(tor_browser_web_driver, locale, \"source-use_tor_browser\")\n\n            tor_browser_web_driver.get(f\"{sd_servers.source_app_base_url}/tor2web-warning\")\n            save_static_data(tor_browser_web_driver, locale, \"source-tor2web_warning\")\n\n            tor_browser_web_driver.get(f\"{sd_servers.source_app_base_url}/why-public-key\")\n            save_static_data(tor_browser_web_driver, locale, \"source-why_journalist_key\")\n\n    def test_instance_metadata(self, sd_servers):\n        # Given a source app, when fetching the instance's metadata\n        url = f\"{sd_servers.source_app_base_url}/metadata\"\n        response = requests.get(url=url, proxies=tor_utils.proxies_for_url(url))\n\n        # Then it succeeds and the right information is returned\n        returned_data = response.json()\n        assert returned_data[\"server_os\"] in [\"20.04\", \"24.04\"]\n        assert returned_data[\"sd_version\"] == __version__\n        assert returned_data[\"gpg_fpr\"]",
              "file": "test_source_static_pages.py"
            },
            {
              "type": "function",
              "name": "test_notfound",
              "code": "def test_notfound(self, locale, sd_servers):\n        # Given a source user accessing the app from their browser\n        with get_web_driver(\n            web_driver_type=WebDriverTypeEnum.TOR_BROWSER,\n            accept_languages=locale,\n        ) as tor_browser_web_driver:\n            # When they try to access a page that does not exist\n            tor_browser_web_driver.get(f\"{sd_servers.source_app_base_url}/does_not_exist\")\n\n            # Then the right error is displayed\n            message = tor_browser_web_driver.find_element(By.ID, \"page-not-found\")\n            assert message.is_displayed()\n\n            save_static_data(tor_browser_web_driver, locale, \"source-notfound\")",
              "file": "test_source_static_pages.py"
            },
            {
              "type": "function",
              "name": "test_static_pages",
              "code": "def test_static_pages(self, locale, sd_servers):\n        # Given a source user accessing the app from their browser\n        with get_web_driver(\n            web_driver_type=WebDriverTypeEnum.TOR_BROWSER,\n            accept_languages=locale,\n        ) as tor_browser_web_driver:\n            # The user can browse to some of the app's static pages\n            tor_browser_web_driver.get(f\"{sd_servers.source_app_base_url}/use-tor\")\n            save_static_data(tor_browser_web_driver, locale, \"source-use_tor_browser\")\n\n            tor_browser_web_driver.get(f\"{sd_servers.source_app_base_url}/tor2web-warning\")\n            save_static_data(tor_browser_web_driver, locale, \"source-tor2web_warning\")\n\n            tor_browser_web_driver.get(f\"{sd_servers.source_app_base_url}/why-public-key\")\n            save_static_data(tor_browser_web_driver, locale, \"source-why_journalist_key\")",
              "file": "test_source_static_pages.py"
            },
            {
              "type": "function",
              "name": "test_instance_metadata",
              "code": "def test_instance_metadata(self, sd_servers):\n        # Given a source app, when fetching the instance's metadata\n        url = f\"{sd_servers.source_app_base_url}/metadata\"\n        response = requests.get(url=url, proxies=tor_utils.proxies_for_url(url))\n\n        # Then it succeeds and the right information is returned\n        returned_data = response.json()\n        assert returned_data[\"server_os\"] in [\"20.04\", \"24.04\"]\n        assert returned_data[\"sd_version\"] == __version__\n        assert returned_data[\"gpg_fpr\"]",
              "file": "test_source_static_pages.py"
            }
          ]
        },
        "app_navigators": {
          "source_app_nav.py": [
            {
              "type": "class",
              "name": "SourceAppNavigator",
              "code": "class SourceAppNavigator:\n    \"\"\"Helper functions to navigate the source app when implementing functional/selenium tests.\n\n    Only logic that needs to be shared across multiple tests within different files should be\n    added to this class, in order to keep this class as small as possible.\n    \"\"\"\n\n    def __init__(\n        self,\n        source_app_base_url: str,\n        web_driver: WebDriver,\n        accept_languages: Optional[str] = None,\n    ) -> None:\n        self._source_app_base_url = source_app_base_url\n        self.nav_helper = NavigationHelper(web_driver)\n        self.driver = web_driver\n\n        # Some string-based tests check this to avoid failing on translated strings.\n        self.accept_languages = accept_languages\n\n    @classmethod\n    @contextmanager\n    def using_tor_browser_web_driver(\n        cls,\n        source_app_base_url: str,\n        accept_languages: Optional[str] = None,\n    ) -> Generator[\"SourceAppNavigator\", None, None]:\n        \"\"\"Convenience method for auto-creating the web driver to be used by the navigator.\"\"\"\n        with get_web_driver(\n            web_driver_type=WebDriverTypeEnum.TOR_BROWSER,\n            accept_languages=accept_languages,\n        ) as tor_browser_web_driver:\n            yield cls(\n                source_app_base_url=source_app_base_url,\n                web_driver=tor_browser_web_driver,\n                accept_languages=accept_languages,\n            )\n\n    def _is_on_source_homepage(self) -> WebElement:\n        return self.nav_helper.wait_for(lambda: self.driver.find_element(By.ID, \"source-index\"))\n\n    def source_visits_source_homepage(self) -> None:\n        self.driver.get(self._source_app_base_url)\n        assert self._is_on_source_homepage()\n\n    def _is_on_generate_page(self) -> WebElement:\n        return self.nav_helper.wait_for(lambda: self.driver.find_element(By.ID, \"source-generate\"))\n\n    def source_clicks_submit_documents_on_homepage(self) -> None:\n        # It's the source's first time visiting this SecureDrop site, so they\n        # choose to \"Submit Documents\".\n        self.nav_helper.safe_click_by_css_selector(\"#started-form button\")\n\n        # The source should now be on the page where they are presented with\n        # a diceware codename they can use for subsequent logins\n        assert self._is_on_generate_page()\n\n    def source_continues_to_submit_page(self) -> None:\n        self.nav_helper.safe_click_by_css_selector(\"#create-form button\")\n\n        def submit_page_loaded() -> None:\n            if not self.accept_languages:\n                headline = self.driver.find_element(By.ID, \"submit-heading\")\n                # Message will either be \"Submit Messages\" or \"Submit Files or Messages\" depending\n                #  on whether file uploads are allowed by the instance's config\n                assert \"Submit\" in headline.text\n                assert \"Messages\" in headline.text\n\n        self.nav_helper.wait_for(submit_page_loaded)\n\n    def _is_on_logout_page(self) -> WebElement:\n        return self.nav_helper.wait_for(lambda: self.driver.find_element(By.ID, \"source-logout\"))\n\n    def source_logs_out(self) -> None:\n        self.nav_helper.safe_click_by_id(\"logout\")\n        assert self._is_on_logout_page()\n\n    def source_retrieves_codename_from_hint(self) -> str:\n        # We use inputs to change CSS states for subsequent elements in the DOM, if it is unchecked\n        # the codename is hidden\n        content = self.driver.find_element(By.ID, \"codename-show-checkbox\")\n        assert content.get_attribute(\"checked\") is None\n\n        self.nav_helper.safe_click_by_id(\"codename-show\")\n\n        assert content.get_attribute(\"checked\") is not None\n        content_content = self.driver.find_element(By.CSS_SELECTOR, \"#codename span\")\n        return content_content.text\n\n    def source_chooses_to_login(self) -> None:\n        self.nav_helper.safe_click_by_css_selector(\"#return-visit a\")\n        self.nav_helper.wait_for(lambda: self.driver.find_elements(By.ID, \"source-login\"))\n\n    def _is_logged_in(self) -> WebElement:\n        return self.nav_helper.wait_for(lambda: self.driver.find_element(By.ID, \"btn-logout\"))\n\n    def source_proceeds_to_login(self, codename: str) -> None:\n        self.nav_helper.safe_send_keys_by_id(\"codename\", codename)\n        self.nav_helper.safe_click_by_css_selector(\".form-controls button\")\n\n        # Check that we've logged in\n        assert self._is_logged_in()\n\n        replies = self.driver.find_elements(By.ID, \"replies\")\n        assert len(replies) == 1\n\n    def source_submits_a_message(self, message: str = \"S3cr3t m3ss4ge\") -> str:\n        # Write the message to submit\n        self.nav_helper.safe_send_keys_by_id(\"msg\", message)\n\n        # Hit the submit button\n        self.nav_helper.safe_click_by_css_selector(\".form-controls button\")\n\n        # Wait for confirmation that the message was submitted\n        def message_submitted():\n            if not self.accept_languages:\n                notification = self.driver.find_element(By.CSS_SELECTOR, \".success\")\n                assert \"Thank\" in notification.text\n                return notification.text\n\n        # Return the confirmation notification\n        return self.nav_helper.wait_for(message_submitted)\n\n    def source_submits_a_file(self, file_content: str = \"S3cr3t f1l3\") -> None:\n        # Write the content to a file\n        with tempfile.NamedTemporaryFile() as file:\n            file.write(file_content.encode(\"utf-8\"))\n            file.seek(0)\n            filename = file.name\n\n            # Submit the file\n            self.nav_helper.safe_send_keys_by_id(\"fh\", filename)\n            self.nav_helper.safe_click_by_css_selector(\".form-controls button\")\n\n            def file_submitted() -> None:\n                if not self.accept_languages:\n                    notification = self.driver.find_element(By.CSS_SELECTOR, \".success\")\n                    expected_notification = \"Thank you for sending this information to us\"\n                    assert expected_notification in notification.text\n\n            # Allow extra time for file uploads\n            self.nav_helper.wait_for(file_submitted, timeout=15)\n\n            # allow time for reply key to be generated\n            time.sleep(3)",
              "file": "source_app_nav.py"
            },
            {
              "type": "function",
              "name": "__init__",
              "code": "def __init__(\n        self,\n        source_app_base_url: str,\n        web_driver: WebDriver,\n        accept_languages: Optional[str] = None,\n    ) -> None:\n        self._source_app_base_url = source_app_base_url\n        self.nav_helper = NavigationHelper(web_driver)\n        self.driver = web_driver\n\n        # Some string-based tests check this to avoid failing on translated strings.\n        self.accept_languages = accept_languages",
              "file": "source_app_nav.py"
            },
            {
              "type": "function",
              "name": "using_tor_browser_web_driver",
              "code": "def using_tor_browser_web_driver(\n        cls,\n        source_app_base_url: str,\n        accept_languages: Optional[str] = None,\n    ) -> Generator[\"SourceAppNavigator\", None, None]:\n        \"\"\"Convenience method for auto-creating the web driver to be used by the navigator.\"\"\"\n        with get_web_driver(\n            web_driver_type=WebDriverTypeEnum.TOR_BROWSER,\n            accept_languages=accept_languages,\n        ) as tor_browser_web_driver:\n            yield cls(\n                source_app_base_url=source_app_base_url,\n                web_driver=tor_browser_web_driver,\n                accept_languages=accept_languages,\n            )",
              "file": "source_app_nav.py"
            },
            {
              "type": "function",
              "name": "_is_on_source_homepage",
              "code": "def _is_on_source_homepage(self) -> WebElement:\n        return self.nav_helper.wait_for(lambda: self.driver.find_element(By.ID, \"source-index\"))",
              "file": "source_app_nav.py"
            },
            {
              "type": "function",
              "name": "source_visits_source_homepage",
              "code": "def source_visits_source_homepage(self) -> None:\n        self.driver.get(self._source_app_base_url)\n        assert self._is_on_source_homepage()",
              "file": "source_app_nav.py"
            },
            {
              "type": "function",
              "name": "_is_on_generate_page",
              "code": "def _is_on_generate_page(self) -> WebElement:\n        return self.nav_helper.wait_for(lambda: self.driver.find_element(By.ID, \"source-generate\"))",
              "file": "source_app_nav.py"
            },
            {
              "type": "function",
              "name": "source_clicks_submit_documents_on_homepage",
              "code": "def source_clicks_submit_documents_on_homepage(self) -> None:\n        # It's the source's first time visiting this SecureDrop site, so they\n        # choose to \"Submit Documents\".\n        self.nav_helper.safe_click_by_css_selector(\"#started-form button\")\n\n        # The source should now be on the page where they are presented with\n        # a diceware codename they can use for subsequent logins\n        assert self._is_on_generate_page()",
              "file": "source_app_nav.py"
            },
            {
              "type": "function",
              "name": "source_continues_to_submit_page",
              "code": "def source_continues_to_submit_page(self) -> None:\n        self.nav_helper.safe_click_by_css_selector(\"#create-form button\")\n\n        def submit_page_loaded() -> None:\n            if not self.accept_languages:\n                headline = self.driver.find_element(By.ID, \"submit-heading\")\n                # Message will either be \"Submit Messages\" or \"Submit Files or Messages\" depending\n                #  on whether file uploads are allowed by the instance's config\n                assert \"Submit\" in headline.text\n                assert \"Messages\" in headline.text\n\n        self.nav_helper.wait_for(submit_page_loaded)",
              "file": "source_app_nav.py"
            },
            {
              "type": "function",
              "name": "submit_page_loaded",
              "code": "def submit_page_loaded() -> None:\n            if not self.accept_languages:\n                headline = self.driver.find_element(By.ID, \"submit-heading\")\n                # Message will either be \"Submit Messages\" or \"Submit Files or Messages\" depending\n                #  on whether file uploads are allowed by the instance's config\n                assert \"Submit\" in headline.text\n                assert \"Messages\" in headline.text",
              "file": "source_app_nav.py"
            },
            {
              "type": "function",
              "name": "_is_on_logout_page",
              "code": "def _is_on_logout_page(self) -> WebElement:\n        return self.nav_helper.wait_for(lambda: self.driver.find_element(By.ID, \"source-logout\"))",
              "file": "source_app_nav.py"
            },
            {
              "type": "function",
              "name": "source_logs_out",
              "code": "def source_logs_out(self) -> None:\n        self.nav_helper.safe_click_by_id(\"logout\")\n        assert self._is_on_logout_page()",
              "file": "source_app_nav.py"
            },
            {
              "type": "function",
              "name": "source_retrieves_codename_from_hint",
              "code": "def source_retrieves_codename_from_hint(self) -> str:\n        # We use inputs to change CSS states for subsequent elements in the DOM, if it is unchecked\n        # the codename is hidden\n        content = self.driver.find_element(By.ID, \"codename-show-checkbox\")\n        assert content.get_attribute(\"checked\") is None\n\n        self.nav_helper.safe_click_by_id(\"codename-show\")\n\n        assert content.get_attribute(\"checked\") is not None\n        content_content = self.driver.find_element(By.CSS_SELECTOR, \"#codename span\")\n        return content_content.text",
              "file": "source_app_nav.py"
            },
            {
              "type": "function",
              "name": "source_chooses_to_login",
              "code": "def source_chooses_to_login(self) -> None:\n        self.nav_helper.safe_click_by_css_selector(\"#return-visit a\")\n        self.nav_helper.wait_for(lambda: self.driver.find_elements(By.ID, \"source-login\"))",
              "file": "source_app_nav.py"
            },
            {
              "type": "function",
              "name": "_is_logged_in",
              "code": "def _is_logged_in(self) -> WebElement:\n        return self.nav_helper.wait_for(lambda: self.driver.find_element(By.ID, \"btn-logout\"))",
              "file": "source_app_nav.py"
            },
            {
              "type": "function",
              "name": "source_proceeds_to_login",
              "code": "def source_proceeds_to_login(self, codename: str) -> None:\n        self.nav_helper.safe_send_keys_by_id(\"codename\", codename)\n        self.nav_helper.safe_click_by_css_selector(\".form-controls button\")\n\n        # Check that we've logged in\n        assert self._is_logged_in()\n\n        replies = self.driver.find_elements(By.ID, \"replies\")\n        assert len(replies) == 1",
              "file": "source_app_nav.py"
            },
            {
              "type": "function",
              "name": "source_submits_a_message",
              "code": "def source_submits_a_message(self, message: str = \"S3cr3t m3ss4ge\") -> str:\n        # Write the message to submit\n        self.nav_helper.safe_send_keys_by_id(\"msg\", message)\n\n        # Hit the submit button\n        self.nav_helper.safe_click_by_css_selector(\".form-controls button\")\n\n        # Wait for confirmation that the message was submitted\n        def message_submitted():\n            if not self.accept_languages:\n                notification = self.driver.find_element(By.CSS_SELECTOR, \".success\")\n                assert \"Thank\" in notification.text\n                return notification.text\n\n        # Return the confirmation notification\n        return self.nav_helper.wait_for(message_submitted)",
              "file": "source_app_nav.py"
            },
            {
              "type": "function",
              "name": "message_submitted",
              "code": "def message_submitted():\n            if not self.accept_languages:\n                notification = self.driver.find_element(By.CSS_SELECTOR, \".success\")\n                assert \"Thank\" in notification.text\n                return notification.text",
              "file": "source_app_nav.py"
            },
            {
              "type": "function",
              "name": "source_submits_a_file",
              "code": "def source_submits_a_file(self, file_content: str = \"S3cr3t f1l3\") -> None:\n        # Write the content to a file\n        with tempfile.NamedTemporaryFile() as file:\n            file.write(file_content.encode(\"utf-8\"))\n            file.seek(0)\n            filename = file.name\n\n            # Submit the file\n            self.nav_helper.safe_send_keys_by_id(\"fh\", filename)\n            self.nav_helper.safe_click_by_css_selector(\".form-controls button\")\n\n            def file_submitted() -> None:\n                if not self.accept_languages:\n                    notification = self.driver.find_element(By.CSS_SELECTOR, \".success\")\n                    expected_notification = \"Thank you for sending this information to us\"\n                    assert expected_notification in notification.text\n\n            # Allow extra time for file uploads\n            self.nav_helper.wait_for(file_submitted, timeout=15)\n\n            # allow time for reply key to be generated\n            time.sleep(3)",
              "file": "source_app_nav.py"
            },
            {
              "type": "function",
              "name": "file_submitted",
              "code": "def file_submitted() -> None:\n                if not self.accept_languages:\n                    notification = self.driver.find_element(By.CSS_SELECTOR, \".success\")\n                    expected_notification = \"Thank you for sending this information to us\"\n                    assert expected_notification in notification.text",
              "file": "source_app_nav.py"
            }
          ],
          "_nav_helper.py": [
            {
              "type": "class",
              "name": "NavigationHelper",
              "code": "class NavigationHelper:\n    _TIMEOUT = 10\n    _POLL_FREQUENCY = 0.1\n\n    def __init__(self, web_driver: WebDriver) -> None:\n        self.driver = web_driver\n\n    def wait_for(self, function_with_assertion, timeout=_TIMEOUT):\n        \"\"\"Polling wait for an arbitrary assertion.\"\"\"\n        # Thanks to\n        # http://chimera.labs.oreilly.com/books/1234000000754/ch20.html#_a_common_selenium_problem_race_conditions\n        start_time = time.time()\n        while time.time() - start_time < timeout:\n            try:\n                return function_with_assertion()\n            except (AssertionError, WebDriverException):\n                time.sleep(self._POLL_FREQUENCY)\n        # one more try, which will raise any errors if they are outstanding\n        return function_with_assertion()\n\n    def safe_click_by_id(self, element_id: str) -> WebElement:\n        \"\"\"\n        Clicks the element with the given ID attribute.\n\n        Returns:\n            el: The element, if found.\n\n        Raises:\n            selenium.common.exceptions.TimeoutException: If the element cannot be found in time.\n\n        \"\"\"\n        el = WebDriverWait(self.driver, self._TIMEOUT, self._POLL_FREQUENCY).until(\n            expected_conditions.element_to_be_clickable((By.ID, element_id))\n        )\n        assert el.location_once_scrolled_into_view\n        el.click()\n        return el\n\n    def safe_click_by_css_selector(self, selector: str) -> WebElement:\n        \"\"\"\n        Clicks the first element with the given CSS selector.\n\n        Returns:\n            el: The element, if found.\n\n        Raises:\n            selenium.common.exceptions.TimeoutException: If the element cannot be found in time.\n\n        \"\"\"\n        el = WebDriverWait(self.driver, self._TIMEOUT, self._POLL_FREQUENCY).until(\n            expected_conditions.element_to_be_clickable((By.CSS_SELECTOR, selector))\n        )\n        el.click()\n        return el\n\n    def safe_click_all_by_css_selector(self, selector: str) -> List[WebElement]:\n        \"\"\"\n        Clicks each element that matches the given CSS selector.\n\n        Returns:\n            els (list): The list of elements that matched the selector.\n\n        Raises:\n            selenium.common.exceptions.TimeoutException: If the element cannot be found in time.\n\n        \"\"\"\n        els = self.wait_for(lambda: self.driver.find_elements(By.CSS_SELECTOR, selector))\n        for _el in els:\n            clickable_el = WebDriverWait(self.driver, self._TIMEOUT, self._POLL_FREQUENCY).until(\n                expected_conditions.element_to_be_clickable((By.CSS_SELECTOR, selector))\n            )\n            clickable_el.click()\n        return els\n\n    def safe_send_keys_by_id(self, element_id: str, text: str) -> WebElement:\n        \"\"\"\n        Sends the given text to the element with the specified ID.\n\n        Returns:\n            el: The element, if found.\n\n        Raises:\n            selenium.common.exceptions.TimeoutException: If the element cannot be found in time.\n\n        \"\"\"\n        el = WebDriverWait(self.driver, self._TIMEOUT, self._POLL_FREQUENCY).until(\n            expected_conditions.element_to_be_clickable((By.ID, element_id))\n        )\n        el.send_keys(text)\n        return el\n\n    def safe_send_keys_by_css_selector(self, selector: str, text: str) -> WebElement:\n        \"\"\"\n        Sends the given text to the first element with the given CSS selector.\n\n        Returns:\n            el: The element, if found.\n\n        Raises:\n            selenium.common.exceptions.TimeoutException: If the element cannot be found in time.\n\n        \"\"\"\n        el = WebDriverWait(self.driver, self._TIMEOUT, self._POLL_FREQUENCY).until(\n            expected_conditions.element_to_be_clickable((By.CSS_SELECTOR, selector))\n        )\n        el.send_keys(text)\n        return el\n\n    def alert_wait(self, timeout: int = _TIMEOUT * 10) -> None:\n        WebDriverWait(self.driver, timeout, self._POLL_FREQUENCY).until(\n            expected_conditions.alert_is_present(), \"Timed out waiting for confirmation popup.\"\n        )\n\n    def alert_accept(self) -> None:\n        # adapted from https://stackoverflow.com/a/34795883/837471\n        def alert_is_not_present(object):\n            \"\"\"Expect an alert to not be present.\"\"\"\n            try:\n                alert = self.driver.switch_to.alert\n                alert.text  # noqa: B018\n                return False\n            except NoAlertPresentException:\n                return True\n\n        self.driver.switch_to.alert.accept()\n        WebDriverWait(self.driver, self._TIMEOUT, self._POLL_FREQUENCY).until(\n            alert_is_not_present, \"Timed out waiting for confirmation popup to disappear.\"\n        )",
              "file": "_nav_helper.py"
            },
            {
              "type": "function",
              "name": "__init__",
              "code": "def __init__(self, web_driver: WebDriver) -> None:\n        self.driver = web_driver",
              "file": "_nav_helper.py"
            },
            {
              "type": "function",
              "name": "wait_for",
              "code": "def wait_for(self, function_with_assertion, timeout=_TIMEOUT):\n        \"\"\"Polling wait for an arbitrary assertion.\"\"\"\n        # Thanks to\n        # http://chimera.labs.oreilly.com/books/1234000000754/ch20.html#_a_common_selenium_problem_race_conditions\n        start_time = time.time()\n        while time.time() - start_time < timeout:\n            try:\n                return function_with_assertion()\n            except (AssertionError, WebDriverException):\n                time.sleep(self._POLL_FREQUENCY)\n        # one more try, which will raise any errors if they are outstanding\n        return function_with_assertion()",
              "file": "_nav_helper.py"
            },
            {
              "type": "function",
              "name": "safe_click_by_id",
              "code": "def safe_click_by_id(self, element_id: str) -> WebElement:\n        \"\"\"\n        Clicks the element with the given ID attribute.\n\n        Returns:\n            el: The element, if found.\n\n        Raises:\n            selenium.common.exceptions.TimeoutException: If the element cannot be found in time.\n\n        \"\"\"\n        el = WebDriverWait(self.driver, self._TIMEOUT, self._POLL_FREQUENCY).until(\n            expected_conditions.element_to_be_clickable((By.ID, element_id))\n        )\n        assert el.location_once_scrolled_into_view\n        el.click()\n        return el",
              "file": "_nav_helper.py"
            },
            {
              "type": "function",
              "name": "safe_click_by_css_selector",
              "code": "def safe_click_by_css_selector(self, selector: str) -> WebElement:\n        \"\"\"\n        Clicks the first element with the given CSS selector.\n\n        Returns:\n            el: The element, if found.\n\n        Raises:\n            selenium.common.exceptions.TimeoutException: If the element cannot be found in time.\n\n        \"\"\"\n        el = WebDriverWait(self.driver, self._TIMEOUT, self._POLL_FREQUENCY).until(\n            expected_conditions.element_to_be_clickable((By.CSS_SELECTOR, selector))\n        )\n        el.click()\n        return el",
              "file": "_nav_helper.py"
            },
            {
              "type": "function",
              "name": "safe_click_all_by_css_selector",
              "code": "def safe_click_all_by_css_selector(self, selector: str) -> List[WebElement]:\n        \"\"\"\n        Clicks each element that matches the given CSS selector.\n\n        Returns:\n            els (list): The list of elements that matched the selector.\n\n        Raises:\n            selenium.common.exceptions.TimeoutException: If the element cannot be found in time.\n\n        \"\"\"\n        els = self.wait_for(lambda: self.driver.find_elements(By.CSS_SELECTOR, selector))\n        for _el in els:\n            clickable_el = WebDriverWait(self.driver, self._TIMEOUT, self._POLL_FREQUENCY).until(\n                expected_conditions.element_to_be_clickable((By.CSS_SELECTOR, selector))\n            )\n            clickable_el.click()\n        return els",
              "file": "_nav_helper.py"
            },
            {
              "type": "function",
              "name": "safe_send_keys_by_id",
              "code": "def safe_send_keys_by_id(self, element_id: str, text: str) -> WebElement:\n        \"\"\"\n        Sends the given text to the element with the specified ID.\n\n        Returns:\n            el: The element, if found.\n\n        Raises:\n            selenium.common.exceptions.TimeoutException: If the element cannot be found in time.\n\n        \"\"\"\n        el = WebDriverWait(self.driver, self._TIMEOUT, self._POLL_FREQUENCY).until(\n            expected_conditions.element_to_be_clickable((By.ID, element_id))\n        )\n        el.send_keys(text)\n        return el",
              "file": "_nav_helper.py"
            },
            {
              "type": "function",
              "name": "safe_send_keys_by_css_selector",
              "code": "def safe_send_keys_by_css_selector(self, selector: str, text: str) -> WebElement:\n        \"\"\"\n        Sends the given text to the first element with the given CSS selector.\n\n        Returns:\n            el: The element, if found.\n\n        Raises:\n            selenium.common.exceptions.TimeoutException: If the element cannot be found in time.\n\n        \"\"\"\n        el = WebDriverWait(self.driver, self._TIMEOUT, self._POLL_FREQUENCY).until(\n            expected_conditions.element_to_be_clickable((By.CSS_SELECTOR, selector))\n        )\n        el.send_keys(text)\n        return el",
              "file": "_nav_helper.py"
            },
            {
              "type": "function",
              "name": "alert_wait",
              "code": "def alert_wait(self, timeout: int = _TIMEOUT * 10) -> None:\n        WebDriverWait(self.driver, timeout, self._POLL_FREQUENCY).until(\n            expected_conditions.alert_is_present(), \"Timed out waiting for confirmation popup.\"\n        )",
              "file": "_nav_helper.py"
            },
            {
              "type": "function",
              "name": "alert_accept",
              "code": "def alert_accept(self) -> None:\n        # adapted from https://stackoverflow.com/a/34795883/837471\n        def alert_is_not_present(object):\n            \"\"\"Expect an alert to not be present.\"\"\"\n            try:\n                alert = self.driver.switch_to.alert\n                alert.text  # noqa: B018\n                return False\n            except NoAlertPresentException:\n                return True\n\n        self.driver.switch_to.alert.accept()\n        WebDriverWait(self.driver, self._TIMEOUT, self._POLL_FREQUENCY).until(\n            alert_is_not_present, \"Timed out waiting for confirmation popup to disappear.\"\n        )",
              "file": "_nav_helper.py"
            },
            {
              "type": "function",
              "name": "alert_is_not_present",
              "code": "def alert_is_not_present(object):\n            \"\"\"Expect an alert to not be present.\"\"\"\n            try:\n                alert = self.driver.switch_to.alert\n                alert.text  # noqa: B018\n                return False\n            except NoAlertPresentException:\n                return True",
              "file": "_nav_helper.py"
            }
          ],
          "journalist_app_nav.py": [
            {
              "type": "class",
              "name": "JournalistAppNavigator",
              "code": "class JournalistAppNavigator:\n    \"\"\"Helper functions to navigate the journalist app when implementing functional/selenium tests.\n\n    Only logic that needs to be shared across multiple tests within different files should be\n    added to this class, in order to keep this class as small as possible.\n    \"\"\"\n\n    def __init__(\n        self,\n        journalist_app_base_url: str,\n        web_driver: WebDriver,\n        accept_languages: Optional[str] = None,\n    ) -> None:\n        self._journalist_app_base_url = journalist_app_base_url\n        self.nav_helper = NavigationHelper(web_driver)\n        self.driver = web_driver\n\n        # Some string-based tests check this to avoid failing on translated strings.\n        self.accept_languages = accept_languages\n\n    def got_expected_language(self, locale: str) -> None:\n        expected = locale.replace(\"_\", \"-\")\n\n        html = self.nav_helper.wait_for(lambda: self.driver.find_element(By.TAG_NAME, \"html\"))\n        actual = html.get_attribute(\"lang\")\n\n        assert actual == expected\n\n    def is_on_journalist_homepage(self) -> WebElement:\n        return self.nav_helper.wait_for(\n            lambda: self.driver.find_element(By.CSS_SELECTOR, \"div.journalist-view-all\")\n        )\n\n    def journalist_goes_to_login_page_and_enters_credentials(\n        self,\n        username: str,\n        password: str,\n        otp_secret: str,\n        should_submit_login_form: bool,\n    ) -> None:\n        self.driver.get(f\"{self._journalist_app_base_url}/login\")\n\n        self.nav_helper.safe_send_keys_by_css_selector('input[name=\"username\"]', username)\n        self.nav_helper.safe_send_keys_by_css_selector('input[name=\"password\"]', password)\n        otp = two_factor.TOTP(otp_secret)\n        self.nav_helper.safe_send_keys_by_css_selector('input[name=\"token\"]', otp.now())\n\n        if should_submit_login_form:\n            self.nav_helper.safe_click_by_css_selector('form#login button[type=\"submit\"]')\n\n    def journalist_logs_in(\n        self,\n        username: str,\n        password: str,\n        otp_secret: str,\n    ) -> None:\n        self.journalist_goes_to_login_page_and_enters_credentials(\n            username=username,\n            password=password,\n            otp_secret=otp_secret,\n            should_submit_login_form=True,\n        )\n\n        # Successful login should redirect to the index\n        self.nav_helper.wait_for(lambda: self.driver.find_element(By.ID, \"btn-logout\"))\n        assert self.is_on_journalist_homepage()\n\n    def journalist_checks_messages(self) -> None:\n        self.driver.get(self._journalist_app_base_url)\n\n        # There should be 1 collection in the list of collections\n        collections_count = self.count_sources_on_index_page()\n        assert collections_count == 1\n\n        if not self.accept_languages:\n            # There should be a \"1 unread\" span in the sole collection entry\n            unread_span = self.driver.find_element(By.CSS_SELECTOR, \"tr.unread\")\n            assert \"1 unread\" in unread_span.text\n\n    @staticmethod\n    def _download_content_at_url(url: str, cookies: Dict[str, str]) -> bytes:\n        r = requests.get(url, cookies=cookies, proxies=proxies_for_url(url), stream=True)\n        if r.status_code != 200:\n            raise Exception(\"Failed to download the data.\")\n        data = b\"\"\n        for chunk in r.iter_content(1024):\n            data += chunk\n        return data\n\n    def journalist_downloads_first_message(self) -> str:\n        # Select the first submission from the first source in the page\n        self.journalist_selects_the_first_source()\n        self.nav_helper.wait_for(\n            lambda: self.driver.find_element(By.CSS_SELECTOR, \"table#submissions\")\n        )\n        submissions = self.driver.find_elements(By.CSS_SELECTOR, \"#submissions a\")\n        assert len(submissions) == 1\n        file_url = submissions[0].get_attribute(\"href\")\n\n        # Downloading files with Selenium is tricky because it cannot automate\n        # the browser's file download dialog. We can directly request the file\n        # using requests, but we need to pass the cookies for logged in user\n        # for Flask to allow this.\n        def cookie_string_from_selenium_cookies(\n            cookies: Iterable[Dict[str, str]],\n        ) -> Dict[str, str]:\n            result = {}\n            for cookie in cookies:\n                result[cookie[\"name\"]] = cookie[\"value\"]\n            return result\n\n        cks = cookie_string_from_selenium_cookies(self.driver.get_cookies())\n        assert isinstance(file_url, str)\n        raw_content = self._download_content_at_url(file_url, cks)\n\n        decryption_result = utils.decrypt_as_journalist(raw_content)\n        if file_url.endswith(\".gz.gpg\"):\n            decrypted_message = gzip.decompress(decryption_result)\n        else:\n            decrypted_message = decryption_result\n\n        return decrypted_message.decode()\n\n    def journalist_selects_the_first_source(self) -> None:\n        self.driver.find_element(By.CSS_SELECTOR, \"#un-starred-source-link-1\").click()\n\n    def journalist_composes_reply_to_source(self, reply_content: str) -> None:\n        self.nav_helper.wait_for(lambda: self.driver.find_element(By.ID, \"reply-text-field\"))\n        self.nav_helper.safe_send_keys_by_id(\"reply-text-field\", reply_content)\n\n    def journalist_sends_reply_to_source(\n        self, reply_content: str = \"Thanks for the documents. Can you submit more? éè\"\n    ) -> None:\n        self.journalist_composes_reply_to_source(reply_content=reply_content)\n        self.driver.find_element(By.ID, \"reply-button\").click()\n\n        def reply_stored() -> None:\n            if not self.accept_languages:\n                assert \"The source will receive your reply\" in self.driver.page_source\n\n        self.nav_helper.wait_for(reply_stored)\n\n    def journalist_visits_col(self) -> None:\n        self.nav_helper.wait_for(\n            lambda: self.driver.find_element(By.CSS_SELECTOR, \"table#collections\")\n        )\n        self.nav_helper.safe_click_by_id(\"un-starred-source-link-1\")\n        self.nav_helper.wait_for(\n            lambda: self.driver.find_element(By.CSS_SELECTOR, \"table#submissions\")\n        )\n\n    def journalist_selects_first_doc(self) -> None:\n        self.nav_helper.safe_click_by_css_selector(\n            'input[type=\"checkbox\"][name=\"doc_names_selected\"]'\n        )\n\n        self.nav_helper.wait_for(\n            lambda: expected_conditions.element_located_to_be_selected(\n                (By.CSS_SELECTOR, 'input[type=\"checkbox\"][name=\"doc_names_selected\"]')\n            )\n        )\n\n        assert self.driver.find_element(\n            By.CSS_SELECTOR, 'input[type=\"checkbox\"][name=\"doc_names_selected\"]'\n        ).is_selected()\n\n    def journalist_clicks_delete_selected_link(self) -> None:\n        self.nav_helper.safe_click_by_css_selector(\"a#delete-selected-link\")\n        self.nav_helper.wait_for(\n            lambda: self.driver.find_element(By.ID, \"delete-selected-confirmation-modal\")\n        )\n\n    def journalist_clicks_delete_all_and_sees_confirmation(self) -> None:\n        self.nav_helper.safe_click_all_by_css_selector(\"[name=doc_names_selected]\")\n        self.nav_helper.safe_click_by_css_selector(\"a#delete-selected-link\")\n\n    def journalist_confirms_delete_selected(self) -> None:\n        self.nav_helper.wait_for(\n            lambda: expected_conditions.element_to_be_clickable((By.ID, \"delete-selected\"))\n        )\n        confirm_btn = self.driver.find_element(By.ID, \"delete-selected\")\n        assert confirm_btn.location_once_scrolled_into_view\n        ActionChains(self.driver).move_to_element(confirm_btn).click().perform()\n\n    def get_submission_checkboxes_on_current_page(self):\n        return self.driver.find_elements(By.NAME, \"doc_names_selected\")\n\n    def count_submissions_on_current_page(self) -> int:\n        return len(self.get_submission_checkboxes_on_current_page())\n\n    def get_sources_on_index_page(self):\n        assert self.is_on_journalist_homepage()\n        return self.driver.find_elements(By.CLASS_NAME, \"code-name\")\n\n    def count_sources_on_index_page(self) -> int:\n        return len(self.get_sources_on_index_page())\n\n    def journalist_confirm_delete_selected(self) -> None:\n        self.nav_helper.wait_for(\n            lambda: expected_conditions.element_to_be_clickable((By.ID, \"delete-selected\"))\n        )\n        confirm_btn = self.driver.find_element(By.ID, \"delete-selected\")\n        assert confirm_btn.location_once_scrolled_into_view\n        ActionChains(self.driver).move_to_element(confirm_btn).click().perform()\n\n    def journalist_sees_link_to_admin_page(self) -> bool:\n        try:\n            self.driver.find_element(By.ID, \"link-admin-index\")\n            return True\n        except NoSuchElementException:\n            return False\n\n    def admin_visits_admin_interface(self) -> None:\n        self.nav_helper.safe_click_by_id(\"link-admin-index\")\n        self.nav_helper.wait_for(lambda: self.driver.find_element(By.ID, \"add-user\"))\n\n    def admin_creates_a_user(\n        self,\n        username: Optional[str] = None,\n        hotp_secret: Optional[str] = None,\n        is_admin: bool = False,\n        callback_before_submitting_add_user_step: Optional[Callable[[], None]] = None,\n        callback_before_submitting_2fa_step: Optional[Callable[[], None]] = None,\n    ) -> Optional[Tuple[str, str, str]]:\n        self.nav_helper.safe_click_by_id(\"add-user\")\n        self.nav_helper.wait_for(lambda: self.driver.find_element(By.ID, \"username\"))\n\n        if not self.accept_languages:\n            # The add user page has a form with an \"ADD USER\" button\n            btns = self.driver.find_elements(By.TAG_NAME, \"button\")\n            assert \"ADD USER\" in [el.text for el in btns]\n\n        password = self.driver.find_element(By.CSS_SELECTOR, \"#password\").text.strip()\n        if not username:\n            final_username = f\"journalist{str(randint(1000, 1000000))}\"\n        else:\n            final_username = username\n\n        # Fill the form\n        self.nav_helper.safe_send_keys_by_css_selector('input[name=\"username\"]', final_username)\n        if hotp_secret:\n            # Create an HOTP user instead of TOTP\n            self.nav_helper.safe_click_all_by_css_selector('input[name=\"is_hotp\"]')\n            self.nav_helper.safe_send_keys_by_css_selector('input[name=\"otp_secret\"]', hotp_secret)\n\n        if is_admin:\n            self.nav_helper.safe_click_by_css_selector('input[name=\"is_admin\"]')\n\n        if callback_before_submitting_add_user_step:\n            callback_before_submitting_add_user_step()\n\n        # Submit the form\n        self.nav_helper.safe_click_by_css_selector(\"form#admin-add-user button[type=submit]\")\n\n        # Submitting the add user form should redirect to the 2FA page\n        self.nav_helper.wait_for(lambda: self.driver.find_element(By.ID, \"check-token\"))\n        if self.accept_languages in [None, \"en-US\"]:\n            expected_title = \"Enable YubiKey (OATH-HOTP)\" if hotp_secret else \"Enable FreeOTP\"\n            h1s = [h1.text for h1 in self.driver.find_elements(By.TAG_NAME, \"h1\")]\n            assert expected_title in h1s\n\n        if hotp_secret:\n            # We created an hotp user\n            otp_secret = hotp_secret\n            hotp_secret_as_hex = unhexlify(hotp_secret.replace(\" \", \"\"))\n            hotp_secret_as_b32 = base64.b32encode(hotp_secret_as_hex).decode(\"ascii\")\n            hotp = two_factor.HOTP(hotp_secret_as_b32)\n            current_2fa_code = hotp.generate(0)\n        else:\n            # We created a totp user\n            otp_secret = (\n                self.driver.find_element(By.CSS_SELECTOR, \"#shared-secret\")\n                .text.strip()\n                .replace(\" \", \"\")\n            )\n            totp = two_factor.TOTP(otp_secret)\n            current_2fa_code = totp.now()\n\n        self.nav_helper.safe_send_keys_by_css_selector('input[name=\"token\"]', current_2fa_code)\n\n        if callback_before_submitting_2fa_step:\n            callback_before_submitting_2fa_step()\n\n        self.nav_helper.safe_click_by_css_selector(\"form#check-token button[type=submit]\")\n\n        # Verify the two-factor authentication\n        def user_token_added():\n            if not self.accept_languages:\n                # Successfully verifying the code should redirect to the admin\n                # interface, and flash a message indicating success\n                flash_msg = self.driver.find_elements(By.CSS_SELECTOR, \".flash\")\n                expected_msg = (\n                    f'The two-factor code for user \"{final_username}\"'\n                    f\" was verified successfully.\"\n                )\n                assert expected_msg in [el.text for el in flash_msg]\n\n        self.nav_helper.wait_for(user_token_added)\n\n        # TODO(AD): Clarify whether the otp_secret that's being returned is totp or hotp\n        return final_username, password, otp_secret\n\n    def journalist_logs_out(self) -> None:\n        # Click the logout link\n        self.nav_helper.safe_click_by_id(\"logout\")\n        self.nav_helper.wait_for(lambda: self.driver.find_element(By.CSS_SELECTOR, \".login-form\"))\n\n        # Logging out should redirect back to the login page\n        def login_page():\n            assert \"Log in to access the journalist interface\" in self.driver.page_source\n\n        self.nav_helper.wait_for(login_page)\n\n    def admin_visits_system_config_page(self):\n        self.nav_helper.safe_click_by_id(\"update-instance-config\")\n\n        def config_page_loaded():\n            assert self.driver.find_element(By.ID, \"test-ossec-alert\")\n\n        self.nav_helper.wait_for(config_page_loaded)\n\n    def journalist_visits_edit_account(self):\n        self.nav_helper.safe_click_by_id(\"link-edit-account\")\n\n    def admin_visits_user_edit_page(self, username_of_journalist_to_edit: str) -> None:\n        # Go to the \"edit user\" page for the supplied journalist's username\n        selector = f'a.edit-user[data-username=\"{username_of_journalist_to_edit}\"]'\n        new_user_edit_links = self.driver.find_elements(By.CSS_SELECTOR, selector)\n        assert len(new_user_edit_links) == 1\n        new_user_edit_links[0].click()\n\n        # Ensure the admin is allowed to edit the journalist\n        def can_edit_user():\n            h = self.driver.find_elements(By.TAG_NAME, \"h1\")[0]\n            if not self.accept_languages:\n                assert f'Edit user \"{username_of_journalist_to_edit}\"' == h.text\n\n        self.nav_helper.wait_for(can_edit_user)",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "__init__",
              "code": "def __init__(\n        self,\n        journalist_app_base_url: str,\n        web_driver: WebDriver,\n        accept_languages: Optional[str] = None,\n    ) -> None:\n        self._journalist_app_base_url = journalist_app_base_url\n        self.nav_helper = NavigationHelper(web_driver)\n        self.driver = web_driver\n\n        # Some string-based tests check this to avoid failing on translated strings.\n        self.accept_languages = accept_languages",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "got_expected_language",
              "code": "def got_expected_language(self, locale: str) -> None:\n        expected = locale.replace(\"_\", \"-\")\n\n        html = self.nav_helper.wait_for(lambda: self.driver.find_element(By.TAG_NAME, \"html\"))\n        actual = html.get_attribute(\"lang\")\n\n        assert actual == expected",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "is_on_journalist_homepage",
              "code": "def is_on_journalist_homepage(self) -> WebElement:\n        return self.nav_helper.wait_for(\n            lambda: self.driver.find_element(By.CSS_SELECTOR, \"div.journalist-view-all\")\n        )",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "journalist_goes_to_login_page_and_enters_credentials",
              "code": "def journalist_goes_to_login_page_and_enters_credentials(\n        self,\n        username: str,\n        password: str,\n        otp_secret: str,\n        should_submit_login_form: bool,\n    ) -> None:\n        self.driver.get(f\"{self._journalist_app_base_url}/login\")\n\n        self.nav_helper.safe_send_keys_by_css_selector('input[name=\"username\"]', username)\n        self.nav_helper.safe_send_keys_by_css_selector('input[name=\"password\"]', password)\n        otp = two_factor.TOTP(otp_secret)\n        self.nav_helper.safe_send_keys_by_css_selector('input[name=\"token\"]', otp.now())\n\n        if should_submit_login_form:\n            self.nav_helper.safe_click_by_css_selector('form#login button[type=\"submit\"]')",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "journalist_logs_in",
              "code": "def journalist_logs_in(\n        self,\n        username: str,\n        password: str,\n        otp_secret: str,\n    ) -> None:\n        self.journalist_goes_to_login_page_and_enters_credentials(\n            username=username,\n            password=password,\n            otp_secret=otp_secret,\n            should_submit_login_form=True,\n        )\n\n        # Successful login should redirect to the index\n        self.nav_helper.wait_for(lambda: self.driver.find_element(By.ID, \"btn-logout\"))\n        assert self.is_on_journalist_homepage()",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "journalist_checks_messages",
              "code": "def journalist_checks_messages(self) -> None:\n        self.driver.get(self._journalist_app_base_url)\n\n        # There should be 1 collection in the list of collections\n        collections_count = self.count_sources_on_index_page()\n        assert collections_count == 1\n\n        if not self.accept_languages:\n            # There should be a \"1 unread\" span in the sole collection entry\n            unread_span = self.driver.find_element(By.CSS_SELECTOR, \"tr.unread\")\n            assert \"1 unread\" in unread_span.text",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "_download_content_at_url",
              "code": "def _download_content_at_url(url: str, cookies: Dict[str, str]) -> bytes:\n        r = requests.get(url, cookies=cookies, proxies=proxies_for_url(url), stream=True)\n        if r.status_code != 200:\n            raise Exception(\"Failed to download the data.\")\n        data = b\"\"\n        for chunk in r.iter_content(1024):\n            data += chunk\n        return data",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "journalist_downloads_first_message",
              "code": "def journalist_downloads_first_message(self) -> str:\n        # Select the first submission from the first source in the page\n        self.journalist_selects_the_first_source()\n        self.nav_helper.wait_for(\n            lambda: self.driver.find_element(By.CSS_SELECTOR, \"table#submissions\")\n        )\n        submissions = self.driver.find_elements(By.CSS_SELECTOR, \"#submissions a\")\n        assert len(submissions) == 1\n        file_url = submissions[0].get_attribute(\"href\")\n\n        # Downloading files with Selenium is tricky because it cannot automate\n        # the browser's file download dialog. We can directly request the file\n        # using requests, but we need to pass the cookies for logged in user\n        # for Flask to allow this.\n        def cookie_string_from_selenium_cookies(\n            cookies: Iterable[Dict[str, str]],\n        ) -> Dict[str, str]:\n            result = {}\n            for cookie in cookies:\n                result[cookie[\"name\"]] = cookie[\"value\"]\n            return result\n\n        cks = cookie_string_from_selenium_cookies(self.driver.get_cookies())\n        assert isinstance(file_url, str)\n        raw_content = self._download_content_at_url(file_url, cks)\n\n        decryption_result = utils.decrypt_as_journalist(raw_content)\n        if file_url.endswith(\".gz.gpg\"):\n            decrypted_message = gzip.decompress(decryption_result)\n        else:\n            decrypted_message = decryption_result\n\n        return decrypted_message.decode()",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "cookie_string_from_selenium_cookies",
              "code": "def cookie_string_from_selenium_cookies(\n            cookies: Iterable[Dict[str, str]],\n        ) -> Dict[str, str]:\n            result = {}\n            for cookie in cookies:\n                result[cookie[\"name\"]] = cookie[\"value\"]\n            return result",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "journalist_selects_the_first_source",
              "code": "def journalist_selects_the_first_source(self) -> None:\n        self.driver.find_element(By.CSS_SELECTOR, \"#un-starred-source-link-1\").click()",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "journalist_composes_reply_to_source",
              "code": "def journalist_composes_reply_to_source(self, reply_content: str) -> None:\n        self.nav_helper.wait_for(lambda: self.driver.find_element(By.ID, \"reply-text-field\"))\n        self.nav_helper.safe_send_keys_by_id(\"reply-text-field\", reply_content)",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "journalist_sends_reply_to_source",
              "code": "def journalist_sends_reply_to_source(\n        self, reply_content: str = \"Thanks for the documents. Can you submit more? éè\"\n    ) -> None:\n        self.journalist_composes_reply_to_source(reply_content=reply_content)\n        self.driver.find_element(By.ID, \"reply-button\").click()\n\n        def reply_stored() -> None:\n            if not self.accept_languages:\n                assert \"The source will receive your reply\" in self.driver.page_source\n\n        self.nav_helper.wait_for(reply_stored)",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "reply_stored",
              "code": "def reply_stored() -> None:\n            if not self.accept_languages:\n                assert \"The source will receive your reply\" in self.driver.page_source",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "journalist_visits_col",
              "code": "def journalist_visits_col(self) -> None:\n        self.nav_helper.wait_for(\n            lambda: self.driver.find_element(By.CSS_SELECTOR, \"table#collections\")\n        )\n        self.nav_helper.safe_click_by_id(\"un-starred-source-link-1\")\n        self.nav_helper.wait_for(\n            lambda: self.driver.find_element(By.CSS_SELECTOR, \"table#submissions\")\n        )",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "journalist_selects_first_doc",
              "code": "def journalist_selects_first_doc(self) -> None:\n        self.nav_helper.safe_click_by_css_selector(\n            'input[type=\"checkbox\"][name=\"doc_names_selected\"]'\n        )\n\n        self.nav_helper.wait_for(\n            lambda: expected_conditions.element_located_to_be_selected(\n                (By.CSS_SELECTOR, 'input[type=\"checkbox\"][name=\"doc_names_selected\"]')\n            )\n        )\n\n        assert self.driver.find_element(\n            By.CSS_SELECTOR, 'input[type=\"checkbox\"][name=\"doc_names_selected\"]'\n        ).is_selected()",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "journalist_clicks_delete_selected_link",
              "code": "def journalist_clicks_delete_selected_link(self) -> None:\n        self.nav_helper.safe_click_by_css_selector(\"a#delete-selected-link\")\n        self.nav_helper.wait_for(\n            lambda: self.driver.find_element(By.ID, \"delete-selected-confirmation-modal\")\n        )",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "journalist_clicks_delete_all_and_sees_confirmation",
              "code": "def journalist_clicks_delete_all_and_sees_confirmation(self) -> None:\n        self.nav_helper.safe_click_all_by_css_selector(\"[name=doc_names_selected]\")\n        self.nav_helper.safe_click_by_css_selector(\"a#delete-selected-link\")",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "journalist_confirms_delete_selected",
              "code": "def journalist_confirms_delete_selected(self) -> None:\n        self.nav_helper.wait_for(\n            lambda: expected_conditions.element_to_be_clickable((By.ID, \"delete-selected\"))\n        )\n        confirm_btn = self.driver.find_element(By.ID, \"delete-selected\")\n        assert confirm_btn.location_once_scrolled_into_view\n        ActionChains(self.driver).move_to_element(confirm_btn).click().perform()",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "get_submission_checkboxes_on_current_page",
              "code": "def get_submission_checkboxes_on_current_page(self):\n        return self.driver.find_elements(By.NAME, \"doc_names_selected\")",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "count_submissions_on_current_page",
              "code": "def count_submissions_on_current_page(self) -> int:\n        return len(self.get_submission_checkboxes_on_current_page())",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "get_sources_on_index_page",
              "code": "def get_sources_on_index_page(self):\n        assert self.is_on_journalist_homepage()\n        return self.driver.find_elements(By.CLASS_NAME, \"code-name\")",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "count_sources_on_index_page",
              "code": "def count_sources_on_index_page(self) -> int:\n        return len(self.get_sources_on_index_page())",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "journalist_confirm_delete_selected",
              "code": "def journalist_confirm_delete_selected(self) -> None:\n        self.nav_helper.wait_for(\n            lambda: expected_conditions.element_to_be_clickable((By.ID, \"delete-selected\"))\n        )\n        confirm_btn = self.driver.find_element(By.ID, \"delete-selected\")\n        assert confirm_btn.location_once_scrolled_into_view\n        ActionChains(self.driver).move_to_element(confirm_btn).click().perform()",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "journalist_sees_link_to_admin_page",
              "code": "def journalist_sees_link_to_admin_page(self) -> bool:\n        try:\n            self.driver.find_element(By.ID, \"link-admin-index\")\n            return True\n        except NoSuchElementException:\n            return False",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "admin_visits_admin_interface",
              "code": "def admin_visits_admin_interface(self) -> None:\n        self.nav_helper.safe_click_by_id(\"link-admin-index\")\n        self.nav_helper.wait_for(lambda: self.driver.find_element(By.ID, \"add-user\"))",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "admin_creates_a_user",
              "code": "def admin_creates_a_user(\n        self,\n        username: Optional[str] = None,\n        hotp_secret: Optional[str] = None,\n        is_admin: bool = False,\n        callback_before_submitting_add_user_step: Optional[Callable[[], None]] = None,\n        callback_before_submitting_2fa_step: Optional[Callable[[], None]] = None,\n    ) -> Optional[Tuple[str, str, str]]:\n        self.nav_helper.safe_click_by_id(\"add-user\")\n        self.nav_helper.wait_for(lambda: self.driver.find_element(By.ID, \"username\"))\n\n        if not self.accept_languages:\n            # The add user page has a form with an \"ADD USER\" button\n            btns = self.driver.find_elements(By.TAG_NAME, \"button\")\n            assert \"ADD USER\" in [el.text for el in btns]\n\n        password = self.driver.find_element(By.CSS_SELECTOR, \"#password\").text.strip()\n        if not username:\n            final_username = f\"journalist{str(randint(1000, 1000000))}\"\n        else:\n            final_username = username\n\n        # Fill the form\n        self.nav_helper.safe_send_keys_by_css_selector('input[name=\"username\"]', final_username)\n        if hotp_secret:\n            # Create an HOTP user instead of TOTP\n            self.nav_helper.safe_click_all_by_css_selector('input[name=\"is_hotp\"]')\n            self.nav_helper.safe_send_keys_by_css_selector('input[name=\"otp_secret\"]', hotp_secret)\n\n        if is_admin:\n            self.nav_helper.safe_click_by_css_selector('input[name=\"is_admin\"]')\n\n        if callback_before_submitting_add_user_step:\n            callback_before_submitting_add_user_step()\n\n        # Submit the form\n        self.nav_helper.safe_click_by_css_selector(\"form#admin-add-user button[type=submit]\")\n\n        # Submitting the add user form should redirect to the 2FA page\n        self.nav_helper.wait_for(lambda: self.driver.find_element(By.ID, \"check-token\"))\n        if self.accept_languages in [None, \"en-US\"]:\n            expected_title = \"Enable YubiKey (OATH-HOTP)\" if hotp_secret else \"Enable FreeOTP\"\n            h1s = [h1.text for h1 in self.driver.find_elements(By.TAG_NAME, \"h1\")]\n            assert expected_title in h1s\n\n        if hotp_secret:\n            # We created an hotp user\n            otp_secret = hotp_secret\n            hotp_secret_as_hex = unhexlify(hotp_secret.replace(\" \", \"\"))\n            hotp_secret_as_b32 = base64.b32encode(hotp_secret_as_hex).decode(\"ascii\")\n            hotp = two_factor.HOTP(hotp_secret_as_b32)\n            current_2fa_code = hotp.generate(0)\n        else:\n            # We created a totp user\n            otp_secret = (\n                self.driver.find_element(By.CSS_SELECTOR, \"#shared-secret\")\n                .text.strip()\n                .replace(\" \", \"\")\n            )\n            totp = two_factor.TOTP(otp_secret)\n            current_2fa_code = totp.now()\n\n        self.nav_helper.safe_send_keys_by_css_selector('input[name=\"token\"]', current_2fa_code)\n\n        if callback_before_submitting_2fa_step:\n            callback_before_submitting_2fa_step()\n\n        self.nav_helper.safe_click_by_css_selector(\"form#check-token button[type=submit]\")\n\n        # Verify the two-factor authentication\n        def user_token_added():\n            if not self.accept_languages:\n                # Successfully verifying the code should redirect to the admin\n                # interface, and flash a message indicating success\n                flash_msg = self.driver.find_elements(By.CSS_SELECTOR, \".flash\")\n                expected_msg = (\n                    f'The two-factor code for user \"{final_username}\"'\n                    f\" was verified successfully.\"\n                )\n                assert expected_msg in [el.text for el in flash_msg]\n\n        self.nav_helper.wait_for(user_token_added)\n\n        # TODO(AD): Clarify whether the otp_secret that's being returned is totp or hotp\n        return final_username, password, otp_secret",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "user_token_added",
              "code": "def user_token_added():\n            if not self.accept_languages:\n                # Successfully verifying the code should redirect to the admin\n                # interface, and flash a message indicating success\n                flash_msg = self.driver.find_elements(By.CSS_SELECTOR, \".flash\")\n                expected_msg = (\n                    f'The two-factor code for user \"{final_username}\"'\n                    f\" was verified successfully.\"\n                )\n                assert expected_msg in [el.text for el in flash_msg]",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "journalist_logs_out",
              "code": "def journalist_logs_out(self) -> None:\n        # Click the logout link\n        self.nav_helper.safe_click_by_id(\"logout\")\n        self.nav_helper.wait_for(lambda: self.driver.find_element(By.CSS_SELECTOR, \".login-form\"))\n\n        # Logging out should redirect back to the login page\n        def login_page():\n            assert \"Log in to access the journalist interface\" in self.driver.page_source\n\n        self.nav_helper.wait_for(login_page)",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "login_page",
              "code": "def login_page():\n            assert \"Log in to access the journalist interface\" in self.driver.page_source",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "admin_visits_system_config_page",
              "code": "def admin_visits_system_config_page(self):\n        self.nav_helper.safe_click_by_id(\"update-instance-config\")\n\n        def config_page_loaded():\n            assert self.driver.find_element(By.ID, \"test-ossec-alert\")\n\n        self.nav_helper.wait_for(config_page_loaded)",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "config_page_loaded",
              "code": "def config_page_loaded():\n            assert self.driver.find_element(By.ID, \"test-ossec-alert\")",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "journalist_visits_edit_account",
              "code": "def journalist_visits_edit_account(self):\n        self.nav_helper.safe_click_by_id(\"link-edit-account\")",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "admin_visits_user_edit_page",
              "code": "def admin_visits_user_edit_page(self, username_of_journalist_to_edit: str) -> None:\n        # Go to the \"edit user\" page for the supplied journalist's username\n        selector = f'a.edit-user[data-username=\"{username_of_journalist_to_edit}\"]'\n        new_user_edit_links = self.driver.find_elements(By.CSS_SELECTOR, selector)\n        assert len(new_user_edit_links) == 1\n        new_user_edit_links[0].click()\n\n        # Ensure the admin is allowed to edit the journalist\n        def can_edit_user():\n            h = self.driver.find_elements(By.TAG_NAME, \"h1\")[0]\n            if not self.accept_languages:\n                assert f'Edit user \"{username_of_journalist_to_edit}\"' == h.text\n\n        self.nav_helper.wait_for(can_edit_user)",
              "file": "journalist_app_nav.py"
            },
            {
              "type": "function",
              "name": "can_edit_user",
              "code": "def can_edit_user():\n            h = self.driver.find_elements(By.TAG_NAME, \"h1\")[0]\n            if not self.accept_languages:\n                assert f'Edit user \"{username_of_journalist_to_edit}\"' == h.text",
              "file": "journalist_app_nav.py"
            }
          ]
        }
      },
      "utils": {
        "db_helper.py": [
          {
            "type": "function",
            "name": "init_journalist",
            "code": "def init_journalist(first_name=None, last_name=None, is_admin=False):\n    \"\"\"Initialize a journalist into the database. Return their\n    :class:`Journalist` object and password string.\n\n    :param bool is_admin: Whether the user is an admin.\n\n    :returns: A 2-tuple. The first entry, an :obj:`Journalist`\n              corresponding to the row just added to the database. The\n              second, their password string.\n    \"\"\"\n    username = PassphraseGenerator.get_default().generate_passphrase()\n    user_pw = PassphraseGenerator.get_default().generate_passphrase()\n    user = Journalist(\n        username=username,\n        password=user_pw,\n        first_name=first_name,\n        last_name=last_name,\n        is_admin=is_admin,\n    )\n    db.session.add(user)\n    db.session.commit()\n    return user, user_pw",
            "file": "db_helper.py"
          },
          {
            "type": "function",
            "name": "delete_journalist",
            "code": "def delete_journalist(journalist):\n    \"\"\"Deletes a journalist from the database.\n\n    :param Journalist journalist: The journalist to delete\n\n    :returns: None\n    \"\"\"\n    journalist.delete()\n    db.session.commit()",
            "file": "db_helper.py"
          },
          {
            "type": "function",
            "name": "reply",
            "code": "def reply(storage, journalist, source, num_replies):\n    \"\"\"Generates and submits *num_replies* replies to *source*\n    from *journalist*. Returns reply objects as a list.\n\n    :param Journalist journalist: The journalist to write the\n                                     reply from.\n\n    :param Source source: The source to send the reply to.\n\n    :param int num_replies: Number of random-data replies to make.\n\n    :returns: A list of the :class:`Reply`s submitted.\n    \"\"\"\n    assert num_replies >= 1\n    replies = []\n    for _ in range(num_replies):\n        source.interaction_count += 1\n        fname = f\"{source.interaction_count}-{source.journalist_filename}-reply.gpg\"\n\n        EncryptionManager.get_default().encrypt_journalist_reply(\n            for_source=source,\n            reply_in=str(os.urandom(1)),\n            encrypted_reply_path_out=storage.path(source.filesystem_id, fname),\n        )\n\n        reply = Reply(journalist, source, fname, storage)\n        replies.append(reply)\n        db.session.add(reply)\n        seen_reply = SeenReply(reply=reply, journalist=journalist)\n        db.session.add(seen_reply)\n\n    db.session.commit()\n    return replies",
            "file": "db_helper.py"
          },
          {
            "type": "function",
            "name": "init_source",
            "code": "def init_source(storage):\n    \"\"\"Initialize a source: create their database record, the\n    filesystem directory that stores their submissions & replies,\n    and their GPG key encrypted with their codename. Return a source\n    object and their codename string.\n\n    :returns: A 2-tuple. The first entry, the :class:`Source`\n    initialized. The second, their codename string.\n    \"\"\"\n    passphrase = PassphraseGenerator.get_default().generate_passphrase()\n    source_user = create_source_user(\n        db_session=db.session,\n        source_passphrase=passphrase,\n        source_app_storage=storage,\n    )\n    return source_user.get_db_record(), passphrase",
            "file": "db_helper.py"
          },
          {
            "type": "function",
            "name": "submit",
            "code": "def submit(storage, source, num_submissions, submission_type=\"message\"):\n    \"\"\"Generates and submits *num_submissions*\n    :class:`Submission`s on behalf of a :class:`Source`\n    *source*.\n\n    :param Storage storage: the Storage object to use.\n\n    :param Source source: The source on who's behalf to make\n                             submissions.\n\n    :param int num_submissions: Number of random-data submissions\n                                to make.\n\n    :returns: A list of the :class:`Submission`s submitted.\n    \"\"\"\n    assert num_submissions >= 1\n    source.last_updated = datetime.datetime.utcnow()\n    db.session.add(source)\n    submissions = []\n    for _ in range(num_submissions):\n        source.interaction_count += 1\n        source.pending = False\n        if submission_type == \"file\":\n            fpath = storage.save_file_submission(\n                source.filesystem_id,\n                source.interaction_count,\n                source.journalist_filename,\n                \"pipe.txt\",\n                io.BytesIO(b\"Ceci n'est pas une pipe.\"),\n            )\n        else:\n            fpath = storage.save_message_submission(\n                source.filesystem_id,\n                source.interaction_count,\n                source.journalist_filename,\n                str(os.urandom(1)),\n            )\n        submission = Submission(source, fpath, storage)\n        submissions.append(submission)\n        db.session.add(source)\n        db.session.add(submission)\n\n    db.session.commit()\n    return submissions",
            "file": "db_helper.py"
          },
          {
            "type": "function",
            "name": "new_codename",
            "code": "def new_codename(client, session):\n    \"\"\"Helper function to go through the \"generate codename\" flow.\"\"\"\n    client.post(\"/generate\", data={\"tor2web_check\": 'href=\"fake.onion\"'})\n    tab_id, codename = next(iter(session[\"codenames\"].items()))\n    client.post(\"/create\", data={\"tab_id\": tab_id})\n    return codename",
            "file": "db_helper.py"
          },
          {
            "type": "function",
            "name": "bulk_setup_for_seen_only",
            "code": "def bulk_setup_for_seen_only(journo: Journalist, storage: Storage) -> List[Dict]:\n    \"\"\"\n    Create some sources with some seen submissions that are not marked as 'downloaded' in the\n    database and some seen replies from journo.\n    \"\"\"\n\n    setup_collection = []\n\n    for _i in range(random.randint(2, 4)):\n        collection = {}\n\n        source, _ = init_source(storage)\n\n        submissions = submit(storage, source, random.randint(2, 4))\n        half = math.ceil(len(submissions) / 2)\n        messages = submissions[half:]\n        files = submissions[:half]\n        replies = reply(storage, journo, source, random.randint(1, 3))\n\n        seen_files = random.sample(files, math.ceil(len(files) / 2))\n        seen_messages = random.sample(messages, math.ceil(len(messages) / 2))\n        seen_replies = random.sample(replies, math.ceil(len(replies) / 2))\n\n        mark_seen(seen_files, journo)\n        mark_seen(seen_messages, journo)\n        mark_seen(seen_replies, journo)\n\n        unseen_files = list(set(files).difference(set(seen_files)))\n        unseen_messages = list(set(messages).difference(set(seen_messages)))\n        unseen_replies = list(set(replies).difference(set(seen_replies)))\n        not_downloaded = list(set(files + messages).difference(set(seen_files + seen_messages)))\n\n        collection[\"source\"] = source\n        collection[\"seen_files\"] = seen_files\n        collection[\"seen_messages\"] = seen_messages\n        collection[\"seen_replies\"] = seen_replies\n        collection[\"unseen_files\"] = unseen_files\n        collection[\"unseen_messages\"] = unseen_messages\n        collection[\"unseen_replies\"] = unseen_replies\n        collection[\"not_downloaded\"] = not_downloaded\n\n        setup_collection.append(collection)\n\n    return setup_collection",
            "file": "db_helper.py"
          },
          {
            "type": "function",
            "name": "reset_database",
            "code": "def reset_database(database_file: Path) -> None:\n    database_file.unlink(missing_ok=True)  # type: ignore\n    database_file.touch()\n    subprocess.check_call([\"sqlite3\", database_file, \".databases\"])",
            "file": "db_helper.py"
          }
        ],
        "api_helper.py": [
          {
            "type": "function",
            "name": "get_api_headers",
            "code": "def get_api_headers(token=\"\"):\n    if token:\n        return {\n            \"Authorization\": f\"Token {token}\",\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/json\",\n        }\n    return {\"Accept\": \"application/json\", \"Content-Type\": \"application/json\"}",
            "file": "api_helper.py"
          }
        ],
        "asynchronous.py": [
          {
            "type": "function",
            "name": "wait_for_assertion",
            "code": "def wait_for_assertion(assertion_expression, timeout=10):\n    \"\"\"Calls an assertion_expression repeatedly, until the assertion\n    passes or a timeout is reached.\n\n    :param assertion_expression: An assertion expression. Generally\n                                 a call to a\n                                 :class:`unittest.TestCase` method.\n\n    :param int timeout: Seconds to wait for the function to return.\n    \"\"\"\n    start_time = time.time()\n    while time.time() - start_time < timeout:\n        try:\n            return assertion_expression()\n        except AssertionError:\n            time.sleep(0.1)\n    # one more try, which will raise any errors if they are outstanding\n    return assertion_expression()",
            "file": "asynchronous.py"
          }
        ],
        "__init__.py": [
          {
            "type": "function",
            "name": "flaky_filter_xfail",
            "code": "def flaky_filter_xfail(err, *args):\n    \"\"\"\n    Tell the pytest flaky plugin to not retry XFailed errors.\n\n    If the test is expected to fail, let's not run it again.\n    \"\"\"\n    return (\n        f\"{err[0].__class__.__module__}.{err[0].__class__.__qualname__}\"\n        == \"_pytest.outcomes.XFailed\"\n    )",
            "file": "__init__.py"
          },
          {
            "type": "function",
            "name": "_reset_journalist_last_token",
            "code": "def _reset_journalist_last_token(username: str) -> None:\n    # This function only works if there's an existing app context\n    assert flask.current_app\n\n    user = db.session.query(models.Journalist).filter_by(username=username).one()\n    user.last_token = None\n    db.session.commit()",
            "file": "__init__.py"
          },
          {
            "type": "function",
            "name": "login_journalist",
            "code": "def login_journalist(\n    app_test_client: FlaskClient,\n    username: str,\n    password: str,\n    otp_secret: str,\n) -> None:\n    \"\"\"Log the journalist in, bypass login hardening measures in order to facilitate testing.\"\"\"\n    # Remove the last_token entry for this user so that the login here can succeed\n    # instead of being blocked due to 2fa token reuse\n    _reset_journalist_last_token(username)\n\n    # Perform the login\n    resp = app_test_client.post(\n        \"/login\",\n        data={\n            \"username\": username,\n            \"password\": password,\n            \"token\": TOTP(otp_secret).now(),\n        },\n        follow_redirects=True,\n    )\n    assert resp.status_code == 200\n\n    # Ensure the user got a session\n    from journalist_app.sessions import session\n\n    assert session.get_user() is not None\n\n    _reset_journalist_last_token(username)",
            "file": "__init__.py"
          },
          {
            "type": "function",
            "name": "extract_password",
            "code": "def extract_password(html: bytes) -> str:\n    search = re.search(r'<input name=\"password\" type=\"hidden\" value=\"(.*?)\">', html.decode())\n    if search:\n        return search.group(1)\n    else:\n        raise ValueError(\"Could not find password in HTML\")",
            "file": "__init__.py"
          },
          {
            "type": "function",
            "name": "prepare_password_change",
            "code": "def prepare_password_change(app_test_client: FlaskClient, id: int, new_password: str) -> None:\n    \"\"\"A reimplementation of utils.set_pending_password() for tests\"\"\"\n    with app_test_client.session_transaction() as session:\n        session[f\"pending_password_{id}\"] = argon2.PasswordHasher(**ARGON2_PARAMS).hash(\n            new_password\n        )",
            "file": "__init__.py"
          },
          {
            "type": "function",
            "name": "decrypt_as_journalist",
            "code": "def decrypt_as_journalist(ciphertext: bytes) -> bytes:\n    return redwood.decrypt(\n        ciphertext=ciphertext,\n        secret_key=JOURNALIST_SECRET_KEY,\n        passphrase=\"correcthorsebatterystaple\",\n    )",
            "file": "__init__.py"
          },
          {
            "type": "function",
            "name": "create_legacy_gpg_key",
            "code": "def create_legacy_gpg_key(\n    manager: EncryptionManager, source_user: SourceUser, source: models.Source\n) -> str:\n    \"\"\"Create a GPG key for the source, so we can test pre-Sequoia behavior\"\"\"\n    # All reply keypairs will be \"created\" on the same day SecureDrop (then\n    # Strongbox) was publicly released for the first time.\n    # https://www.newyorker.com/news/news-desk/strongbox-and-aaron-swartz\n    default_key_creation_date = datetime.date(2013, 5, 14)\n    gen_key_input = manager.gpg().gen_key_input(\n        passphrase=source_user.gpg_secret,\n        name_email=source_user.filesystem_id,\n        key_type=\"RSA\",\n        key_length=4096,\n        name_real=\"Source Key\",\n        creation_date=default_key_creation_date.isoformat(),\n        # '0' is the magic value that tells GPG's batch key generation not\n        # to set an expiration date.\n        expire_date=\"0\",\n    )\n    result = manager.gpg().gen_key(gen_key_input)\n\n    # Delete the Sequoia-generated keys\n    source.pgp_public_key = None\n    source.pgp_fingerprint = None\n    source.pgp_secret_key = None\n    db.session.add(source)\n    db.session.commit()\n    return result.fingerprint",
            "file": "__init__.py"
          },
          {
            "type": "function",
            "name": "_session_from_cookiejar",
            "code": "def _session_from_cookiejar(cookie_jar, flask_app):\n    return next(\n        (cookie for cookie in cookie_jar if cookie.name == flask_app.config[\"SESSION_COOKIE_NAME\"]),\n        None,\n    )",
            "file": "__init__.py"
          }
        ],
        "instrument.py": [
          {
            "type": "class",
            "name": "ContextVariableDoesNotExist",
            "code": "class ContextVariableDoesNotExist(Exception):\n    pass",
            "file": "instrument.py"
          },
          {
            "type": "class",
            "name": "InstrumentedApp",
            "code": "class InstrumentedApp:\n    def __init__(self, app):\n        self.app = app\n\n    def __enter__(self):\n        self.templates = []\n        self.flashed_messages = []\n        template_rendered.connect(self._add_template)\n        message_flashed.connect(self._add_flash_message)\n        return self\n\n    def __exit__(self, *nargs):\n        if getattr(self, \"app\", None) is not None:\n            del self.app\n\n        del self.templates[:]\n        del self.flashed_messages[:]\n\n        template_rendered.disconnect(self._add_template)\n        message_flashed.disconnect(self._add_flash_message)\n\n    def _add_flash_message(self, app, message, category):\n        self.flashed_messages.append((message, category))\n\n    def _add_template(self, app, template, context):\n        if len(self.templates) > 0:\n            self.templates = []\n        self.templates.append((template, context))\n\n    def assert_message_flashed(self, message, category=\"message\"):\n        \"\"\"\n        Checks if a given message was flashed.\n\n        :param message: expected message\n        :param category: expected message category\n        \"\"\"\n        for _message, _category in self.flashed_messages:\n            if _message == message and _category == category:\n                return True\n\n        raise AssertionError(f\"Message '{message}' in category '{category}' wasn't flashed\")\n\n    def assert_template_used(self, name, tmpl_name_attribute=\"name\"):\n        \"\"\"\n        Checks if a given template is used in the request. If the template\n        engine used is not Jinja2, provide ``tmpl_name_attribute`` with a\n        value of its `Template` class attribute name which contains the\n        provided ``name`` value.\n\n        :param name: template name\n        :param tmpl_name_attribute: template engine specific attribute name\n        \"\"\"\n        used_templates = []\n\n        for template, _context in self.templates:\n            if getattr(template, tmpl_name_attribute) == name:\n                return True\n\n            used_templates.append(template)\n\n        raise AssertionError(\n            \"Template {} not used. Templates were used: {}\".format(\n                name, \" \".join(repr(used_templates))\n            )\n        )\n\n    def get_context_variable(self, name):\n        \"\"\"\n        Returns a variable from the context passed to the template.\n\n        Raises a ContextVariableDoesNotExist exception if does not exist in\n        context.\n\n        :param name: name of variable\n        \"\"\"\n        for _template, context in self.templates:\n            if name in context:\n                return context[name]\n        raise ContextVariableDoesNotExist\n\n    def assert_context(self, name, value, message=None):\n        \"\"\"\n        Checks if given name exists in the template context\n        and equals the given value.\n\n        :versionadded: 0.2\n        :param name: name of context variable\n        :param value: value to check against\n        \"\"\"\n\n        try:\n            assert self.get_context_variable(name) == value, message\n        except ContextVariableDoesNotExist:\n            pytest.fail(message or f\"Context variable does not exist: {name}\")\n\n    def assert_redirects(self, response, expected_location, message=None):\n        \"\"\"\n        Checks if response is an HTTP redirect to the\n        given location.\n\n        :param response: Flask response\n        :param location: relative URL path or an absolute URL\n        \"\"\"\n        valid_status_codes = (301, 302, 303, 305, 307)\n        valid_status_code_str = \", \".join([str(code) for code in valid_status_codes])\n        not_redirect = (\n            f\"HTTP Status {valid_status_code_str} expected but got {response.status_code}\"\n        )\n        assert response.status_code in (valid_status_codes, message) or not_redirect\n        assert response.location == expected_location, message",
            "file": "instrument.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, app):\n        self.app = app",
            "file": "instrument.py"
          },
          {
            "type": "function",
            "name": "__enter__",
            "code": "def __enter__(self):\n        self.templates = []\n        self.flashed_messages = []\n        template_rendered.connect(self._add_template)\n        message_flashed.connect(self._add_flash_message)\n        return self",
            "file": "instrument.py"
          },
          {
            "type": "function",
            "name": "__exit__",
            "code": "def __exit__(self, *nargs):\n        if getattr(self, \"app\", None) is not None:\n            del self.app\n\n        del self.templates[:]\n        del self.flashed_messages[:]\n\n        template_rendered.disconnect(self._add_template)\n        message_flashed.disconnect(self._add_flash_message)",
            "file": "instrument.py"
          },
          {
            "type": "function",
            "name": "_add_flash_message",
            "code": "def _add_flash_message(self, app, message, category):\n        self.flashed_messages.append((message, category))",
            "file": "instrument.py"
          },
          {
            "type": "function",
            "name": "_add_template",
            "code": "def _add_template(self, app, template, context):\n        if len(self.templates) > 0:\n            self.templates = []\n        self.templates.append((template, context))",
            "file": "instrument.py"
          },
          {
            "type": "function",
            "name": "assert_message_flashed",
            "code": "def assert_message_flashed(self, message, category=\"message\"):\n        \"\"\"\n        Checks if a given message was flashed.\n\n        :param message: expected message\n        :param category: expected message category\n        \"\"\"\n        for _message, _category in self.flashed_messages:\n            if _message == message and _category == category:\n                return True\n\n        raise AssertionError(f\"Message '{message}' in category '{category}' wasn't flashed\")",
            "file": "instrument.py"
          },
          {
            "type": "function",
            "name": "assert_template_used",
            "code": "def assert_template_used(self, name, tmpl_name_attribute=\"name\"):\n        \"\"\"\n        Checks if a given template is used in the request. If the template\n        engine used is not Jinja2, provide ``tmpl_name_attribute`` with a\n        value of its `Template` class attribute name which contains the\n        provided ``name`` value.\n\n        :param name: template name\n        :param tmpl_name_attribute: template engine specific attribute name\n        \"\"\"\n        used_templates = []\n\n        for template, _context in self.templates:\n            if getattr(template, tmpl_name_attribute) == name:\n                return True\n\n            used_templates.append(template)\n\n        raise AssertionError(\n            \"Template {} not used. Templates were used: {}\".format(\n                name, \" \".join(repr(used_templates))\n            )\n        )",
            "file": "instrument.py"
          },
          {
            "type": "function",
            "name": "get_context_variable",
            "code": "def get_context_variable(self, name):\n        \"\"\"\n        Returns a variable from the context passed to the template.\n\n        Raises a ContextVariableDoesNotExist exception if does not exist in\n        context.\n\n        :param name: name of variable\n        \"\"\"\n        for _template, context in self.templates:\n            if name in context:\n                return context[name]\n        raise ContextVariableDoesNotExist",
            "file": "instrument.py"
          },
          {
            "type": "function",
            "name": "assert_context",
            "code": "def assert_context(self, name, value, message=None):\n        \"\"\"\n        Checks if given name exists in the template context\n        and equals the given value.\n\n        :versionadded: 0.2\n        :param name: name of context variable\n        :param value: value to check against\n        \"\"\"\n\n        try:\n            assert self.get_context_variable(name) == value, message\n        except ContextVariableDoesNotExist:\n            pytest.fail(message or f\"Context variable does not exist: {name}\")",
            "file": "instrument.py"
          },
          {
            "type": "function",
            "name": "assert_redirects",
            "code": "def assert_redirects(self, response, expected_location, message=None):\n        \"\"\"\n        Checks if response is an HTTP redirect to the\n        given location.\n\n        :param response: Flask response\n        :param location: relative URL path or an absolute URL\n        \"\"\"\n        valid_status_codes = (301, 302, 303, 305, 307)\n        valid_status_code_str = \", \".join([str(code) for code in valid_status_codes])\n        not_redirect = (\n            f\"HTTP Status {valid_status_code_str} expected but got {response.status_code}\"\n        )\n        assert response.status_code in (valid_status_codes, message) or not_redirect\n        assert response.location == expected_location, message",
            "file": "instrument.py"
          }
        ],
        "i18n.py": [
          {
            "type": "function",
            "name": "get_test_locales",
            "code": "def get_test_locales(default_locale: str = \"en_US\") -> List[str]:\n    locales = set(os.environ.get(\"TEST_LOCALES\", \"ar\").split())\n    if default_locale:\n        locales.add(default_locale)\n    return sorted(list(locales))",
            "file": "i18n.py"
          },
          {
            "type": "function",
            "name": "get_plural_tests",
            "code": "def get_plural_tests() -> Dict[str, Tuple[int, ...]]:\n    return collections.defaultdict(\n        lambda: (0, 1, 2),\n        ar=(0, 1, 2, 3, 11, 100),\n        cs=(1, 2, 5),\n        ro=(0, 1, 2, 20),\n        ru=(0, 1, 2, 20),\n        sk=(0, 1, 2, 5, 10),\n        zh_Hans=(1,),\n        zh_Hant=(1,),\n    )",
            "file": "i18n.py"
          },
          {
            "type": "function",
            "name": "language_tag",
            "code": "def language_tag(locale: str) -> str:\n    \"\"\"\n    Returns a BCP47/RFC5646 language tag for the locale.\n\n    For example, it will convert \"fr_FR\" to \"fr-FR\".\n    \"\"\"\n    return get_locale_identifier(parse_locale(locale), sep=\"-\")",
            "file": "i18n.py"
          },
          {
            "type": "function",
            "name": "message_catalog",
            "code": "def message_catalog(translation_dir: Path, locale: str) -> Catalog:\n    \"\"\"\n    Returns the gettext message catalog for the given locale.\n\n    With the catalog, tests can check whether a gettext call returned\n    an actual translation or merely the result of falling back to the\n    default locale.\n\n    >>> german = message_catalog(translation_dir, 'de_DE')\n    >>> m = german.get(\"a string that has been added to the catalog but not translated\")\n    >>> m.string\n    ''\n    >>> german.get(\"Password\").string\n    'Passwort'\n    \"\"\"\n    return read_po(open(translation_dir / locale / \"LC_MESSAGES\" / \"messages.po\"))",
            "file": "i18n.py"
          },
          {
            "type": "function",
            "name": "page_language",
            "code": "def page_language(page_text: str) -> Optional[str]:\n    \"\"\"\n    Returns the \"lang\" attribute of the page's \"html\" element.\n    \"\"\"\n    soup = BeautifulSoup(page_text, \"html.parser\")\n    return soup.find(\"html\").get(\"lang\")",
            "file": "i18n.py"
          },
          {
            "type": "function",
            "name": "xfail_untranslated_messages",
            "code": "def xfail_untranslated_messages(\n    config: SecureDropConfig, locale: str, msgids: Iterable[str]\n) -> Generator[None, None, None]:\n    \"\"\"\n    Trigger pytest.xfail for untranslated strings.\n\n    Given a list of gettext message IDs (strings marked for\n    translation with gettext or ngettext in source code) used in this\n    context manager's block, check that each has been translated in\n    `locale`. Call pytest.xfail if any has not.\n\n    Without this, to detect when gettext fell back to English, we'd\n    have to hard-code the expected translations, which has obvious\n    maintenance problems. You also can't just check that the result of\n    a gettext call isn't the source string, because some translations\n    are the same.\n    \"\"\"\n    with force_locale(locale):\n        if locale != \"en_US\":\n            catalog = message_catalog(config.TRANSLATION_DIRS, locale)\n            for msgid in msgids:\n                m = catalog.get(msgid)\n                if not m:\n                    pytest.xfail(f\"locale {locale} message catalog lacks msgid: {msgid}\")\n                if not m.string:\n                    pytest.xfail(f\"locale {locale} has no translation for msgid: {msgid}\")\n        yield",
            "file": "i18n.py"
          }
        ]
      },
      "migrations": {
        "migration_c5a02eb52f2d.py": [
          {
            "type": "class",
            "name": "Helper",
            "code": "class Helper:\n    def __init__(self):\n        self.journalist_id = None",
            "file": "migration_c5a02eb52f2d.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self):\n        self.journalist_id = None",
            "file": "migration_c5a02eb52f2d.py"
          },
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester(Helper):\n    def __init__(self, config):\n        Helper.__init__(self)\n        self.config = config\n        self.app = create_app(config)\n\n    def create_journalist(self):\n        params = {\n            \"uuid\": str(uuid.uuid4()),\n            \"username\": random_chars(50),\n            \"nonce\": random.randint(20, 100),\n        }\n        sql = \"\"\"INSERT INTO journalists (uuid, username, session_nonce)\n                 VALUES (:uuid, :username, :nonce)\"\"\"\n        return db.engine.execute(text(sql), **params).lastrowid\n\n    def add_revoked_token(self):\n        params = {\n            \"journalist_id\": self.journalist_id,\n            \"token\": \"abc123\",\n        }\n        sql = \"\"\"INSERT INTO revoked_tokens (journalist_id, token)\n                 VALUES (:journalist_id, :token)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    def load_data(self):\n        with self.app.app_context():\n            self.journalist_id = self.create_journalist()\n            self.add_revoked_token()\n\n    def check_upgrade(self):\n        with self.app.app_context():\n            sql = \"SELECT session_nonce FROM journalists WHERE id = :id\"\n            params = {\"id\": self.journalist_id}\n            try:\n                db.engine.execute(text(sql), **params).fetchall()\n            except OperationalError:\n                pass\n            sql = \"SELECT * FROM revoked_tokens WHERE id = :id\"\n            try:\n                db.engine.execute(text(sql), **params).fetchall()\n            except OperationalError:\n                pass",
            "file": "migration_c5a02eb52f2d.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        Helper.__init__(self)\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_c5a02eb52f2d.py"
          },
          {
            "type": "function",
            "name": "create_journalist",
            "code": "def create_journalist(self):\n        params = {\n            \"uuid\": str(uuid.uuid4()),\n            \"username\": random_chars(50),\n            \"nonce\": random.randint(20, 100),\n        }\n        sql = \"\"\"INSERT INTO journalists (uuid, username, session_nonce)\n                 VALUES (:uuid, :username, :nonce)\"\"\"\n        return db.engine.execute(text(sql), **params).lastrowid",
            "file": "migration_c5a02eb52f2d.py"
          },
          {
            "type": "function",
            "name": "add_revoked_token",
            "code": "def add_revoked_token(self):\n        params = {\n            \"journalist_id\": self.journalist_id,\n            \"token\": \"abc123\",\n        }\n        sql = \"\"\"INSERT INTO revoked_tokens (journalist_id, token)\n                 VALUES (:journalist_id, :token)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_c5a02eb52f2d.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            self.journalist_id = self.create_journalist()\n            self.add_revoked_token()",
            "file": "migration_c5a02eb52f2d.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        with self.app.app_context():\n            sql = \"SELECT session_nonce FROM journalists WHERE id = :id\"\n            params = {\"id\": self.journalist_id}\n            try:\n                db.engine.execute(text(sql), **params).fetchall()\n            except OperationalError:\n                pass\n            sql = \"SELECT * FROM revoked_tokens WHERE id = :id\"\n            try:\n                db.engine.execute(text(sql), **params).fetchall()\n            except OperationalError:\n                pass",
            "file": "migration_c5a02eb52f2d.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester(Helper):\n    def __init__(self, config):\n        Helper.__init__(self)\n        self.config = config\n        self.app = create_app(config)\n\n    def create_journalist(self):\n        params = {\"uuid\": str(uuid.uuid4()), \"username\": random_chars(50)}\n        sql = \"\"\"INSERT INTO journalists (uuid, username)\n                 VALUES (:uuid, :username)\"\"\"\n        return db.engine.execute(text(sql), **params).lastrowid\n\n    def load_data(self):\n        with self.app.app_context():\n            self.journalist_id = self.create_journalist()\n\n    def check_downgrade(self):\n        with self.app.app_context():\n            sql = \"SELECT session_nonce FROM journalists WHERE id = :id\"\n            params = {\"id\": self.journalist_id}\n            res = db.engine.execute(text(sql), **params).fetchone()\n            assert isinstance(res[\"session_nonce\"], int)\n            sql = \"\"\"INSERT INTO revoked_tokens (journalist_id, token)\n                   VALUES (:journalist_id, :token)\"\"\"\n            params = {\"journalist_id\": self.journalist_id, \"token\": \"abc789\"}\n            res = db.engine.execute(text(sql), **params).lastrowid\n            assert isinstance(res, int)",
            "file": "migration_c5a02eb52f2d.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        Helper.__init__(self)\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_c5a02eb52f2d.py"
          },
          {
            "type": "function",
            "name": "create_journalist",
            "code": "def create_journalist(self):\n        params = {\"uuid\": str(uuid.uuid4()), \"username\": random_chars(50)}\n        sql = \"\"\"INSERT INTO journalists (uuid, username)\n                 VALUES (:uuid, :username)\"\"\"\n        return db.engine.execute(text(sql), **params).lastrowid",
            "file": "migration_c5a02eb52f2d.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            self.journalist_id = self.create_journalist()",
            "file": "migration_c5a02eb52f2d.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        with self.app.app_context():\n            sql = \"SELECT session_nonce FROM journalists WHERE id = :id\"\n            params = {\"id\": self.journalist_id}\n            res = db.engine.execute(text(sql), **params).fetchone()\n            assert isinstance(res[\"session_nonce\"], int)\n            sql = \"\"\"INSERT INTO revoked_tokens (journalist_id, token)\n                   VALUES (:journalist_id, :token)\"\"\"\n            params = {\"journalist_id\": self.journalist_id, \"token\": \"abc789\"}\n            res = db.engine.execute(text(sql), **params).lastrowid\n            assert isinstance(res, int)",
            "file": "migration_c5a02eb52f2d.py"
          }
        ],
        "migration_48a75abc0121.py": [
          {
            "type": "class",
            "name": "Helper",
            "code": "class Helper:\n    @staticmethod\n    def add_source():\n        filesystem_id = random_chars(96) if random_bool() else None\n        params = {\n            \"uuid\": str(uuid4()),\n            \"filesystem_id\": filesystem_id,\n            \"journalist_designation\": random_chars(50),\n            \"flagged\": bool_or_none(),\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": bool_or_none(),\n            \"interaction_count\": random.randint(0, 1000),\n        }\n        sql = \"\"\"\n        INSERT INTO sources (\n            uuid,\n            filesystem_id,\n            journalist_designation,\n            flagged,\n            last_updated,\n            pending,\n            interaction_count\n        )\n        VALUES (\n            :uuid,\n            :filesystem_id,\n            :journalist_designation,\n            :flagged,\n            :last_updated,\n            :pending,\n            :interaction_count\n        )\n        \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    @staticmethod\n    def add_journalist():\n        if random_bool():\n            otp_secret = random_chars(16, string.ascii_uppercase + \"234567\")\n        else:\n            otp_secret = None\n\n        is_totp = random_bool()\n        if is_totp:\n            hotp_counter = 0 if random_bool() else None\n        else:\n            hotp_counter = random.randint(0, 10000) if random_bool() else None\n\n        last_token = random_chars(6, string.digits) if random_bool() else None\n\n        params = {\n            \"uuid\": str(uuid4()),\n            \"username\": random_username(),\n            \"session_nonce\": 0,\n            \"pw_salt\": random_bytes(1, 64, nullable=True),\n            \"pw_hash\": random_bytes(32, 64, nullable=True),\n            \"is_admin\": bool_or_none(),\n            \"otp_secret\": otp_secret,\n            \"is_totp\": is_totp,\n            \"hotp_counter\": hotp_counter,\n            \"last_token\": last_token,\n            \"created_on\": random_datetime(nullable=True),\n            \"last_access\": random_datetime(nullable=True),\n        }\n        sql = \"\"\"\n        INSERT INTO journalists (\n            uuid,\n            username,\n            session_nonce,\n            pw_salt,\n            pw_hash,\n            is_admin,\n            otp_secret,\n            is_totp,\n            hotp_counter,\n            last_token,\n            created_on,\n            last_access\n        )\n        VALUES (\n            :uuid,\n            :username,\n            :session_nonce,\n            :pw_salt,\n            :pw_hash,\n            :is_admin,\n            :otp_secret,\n            :is_totp,\n            :hotp_counter,\n            :last_token,\n            :created_on,\n            :last_access\n        );\n        \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    @staticmethod\n    def add_reply(journalist_id, source_id):\n        params = {\n            \"uuid\": str(uuid4()),\n            \"journalist_id\": journalist_id,\n            \"source_id\": source_id,\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"deleted_by_source\": 0,\n        }\n        sql = \"\"\"\n        INSERT INTO replies (uuid, journalist_id, source_id, filename, size, deleted_by_source)\n        VALUES (:uuid, :journalist_id, :source_id, :filename, :size, :deleted_by_source)\n        \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    @staticmethod\n    def add_message(source_id):\n        params = {\n            \"uuid\": str(uuid4()),\n            \"source_id\": source_id,\n            \"filename\": random_chars(50) + \"-msg.gpg\",\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"downloaded\": secrets.choice([True, False]),\n        }\n        sql = \"\"\"\n        INSERT INTO submissions (uuid, source_id, filename, size, downloaded)\n        VALUES (:uuid, :source_id, :filename, :size, :downloaded)\n        \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    @staticmethod\n    def add_file(source_id):\n        params = {\n            \"uuid\": str(uuid4()),\n            \"source_id\": source_id,\n            \"filename\": random_chars(50) + \"-doc.gz.gpg\",\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"downloaded\": secrets.choice([True, False]),\n            \"checksum\": \"sha256:\" + random_chars(64),\n        }\n        sql = \"\"\"\n        INSERT INTO submissions (uuid, source_id, filename, size, downloaded, checksum)\n        VALUES (:uuid, :source_id, :filename, :size, :downloaded, :checksum)\n        \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    @staticmethod\n    def mark_reply_as_seen(reply_id, journalist_id):\n        params = {\n            \"reply_id\": reply_id,\n            \"journalist_id\": journalist_id,\n        }\n        sql = \"\"\"\n        INSERT INTO seen_replies (reply_id, journalist_id)\n        VALUES (:reply_id, :journalist_id)\n        \"\"\"\n        try:\n            db.engine.execute(text(sql), **params)\n        except IntegrityError:\n            pass\n\n    @staticmethod\n    def mark_file_as_seen(file_id, journalist_id):\n        params = {\n            \"file_id\": file_id,\n            \"journalist_id\": journalist_id,\n        }\n        sql = \"\"\"\n        INSERT INTO seen_files (file_id, journalist_id)\n        VALUES (:file_id, :journalist_id)\n        \"\"\"\n        try:\n            db.engine.execute(text(sql), **params)\n        except IntegrityError:\n            pass\n\n    @staticmethod\n    def mark_message_as_seen(message_id, journalist_id):\n        params = {\n            \"message_id\": message_id,\n            \"journalist_id\": journalist_id,\n        }\n        sql = \"\"\"\n        INSERT INTO seen_messages (message_id, journalist_id)\n        VALUES (:message_id, :journalist_id)\n        \"\"\"\n        try:\n            db.engine.execute(text(sql), **params)\n        except IntegrityError:\n            pass",
            "file": "migration_48a75abc0121.py"
          },
          {
            "type": "function",
            "name": "add_source",
            "code": "def add_source():\n        filesystem_id = random_chars(96) if random_bool() else None\n        params = {\n            \"uuid\": str(uuid4()),\n            \"filesystem_id\": filesystem_id,\n            \"journalist_designation\": random_chars(50),\n            \"flagged\": bool_or_none(),\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": bool_or_none(),\n            \"interaction_count\": random.randint(0, 1000),\n        }\n        sql = \"\"\"\n        INSERT INTO sources (\n            uuid,\n            filesystem_id,\n            journalist_designation,\n            flagged,\n            last_updated,\n            pending,\n            interaction_count\n        )\n        VALUES (\n            :uuid,\n            :filesystem_id,\n            :journalist_designation,\n            :flagged,\n            :last_updated,\n            :pending,\n            :interaction_count\n        )\n        \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_48a75abc0121.py"
          },
          {
            "type": "function",
            "name": "add_journalist",
            "code": "def add_journalist():\n        if random_bool():\n            otp_secret = random_chars(16, string.ascii_uppercase + \"234567\")\n        else:\n            otp_secret = None\n\n        is_totp = random_bool()\n        if is_totp:\n            hotp_counter = 0 if random_bool() else None\n        else:\n            hotp_counter = random.randint(0, 10000) if random_bool() else None\n\n        last_token = random_chars(6, string.digits) if random_bool() else None\n\n        params = {\n            \"uuid\": str(uuid4()),\n            \"username\": random_username(),\n            \"session_nonce\": 0,\n            \"pw_salt\": random_bytes(1, 64, nullable=True),\n            \"pw_hash\": random_bytes(32, 64, nullable=True),\n            \"is_admin\": bool_or_none(),\n            \"otp_secret\": otp_secret,\n            \"is_totp\": is_totp,\n            \"hotp_counter\": hotp_counter,\n            \"last_token\": last_token,\n            \"created_on\": random_datetime(nullable=True),\n            \"last_access\": random_datetime(nullable=True),\n        }\n        sql = \"\"\"\n        INSERT INTO journalists (\n            uuid,\n            username,\n            session_nonce,\n            pw_salt,\n            pw_hash,\n            is_admin,\n            otp_secret,\n            is_totp,\n            hotp_counter,\n            last_token,\n            created_on,\n            last_access\n        )\n        VALUES (\n            :uuid,\n            :username,\n            :session_nonce,\n            :pw_salt,\n            :pw_hash,\n            :is_admin,\n            :otp_secret,\n            :is_totp,\n            :hotp_counter,\n            :last_token,\n            :created_on,\n            :last_access\n        );\n        \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_48a75abc0121.py"
          },
          {
            "type": "function",
            "name": "add_reply",
            "code": "def add_reply(journalist_id, source_id):\n        params = {\n            \"uuid\": str(uuid4()),\n            \"journalist_id\": journalist_id,\n            \"source_id\": source_id,\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"deleted_by_source\": 0,\n        }\n        sql = \"\"\"\n        INSERT INTO replies (uuid, journalist_id, source_id, filename, size, deleted_by_source)\n        VALUES (:uuid, :journalist_id, :source_id, :filename, :size, :deleted_by_source)\n        \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_48a75abc0121.py"
          },
          {
            "type": "function",
            "name": "add_message",
            "code": "def add_message(source_id):\n        params = {\n            \"uuid\": str(uuid4()),\n            \"source_id\": source_id,\n            \"filename\": random_chars(50) + \"-msg.gpg\",\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"downloaded\": secrets.choice([True, False]),\n        }\n        sql = \"\"\"\n        INSERT INTO submissions (uuid, source_id, filename, size, downloaded)\n        VALUES (:uuid, :source_id, :filename, :size, :downloaded)\n        \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_48a75abc0121.py"
          },
          {
            "type": "function",
            "name": "add_file",
            "code": "def add_file(source_id):\n        params = {\n            \"uuid\": str(uuid4()),\n            \"source_id\": source_id,\n            \"filename\": random_chars(50) + \"-doc.gz.gpg\",\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"downloaded\": secrets.choice([True, False]),\n            \"checksum\": \"sha256:\" + random_chars(64),\n        }\n        sql = \"\"\"\n        INSERT INTO submissions (uuid, source_id, filename, size, downloaded, checksum)\n        VALUES (:uuid, :source_id, :filename, :size, :downloaded, :checksum)\n        \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_48a75abc0121.py"
          },
          {
            "type": "function",
            "name": "mark_reply_as_seen",
            "code": "def mark_reply_as_seen(reply_id, journalist_id):\n        params = {\n            \"reply_id\": reply_id,\n            \"journalist_id\": journalist_id,\n        }\n        sql = \"\"\"\n        INSERT INTO seen_replies (reply_id, journalist_id)\n        VALUES (:reply_id, :journalist_id)\n        \"\"\"\n        try:\n            db.engine.execute(text(sql), **params)\n        except IntegrityError:\n            pass",
            "file": "migration_48a75abc0121.py"
          },
          {
            "type": "function",
            "name": "mark_file_as_seen",
            "code": "def mark_file_as_seen(file_id, journalist_id):\n        params = {\n            \"file_id\": file_id,\n            \"journalist_id\": journalist_id,\n        }\n        sql = \"\"\"\n        INSERT INTO seen_files (file_id, journalist_id)\n        VALUES (:file_id, :journalist_id)\n        \"\"\"\n        try:\n            db.engine.execute(text(sql), **params)\n        except IntegrityError:\n            pass",
            "file": "migration_48a75abc0121.py"
          },
          {
            "type": "function",
            "name": "mark_message_as_seen",
            "code": "def mark_message_as_seen(message_id, journalist_id):\n        params = {\n            \"message_id\": message_id,\n            \"journalist_id\": journalist_id,\n        }\n        sql = \"\"\"\n        INSERT INTO seen_messages (message_id, journalist_id)\n        VALUES (:message_id, :journalist_id)\n        \"\"\"\n        try:\n            db.engine.execute(text(sql), **params)\n        except IntegrityError:\n            pass",
            "file": "migration_48a75abc0121.py"
          },
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester(Helper):\n    \"\"\"\n    This migration verifies that the seen_files, seen_messages, and seen_replies association tables\n    now exist, and that the data migration completes successfully.\n    \"\"\"\n\n    JOURNO_NUM = 20\n    SOURCE_NUM = 20\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.SOURCE_NUM):\n                self.add_source()\n\n            for _ in range(self.JOURNO_NUM):\n                self.add_journalist()\n\n            for i in range(self.SOURCE_NUM):\n                # add 1-3 messages from each source, some messages are set to downloaded\n                for _ in range(random.randint(1, 3)):\n                    self.add_message(i)\n                # add 0-2 files from each source, some files are set to downloaded\n                for _ in range(random.randint(0, 2)):\n                    self.add_file(i)\n\n            # add 30 replies from randomly-selected journalists to randomly-selected sources\n            for _i in range(30):\n                selected_journo = random.randint(0, self.JOURNO_NUM)\n                selected_source = random.randint(0, self.SOURCE_NUM)\n                self.add_reply(selected_journo, selected_source)\n\n    def check_upgrade(self):\n        with self.app.app_context():\n            sql = \"SELECT * FROM submissions\"\n            submissions = db.engine.execute(text(sql)).fetchall()\n\n            sql = \"SELECT * FROM replies\"\n            replies = db.engine.execute(text(sql)).fetchall()\n\n            # Now seen tables exist, so you should be able to mark some files, messages, and replies\n            # as seen\n            for submission in submissions:\n                if submission.filename.endswith(\"-doc.gz.gpg\") and secrets.choice([0, 1]):\n                    selected_journo_id = random.randint(0, self.JOURNO_NUM)\n                    self.mark_file_as_seen(submission.id, selected_journo_id)\n                elif secrets.choice([0, 1]):\n                    selected_journo_id = random.randint(0, self.JOURNO_NUM)\n                    self.mark_message_as_seen(submission.id, selected_journo_id)\n            for reply in replies:\n                if secrets.choice([0, 1]):\n                    selected_journo_id = random.randint(0, self.JOURNO_NUM)\n                    self.mark_reply_as_seen(reply.id, selected_journo_id)\n\n            # Check unique constraint on (reply_id, journalist_id)\n            params = {\"reply_id\": 100, \"journalist_id\": 100}\n            sql = \"\"\"\n            INSERT INTO seen_replies (reply_id, journalist_id)\n            VALUES (:reply_id, :journalist_id);\n            \"\"\"\n            db.engine.execute(text(sql), **params)\n            with pytest.raises(IntegrityError):\n                db.engine.execute(text(sql), **params)\n\n            # Check unique constraint on (message_id, journalist_id)\n            params = {\"message_id\": 100, \"journalist_id\": 100}\n            sql = \"\"\"\n            INSERT INTO seen_messages (message_id, journalist_id)\n            VALUES (:message_id, :journalist_id);\n            \"\"\"\n            db.engine.execute(text(sql), **params)\n            with pytest.raises(IntegrityError):\n                db.engine.execute(text(sql), **params)\n\n            # Check unique constraint on (file_id, journalist_id)\n            params = {\"file_id\": 101, \"journalist_id\": 100}\n            sql = \"\"\"\n            INSERT INTO seen_files (file_id, journalist_id)\n            VALUES (:file_id, :journalist_id);\n            \"\"\"\n            db.engine.execute(text(sql), **params)\n            with pytest.raises(IntegrityError):\n                db.engine.execute(text(sql), **params)",
            "file": "migration_48a75abc0121.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_48a75abc0121.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.SOURCE_NUM):\n                self.add_source()\n\n            for _ in range(self.JOURNO_NUM):\n                self.add_journalist()\n\n            for i in range(self.SOURCE_NUM):\n                # add 1-3 messages from each source, some messages are set to downloaded\n                for _ in range(random.randint(1, 3)):\n                    self.add_message(i)\n                # add 0-2 files from each source, some files are set to downloaded\n                for _ in range(random.randint(0, 2)):\n                    self.add_file(i)\n\n            # add 30 replies from randomly-selected journalists to randomly-selected sources\n            for _i in range(30):\n                selected_journo = random.randint(0, self.JOURNO_NUM)\n                selected_source = random.randint(0, self.SOURCE_NUM)\n                self.add_reply(selected_journo, selected_source)",
            "file": "migration_48a75abc0121.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        with self.app.app_context():\n            sql = \"SELECT * FROM submissions\"\n            submissions = db.engine.execute(text(sql)).fetchall()\n\n            sql = \"SELECT * FROM replies\"\n            replies = db.engine.execute(text(sql)).fetchall()\n\n            # Now seen tables exist, so you should be able to mark some files, messages, and replies\n            # as seen\n            for submission in submissions:\n                if submission.filename.endswith(\"-doc.gz.gpg\") and secrets.choice([0, 1]):\n                    selected_journo_id = random.randint(0, self.JOURNO_NUM)\n                    self.mark_file_as_seen(submission.id, selected_journo_id)\n                elif secrets.choice([0, 1]):\n                    selected_journo_id = random.randint(0, self.JOURNO_NUM)\n                    self.mark_message_as_seen(submission.id, selected_journo_id)\n            for reply in replies:\n                if secrets.choice([0, 1]):\n                    selected_journo_id = random.randint(0, self.JOURNO_NUM)\n                    self.mark_reply_as_seen(reply.id, selected_journo_id)\n\n            # Check unique constraint on (reply_id, journalist_id)\n            params = {\"reply_id\": 100, \"journalist_id\": 100}\n            sql = \"\"\"\n            INSERT INTO seen_replies (reply_id, journalist_id)\n            VALUES (:reply_id, :journalist_id);\n            \"\"\"\n            db.engine.execute(text(sql), **params)\n            with pytest.raises(IntegrityError):\n                db.engine.execute(text(sql), **params)\n\n            # Check unique constraint on (message_id, journalist_id)\n            params = {\"message_id\": 100, \"journalist_id\": 100}\n            sql = \"\"\"\n            INSERT INTO seen_messages (message_id, journalist_id)\n            VALUES (:message_id, :journalist_id);\n            \"\"\"\n            db.engine.execute(text(sql), **params)\n            with pytest.raises(IntegrityError):\n                db.engine.execute(text(sql), **params)\n\n            # Check unique constraint on (file_id, journalist_id)\n            params = {\"file_id\": 101, \"journalist_id\": 100}\n            sql = \"\"\"\n            INSERT INTO seen_files (file_id, journalist_id)\n            VALUES (:file_id, :journalist_id);\n            \"\"\"\n            db.engine.execute(text(sql), **params)\n            with pytest.raises(IntegrityError):\n                db.engine.execute(text(sql), **params)",
            "file": "migration_48a75abc0121.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester(Helper):\n    \"\"\"\n    This migration verifies that the seen_files, seen_messages, and seen_replies association tables\n    are removed.\n    \"\"\"\n\n    JOURNO_NUM = 20\n    SOURCE_NUM = 20\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.SOURCE_NUM):\n                self.add_source()\n\n            for _ in range(self.JOURNO_NUM):\n                self.add_journalist()\n\n            for i in range(self.SOURCE_NUM):\n                # add 1-3 messages from each source, some messages are set to downloaded\n                for _ in range(random.randint(1, 3)):\n                    self.add_message(i)\n                # add 0-2 files from each source, some files are set to downloaded\n                for _ in range(random.randint(0, 2)):\n                    self.add_file(i)\n\n            # add 30 replies from randomly-selected journalists to randomly-selected sources\n            for _i in range(30):\n                selected_journo = random.randint(0, self.JOURNO_NUM)\n                selected_source = random.randint(0, self.SOURCE_NUM)\n                self.add_reply(selected_journo, selected_source)\n\n            # mark some files, messages, and replies as seen\n            sql = \"SELECT * FROM submissions\"\n            submissions = db.engine.execute(text(sql)).fetchall()\n            for submission in submissions:\n                if submission.filename.endswith(\"-doc.gz.gpg\") and secrets.choice([0, 1]):\n                    selected_journo_id = random.randint(0, self.JOURNO_NUM)\n                    self.mark_file_as_seen(submission.id, selected_journo_id)\n                elif secrets.choice([0, 1]):\n                    selected_journo_id = random.randint(0, self.JOURNO_NUM)\n                    self.mark_message_as_seen(submission.id, selected_journo_id)\n\n            sql = \"SELECT * FROM replies\"\n            replies = db.engine.execute(text(sql)).fetchall()\n            for reply in replies:\n                if secrets.choice([0, 1]):\n                    selected_journo_id = random.randint(0, self.JOURNO_NUM)\n                    self.mark_reply_as_seen(reply.id, selected_journo_id)\n\n            # Mark some files, messages, and replies as seen\n            for submission in submissions:\n                if submission.filename.endswith(\"-doc.gz.gpg\") and secrets.choice([0, 1]):\n                    selected_journo_id = random.randint(0, self.JOURNO_NUM)\n                    self.mark_file_as_seen(submission.id, selected_journo_id)\n                elif secrets.choice([0, 1]):\n                    selected_journo_id = random.randint(0, self.JOURNO_NUM)\n                    self.mark_message_as_seen(submission.id, selected_journo_id)\n            for reply in replies:\n                if secrets.choice([0, 1]):\n                    selected_journo_id = random.randint(0, self.JOURNO_NUM)\n                    self.mark_reply_as_seen(reply.id, selected_journo_id)\n\n    def check_downgrade(self):\n        with self.app.app_context():\n            # Check that seen tables no longer exist\n            params = {\"table_name\": \"seen_files\"}\n            sql = \"SELECT name FROM sqlite_master WHERE type='table' AND name=:table_name;\"\n            seen_files_exists = db.engine.execute(text(sql), **params).fetchall()\n            assert not seen_files_exists\n            params = {\"table_name\": \"seen_messages\"}\n            sql = \"SELECT name FROM sqlite_master WHERE type='table' AND name=:table_name;\"\n            seen_messages_exists = db.engine.execute(text(sql), **params).fetchall()\n            assert not seen_messages_exists\n            params = {\"table_name\": \"seen_replies\"}\n            sql = \"SELECT name FROM sqlite_master WHERE type='table' AND name=:table_name;\"\n            seen_replies_exists = db.engine.execute(text(sql), **params).fetchall()\n            assert not seen_replies_exists",
            "file": "migration_48a75abc0121.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_48a75abc0121.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.SOURCE_NUM):\n                self.add_source()\n\n            for _ in range(self.JOURNO_NUM):\n                self.add_journalist()\n\n            for i in range(self.SOURCE_NUM):\n                # add 1-3 messages from each source, some messages are set to downloaded\n                for _ in range(random.randint(1, 3)):\n                    self.add_message(i)\n                # add 0-2 files from each source, some files are set to downloaded\n                for _ in range(random.randint(0, 2)):\n                    self.add_file(i)\n\n            # add 30 replies from randomly-selected journalists to randomly-selected sources\n            for _i in range(30):\n                selected_journo = random.randint(0, self.JOURNO_NUM)\n                selected_source = random.randint(0, self.SOURCE_NUM)\n                self.add_reply(selected_journo, selected_source)\n\n            # mark some files, messages, and replies as seen\n            sql = \"SELECT * FROM submissions\"\n            submissions = db.engine.execute(text(sql)).fetchall()\n            for submission in submissions:\n                if submission.filename.endswith(\"-doc.gz.gpg\") and secrets.choice([0, 1]):\n                    selected_journo_id = random.randint(0, self.JOURNO_NUM)\n                    self.mark_file_as_seen(submission.id, selected_journo_id)\n                elif secrets.choice([0, 1]):\n                    selected_journo_id = random.randint(0, self.JOURNO_NUM)\n                    self.mark_message_as_seen(submission.id, selected_journo_id)\n\n            sql = \"SELECT * FROM replies\"\n            replies = db.engine.execute(text(sql)).fetchall()\n            for reply in replies:\n                if secrets.choice([0, 1]):\n                    selected_journo_id = random.randint(0, self.JOURNO_NUM)\n                    self.mark_reply_as_seen(reply.id, selected_journo_id)\n\n            # Mark some files, messages, and replies as seen\n            for submission in submissions:\n                if submission.filename.endswith(\"-doc.gz.gpg\") and secrets.choice([0, 1]):\n                    selected_journo_id = random.randint(0, self.JOURNO_NUM)\n                    self.mark_file_as_seen(submission.id, selected_journo_id)\n                elif secrets.choice([0, 1]):\n                    selected_journo_id = random.randint(0, self.JOURNO_NUM)\n                    self.mark_message_as_seen(submission.id, selected_journo_id)\n            for reply in replies:\n                if secrets.choice([0, 1]):\n                    selected_journo_id = random.randint(0, self.JOURNO_NUM)\n                    self.mark_reply_as_seen(reply.id, selected_journo_id)",
            "file": "migration_48a75abc0121.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        with self.app.app_context():\n            # Check that seen tables no longer exist\n            params = {\"table_name\": \"seen_files\"}\n            sql = \"SELECT name FROM sqlite_master WHERE type='table' AND name=:table_name;\"\n            seen_files_exists = db.engine.execute(text(sql), **params).fetchall()\n            assert not seen_files_exists\n            params = {\"table_name\": \"seen_messages\"}\n            sql = \"SELECT name FROM sqlite_master WHERE type='table' AND name=:table_name;\"\n            seen_messages_exists = db.engine.execute(text(sql), **params).fetchall()\n            assert not seen_messages_exists\n            params = {\"table_name\": \"seen_replies\"}\n            sql = \"SELECT name FROM sqlite_master WHERE type='table' AND name=:table_name;\"\n            seen_replies_exists = db.engine.execute(text(sql), **params).fetchall()\n            assert not seen_replies_exists",
            "file": "migration_48a75abc0121.py"
          }
        ],
        "migration_60f41bb14d98.py": [
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester:\n    \"\"\"This migration verifies that the session_nonce column now exists, and\n    that the data migration completed successfully.\n    \"\"\"\n\n    JOURNO_NUM = 20\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                self.add_journalist()\n            db.session.commit()\n\n    @staticmethod\n    def add_journalist():\n        if random_bool():\n            otp_secret = random_chars(16, string.ascii_uppercase + \"234567\")\n        else:\n            otp_secret = None\n\n        is_totp = random_bool()\n        if is_totp:\n            hotp_counter = 0 if random_bool() else None\n        else:\n            hotp_counter = random.randint(0, 10000) if random_bool() else None\n\n        last_token = random_chars(6, string.digits) if random_bool() else None\n\n        params = {\n            \"username\": random_username(),\n            \"uuid\": str(uuid.uuid4()),\n            \"first_name\": random_name(),\n            \"last_name\": random_name(),\n            \"pw_salt\": random_bytes(1, 64, nullable=True),\n            \"pw_hash\": random_bytes(32, 64, nullable=True),\n            \"is_admin\": bool_or_none(),\n            \"otp_secret\": otp_secret,\n            \"is_totp\": is_totp,\n            \"hotp_counter\": hotp_counter,\n            \"last_token\": last_token,\n            \"created_on\": random_datetime(nullable=True),\n            \"last_access\": random_datetime(nullable=True),\n            \"passphrase_hash\": random_bytes(32, 64, nullable=True),\n        }\n        sql = \"\"\"INSERT INTO journalists (username, uuid, first_name, last_name,\n        pw_salt, pw_hash, is_admin, otp_secret, is_totp, hotp_counter,\n        last_token, created_on, last_access, passphrase_hash)\n                 VALUES (:username, :uuid, :first_name, :last_name, :pw_salt,\n                    :pw_hash, :is_admin, :otp_secret, :is_totp, :hotp_counter,\n                    :last_token, :created_on, :last_access, :passphrase_hash);\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    def check_upgrade(self):\n        with self.app.app_context():\n            journalists = db.engine.execute(text(\"SELECT * FROM journalists\")).fetchall()\n\n            for journalist in journalists:\n                assert journalist.session_nonce is not None",
            "file": "migration_60f41bb14d98.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_60f41bb14d98.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                self.add_journalist()\n            db.session.commit()",
            "file": "migration_60f41bb14d98.py"
          },
          {
            "type": "function",
            "name": "add_journalist",
            "code": "def add_journalist():\n        if random_bool():\n            otp_secret = random_chars(16, string.ascii_uppercase + \"234567\")\n        else:\n            otp_secret = None\n\n        is_totp = random_bool()\n        if is_totp:\n            hotp_counter = 0 if random_bool() else None\n        else:\n            hotp_counter = random.randint(0, 10000) if random_bool() else None\n\n        last_token = random_chars(6, string.digits) if random_bool() else None\n\n        params = {\n            \"username\": random_username(),\n            \"uuid\": str(uuid.uuid4()),\n            \"first_name\": random_name(),\n            \"last_name\": random_name(),\n            \"pw_salt\": random_bytes(1, 64, nullable=True),\n            \"pw_hash\": random_bytes(32, 64, nullable=True),\n            \"is_admin\": bool_or_none(),\n            \"otp_secret\": otp_secret,\n            \"is_totp\": is_totp,\n            \"hotp_counter\": hotp_counter,\n            \"last_token\": last_token,\n            \"created_on\": random_datetime(nullable=True),\n            \"last_access\": random_datetime(nullable=True),\n            \"passphrase_hash\": random_bytes(32, 64, nullable=True),\n        }\n        sql = \"\"\"INSERT INTO journalists (username, uuid, first_name, last_name,\n        pw_salt, pw_hash, is_admin, otp_secret, is_totp, hotp_counter,\n        last_token, created_on, last_access, passphrase_hash)\n                 VALUES (:username, :uuid, :first_name, :last_name, :pw_salt,\n                    :pw_hash, :is_admin, :otp_secret, :is_totp, :hotp_counter,\n                    :last_token, :created_on, :last_access, :passphrase_hash);\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_60f41bb14d98.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        with self.app.app_context():\n            journalists = db.engine.execute(text(\"SELECT * FROM journalists\")).fetchall()\n\n            for journalist in journalists:\n                assert journalist.session_nonce is not None",
            "file": "migration_60f41bb14d98.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester:\n    JOURNO_NUM = 20\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                self.add_journalist()\n            db.session.commit()\n\n    @staticmethod\n    def add_journalist():\n        if random_bool():\n            otp_secret = random_chars(16, string.ascii_uppercase + \"234567\")\n        else:\n            otp_secret = None\n\n        is_totp = random_bool()\n        if is_totp:\n            hotp_counter = 0 if random_bool() else None\n        else:\n            hotp_counter = random.randint(0, 10000) if random_bool() else None\n\n        last_token = random_chars(6, string.digits) if random_bool() else None\n\n        params = {\n            \"username\": random_username(),\n            \"uuid\": str(uuid.uuid4()),\n            \"first_name\": random_name(),\n            \"last_name\": random_name(),\n            \"pw_salt\": random_bytes(1, 64, nullable=True),\n            \"pw_hash\": random_bytes(32, 64, nullable=True),\n            \"is_admin\": bool_or_none(),\n            \"session_nonce\": random.randint(0, 10000),\n            \"otp_secret\": otp_secret,\n            \"is_totp\": is_totp,\n            \"hotp_counter\": hotp_counter,\n            \"last_token\": last_token,\n            \"created_on\": random_datetime(nullable=True),\n            \"last_access\": random_datetime(nullable=True),\n            \"passphrase_hash\": random_bytes(32, 64, nullable=True),\n        }\n        sql = \"\"\"INSERT INTO journalists (username, uuid, first_name, last_name,\n        pw_salt, pw_hash, is_admin, session_nonce, otp_secret, is_totp,\n        hotp_counter, last_token, created_on, last_access, passphrase_hash)\n                 VALUES (:username, :uuid, :first_name, :last_name, :pw_salt,\n                    :pw_hash, :is_admin, :session_nonce, :otp_secret, :is_totp,\n                    :hotp_counter, :last_token, :created_on, :last_access,\n                    :passphrase_hash);\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    def check_downgrade(self):\n        \"\"\"Verify that the session_nonce column is now gone, but otherwise the\n        table has the expected number of rows.\n        \"\"\"\n        with self.app.app_context():\n            sql = \"SELECT * FROM journalists\"\n            journalists = db.engine.execute(text(sql)).fetchall()\n\n            for journalist in journalists:\n                try:\n                    # This should produce an exception, as the column (should)\n                    # be gone.\n                    assert journalist[\"session_nonce\"] is None\n                except NoSuchColumnError:\n                    pass",
            "file": "migration_60f41bb14d98.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_60f41bb14d98.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                self.add_journalist()\n            db.session.commit()",
            "file": "migration_60f41bb14d98.py"
          },
          {
            "type": "function",
            "name": "add_journalist",
            "code": "def add_journalist():\n        if random_bool():\n            otp_secret = random_chars(16, string.ascii_uppercase + \"234567\")\n        else:\n            otp_secret = None\n\n        is_totp = random_bool()\n        if is_totp:\n            hotp_counter = 0 if random_bool() else None\n        else:\n            hotp_counter = random.randint(0, 10000) if random_bool() else None\n\n        last_token = random_chars(6, string.digits) if random_bool() else None\n\n        params = {\n            \"username\": random_username(),\n            \"uuid\": str(uuid.uuid4()),\n            \"first_name\": random_name(),\n            \"last_name\": random_name(),\n            \"pw_salt\": random_bytes(1, 64, nullable=True),\n            \"pw_hash\": random_bytes(32, 64, nullable=True),\n            \"is_admin\": bool_or_none(),\n            \"session_nonce\": random.randint(0, 10000),\n            \"otp_secret\": otp_secret,\n            \"is_totp\": is_totp,\n            \"hotp_counter\": hotp_counter,\n            \"last_token\": last_token,\n            \"created_on\": random_datetime(nullable=True),\n            \"last_access\": random_datetime(nullable=True),\n            \"passphrase_hash\": random_bytes(32, 64, nullable=True),\n        }\n        sql = \"\"\"INSERT INTO journalists (username, uuid, first_name, last_name,\n        pw_salt, pw_hash, is_admin, session_nonce, otp_secret, is_totp,\n        hotp_counter, last_token, created_on, last_access, passphrase_hash)\n                 VALUES (:username, :uuid, :first_name, :last_name, :pw_salt,\n                    :pw_hash, :is_admin, :session_nonce, :otp_secret, :is_totp,\n                    :hotp_counter, :last_token, :created_on, :last_access,\n                    :passphrase_hash);\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_60f41bb14d98.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        \"\"\"Verify that the session_nonce column is now gone, but otherwise the\n        table has the expected number of rows.\n        \"\"\"\n        with self.app.app_context():\n            sql = \"SELECT * FROM journalists\"\n            journalists = db.engine.execute(text(sql)).fetchall()\n\n            for journalist in journalists:\n                try:\n                    # This should produce an exception, as the column (should)\n                    # be gone.\n                    assert journalist[\"session_nonce\"] is None\n                except NoSuchColumnError:\n                    pass",
            "file": "migration_60f41bb14d98.py"
          }
        ],
        "migration_2e24fc7536e8.py": [
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester:\n    \"\"\"Insert a Reply, SeenReply and JournalistLoginAttempt with journalist_id=NULL.\n    Verify that the first two are reassociated to the \"Deleted\" user, while the last\n    is deleted outright.\n    \"\"\"\n\n    def __init__(self, config):\n        \"\"\"This function MUST accept an argument named `config`.\n        You will likely want to save a reference to the config in your\n        class, so you can access the database later.\n        \"\"\"\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        \"\"\"This function loads data into the database and filesystem. It is\n        executed before the upgrade.\n        \"\"\"\n        with self.app.app_context():\n            params = {\n                \"uuid\": str(uuid.uuid4()),\n                \"journalist_id\": None,\n                \"source_id\": 0,\n                \"filename\": \"dummy.txt\",\n                \"size\": 1,\n                \"checksum\": \"\",\n                \"deleted_by_source\": False,\n            }\n            sql = \"\"\"\\\n                INSERT INTO replies (uuid, journalist_id, source_id, filename,\n                    size, checksum, deleted_by_source)\n                 VALUES (:uuid, :journalist_id, :source_id, :filename,\n                        :size, :checksum, :deleted_by_source);\"\"\"\n            db.engine.execute(text(sql), **params)\n            # Insert two SeenReply instances corresponding to the just-inserted reply, which also\n            # verifies our handling of duplicate keys.\n            for _ in range(2):\n                db.engine.execute(\n                    text(\n                        \"\"\"\\\n                    INSERT INTO seen_replies (reply_id, journalist_id)\n                    VALUES (1, NULL);\n                    \"\"\"\n                    )\n                )\n            # Insert a JournalistLoginAttempt\n            db.engine.execute(\n                text(\n                    \"\"\"\\\n                INSERT INTO journalist_login_attempt (timestamp, journalist_id)\n                VALUES (:timestamp, NULL)\n                \"\"\"\n                ),\n                timestamp=random_datetime(nullable=False),\n            )\n\n    def check_upgrade(self):\n        \"\"\"This function is run after the upgrade and verifies the state\n        of the database or filesystem. It MUST raise an exception if the\n        check fails.\n        \"\"\"\n        with self.app.app_context():\n            deleted = db.engine.execute(\n                'SELECT id, passphrase_hash, otp_secret FROM journalists WHERE username=\"deleted\"'\n            ).first()\n            # A passphrase_hash is set\n            assert deleted[1].startswith(\"$argon2\")\n            # And a TOTP secret is set\n            assert len(deleted[2]) == 32\n            deleted_id = deleted[0]\n            replies = db.engine.execute(text(\"SELECT journalist_id FROM replies\")).fetchall()\n            assert len(replies) == 1\n            # And the journalist_id matches our \"deleted\" one\n            assert replies[0][0] == deleted_id\n            seen_replies = db.engine.execute(\n                text(\"SELECT journalist_id FROM seen_replies\")\n            ).fetchall()\n            assert len(seen_replies) == 1\n            # And the journalist_id matches our \"deleted\" one\n            assert seen_replies[0][0] == deleted_id\n            login_attempts = db.engine.execute(\n                text(\"SELECT * FROM journalist_login_attempt\")\n            ).fetchall()\n            # The NULL login attempt was deleted outright\n            assert login_attempts == []",
            "file": "migration_2e24fc7536e8.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        \"\"\"This function MUST accept an argument named `config`.\n        You will likely want to save a reference to the config in your\n        class, so you can access the database later.\n        \"\"\"\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_2e24fc7536e8.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        \"\"\"This function loads data into the database and filesystem. It is\n        executed before the upgrade.\n        \"\"\"\n        with self.app.app_context():\n            params = {\n                \"uuid\": str(uuid.uuid4()),\n                \"journalist_id\": None,\n                \"source_id\": 0,\n                \"filename\": \"dummy.txt\",\n                \"size\": 1,\n                \"checksum\": \"\",\n                \"deleted_by_source\": False,\n            }\n            sql = \"\"\"\\\n                INSERT INTO replies (uuid, journalist_id, source_id, filename,\n                    size, checksum, deleted_by_source)\n                 VALUES (:uuid, :journalist_id, :source_id, :filename,\n                        :size, :checksum, :deleted_by_source);\"\"\"\n            db.engine.execute(text(sql), **params)\n            # Insert two SeenReply instances corresponding to the just-inserted reply, which also\n            # verifies our handling of duplicate keys.\n            for _ in range(2):\n                db.engine.execute(\n                    text(\n                        \"\"\"\\\n                    INSERT INTO seen_replies (reply_id, journalist_id)\n                    VALUES (1, NULL);\n                    \"\"\"\n                    )\n                )\n            # Insert a JournalistLoginAttempt\n            db.engine.execute(\n                text(\n                    \"\"\"\\\n                INSERT INTO journalist_login_attempt (timestamp, journalist_id)\n                VALUES (:timestamp, NULL)\n                \"\"\"\n                ),\n                timestamp=random_datetime(nullable=False),\n            )",
            "file": "migration_2e24fc7536e8.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        \"\"\"This function is run after the upgrade and verifies the state\n        of the database or filesystem. It MUST raise an exception if the\n        check fails.\n        \"\"\"\n        with self.app.app_context():\n            deleted = db.engine.execute(\n                'SELECT id, passphrase_hash, otp_secret FROM journalists WHERE username=\"deleted\"'\n            ).first()\n            # A passphrase_hash is set\n            assert deleted[1].startswith(\"$argon2\")\n            # And a TOTP secret is set\n            assert len(deleted[2]) == 32\n            deleted_id = deleted[0]\n            replies = db.engine.execute(text(\"SELECT journalist_id FROM replies\")).fetchall()\n            assert len(replies) == 1\n            # And the journalist_id matches our \"deleted\" one\n            assert replies[0][0] == deleted_id\n            seen_replies = db.engine.execute(\n                text(\"SELECT journalist_id FROM seen_replies\")\n            ).fetchall()\n            assert len(seen_replies) == 1\n            # And the journalist_id matches our \"deleted\" one\n            assert seen_replies[0][0] == deleted_id\n            login_attempts = db.engine.execute(\n                text(\"SELECT * FROM journalist_login_attempt\")\n            ).fetchall()\n            # The NULL login attempt was deleted outright\n            assert login_attempts == []",
            "file": "migration_2e24fc7536e8.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester:\n    \"\"\"Downgrading only makes fields nullable again, which is a\n    non-destructive and safe operation\"\"\"\n\n    def __init__(self, config):\n        \"\"\"This function MUST accept an argument named `config`.\n        You will likely want to save a reference to the config in your\n        class, so you can access the database later.\n        \"\"\"\n        self.config = config\n\n    def load_data(self):\n        \"\"\"This function loads data into the database and filesystem. It is\n        executed before the downgrade.\n        \"\"\"\n\n    def check_downgrade(self):\n        \"\"\"This function is run after the downgrade and verifies the state\n        of the database or filesystem. It MUST raise an exception if the\n        check fails.\n        \"\"\"",
            "file": "migration_2e24fc7536e8.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        \"\"\"This function MUST accept an argument named `config`.\n        You will likely want to save a reference to the config in your\n        class, so you can access the database later.\n        \"\"\"\n        self.config = config",
            "file": "migration_2e24fc7536e8.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        \"\"\"This function loads data into the database and filesystem. It is\n        executed before the downgrade.\n        \"\"\"",
            "file": "migration_2e24fc7536e8.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        \"\"\"This function is run after the downgrade and verifies the state\n        of the database or filesystem. It MUST raise an exception if the\n        check fails.\n        \"\"\"",
            "file": "migration_2e24fc7536e8.py"
          }
        ],
        "migration_faac8092c123.py": [
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester:\n    \"\"\"This migration has no upgrade because it is only the enabling of\n    pragmas which do not affect database contents.\n    \"\"\"\n\n    def __init__(self, config):\n        pass\n\n    def load_data(self):\n        pass\n\n    def check_upgrade(self):\n        pass",
            "file": "migration_faac8092c123.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        pass",
            "file": "migration_faac8092c123.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        pass",
            "file": "migration_faac8092c123.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        pass",
            "file": "migration_faac8092c123.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester:\n    \"\"\"This migration has no downgrade because it is only the enabling of\n    pragmas, so we don't need to test the downgrade.\n    \"\"\"\n\n    def __init__(self, config):\n        pass\n\n    def load_data(self):\n        pass\n\n    def check_downgrade(self):\n        pass",
            "file": "migration_faac8092c123.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        pass",
            "file": "migration_faac8092c123.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        pass",
            "file": "migration_faac8092c123.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        pass",
            "file": "migration_faac8092c123.py"
          }
        ],
        "migration_b7f98cfd6a70.py": [
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester:\n    \"\"\"Insert a source, verify the filesystem_id makes it through untouched\"\"\"\n\n    def __init__(self, config):\n        \"\"\"This function MUST accept an argument named `config`.\n        You will likely want to save a reference to the config in your\n        class, so you can access the database later.\n        \"\"\"\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        \"\"\"This function loads data into the database and filesystem. It is\n        executed before the upgrade.\n        \"\"\"\n        with self.app.app_context():\n            sources = [\n                {\n                    \"uuid\": str(uuid.uuid4()),\n                    \"filesystem_id\": FILESYSTEM_ID,\n                    \"journalist_designation\": \"sunburned arraignment\",\n                    \"interaction_count\": 0,\n                },\n                {\n                    \"uuid\": str(uuid.uuid4()),\n                    \"filesystem_id\": None,\n                    \"journalist_designation\": \"needy transponder\",\n                    \"interaction_count\": 0,\n                },\n            ]\n            sql = \"\"\"\\\n                INSERT INTO sources (uuid, filesystem_id, journalist_designation, interaction_count)\n                VALUES (:uuid, :filesystem_id, :journalist_designation, :interaction_count)\"\"\"\n            for params in sources:\n                db.engine.execute(text(sql), **params)\n            # Insert a source_stars row associated each source\n            for source_id in (1, 2):\n                db.engine.execute(\n                    text(\n                        \"INSERT INTO source_stars (source_id, starred) \"\n                        \"VALUES (:source_id, :starred)\"\n                    ),\n                    {\"source_id\": source_id, \"starred\": True},\n                )\n\n    def check_upgrade(self):\n        \"\"\"This function is run after the upgrade and verifies the state\n        of the database or filesystem. It MUST raise an exception if the\n        check fails.\n        \"\"\"\n        with self.app.app_context():\n            # Verify remaining single source is the one with a non-NULL filesystem_id\n            sources = db.engine.execute(\"SELECT filesystem_id FROM sources\").fetchall()\n            assert len(sources) == 1\n            assert sources[0][0] == FILESYSTEM_ID\n            # Verify that the source_stars #2 row got deleted\n            stars = db.engine.execute(\"SELECT source_id FROM source_stars\").fetchall()\n            assert stars == [(1,)]",
            "file": "migration_b7f98cfd6a70.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        \"\"\"This function MUST accept an argument named `config`.\n        You will likely want to save a reference to the config in your\n        class, so you can access the database later.\n        \"\"\"\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_b7f98cfd6a70.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        \"\"\"This function loads data into the database and filesystem. It is\n        executed before the upgrade.\n        \"\"\"\n        with self.app.app_context():\n            sources = [\n                {\n                    \"uuid\": str(uuid.uuid4()),\n                    \"filesystem_id\": FILESYSTEM_ID,\n                    \"journalist_designation\": \"sunburned arraignment\",\n                    \"interaction_count\": 0,\n                },\n                {\n                    \"uuid\": str(uuid.uuid4()),\n                    \"filesystem_id\": None,\n                    \"journalist_designation\": \"needy transponder\",\n                    \"interaction_count\": 0,\n                },\n            ]\n            sql = \"\"\"\\\n                INSERT INTO sources (uuid, filesystem_id, journalist_designation, interaction_count)\n                VALUES (:uuid, :filesystem_id, :journalist_designation, :interaction_count)\"\"\"\n            for params in sources:\n                db.engine.execute(text(sql), **params)\n            # Insert a source_stars row associated each source\n            for source_id in (1, 2):\n                db.engine.execute(\n                    text(\n                        \"INSERT INTO source_stars (source_id, starred) \"\n                        \"VALUES (:source_id, :starred)\"\n                    ),\n                    {\"source_id\": source_id, \"starred\": True},\n                )",
            "file": "migration_b7f98cfd6a70.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        \"\"\"This function is run after the upgrade and verifies the state\n        of the database or filesystem. It MUST raise an exception if the\n        check fails.\n        \"\"\"\n        with self.app.app_context():\n            # Verify remaining single source is the one with a non-NULL filesystem_id\n            sources = db.engine.execute(\"SELECT filesystem_id FROM sources\").fetchall()\n            assert len(sources) == 1\n            assert sources[0][0] == FILESYSTEM_ID\n            # Verify that the source_stars #2 row got deleted\n            stars = db.engine.execute(\"SELECT source_id FROM source_stars\").fetchall()\n            assert stars == [(1,)]",
            "file": "migration_b7f98cfd6a70.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester:\n    \"\"\"Downgrading only makes fields nullable again, which is a\n    non-destructive and safe operation\"\"\"\n\n    def __init__(self, config):\n        \"\"\"This function MUST accept an argument named `config`.\n        You will likely want to save a reference to the config in your\n        class, so you can access the database later.\n        \"\"\"\n        self.config = config\n\n    def load_data(self):\n        \"\"\"This function loads data into the database and filesystem. It is\n        executed before the downgrade.\n        \"\"\"\n\n    def check_downgrade(self):\n        \"\"\"This function is run after the downgrade and verifies the state\n        of the database or filesystem. It MUST raise an exception if the\n        check fails.\n        \"\"\"",
            "file": "migration_b7f98cfd6a70.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        \"\"\"This function MUST accept an argument named `config`.\n        You will likely want to save a reference to the config in your\n        class, so you can access the database later.\n        \"\"\"\n        self.config = config",
            "file": "migration_b7f98cfd6a70.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        \"\"\"This function loads data into the database and filesystem. It is\n        executed before the downgrade.\n        \"\"\"",
            "file": "migration_b7f98cfd6a70.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        \"\"\"This function is run after the downgrade and verifies the state\n        of the database or filesystem. It MUST raise an exception if the\n        check fails.\n        \"\"\"",
            "file": "migration_b7f98cfd6a70.py"
          }
        ],
        "migration_f2833ac34bb6.py": [
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester:\n    \"\"\"This migration verifies that the UUID column now exists, and that\n    the data migration completed successfully.\n    \"\"\"\n\n    JOURNO_NUM = 20\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                self.add_journalist()\n            db.session.commit()\n\n    @staticmethod\n    def add_journalist():\n        if random_bool():\n            otp_secret = random_chars(16, string.ascii_uppercase + \"234567\")\n        else:\n            otp_secret = None\n\n        is_totp = random_bool()\n        if is_totp:\n            hotp_counter = 0 if random_bool() else None\n        else:\n            hotp_counter = random.randint(0, 10000) if random_bool() else None\n\n        last_token = random_chars(6, string.digits) if random_bool() else None\n\n        params = {\n            \"username\": random_username(),\n            \"pw_salt\": random_bytes(1, 64, nullable=True),\n            \"pw_hash\": random_bytes(32, 64, nullable=True),\n            \"is_admin\": bool_or_none(),\n            \"otp_secret\": otp_secret,\n            \"is_totp\": is_totp,\n            \"hotp_counter\": hotp_counter,\n            \"last_token\": last_token,\n            \"created_on\": random_datetime(nullable=True),\n            \"last_access\": random_datetime(nullable=True),\n            \"passphrase_hash\": random_bytes(32, 64, nullable=True),\n        }\n        sql = \"\"\"INSERT INTO journalists (username, pw_salt, pw_hash,\n                    is_admin, otp_secret, is_totp, hotp_counter, last_token,\n                    created_on, last_access, passphrase_hash)\n                 VALUES (:username, :pw_salt, :pw_hash, :is_admin,\n                    :otp_secret, :is_totp, :hotp_counter, :last_token,\n                    :created_on, :last_access, :passphrase_hash);\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    def check_upgrade(self):\n        with self.app.app_context():\n            journalists = db.engine.execute(text(\"SELECT * FROM journalists\")).fetchall()\n\n            for journalist in journalists:\n                assert journalist.uuid is not None",
            "file": "migration_f2833ac34bb6.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_f2833ac34bb6.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                self.add_journalist()\n            db.session.commit()",
            "file": "migration_f2833ac34bb6.py"
          },
          {
            "type": "function",
            "name": "add_journalist",
            "code": "def add_journalist():\n        if random_bool():\n            otp_secret = random_chars(16, string.ascii_uppercase + \"234567\")\n        else:\n            otp_secret = None\n\n        is_totp = random_bool()\n        if is_totp:\n            hotp_counter = 0 if random_bool() else None\n        else:\n            hotp_counter = random.randint(0, 10000) if random_bool() else None\n\n        last_token = random_chars(6, string.digits) if random_bool() else None\n\n        params = {\n            \"username\": random_username(),\n            \"pw_salt\": random_bytes(1, 64, nullable=True),\n            \"pw_hash\": random_bytes(32, 64, nullable=True),\n            \"is_admin\": bool_or_none(),\n            \"otp_secret\": otp_secret,\n            \"is_totp\": is_totp,\n            \"hotp_counter\": hotp_counter,\n            \"last_token\": last_token,\n            \"created_on\": random_datetime(nullable=True),\n            \"last_access\": random_datetime(nullable=True),\n            \"passphrase_hash\": random_bytes(32, 64, nullable=True),\n        }\n        sql = \"\"\"INSERT INTO journalists (username, pw_salt, pw_hash,\n                    is_admin, otp_secret, is_totp, hotp_counter, last_token,\n                    created_on, last_access, passphrase_hash)\n                 VALUES (:username, :pw_salt, :pw_hash, :is_admin,\n                    :otp_secret, :is_totp, :hotp_counter, :last_token,\n                    :created_on, :last_access, :passphrase_hash);\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_f2833ac34bb6.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        with self.app.app_context():\n            journalists = db.engine.execute(text(\"SELECT * FROM journalists\")).fetchall()\n\n            for journalist in journalists:\n                assert journalist.uuid is not None",
            "file": "migration_f2833ac34bb6.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester:\n    JOURNO_NUM = 20\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                self.add_journalist()\n            db.session.commit()\n\n    @staticmethod\n    def add_journalist():\n        if random_bool():\n            otp_secret = random_chars(16, string.ascii_uppercase + \"234567\")\n        else:\n            otp_secret = None\n\n        is_totp = random_bool()\n        if is_totp:\n            hotp_counter = 0 if random_bool() else None\n        else:\n            hotp_counter = random.randint(0, 10000) if random_bool() else None\n\n        last_token = random_chars(6, string.digits) if random_bool() else None\n\n        params = {\n            \"username\": random_username(),\n            \"uuid\": str(uuid.uuid4()),\n            \"pw_salt\": random_bytes(1, 64, nullable=True),\n            \"pw_hash\": random_bytes(32, 64, nullable=True),\n            \"is_admin\": bool_or_none(),\n            \"otp_secret\": otp_secret,\n            \"is_totp\": is_totp,\n            \"hotp_counter\": hotp_counter,\n            \"last_token\": last_token,\n            \"created_on\": random_datetime(nullable=True),\n            \"last_access\": random_datetime(nullable=True),\n            \"passphrase_hash\": random_bytes(32, 64, nullable=True),\n        }\n        sql = \"\"\"INSERT INTO journalists (username, uuid, pw_salt, pw_hash,\n                    is_admin, otp_secret, is_totp, hotp_counter, last_token,\n                    created_on, last_access, passphrase_hash)\n                 VALUES (:username, :uuid, :pw_salt, :pw_hash, :is_admin,\n                    :otp_secret, :is_totp, :hotp_counter, :last_token,\n                    :created_on, :last_access, :passphrase_hash);\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    def check_downgrade(self):\n        \"\"\"Verify that the UUID column is now gone, but otherwise the table\n        has the expected number of rows.\n        \"\"\"\n        with self.app.app_context():\n            sql = \"SELECT * FROM journalists\"\n            journalists = db.engine.execute(text(sql)).fetchall()\n\n            for journalist in journalists:\n                try:\n                    # This should produce an exception, as the column (should)\n                    # be gone.\n                    assert journalist[\"uuid\"] is None\n                except NoSuchColumnError:\n                    pass",
            "file": "migration_f2833ac34bb6.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_f2833ac34bb6.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                self.add_journalist()\n            db.session.commit()",
            "file": "migration_f2833ac34bb6.py"
          },
          {
            "type": "function",
            "name": "add_journalist",
            "code": "def add_journalist():\n        if random_bool():\n            otp_secret = random_chars(16, string.ascii_uppercase + \"234567\")\n        else:\n            otp_secret = None\n\n        is_totp = random_bool()\n        if is_totp:\n            hotp_counter = 0 if random_bool() else None\n        else:\n            hotp_counter = random.randint(0, 10000) if random_bool() else None\n\n        last_token = random_chars(6, string.digits) if random_bool() else None\n\n        params = {\n            \"username\": random_username(),\n            \"uuid\": str(uuid.uuid4()),\n            \"pw_salt\": random_bytes(1, 64, nullable=True),\n            \"pw_hash\": random_bytes(32, 64, nullable=True),\n            \"is_admin\": bool_or_none(),\n            \"otp_secret\": otp_secret,\n            \"is_totp\": is_totp,\n            \"hotp_counter\": hotp_counter,\n            \"last_token\": last_token,\n            \"created_on\": random_datetime(nullable=True),\n            \"last_access\": random_datetime(nullable=True),\n            \"passphrase_hash\": random_bytes(32, 64, nullable=True),\n        }\n        sql = \"\"\"INSERT INTO journalists (username, uuid, pw_salt, pw_hash,\n                    is_admin, otp_secret, is_totp, hotp_counter, last_token,\n                    created_on, last_access, passphrase_hash)\n                 VALUES (:username, :uuid, :pw_salt, :pw_hash, :is_admin,\n                    :otp_secret, :is_totp, :hotp_counter, :last_token,\n                    :created_on, :last_access, :passphrase_hash);\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_f2833ac34bb6.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        \"\"\"Verify that the UUID column is now gone, but otherwise the table\n        has the expected number of rows.\n        \"\"\"\n        with self.app.app_context():\n            sql = \"SELECT * FROM journalists\"\n            journalists = db.engine.execute(text(sql)).fetchall()\n\n            for journalist in journalists:\n                try:\n                    # This should produce an exception, as the column (should)\n                    # be gone.\n                    assert journalist[\"uuid\"] is None\n                except NoSuchColumnError:\n                    pass",
            "file": "migration_f2833ac34bb6.py"
          }
        ],
        "migration_15ac9509fc68.py": [
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester:\n    \"\"\"This migration has no upgrade because there are no tables in the\n    database prior to running, so there is no data to load or test.\n    \"\"\"\n\n    def __init__(self, config):\n        pass\n\n    def load_data(self):\n        pass\n\n    def check_upgrade(self):\n        pass",
            "file": "migration_15ac9509fc68.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        pass",
            "file": "migration_15ac9509fc68.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        pass",
            "file": "migration_15ac9509fc68.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        pass",
            "file": "migration_15ac9509fc68.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester:\n    JOURNO_NUM = 200\n    SOURCE_NUM = 200\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                self.add_journalist()\n\n            for _ in range(self.SOURCE_NUM):\n                self.add_source()\n\n            for jid in range(1, self.JOURNO_NUM, 10):\n                for _ in range(random.randint(1, 3)):\n                    self.add_journalist_login_attempt(jid)\n\n            for jid in range(1, self.JOURNO_NUM, 10):\n                for sid in range(1, self.SOURCE_NUM, 10):\n                    self.add_reply(jid, sid)\n\n            for sid in range(1, self.SOURCE_NUM, 10):\n                self.add_source_star(sid)\n\n            for sid in range(1, self.SOURCE_NUM, 8):\n                for _ in range(random.randint(1, 3)):\n                    self.add_submission(sid)\n\n            # create \"abandoned\" submissions (issue #1189)\n            for sid in range(self.SOURCE_NUM, self.SOURCE_NUM + 50):\n                self.add_submission(sid)\n\n            db.session.commit()\n\n    @staticmethod\n    def add_journalist():\n        if random_bool():\n            otp_secret = random_chars(16, string.ascii_uppercase + \"234567\")\n        else:\n            otp_secret = None\n\n        is_totp = random_bool()\n        if is_totp:\n            hotp_counter = 0 if random_bool() else None\n        else:\n            hotp_counter = random.randint(0, 10000) if random_bool() else None\n\n        last_token = random_chars(6, string.digits) if random_bool() else None\n\n        params = {\n            \"username\": random_username(),\n            \"pw_salt\": random_bytes(1, 64, nullable=True),\n            \"pw_hash\": random_bytes(32, 64, nullable=True),\n            \"is_admin\": bool_or_none(),\n            \"otp_secret\": otp_secret,\n            \"is_totp\": is_totp,\n            \"hotp_counter\": hotp_counter,\n            \"last_token\": last_token,\n            \"created_on\": random_datetime(nullable=True),\n            \"last_access\": random_datetime(nullable=True),\n        }\n        sql = \"\"\"INSERT INTO journalists (username, pw_salt, pw_hash,\n                    is_admin, otp_secret, is_totp, hotp_counter, last_token,\n                    created_on, last_access)\n                 VALUES (:username, :pw_salt, :pw_hash, :is_admin,\n                    :otp_secret, :is_totp, :hotp_counter, :last_token,\n                    :created_on, :last_access);\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    @staticmethod\n    def add_source():\n        filesystem_id = random_chars(96) if random_bool() else None\n        params = {\n            \"filesystem_id\": filesystem_id,\n            \"journalist_designation\": random_chars(50),\n            \"flagged\": bool_or_none(),\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": bool_or_none(),\n            \"interaction_count\": random.randint(0, 1000),\n        }\n        sql = \"\"\"INSERT INTO sources (filesystem_id, journalist_designation,\n                    flagged, last_updated, pending, interaction_count)\n                 VALUES (:filesystem_id, :journalist_designation, :flagged,\n                    :last_updated, :pending, :interaction_count)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    @staticmethod\n    def add_journalist_login_attempt(journalist_id):\n        params = {\n            \"timestamp\": random_datetime(nullable=True),\n            \"journalist_id\": journalist_id,\n        }\n        sql = \"\"\"INSERT INTO journalist_login_attempt (timestamp,\n                    journalist_id)\n                 VALUES (:timestamp, :journalist_id)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    @staticmethod\n    def add_reply(journalist_id, source_id):\n        params = {\n            \"journalist_id\": journalist_id,\n            \"source_id\": source_id,\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n        }\n        sql = \"\"\"INSERT INTO replies (journalist_id, source_id, filename,\n                    size)\n                 VALUES (:journalist_id, :source_id, :filename, :size)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    @staticmethod\n    def add_source_star(source_id):\n        params = {\n            \"source_id\": source_id,\n            \"starred\": bool_or_none(),\n        }\n        sql = \"\"\"INSERT INTO source_stars (source_id, starred)\n                 VALUES (:source_id, :starred)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    @staticmethod\n    def add_submission(source_id):\n        params = {\n            \"source_id\": source_id,\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"downloaded\": bool_or_none(),\n        }\n        sql = \"\"\"INSERT INTO submissions (source_id, filename, size,\n                    downloaded)\n                 VALUES (:source_id, :filename, :size, :downloaded)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    def check_downgrade(self):\n        \"\"\"We don't need to check anything on this downgrade because the\n        migration drops all the tables. Thus, there is nothing to do.\n        \"\"\"",
            "file": "migration_15ac9509fc68.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_15ac9509fc68.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                self.add_journalist()\n\n            for _ in range(self.SOURCE_NUM):\n                self.add_source()\n\n            for jid in range(1, self.JOURNO_NUM, 10):\n                for _ in range(random.randint(1, 3)):\n                    self.add_journalist_login_attempt(jid)\n\n            for jid in range(1, self.JOURNO_NUM, 10):\n                for sid in range(1, self.SOURCE_NUM, 10):\n                    self.add_reply(jid, sid)\n\n            for sid in range(1, self.SOURCE_NUM, 10):\n                self.add_source_star(sid)\n\n            for sid in range(1, self.SOURCE_NUM, 8):\n                for _ in range(random.randint(1, 3)):\n                    self.add_submission(sid)\n\n            # create \"abandoned\" submissions (issue #1189)\n            for sid in range(self.SOURCE_NUM, self.SOURCE_NUM + 50):\n                self.add_submission(sid)\n\n            db.session.commit()",
            "file": "migration_15ac9509fc68.py"
          },
          {
            "type": "function",
            "name": "add_journalist",
            "code": "def add_journalist():\n        if random_bool():\n            otp_secret = random_chars(16, string.ascii_uppercase + \"234567\")\n        else:\n            otp_secret = None\n\n        is_totp = random_bool()\n        if is_totp:\n            hotp_counter = 0 if random_bool() else None\n        else:\n            hotp_counter = random.randint(0, 10000) if random_bool() else None\n\n        last_token = random_chars(6, string.digits) if random_bool() else None\n\n        params = {\n            \"username\": random_username(),\n            \"pw_salt\": random_bytes(1, 64, nullable=True),\n            \"pw_hash\": random_bytes(32, 64, nullable=True),\n            \"is_admin\": bool_or_none(),\n            \"otp_secret\": otp_secret,\n            \"is_totp\": is_totp,\n            \"hotp_counter\": hotp_counter,\n            \"last_token\": last_token,\n            \"created_on\": random_datetime(nullable=True),\n            \"last_access\": random_datetime(nullable=True),\n        }\n        sql = \"\"\"INSERT INTO journalists (username, pw_salt, pw_hash,\n                    is_admin, otp_secret, is_totp, hotp_counter, last_token,\n                    created_on, last_access)\n                 VALUES (:username, :pw_salt, :pw_hash, :is_admin,\n                    :otp_secret, :is_totp, :hotp_counter, :last_token,\n                    :created_on, :last_access);\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_15ac9509fc68.py"
          },
          {
            "type": "function",
            "name": "add_source",
            "code": "def add_source():\n        filesystem_id = random_chars(96) if random_bool() else None\n        params = {\n            \"filesystem_id\": filesystem_id,\n            \"journalist_designation\": random_chars(50),\n            \"flagged\": bool_or_none(),\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": bool_or_none(),\n            \"interaction_count\": random.randint(0, 1000),\n        }\n        sql = \"\"\"INSERT INTO sources (filesystem_id, journalist_designation,\n                    flagged, last_updated, pending, interaction_count)\n                 VALUES (:filesystem_id, :journalist_designation, :flagged,\n                    :last_updated, :pending, :interaction_count)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_15ac9509fc68.py"
          },
          {
            "type": "function",
            "name": "add_journalist_login_attempt",
            "code": "def add_journalist_login_attempt(journalist_id):\n        params = {\n            \"timestamp\": random_datetime(nullable=True),\n            \"journalist_id\": journalist_id,\n        }\n        sql = \"\"\"INSERT INTO journalist_login_attempt (timestamp,\n                    journalist_id)\n                 VALUES (:timestamp, :journalist_id)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_15ac9509fc68.py"
          },
          {
            "type": "function",
            "name": "add_reply",
            "code": "def add_reply(journalist_id, source_id):\n        params = {\n            \"journalist_id\": journalist_id,\n            \"source_id\": source_id,\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n        }\n        sql = \"\"\"INSERT INTO replies (journalist_id, source_id, filename,\n                    size)\n                 VALUES (:journalist_id, :source_id, :filename, :size)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_15ac9509fc68.py"
          },
          {
            "type": "function",
            "name": "add_source_star",
            "code": "def add_source_star(source_id):\n        params = {\n            \"source_id\": source_id,\n            \"starred\": bool_or_none(),\n        }\n        sql = \"\"\"INSERT INTO source_stars (source_id, starred)\n                 VALUES (:source_id, :starred)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_15ac9509fc68.py"
          },
          {
            "type": "function",
            "name": "add_submission",
            "code": "def add_submission(source_id):\n        params = {\n            \"source_id\": source_id,\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"downloaded\": bool_or_none(),\n        }\n        sql = \"\"\"INSERT INTO submissions (source_id, filename, size,\n                    downloaded)\n                 VALUES (:source_id, :filename, :size, :downloaded)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_15ac9509fc68.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        \"\"\"We don't need to check anything on this downgrade because the\n        migration drops all the tables. Thus, there is nothing to do.\n        \"\"\"",
            "file": "migration_15ac9509fc68.py"
          }
        ],
        "migration_811334d7105f.py": [
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester:\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(self.config)\n        self.uuid = str(uuid.uuid4())\n\n    def load_data(self):\n        \"\"\"Create a source\"\"\"\n        with self.app.app_context():\n            source = {\n                \"uuid\": self.uuid,\n                \"filesystem_id\": \"5678\",\n                \"journalist_designation\": \"alienated licensee\",\n                \"interaction_count\": 0,\n            }\n            sql = \"\"\"\\\n                INSERT INTO sources (uuid, filesystem_id, journalist_designation,\n                    interaction_count)\n                VALUES (:uuid, :filesystem_id, :journalist_designation,\n                    :interaction_count)\"\"\"\n            db.engine.execute(text(sql), **source)\n\n    def check_upgrade(self):\n        \"\"\"Verify PGP fields can be queried and modified\"\"\"\n        with self.app.app_context():\n            query_sql = \"\"\"\\\n            SELECT pgp_fingerprint, pgp_public_key, pgp_secret_key\n            FROM sources\n            WHERE uuid = :uuid\"\"\"\n            source = db.engine.execute(\n                text(query_sql),\n                uuid=self.uuid,\n            ).fetchone()\n            # Fields are set to NULL by default\n            assert source == (None, None, None)\n            update_sql = \"\"\"\\\n            UPDATE sources\n            SET pgp_fingerprint=:pgp_fingerprint, pgp_public_key=:pgp_public_key,\n                pgp_secret_key=:pgp_secret_key\n            WHERE uuid = :uuid\"\"\"\n            db.engine.execute(\n                text(update_sql),\n                pgp_fingerprint=\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\n                pgp_public_key=\"a public key!\",\n                pgp_secret_key=\"a secret key!\",\n                uuid=self.uuid,\n            )\n            source = db.engine.execute(text(query_sql), uuid=self.uuid).fetchone()\n            # Fields are the values we set them to\n            assert source == (\n                \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\n                \"a public key!\",\n                \"a secret key!\",\n            )",
            "file": "migration_811334d7105f.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(self.config)\n        self.uuid = str(uuid.uuid4())",
            "file": "migration_811334d7105f.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        \"\"\"Create a source\"\"\"\n        with self.app.app_context():\n            source = {\n                \"uuid\": self.uuid,\n                \"filesystem_id\": \"5678\",\n                \"journalist_designation\": \"alienated licensee\",\n                \"interaction_count\": 0,\n            }\n            sql = \"\"\"\\\n                INSERT INTO sources (uuid, filesystem_id, journalist_designation,\n                    interaction_count)\n                VALUES (:uuid, :filesystem_id, :journalist_designation,\n                    :interaction_count)\"\"\"\n            db.engine.execute(text(sql), **source)",
            "file": "migration_811334d7105f.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        \"\"\"Verify PGP fields can be queried and modified\"\"\"\n        with self.app.app_context():\n            query_sql = \"\"\"\\\n            SELECT pgp_fingerprint, pgp_public_key, pgp_secret_key\n            FROM sources\n            WHERE uuid = :uuid\"\"\"\n            source = db.engine.execute(\n                text(query_sql),\n                uuid=self.uuid,\n            ).fetchone()\n            # Fields are set to NULL by default\n            assert source == (None, None, None)\n            update_sql = \"\"\"\\\n            UPDATE sources\n            SET pgp_fingerprint=:pgp_fingerprint, pgp_public_key=:pgp_public_key,\n                pgp_secret_key=:pgp_secret_key\n            WHERE uuid = :uuid\"\"\"\n            db.engine.execute(\n                text(update_sql),\n                pgp_fingerprint=\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\n                pgp_public_key=\"a public key!\",\n                pgp_secret_key=\"a secret key!\",\n                uuid=self.uuid,\n            )\n            source = db.engine.execute(text(query_sql), uuid=self.uuid).fetchone()\n            # Fields are the values we set them to\n            assert source == (\n                \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\n                \"a public key!\",\n                \"a secret key!\",\n            )",
            "file": "migration_811334d7105f.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester:\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(self.config)\n        self.uuid = str(uuid.uuid4())\n\n    def load_data(self):\n        \"\"\"Create a source with a PGP key pair stored\"\"\"\n        with self.app.app_context():\n            source = {\n                \"uuid\": self.uuid,\n                \"filesystem_id\": \"1234\",\n                \"journalist_designation\": \"mucky pine\",\n                \"interaction_count\": 0,\n                \"pgp_fingerprint\": \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\n                \"pgp_public_key\": \"very public\",\n                \"pgp_secret_key\": \"very secret\",\n            }\n            sql = \"\"\"\\\n                INSERT INTO sources (uuid, filesystem_id, journalist_designation,\n                    interaction_count, pgp_fingerprint, pgp_public_key, pgp_secret_key)\n                VALUES (:uuid, :filesystem_id, :journalist_designation,\n                    :interaction_count, :pgp_fingerprint, :pgp_public_key, :pgp_secret_key)\"\"\"\n            db.engine.execute(text(sql), **source)\n\n    def check_downgrade(self):\n        \"\"\"Verify the downgrade does nothing, i.e. the PGP fields are still there\"\"\"\n        with self.app.app_context():\n            sql = \"\"\"\\\n            SELECT pgp_fingerprint, pgp_public_key, pgp_secret_key\n            FROM sources\n            WHERE uuid = :uuid\"\"\"\n            source = db.engine.execute(\n                text(sql),\n                uuid=self.uuid,\n            ).fetchone()\n            print(source)\n            assert source == (\n                \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\n                \"very public\",\n                \"very secret\",\n            )",
            "file": "migration_811334d7105f.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(self.config)\n        self.uuid = str(uuid.uuid4())",
            "file": "migration_811334d7105f.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        \"\"\"Create a source with a PGP key pair stored\"\"\"\n        with self.app.app_context():\n            source = {\n                \"uuid\": self.uuid,\n                \"filesystem_id\": \"1234\",\n                \"journalist_designation\": \"mucky pine\",\n                \"interaction_count\": 0,\n                \"pgp_fingerprint\": \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\n                \"pgp_public_key\": \"very public\",\n                \"pgp_secret_key\": \"very secret\",\n            }\n            sql = \"\"\"\\\n                INSERT INTO sources (uuid, filesystem_id, journalist_designation,\n                    interaction_count, pgp_fingerprint, pgp_public_key, pgp_secret_key)\n                VALUES (:uuid, :filesystem_id, :journalist_designation,\n                    :interaction_count, :pgp_fingerprint, :pgp_public_key, :pgp_secret_key)\"\"\"\n            db.engine.execute(text(sql), **source)",
            "file": "migration_811334d7105f.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        \"\"\"Verify the downgrade does nothing, i.e. the PGP fields are still there\"\"\"\n        with self.app.app_context():\n            sql = \"\"\"\\\n            SELECT pgp_fingerprint, pgp_public_key, pgp_secret_key\n            FROM sources\n            WHERE uuid = :uuid\"\"\"\n            source = db.engine.execute(\n                text(sql),\n                uuid=self.uuid,\n            ).fetchone()\n            print(source)\n            assert source == (\n                \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\n                \"very public\",\n                \"very secret\",\n            )",
            "file": "migration_811334d7105f.py"
          }
        ],
        "migration_3d91d6948753.py": [
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester:\n    \"\"\"This migration verifies that the UUID column now exists, and that\n    the data migration completed successfully.\n    \"\"\"\n\n    SOURCE_NUM = 200\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.SOURCE_NUM):\n                self.add_source()\n\n            db.session.commit()\n\n    @staticmethod\n    def add_source():\n        filesystem_id = random_chars(96) if random_bool() else None\n        params = {\n            \"filesystem_id\": filesystem_id,\n            \"journalist_designation\": random_chars(50),\n            \"flagged\": bool_or_none(),\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": bool_or_none(),\n            \"interaction_count\": random.randint(0, 1000),\n        }\n        sql = \"\"\"INSERT INTO sources (filesystem_id, journalist_designation,\n                    flagged, last_updated, pending, interaction_count)\n                 VALUES (:filesystem_id, :journalist_designation, :flagged,\n                    :last_updated, :pending, :interaction_count)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    def check_upgrade(self):\n        with self.app.app_context():\n            sources = db.engine.execute(text(\"SELECT * FROM sources\")).fetchall()\n            assert len(sources) == self.SOURCE_NUM\n\n            for source in sources:\n                assert source.uuid is not None",
            "file": "migration_3d91d6948753.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_3d91d6948753.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.SOURCE_NUM):\n                self.add_source()\n\n            db.session.commit()",
            "file": "migration_3d91d6948753.py"
          },
          {
            "type": "function",
            "name": "add_source",
            "code": "def add_source():\n        filesystem_id = random_chars(96) if random_bool() else None\n        params = {\n            \"filesystem_id\": filesystem_id,\n            \"journalist_designation\": random_chars(50),\n            \"flagged\": bool_or_none(),\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": bool_or_none(),\n            \"interaction_count\": random.randint(0, 1000),\n        }\n        sql = \"\"\"INSERT INTO sources (filesystem_id, journalist_designation,\n                    flagged, last_updated, pending, interaction_count)\n                 VALUES (:filesystem_id, :journalist_designation, :flagged,\n                    :last_updated, :pending, :interaction_count)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_3d91d6948753.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        with self.app.app_context():\n            sources = db.engine.execute(text(\"SELECT * FROM sources\")).fetchall()\n            assert len(sources) == self.SOURCE_NUM\n\n            for source in sources:\n                assert source.uuid is not None",
            "file": "migration_3d91d6948753.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester:\n    SOURCE_NUM = 200\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.SOURCE_NUM):\n                self.add_source()\n\n            db.session.commit()\n\n    @staticmethod\n    def add_source():\n        filesystem_id = random_chars(96) if random_bool() else None\n        params = {\n            \"filesystem_id\": filesystem_id,\n            \"uuid\": str(uuid.uuid4()),\n            \"journalist_designation\": random_chars(50),\n            \"flagged\": bool_or_none(),\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": bool_or_none(),\n            \"interaction_count\": random.randint(0, 1000),\n        }\n        sql = \"\"\"INSERT INTO sources (filesystem_id, uuid,\n                    journalist_designation, flagged, last_updated, pending,\n                    interaction_count)\n                 VALUES (:filesystem_id, :uuid, :journalist_designation,\n                    :flagged, :last_updated, :pending, :interaction_count)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    def check_downgrade(self):\n        \"\"\"Verify that the UUID column is now gone, but otherwise the table\n        has the expected number of rows.\n        \"\"\"\n        with self.app.app_context():\n            sql = \"SELECT * FROM sources\"\n            sources = db.engine.execute(text(sql)).fetchall()\n\n            for source in sources:\n                try:\n                    # This should produce an exception, as the column (should)\n                    # be gone.\n                    assert source[\"uuid\"] is None\n                except NoSuchColumnError:\n                    pass\n\n            assert len(sources) == self.SOURCE_NUM",
            "file": "migration_3d91d6948753.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_3d91d6948753.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.SOURCE_NUM):\n                self.add_source()\n\n            db.session.commit()",
            "file": "migration_3d91d6948753.py"
          },
          {
            "type": "function",
            "name": "add_source",
            "code": "def add_source():\n        filesystem_id = random_chars(96) if random_bool() else None\n        params = {\n            \"filesystem_id\": filesystem_id,\n            \"uuid\": str(uuid.uuid4()),\n            \"journalist_designation\": random_chars(50),\n            \"flagged\": bool_or_none(),\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": bool_or_none(),\n            \"interaction_count\": random.randint(0, 1000),\n        }\n        sql = \"\"\"INSERT INTO sources (filesystem_id, uuid,\n                    journalist_designation, flagged, last_updated, pending,\n                    interaction_count)\n                 VALUES (:filesystem_id, :uuid, :journalist_designation,\n                    :flagged, :last_updated, :pending, :interaction_count)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_3d91d6948753.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        \"\"\"Verify that the UUID column is now gone, but otherwise the table\n        has the expected number of rows.\n        \"\"\"\n        with self.app.app_context():\n            sql = \"SELECT * FROM sources\"\n            sources = db.engine.execute(text(sql)).fetchall()\n\n            for source in sources:\n                try:\n                    # This should produce an exception, as the column (should)\n                    # be gone.\n                    assert source[\"uuid\"] is None\n                except NoSuchColumnError:\n                    pass\n\n            assert len(sources) == self.SOURCE_NUM",
            "file": "migration_3d91d6948753.py"
          }
        ],
        "helpers.py": [
          {
            "type": "function",
            "name": "random_bool",
            "code": "def random_bool():\n    return bool(secrets.randbits(1))",
            "file": "helpers.py"
          },
          {
            "type": "function",
            "name": "bool_or_none",
            "code": "def bool_or_none():\n    return secrets.choice([None, True, False])",
            "file": "helpers.py"
          },
          {
            "type": "function",
            "name": "random_bytes",
            "code": "def random_bytes(min, max, nullable):\n    if nullable and random_bool():\n        return None\n    else:\n        # python2 just wants strings, fix this in python3\n        return random_chars(random.randint(min, max))",
            "file": "helpers.py"
          },
          {
            "type": "function",
            "name": "random_name",
            "code": "def random_name():\n    len = random.randint(1, 100)\n    return random_chars(len)",
            "file": "helpers.py"
          },
          {
            "type": "function",
            "name": "random_username",
            "code": "def random_username():\n    len = random.randint(3, 64)\n    return random_chars(len)",
            "file": "helpers.py"
          },
          {
            "type": "function",
            "name": "random_chars",
            "code": "def random_chars(len, chars=string.printable):\n    return \"\".join([secrets.choice(chars) for _ in range(len)])",
            "file": "helpers.py"
          },
          {
            "type": "function",
            "name": "random_ascii_chars",
            "code": "def random_ascii_chars(len, chars=string.ascii_lowercase):\n    return \"\".join([secrets.choice(chars) for _ in range(len)])",
            "file": "helpers.py"
          },
          {
            "type": "function",
            "name": "random_datetime",
            "code": "def random_datetime(nullable):\n    if nullable and random_bool():\n        return None\n    else:\n        return datetime(\n            year=random.randint(1, 9999),\n            month=random.randint(1, 12),\n            day=random.randint(1, 28),\n            hour=random.randint(0, 23),\n            minute=random.randint(0, 59),\n            second=random.randint(0, 59),\n            microsecond=random.randint(0, 1000),\n        )",
            "file": "helpers.py"
          }
        ],
        "migration_de00920916bf.py": [
          {
            "type": "class",
            "name": "Helper",
            "code": "class Helper:\n    def __init__(self):\n        self.journalist_id = None\n\n    def create_journalist(self):\n        if self.journalist_id is not None:\n            raise RuntimeError(\"Journalist already created\")\n\n        params = {\n            \"uuid\": str(uuid.uuid4()),\n            \"username\": random_chars(50),\n            \"session_nonce\": 0,\n            \"otp_secret\": \"ABCDEFGHIJKLMNOPQRSTUVWXYZ234567\",\n        }\n        sql = \"\"\"INSERT INTO journalists (uuid, username, otp_secret, session_nonce)\n                 VALUES (:uuid, :username, :otp_secret, :session_nonce)\n              \"\"\"\n        self.journalist_id = db.engine.execute(text(sql), **params).lastrowid",
            "file": "migration_de00920916bf.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self):\n        self.journalist_id = None",
            "file": "migration_de00920916bf.py"
          },
          {
            "type": "function",
            "name": "create_journalist",
            "code": "def create_journalist(self):\n        if self.journalist_id is not None:\n            raise RuntimeError(\"Journalist already created\")\n\n        params = {\n            \"uuid\": str(uuid.uuid4()),\n            \"username\": random_chars(50),\n            \"session_nonce\": 0,\n            \"otp_secret\": \"ABCDEFGHIJKLMNOPQRSTUVWXYZ234567\",\n        }\n        sql = \"\"\"INSERT INTO journalists (uuid, username, otp_secret, session_nonce)\n                 VALUES (:uuid, :username, :otp_secret, :session_nonce)\n              \"\"\"\n        self.journalist_id = db.engine.execute(text(sql), **params).lastrowid",
            "file": "migration_de00920916bf.py"
          },
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester(Helper):\n    \"\"\"\n    Checks schema to verify that the otp_secret varchar \"length\" has been updated.\n    Varchar specified length isn't enforced by sqlite but it's good to verify that\n    the migration worked as expected.\n    \"\"\"\n\n    def __init__(self, config):\n        Helper.__init__(self)\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            self.create_journalist()\n\n    def check_upgrade(self):\n        with self.app.app_context():\n            journalists_sql = \"SELECT * FROM journalists\"\n            journalist = db.engine.execute(text(journalists_sql)).first()\n            assert len(journalist[\"otp_secret\"]) == 32  # Varchar ignores length",
            "file": "migration_de00920916bf.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        Helper.__init__(self)\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_de00920916bf.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            self.create_journalist()",
            "file": "migration_de00920916bf.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        with self.app.app_context():\n            journalists_sql = \"SELECT * FROM journalists\"\n            journalist = db.engine.execute(text(journalists_sql)).first()\n            assert len(journalist[\"otp_secret\"]) == 32  # Varchar ignores length",
            "file": "migration_de00920916bf.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester(Helper):\n    def __init__(self, config):\n        Helper.__init__(self)\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            self.create_journalist()\n\n    def check_downgrade(self):\n        with self.app.app_context():\n            journalists_sql = \"SELECT * FROM journalists\"\n            journalist = db.engine.execute(text(journalists_sql)).first()\n            assert len(journalist[\"otp_secret\"]) == 32  # Varchar ignores length",
            "file": "migration_de00920916bf.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        Helper.__init__(self)\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_de00920916bf.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            self.create_journalist()",
            "file": "migration_de00920916bf.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        with self.app.app_context():\n            journalists_sql = \"SELECT * FROM journalists\"\n            journalist = db.engine.execute(text(journalists_sql)).first()\n            assert len(journalist[\"otp_secret\"]) == 32  # Varchar ignores length",
            "file": "migration_de00920916bf.py"
          }
        ],
        "migration_e0a525cbab83.py": [
          {
            "type": "function",
            "name": "add_source",
            "code": "def add_source():\n    filesystem_id = random_chars(96) if random_bool() else None\n    params = {\n        \"uuid\": str(uuid.uuid4()),\n        \"filesystem_id\": filesystem_id,\n        \"journalist_designation\": random_chars(50),\n        \"flagged\": bool_or_none(),\n        \"last_updated\": random_datetime(nullable=True),\n        \"pending\": bool_or_none(),\n        \"interaction_count\": random.randint(0, 1000),\n    }\n    sql = \"\"\"INSERT INTO sources (uuid, filesystem_id,\n                journalist_designation, flagged, last_updated, pending,\n                interaction_count)\n             VALUES (:uuid, :filesystem_id, :journalist_designation,\n                :flagged, :last_updated, :pending, :interaction_count)\n          \"\"\"\n    db.engine.execute(text(sql), **params)",
            "file": "migration_e0a525cbab83.py"
          },
          {
            "type": "function",
            "name": "add_journalist",
            "code": "def add_journalist():\n    if random_bool():\n        otp_secret = random_chars(16, string.ascii_uppercase + \"234567\")\n    else:\n        otp_secret = None\n\n    is_totp = random_bool()\n    if is_totp:\n        hotp_counter = 0 if random_bool() else None\n    else:\n        hotp_counter = random.randint(0, 10000) if random_bool() else None\n\n    last_token = random_chars(6, string.digits) if random_bool() else None\n\n    params = {\n        \"username\": random_username(),\n        \"pw_salt\": random_bytes(1, 64, nullable=True),\n        \"pw_hash\": random_bytes(32, 64, nullable=True),\n        \"is_admin\": bool_or_none(),\n        \"otp_secret\": otp_secret,\n        \"is_totp\": is_totp,\n        \"hotp_counter\": hotp_counter,\n        \"last_token\": last_token,\n        \"created_on\": random_datetime(nullable=True),\n        \"last_access\": random_datetime(nullable=True),\n        \"passphrase_hash\": random_bytes(32, 64, nullable=True),\n    }\n    sql = \"\"\"INSERT INTO journalists (username, pw_salt, pw_hash,\n                is_admin, otp_secret, is_totp, hotp_counter, last_token,\n                created_on, last_access, passphrase_hash)\n             VALUES (:username, :pw_salt, :pw_hash, :is_admin,\n                :otp_secret, :is_totp, :hotp_counter, :last_token,\n                :created_on, :last_access, :passphrase_hash);\n          \"\"\"\n    db.engine.execute(text(sql), **params)",
            "file": "migration_e0a525cbab83.py"
          },
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester:\n    \"\"\"This migration verifies that the deleted_by_source column now exists,\n    and that the data migration completed successfully.\n    \"\"\"\n\n    SOURCE_NUM = 200\n    JOURNO_NUM = 20\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                add_journalist()\n\n            add_source()\n\n            for jid in range(1, self.JOURNO_NUM):\n                self.add_reply(jid, 1)\n\n            db.session.commit()\n\n    @staticmethod\n    def add_reply(journalist_id, source_id):\n        params = {\n            \"journalist_id\": journalist_id,\n            \"source_id\": source_id,\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n        }\n        sql = \"\"\"INSERT INTO replies (journalist_id, source_id, filename,\n                    size)\n                 VALUES (:journalist_id, :source_id, :filename, :size)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    def check_upgrade(self):\n        with self.app.app_context():\n            replies = db.engine.execute(text(\"SELECT * FROM replies\")).fetchall()\n            assert len(replies) == self.JOURNO_NUM - 1\n\n            for reply in replies:\n                assert reply.deleted_by_source == False",
            "file": "migration_e0a525cbab83.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_e0a525cbab83.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                add_journalist()\n\n            add_source()\n\n            for jid in range(1, self.JOURNO_NUM):\n                self.add_reply(jid, 1)\n\n            db.session.commit()",
            "file": "migration_e0a525cbab83.py"
          },
          {
            "type": "function",
            "name": "add_reply",
            "code": "def add_reply(journalist_id, source_id):\n        params = {\n            \"journalist_id\": journalist_id,\n            \"source_id\": source_id,\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n        }\n        sql = \"\"\"INSERT INTO replies (journalist_id, source_id, filename,\n                    size)\n                 VALUES (:journalist_id, :source_id, :filename, :size)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_e0a525cbab83.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        with self.app.app_context():\n            replies = db.engine.execute(text(\"SELECT * FROM replies\")).fetchall()\n            assert len(replies) == self.JOURNO_NUM - 1\n\n            for reply in replies:\n                assert reply.deleted_by_source == False",
            "file": "migration_e0a525cbab83.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester:\n    SOURCE_NUM = 200\n    JOURNO_NUM = 20\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                add_journalist()\n\n            add_source()\n\n            for jid in range(1, self.JOURNO_NUM):\n                self.add_reply(jid, 1)\n\n            db.session.commit()\n\n    @staticmethod\n    def add_reply(journalist_id, source_id):\n        params = {\n            \"journalist_id\": journalist_id,\n            \"source_id\": source_id,\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"deleted_by_source\": False,\n        }\n        sql = \"\"\"INSERT INTO replies (journalist_id, source_id, filename,\n                    size, deleted_by_source)\n                 VALUES (:journalist_id, :source_id, :filename, :size,\n                    :deleted_by_source)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    def check_downgrade(self):\n        \"\"\"Verify that the deleted_by_source column is now gone, and\n        otherwise the table has the expected number of rows.\n        \"\"\"\n        with self.app.app_context():\n            sql = \"SELECT * FROM replies\"\n            replies = db.engine.execute(text(sql)).fetchall()\n\n            for reply in replies:\n                try:\n                    # This should produce an exception, as the column (should)\n                    # be gone.\n                    assert reply[\"deleted_by_source\"] is None\n                except NoSuchColumnError:\n                    pass\n\n            assert len(replies) == self.JOURNO_NUM - 1",
            "file": "migration_e0a525cbab83.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_e0a525cbab83.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                add_journalist()\n\n            add_source()\n\n            for jid in range(1, self.JOURNO_NUM):\n                self.add_reply(jid, 1)\n\n            db.session.commit()",
            "file": "migration_e0a525cbab83.py"
          },
          {
            "type": "function",
            "name": "add_reply",
            "code": "def add_reply(journalist_id, source_id):\n        params = {\n            \"journalist_id\": journalist_id,\n            \"source_id\": source_id,\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"deleted_by_source\": False,\n        }\n        sql = \"\"\"INSERT INTO replies (journalist_id, source_id, filename,\n                    size, deleted_by_source)\n                 VALUES (:journalist_id, :source_id, :filename, :size,\n                    :deleted_by_source)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_e0a525cbab83.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        \"\"\"Verify that the deleted_by_source column is now gone, and\n        otherwise the table has the expected number of rows.\n        \"\"\"\n        with self.app.app_context():\n            sql = \"SELECT * FROM replies\"\n            replies = db.engine.execute(text(sql)).fetchall()\n\n            for reply in replies:\n                try:\n                    # This should produce an exception, as the column (should)\n                    # be gone.\n                    assert reply[\"deleted_by_source\"] is None\n                except NoSuchColumnError:\n                    pass\n\n            assert len(replies) == self.JOURNO_NUM - 1",
            "file": "migration_e0a525cbab83.py"
          }
        ],
        "migration_35513370ba0d.py": [
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester:\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            self.add_source()\n            self.valid_source_id = 1\n\n            db.session.commit()\n\n    @staticmethod\n    def add_source():\n        filesystem_id = random_chars(96) if random_bool() else None\n        params = {\n            \"uuid\": str(uuid4()),\n            \"filesystem_id\": filesystem_id,\n            \"journalist_designation\": random_chars(50),\n            \"flagged\": bool_or_none(),\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": bool_or_none(),\n            \"interaction_count\": random.randint(0, 1000),\n        }\n        sql = \"\"\"\n        INSERT INTO sources (\n            uuid, filesystem_id, journalist_designation, flagged, last_updated,\n            pending, interaction_count\n        ) VALUES (\n            :uuid, :filesystem_id, :journalist_designation, :flagged, :last_updated,\n            :pending, :interaction_count\n        )\n        \"\"\"\n\n        db.engine.execute(sqlalchemy.text(sql), **params)\n\n    def check_upgrade(self):\n        \"\"\"\n        Check the new `deleted_at` column\n\n        Querying `deleted_at` shouldn't cause an error, and no source\n        should already have it set.\n        \"\"\"\n        with self.app.app_context():\n            sources = db.engine.execute(\n                sqlalchemy.text(\"SELECT * FROM sources WHERE deleted_at IS NOT NULL\")\n            ).fetchall()\n            assert len(sources) == 0",
            "file": "migration_35513370ba0d.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_35513370ba0d.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            self.add_source()\n            self.valid_source_id = 1\n\n            db.session.commit()",
            "file": "migration_35513370ba0d.py"
          },
          {
            "type": "function",
            "name": "add_source",
            "code": "def add_source():\n        filesystem_id = random_chars(96) if random_bool() else None\n        params = {\n            \"uuid\": str(uuid4()),\n            \"filesystem_id\": filesystem_id,\n            \"journalist_designation\": random_chars(50),\n            \"flagged\": bool_or_none(),\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": bool_or_none(),\n            \"interaction_count\": random.randint(0, 1000),\n        }\n        sql = \"\"\"\n        INSERT INTO sources (\n            uuid, filesystem_id, journalist_designation, flagged, last_updated,\n            pending, interaction_count\n        ) VALUES (\n            :uuid, :filesystem_id, :journalist_designation, :flagged, :last_updated,\n            :pending, :interaction_count\n        )\n        \"\"\"\n\n        db.engine.execute(sqlalchemy.text(sql), **params)",
            "file": "migration_35513370ba0d.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        \"\"\"\n        Check the new `deleted_at` column\n\n        Querying `deleted_at` shouldn't cause an error, and no source\n        should already have it set.\n        \"\"\"\n        with self.app.app_context():\n            sources = db.engine.execute(\n                sqlalchemy.text(\"SELECT * FROM sources WHERE deleted_at IS NOT NULL\")\n            ).fetchall()\n            assert len(sources) == 0",
            "file": "migration_35513370ba0d.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester:\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        pass\n\n    def check_downgrade(self):\n        \"\"\"\n        After downgrade, using `deleted_at` in a query should raise an exception\n        \"\"\"\n        with self.app.app_context(), pytest.raises(sqlalchemy.exc.OperationalError):\n            db.engine.execute(\n                sqlalchemy.text(\"SELECT * FROM sources WHERE deleted_at IS NOT NULL\")\n            ).fetchall()",
            "file": "migration_35513370ba0d.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_35513370ba0d.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        pass",
            "file": "migration_35513370ba0d.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        \"\"\"\n        After downgrade, using `deleted_at` in a query should raise an exception\n        \"\"\"\n        with self.app.app_context(), pytest.raises(sqlalchemy.exc.OperationalError):\n            db.engine.execute(\n                sqlalchemy.text(\"SELECT * FROM sources WHERE deleted_at IS NOT NULL\")\n            ).fetchall()",
            "file": "migration_35513370ba0d.py"
          }
        ],
        "migration_2d0ce3ee5bdc.py": [
          {
            "type": "class",
            "name": "Helper",
            "code": "class Helper:\n    @staticmethod\n    def add_source():\n        filesystem_id = random_chars(96) if random_bool() else None\n        params = {\n            \"uuid\": str(uuid4()),\n            \"filesystem_id\": filesystem_id,\n            \"journalist_designation\": random_chars(50),\n            \"flagged\": bool_or_none(),\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": bool_or_none(),\n            \"interaction_count\": random.randint(0, 1000),\n        }\n        sql = \"\"\"INSERT INTO sources (uuid, filesystem_id,\n                    journalist_designation, flagged, last_updated, pending,\n                    interaction_count)\n                 VALUES (:uuid, :filesystem_id, :journalist_designation,\n                    :flagged, :last_updated, :pending, :interaction_count)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    @staticmethod\n    def add_journalist_login_attempt(journalist_id):\n        params = {\n            \"timestamp\": random_datetime(nullable=True),\n            \"journalist_id\": journalist_id,\n        }\n        sql = \"\"\"INSERT INTO journalist_login_attempt (timestamp,\n                    journalist_id)\n                 VALUES (:timestamp, :journalist_id)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    @staticmethod\n    def add_reply(journalist_id, source_id):\n        params = {\n            \"journalist_id\": journalist_id,\n            \"source_id\": source_id,\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n        }\n        sql = \"\"\"INSERT INTO replies (journalist_id, source_id, filename,\n                    size)\n                 VALUES (:journalist_id, :source_id, :filename, :size)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    @staticmethod\n    def extract(app):\n        with app.app_context():\n            sql = \"\"\"SELECT j.id, count(distinct a.id), count(distinct r.id)\n                     FROM journalists AS j\n                     LEFT OUTER JOIN journalist_login_attempt AS a\n                     ON a.journalist_id = j.id\n                     LEFT OUTER JOIN replies AS r\n                     ON r.journalist_id = j.id\n                     GROUP BY j.id\n                     ORDER BY j.id\n                  \"\"\"\n            return list(db.session.execute(text(sql)))",
            "file": "migration_2d0ce3ee5bdc.py"
          },
          {
            "type": "function",
            "name": "add_source",
            "code": "def add_source():\n        filesystem_id = random_chars(96) if random_bool() else None\n        params = {\n            \"uuid\": str(uuid4()),\n            \"filesystem_id\": filesystem_id,\n            \"journalist_designation\": random_chars(50),\n            \"flagged\": bool_or_none(),\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": bool_or_none(),\n            \"interaction_count\": random.randint(0, 1000),\n        }\n        sql = \"\"\"INSERT INTO sources (uuid, filesystem_id,\n                    journalist_designation, flagged, last_updated, pending,\n                    interaction_count)\n                 VALUES (:uuid, :filesystem_id, :journalist_designation,\n                    :flagged, :last_updated, :pending, :interaction_count)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_2d0ce3ee5bdc.py"
          },
          {
            "type": "function",
            "name": "add_journalist_login_attempt",
            "code": "def add_journalist_login_attempt(journalist_id):\n        params = {\n            \"timestamp\": random_datetime(nullable=True),\n            \"journalist_id\": journalist_id,\n        }\n        sql = \"\"\"INSERT INTO journalist_login_attempt (timestamp,\n                    journalist_id)\n                 VALUES (:timestamp, :journalist_id)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_2d0ce3ee5bdc.py"
          },
          {
            "type": "function",
            "name": "add_reply",
            "code": "def add_reply(journalist_id, source_id):\n        params = {\n            \"journalist_id\": journalist_id,\n            \"source_id\": source_id,\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n        }\n        sql = \"\"\"INSERT INTO replies (journalist_id, source_id, filename,\n                    size)\n                 VALUES (:journalist_id, :source_id, :filename, :size)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_2d0ce3ee5bdc.py"
          },
          {
            "type": "function",
            "name": "extract",
            "code": "def extract(app):\n        with app.app_context():\n            sql = \"\"\"SELECT j.id, count(distinct a.id), count(distinct r.id)\n                     FROM journalists AS j\n                     LEFT OUTER JOIN journalist_login_attempt AS a\n                     ON a.journalist_id = j.id\n                     LEFT OUTER JOIN replies AS r\n                     ON r.journalist_id = j.id\n                     GROUP BY j.id\n                     ORDER BY j.id\n                  \"\"\"\n            return list(db.session.execute(text(sql)))",
            "file": "migration_2d0ce3ee5bdc.py"
          },
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester(Helper):\n    JOURNO_NUM = 100\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n        self.initial_data = None\n\n    def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                self.add_journalist()\n\n            self.add_source()\n\n            for jid in range(1, self.JOURNO_NUM):\n                for _ in range(random.randint(1, 3)):\n                    self.add_journalist_login_attempt(jid)\n\n            for jid in range(1, self.JOURNO_NUM):\n                self.add_reply(jid, 1)\n\n            db.session.commit()\n            self.initial_data = self.extract(self.app)\n\n    def check_upgrade(self):\n        extracted = self.extract(self.app)\n        assert len(extracted) == self.JOURNO_NUM\n        assert extracted == self.initial_data\n\n    @staticmethod\n    def add_journalist():\n        if random_bool():\n            otp_secret = random_chars(16, string.ascii_uppercase + \"234567\")\n        else:\n            otp_secret = None\n\n        is_totp = random_bool()\n        if is_totp:\n            hotp_counter = 0 if random_bool() else None\n        else:\n            hotp_counter = random.randint(0, 10000) if random_bool() else None\n\n        last_token = random_chars(6, string.digits) if random_bool() else None\n\n        params = {\n            \"username\": random_username(),\n            \"pw_salt\": random_bytes(1, 64, nullable=True),\n            \"pw_hash\": random_bytes(32, 64, nullable=True),\n            \"is_admin\": bool_or_none(),\n            \"otp_secret\": otp_secret,\n            \"is_totp\": is_totp,\n            \"hotp_counter\": hotp_counter,\n            \"last_token\": last_token,\n            \"created_on\": random_datetime(nullable=True),\n            \"last_access\": random_datetime(nullable=True),\n        }\n        sql = \"\"\"INSERT INTO journalists (username, pw_salt, pw_hash,\n                    is_admin, otp_secret, is_totp, hotp_counter, last_token,\n                    created_on, last_access)\n                 VALUES (:username, :pw_salt, :pw_hash, :is_admin,\n                    :otp_secret, :is_totp, :hotp_counter, :last_token,\n                    :created_on, :last_access);\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_2d0ce3ee5bdc.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n        self.initial_data = None",
            "file": "migration_2d0ce3ee5bdc.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                self.add_journalist()\n\n            self.add_source()\n\n            for jid in range(1, self.JOURNO_NUM):\n                for _ in range(random.randint(1, 3)):\n                    self.add_journalist_login_attempt(jid)\n\n            for jid in range(1, self.JOURNO_NUM):\n                self.add_reply(jid, 1)\n\n            db.session.commit()\n            self.initial_data = self.extract(self.app)",
            "file": "migration_2d0ce3ee5bdc.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        extracted = self.extract(self.app)\n        assert len(extracted) == self.JOURNO_NUM\n        assert extracted == self.initial_data",
            "file": "migration_2d0ce3ee5bdc.py"
          },
          {
            "type": "function",
            "name": "add_journalist",
            "code": "def add_journalist():\n        if random_bool():\n            otp_secret = random_chars(16, string.ascii_uppercase + \"234567\")\n        else:\n            otp_secret = None\n\n        is_totp = random_bool()\n        if is_totp:\n            hotp_counter = 0 if random_bool() else None\n        else:\n            hotp_counter = random.randint(0, 10000) if random_bool() else None\n\n        last_token = random_chars(6, string.digits) if random_bool() else None\n\n        params = {\n            \"username\": random_username(),\n            \"pw_salt\": random_bytes(1, 64, nullable=True),\n            \"pw_hash\": random_bytes(32, 64, nullable=True),\n            \"is_admin\": bool_or_none(),\n            \"otp_secret\": otp_secret,\n            \"is_totp\": is_totp,\n            \"hotp_counter\": hotp_counter,\n            \"last_token\": last_token,\n            \"created_on\": random_datetime(nullable=True),\n            \"last_access\": random_datetime(nullable=True),\n        }\n        sql = \"\"\"INSERT INTO journalists (username, pw_salt, pw_hash,\n                    is_admin, otp_secret, is_totp, hotp_counter, last_token,\n                    created_on, last_access)\n                 VALUES (:username, :pw_salt, :pw_hash, :is_admin,\n                    :otp_secret, :is_totp, :hotp_counter, :last_token,\n                    :created_on, :last_access);\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_2d0ce3ee5bdc.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester(Helper):\n    JOURNO_NUM = 100\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n        self.initial_data = None\n\n    def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                self.add_journalist()\n\n            self.add_source()\n\n            for jid in range(1, self.JOURNO_NUM):\n                for _ in range(random.randint(1, 3)):\n                    self.add_journalist_login_attempt(jid)\n\n            for jid in range(1, self.JOURNO_NUM):\n                self.add_reply(jid, 1)\n\n            db.session.commit()\n            self.initial_data = self.extract(self.app)\n\n    def check_downgrade(self):\n        extracted = self.extract(self.app)\n        assert len(extracted) == self.JOURNO_NUM\n        assert extracted == self.initial_data\n\n    @staticmethod\n    def add_journalist():\n        if random_bool():\n            otp_secret = random_chars(16, string.ascii_uppercase + \"234567\")\n        else:\n            otp_secret = None\n\n        is_totp = random_bool()\n        if is_totp:\n            hotp_counter = 0 if random_bool() else None\n        else:\n            hotp_counter = random.randint(0, 10000) if random_bool() else None\n\n        last_token = random_chars(6, string.digits) if random_bool() else None\n\n        params = {\n            \"username\": random_username(),\n            \"pw_salt\": random_bytes(1, 64, nullable=True),\n            \"pw_hash\": random_bytes(32, 64, nullable=True),\n            \"is_admin\": bool_or_none(),\n            \"otp_secret\": otp_secret,\n            \"is_totp\": is_totp,\n            \"hotp_counter\": hotp_counter,\n            \"last_token\": last_token,\n            \"created_on\": random_datetime(nullable=True),\n            \"last_access\": random_datetime(nullable=True),\n            \"passphrase_hash\": random_bytes(32, 64, nullable=True),\n        }\n        sql = \"\"\"INSERT INTO journalists (username, pw_salt, pw_hash,\n                    is_admin, otp_secret, is_totp, hotp_counter, last_token,\n                    created_on, last_access, passphrase_hash)\n                 VALUES (:username, :pw_salt, :pw_hash, :is_admin,\n                    :otp_secret, :is_totp, :hotp_counter, :last_token,\n                    :created_on, :last_access, :passphrase_hash);\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_2d0ce3ee5bdc.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n        self.initial_data = None",
            "file": "migration_2d0ce3ee5bdc.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                self.add_journalist()\n\n            self.add_source()\n\n            for jid in range(1, self.JOURNO_NUM):\n                for _ in range(random.randint(1, 3)):\n                    self.add_journalist_login_attempt(jid)\n\n            for jid in range(1, self.JOURNO_NUM):\n                self.add_reply(jid, 1)\n\n            db.session.commit()\n            self.initial_data = self.extract(self.app)",
            "file": "migration_2d0ce3ee5bdc.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        extracted = self.extract(self.app)\n        assert len(extracted) == self.JOURNO_NUM\n        assert extracted == self.initial_data",
            "file": "migration_2d0ce3ee5bdc.py"
          },
          {
            "type": "function",
            "name": "add_journalist",
            "code": "def add_journalist():\n        if random_bool():\n            otp_secret = random_chars(16, string.ascii_uppercase + \"234567\")\n        else:\n            otp_secret = None\n\n        is_totp = random_bool()\n        if is_totp:\n            hotp_counter = 0 if random_bool() else None\n        else:\n            hotp_counter = random.randint(0, 10000) if random_bool() else None\n\n        last_token = random_chars(6, string.digits) if random_bool() else None\n\n        params = {\n            \"username\": random_username(),\n            \"pw_salt\": random_bytes(1, 64, nullable=True),\n            \"pw_hash\": random_bytes(32, 64, nullable=True),\n            \"is_admin\": bool_or_none(),\n            \"otp_secret\": otp_secret,\n            \"is_totp\": is_totp,\n            \"hotp_counter\": hotp_counter,\n            \"last_token\": last_token,\n            \"created_on\": random_datetime(nullable=True),\n            \"last_access\": random_datetime(nullable=True),\n            \"passphrase_hash\": random_bytes(32, 64, nullable=True),\n        }\n        sql = \"\"\"INSERT INTO journalists (username, pw_salt, pw_hash,\n                    is_admin, otp_secret, is_totp, hotp_counter, last_token,\n                    created_on, last_access, passphrase_hash)\n                 VALUES (:username, :pw_salt, :pw_hash, :is_admin,\n                    :otp_secret, :is_totp, :hotp_counter, :last_token,\n                    :created_on, :last_access, :passphrase_hash);\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_2d0ce3ee5bdc.py"
          }
        ],
        "migration_3da3fcab826a.py": [
          {
            "type": "function",
            "name": "create_file_in_dummy_source_dir",
            "code": "def create_file_in_dummy_source_dir(filename):\n    filesystem_id = \"dummy\"\n    basedir = os.path.join(TEST_DATA_DIR, filesystem_id)\n\n    if not os.path.exists(basedir):\n        os.makedirs(basedir)\n\n    path_to_file = os.path.join(basedir, filename)\n    with open(path_to_file, \"a\"):\n        os.utime(path_to_file, None)",
            "file": "migration_3da3fcab826a.py"
          },
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester:\n    \"\"\"This migration verifies that any orphaned submission or reply data from\n    deleted sources is also deleted.\n    \"\"\"\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n        self.journalist_id = None\n\n    def load_data(self):\n        with self.app.app_context():\n            self.create_journalist()\n            self.add_source()\n            self.valid_source_id = 1\n            deleted_source_id = 2\n\n            # Add submissions and replies with and without a valid source\n            self.add_submission(self.valid_source_id)\n            self.add_submission(deleted_source_id)\n            self.add_submission(deleted_source_id, with_file=False)\n            self.add_submission(None)  # NULL source\n\n            self.add_reply(self.journalist_id, self.valid_source_id)\n            self.add_reply(self.journalist_id, deleted_source_id)\n            self.add_reply(self.journalist_id, deleted_source_id, with_file=False)\n            self.add_reply(self.journalist_id, None)  # NULL source\n\n            db.session.commit()\n\n    def create_journalist(self):\n        if self.journalist_id is not None:\n            raise RuntimeError(\"Journalist already created\")\n\n        params = {\"uuid\": str(uuid4()), \"username\": random_chars(50), \"session_nonce\": 0}\n        sql = \"\"\"INSERT INTO journalists (uuid, username, session_nonce)\n                 VALUES (:uuid, :username, :session_nonce)\n              \"\"\"\n        self.journalist_id = db.engine.execute(text(sql), **params).lastrowid\n\n    def add_reply(self, journalist_id, source_id, with_file=True):\n        filename = \"1-\" + random_ascii_chars(5) + \"-\" + random_ascii_chars(5) + \"-reply.gpg\"\n        params = {\n            \"uuid\": str(uuid4()),\n            \"journalist_id\": journalist_id,\n            \"source_id\": source_id,\n            \"filename\": filename,\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"deleted_by_source\": False,\n        }\n        sql = \"\"\"INSERT INTO replies (journalist_id, uuid, source_id, filename,\n                    size, deleted_by_source)\n                 VALUES (:journalist_id, :uuid, :source_id, :filename, :size,\n                         :deleted_by_source)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n        if with_file:\n            create_file_in_dummy_source_dir(filename)\n\n    @staticmethod\n    def add_source():\n        filesystem_id = random_chars(96) if random_bool() else None\n        params = {\n            \"uuid\": str(uuid4()),\n            \"filesystem_id\": filesystem_id,\n            \"journalist_designation\": random_chars(50),\n            \"flagged\": bool_or_none(),\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": bool_or_none(),\n            \"interaction_count\": random.randint(0, 1000),\n        }\n        sql = \"\"\"INSERT INTO sources (uuid, filesystem_id,\n                    journalist_designation, flagged, last_updated, pending,\n                    interaction_count)\n                 VALUES (:uuid, :filesystem_id, :journalist_designation,\n                    :flagged, :last_updated, :pending, :interaction_count)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    def add_submission(self, source_id, with_file=True):\n        filename = \"1-\" + random_ascii_chars(5) + \"-\" + random_ascii_chars(5) + \"-doc.gz.gpg\"\n        params = {\n            \"uuid\": str(uuid4()),\n            \"source_id\": source_id,\n            \"filename\": filename,\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"downloaded\": bool_or_none(),\n        }\n        sql = \"\"\"INSERT INTO submissions (uuid, source_id, filename, size,\n                    downloaded)\n                 VALUES (:uuid, :source_id, :filename, :size, :downloaded)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n        if with_file:\n            create_file_in_dummy_source_dir(filename)\n\n    def check_upgrade(self):\n        with self.app.app_context():\n            submissions = db.engine.execute(text(\"SELECT * FROM submissions\")).fetchall()\n\n            # Submissions without a source should be deleted\n            assert len(submissions) == 1\n            for submission in submissions:\n                assert submission.source_id == self.valid_source_id\n\n            replies = db.engine.execute(text(\"SELECT * FROM replies\")).fetchall()\n\n            # Replies without a source should be deleted\n            assert len(replies) == 1\n            for reply in replies:\n                assert reply.source_id == self.valid_source_id",
            "file": "migration_3da3fcab826a.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n        self.journalist_id = None",
            "file": "migration_3da3fcab826a.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            self.create_journalist()\n            self.add_source()\n            self.valid_source_id = 1\n            deleted_source_id = 2\n\n            # Add submissions and replies with and without a valid source\n            self.add_submission(self.valid_source_id)\n            self.add_submission(deleted_source_id)\n            self.add_submission(deleted_source_id, with_file=False)\n            self.add_submission(None)  # NULL source\n\n            self.add_reply(self.journalist_id, self.valid_source_id)\n            self.add_reply(self.journalist_id, deleted_source_id)\n            self.add_reply(self.journalist_id, deleted_source_id, with_file=False)\n            self.add_reply(self.journalist_id, None)  # NULL source\n\n            db.session.commit()",
            "file": "migration_3da3fcab826a.py"
          },
          {
            "type": "function",
            "name": "create_journalist",
            "code": "def create_journalist(self):\n        if self.journalist_id is not None:\n            raise RuntimeError(\"Journalist already created\")\n\n        params = {\"uuid\": str(uuid4()), \"username\": random_chars(50), \"session_nonce\": 0}\n        sql = \"\"\"INSERT INTO journalists (uuid, username, session_nonce)\n                 VALUES (:uuid, :username, :session_nonce)\n              \"\"\"\n        self.journalist_id = db.engine.execute(text(sql), **params).lastrowid",
            "file": "migration_3da3fcab826a.py"
          },
          {
            "type": "function",
            "name": "add_reply",
            "code": "def add_reply(self, journalist_id, source_id, with_file=True):\n        filename = \"1-\" + random_ascii_chars(5) + \"-\" + random_ascii_chars(5) + \"-reply.gpg\"\n        params = {\n            \"uuid\": str(uuid4()),\n            \"journalist_id\": journalist_id,\n            \"source_id\": source_id,\n            \"filename\": filename,\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"deleted_by_source\": False,\n        }\n        sql = \"\"\"INSERT INTO replies (journalist_id, uuid, source_id, filename,\n                    size, deleted_by_source)\n                 VALUES (:journalist_id, :uuid, :source_id, :filename, :size,\n                         :deleted_by_source)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n        if with_file:\n            create_file_in_dummy_source_dir(filename)",
            "file": "migration_3da3fcab826a.py"
          },
          {
            "type": "function",
            "name": "add_source",
            "code": "def add_source():\n        filesystem_id = random_chars(96) if random_bool() else None\n        params = {\n            \"uuid\": str(uuid4()),\n            \"filesystem_id\": filesystem_id,\n            \"journalist_designation\": random_chars(50),\n            \"flagged\": bool_or_none(),\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": bool_or_none(),\n            \"interaction_count\": random.randint(0, 1000),\n        }\n        sql = \"\"\"INSERT INTO sources (uuid, filesystem_id,\n                    journalist_designation, flagged, last_updated, pending,\n                    interaction_count)\n                 VALUES (:uuid, :filesystem_id, :journalist_designation,\n                    :flagged, :last_updated, :pending, :interaction_count)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_3da3fcab826a.py"
          },
          {
            "type": "function",
            "name": "add_submission",
            "code": "def add_submission(self, source_id, with_file=True):\n        filename = \"1-\" + random_ascii_chars(5) + \"-\" + random_ascii_chars(5) + \"-doc.gz.gpg\"\n        params = {\n            \"uuid\": str(uuid4()),\n            \"source_id\": source_id,\n            \"filename\": filename,\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"downloaded\": bool_or_none(),\n        }\n        sql = \"\"\"INSERT INTO submissions (uuid, source_id, filename, size,\n                    downloaded)\n                 VALUES (:uuid, :source_id, :filename, :size, :downloaded)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n        if with_file:\n            create_file_in_dummy_source_dir(filename)",
            "file": "migration_3da3fcab826a.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        with self.app.app_context():\n            submissions = db.engine.execute(text(\"SELECT * FROM submissions\")).fetchall()\n\n            # Submissions without a source should be deleted\n            assert len(submissions) == 1\n            for submission in submissions:\n                assert submission.source_id == self.valid_source_id\n\n            replies = db.engine.execute(text(\"SELECT * FROM replies\")).fetchall()\n\n            # Replies without a source should be deleted\n            assert len(replies) == 1\n            for reply in replies:\n                assert reply.source_id == self.valid_source_id",
            "file": "migration_3da3fcab826a.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester:\n    # This is a destructive alembic migration, it cannot be downgraded\n\n    def __init__(self, config):\n        self.config = config\n\n    def load_data(self):\n        pass\n\n    def check_downgrade(self):\n        pass",
            "file": "migration_3da3fcab826a.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config",
            "file": "migration_3da3fcab826a.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        pass",
            "file": "migration_3da3fcab826a.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        pass",
            "file": "migration_3da3fcab826a.py"
          }
        ],
        "migration_b58139cfdc8c.py": [
          {
            "type": "class",
            "name": "Helper",
            "code": "class Helper:\n    def __init__(self):\n        self.journalist_id = None\n        self.source_id = None\n        self._counter = 0\n\n    @property\n    def counter(self):\n        self._counter += 1\n        return self._counter\n\n    def create_journalist(self):\n        if self.journalist_id is not None:\n            raise RuntimeError(\"Journalist already created\")\n\n        params = {\n            \"uuid\": str(uuid.uuid4()),\n            \"username\": random_chars(50),\n        }\n        sql = \"\"\"INSERT INTO journalists (uuid, username)\n                 VALUES (:uuid, :username)\n              \"\"\"\n        self.journalist_id = db.engine.execute(text(sql), **params).lastrowid\n\n    def create_source(self):\n        if self.source_id is not None:\n            raise RuntimeError(\"Source already created\")\n\n        self.source_filesystem_id = f\"aliruhglaiurhgliaurg-{self.counter}\"\n        params = {\n            \"filesystem_id\": self.source_filesystem_id,\n            \"uuid\": str(uuid.uuid4()),\n            \"journalist_designation\": random_chars(50),\n            \"flagged\": False,\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": False,\n            \"interaction_count\": 0,\n        }\n        sql = \"\"\"INSERT INTO sources (filesystem_id, uuid, journalist_designation, flagged,\n                    last_updated, pending, interaction_count)\n                 VALUES (:filesystem_id, :uuid, :journalist_designation, :flagged, :last_updated,\n                    :pending, :interaction_count)\n              \"\"\"\n        self.source_id = db.engine.execute(text(sql), **params).lastrowid\n\n    def create_submission(self, checksum=False):\n        filename = str(uuid.uuid4())\n        params = {\n            \"uuid\": str(uuid.uuid4()),\n            \"source_id\": self.source_id,\n            \"filename\": filename,\n            \"size\": random.randint(10, 1000),\n            \"downloaded\": False,\n        }\n\n        if checksum:\n            params[\"checksum\"] = (\n                \"sha256:f00a787f7492a95e165b470702f4fe9373583fbdc025b2c8bdf0262cc48fcff4\"\n            )\n            sql = \"\"\"INSERT INTO submissions (uuid, source_id, filename, size, downloaded, checksum)\n                     VALUES (:uuid, :source_id, :filename, :size, :downloaded, :checksum)\n                  \"\"\"\n        else:\n            sql = \"\"\"INSERT INTO submissions (uuid, source_id, filename, size, downloaded)\n                     VALUES (:uuid, :source_id, :filename, :size, :downloaded)\n                  \"\"\"\n\n        return (db.engine.execute(text(sql), **params).lastrowid, filename)\n\n    def create_reply(self, checksum=False):\n        filename = str(uuid.uuid4())\n        params = {\n            \"uuid\": str(uuid.uuid4()),\n            \"source_id\": self.source_id,\n            \"journalist_id\": self.journalist_id,\n            \"filename\": filename,\n            \"size\": random.randint(10, 1000),\n            \"deleted_by_source\": False,\n        }\n\n        if checksum:\n            params[\"checksum\"] = (\n                \"sha256:f00a787f7492a95e165b470702f4fe9373583fbdc025b2c8bdf0262cc48fcff4\"\n            )\n            sql = \"\"\"INSERT INTO replies (uuid, source_id, journalist_id, filename, size,\n                        deleted_by_source, checksum)\n                     VALUES (:uuid, :source_id, :journalist_id, :filename, :size,\n                        :deleted_by_source, :checksum)\n                  \"\"\"\n        else:\n            sql = \"\"\"INSERT INTO replies (uuid, source_id, journalist_id, filename, size,\n                        deleted_by_source)\n                     VALUES (:uuid, :source_id, :journalist_id, :filename, :size,\n                        :deleted_by_source)\n                  \"\"\"\n        return (db.engine.execute(text(sql), **params).lastrowid, filename)",
            "file": "migration_b58139cfdc8c.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self):\n        self.journalist_id = None\n        self.source_id = None\n        self._counter = 0",
            "file": "migration_b58139cfdc8c.py"
          },
          {
            "type": "function",
            "name": "counter",
            "code": "def counter(self):\n        self._counter += 1\n        return self._counter",
            "file": "migration_b58139cfdc8c.py"
          },
          {
            "type": "function",
            "name": "create_journalist",
            "code": "def create_journalist(self):\n        if self.journalist_id is not None:\n            raise RuntimeError(\"Journalist already created\")\n\n        params = {\n            \"uuid\": str(uuid.uuid4()),\n            \"username\": random_chars(50),\n        }\n        sql = \"\"\"INSERT INTO journalists (uuid, username)\n                 VALUES (:uuid, :username)\n              \"\"\"\n        self.journalist_id = db.engine.execute(text(sql), **params).lastrowid",
            "file": "migration_b58139cfdc8c.py"
          },
          {
            "type": "function",
            "name": "create_source",
            "code": "def create_source(self):\n        if self.source_id is not None:\n            raise RuntimeError(\"Source already created\")\n\n        self.source_filesystem_id = f\"aliruhglaiurhgliaurg-{self.counter}\"\n        params = {\n            \"filesystem_id\": self.source_filesystem_id,\n            \"uuid\": str(uuid.uuid4()),\n            \"journalist_designation\": random_chars(50),\n            \"flagged\": False,\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": False,\n            \"interaction_count\": 0,\n        }\n        sql = \"\"\"INSERT INTO sources (filesystem_id, uuid, journalist_designation, flagged,\n                    last_updated, pending, interaction_count)\n                 VALUES (:filesystem_id, :uuid, :journalist_designation, :flagged, :last_updated,\n                    :pending, :interaction_count)\n              \"\"\"\n        self.source_id = db.engine.execute(text(sql), **params).lastrowid",
            "file": "migration_b58139cfdc8c.py"
          },
          {
            "type": "function",
            "name": "create_submission",
            "code": "def create_submission(self, checksum=False):\n        filename = str(uuid.uuid4())\n        params = {\n            \"uuid\": str(uuid.uuid4()),\n            \"source_id\": self.source_id,\n            \"filename\": filename,\n            \"size\": random.randint(10, 1000),\n            \"downloaded\": False,\n        }\n\n        if checksum:\n            params[\"checksum\"] = (\n                \"sha256:f00a787f7492a95e165b470702f4fe9373583fbdc025b2c8bdf0262cc48fcff4\"\n            )\n            sql = \"\"\"INSERT INTO submissions (uuid, source_id, filename, size, downloaded, checksum)\n                     VALUES (:uuid, :source_id, :filename, :size, :downloaded, :checksum)\n                  \"\"\"\n        else:\n            sql = \"\"\"INSERT INTO submissions (uuid, source_id, filename, size, downloaded)\n                     VALUES (:uuid, :source_id, :filename, :size, :downloaded)\n                  \"\"\"\n\n        return (db.engine.execute(text(sql), **params).lastrowid, filename)",
            "file": "migration_b58139cfdc8c.py"
          },
          {
            "type": "function",
            "name": "create_reply",
            "code": "def create_reply(self, checksum=False):\n        filename = str(uuid.uuid4())\n        params = {\n            \"uuid\": str(uuid.uuid4()),\n            \"source_id\": self.source_id,\n            \"journalist_id\": self.journalist_id,\n            \"filename\": filename,\n            \"size\": random.randint(10, 1000),\n            \"deleted_by_source\": False,\n        }\n\n        if checksum:\n            params[\"checksum\"] = (\n                \"sha256:f00a787f7492a95e165b470702f4fe9373583fbdc025b2c8bdf0262cc48fcff4\"\n            )\n            sql = \"\"\"INSERT INTO replies (uuid, source_id, journalist_id, filename, size,\n                        deleted_by_source, checksum)\n                     VALUES (:uuid, :source_id, :journalist_id, :filename, :size,\n                        :deleted_by_source, :checksum)\n                  \"\"\"\n        else:\n            sql = \"\"\"INSERT INTO replies (uuid, source_id, journalist_id, filename, size,\n                        deleted_by_source)\n                     VALUES (:uuid, :source_id, :journalist_id, :filename, :size,\n                        :deleted_by_source)\n                  \"\"\"\n        return (db.engine.execute(text(sql), **params).lastrowid, filename)",
            "file": "migration_b58139cfdc8c.py"
          },
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester(Helper):\n    def __init__(self, config):\n        Helper.__init__(self)\n        self.config = config\n        self.app = create_app(config)\n\n        # as this class requires access to the Storage object, which is no longer\n        # attached to app, we create it here and mock the call to return it below.\n        self.storage = Storage(str(config.STORE_DIR), str(config.TEMP_DIR))\n\n    def load_data(self):\n        with mock.patch(\"store.Storage.get_default\") as mock_storage_global:\n            mock_storage_global.return_value = self.storage\n            with self.app.app_context():\n                self.create_journalist()\n                self.create_source()\n\n                submission_id, submission_filename = self.create_submission()\n                reply_id, reply_filename = self.create_reply()\n\n                # we need to actually create files and write data to them so the\n                # RQ worker can hash them\n                for fn in [submission_filename, reply_filename]:\n                    full_path = Storage.get_default().path(self.source_filesystem_id, fn)\n\n                    dirname = path.dirname(full_path)\n                    if not path.exists(dirname):\n                        os.mkdir(dirname)\n\n                    with open(full_path, \"wb\") as f:\n                        f.write(DATA)\n\n    def check_upgrade(self):\n        \"\"\"\n        We cannot inject the `SDConfig` object provided by the fixture `config` into the alembic\n        subprocess that actually performs the migration. This is needed to get both the value of the\n        DB URL and access to the function `storage.path`. These values are passed to the `rqworker`,\n        and without being able to inject this config, the checksum function won't succeed. The above\n        `load_data` function provides data that can be manually verified by checking the `rqworker`\n        log file in `/tmp/`.\n        The other part of the migration, creating a table, cannot be tested regardless.\n        \"\"\"",
            "file": "migration_b58139cfdc8c.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        Helper.__init__(self)\n        self.config = config\n        self.app = create_app(config)\n\n        # as this class requires access to the Storage object, which is no longer\n        # attached to app, we create it here and mock the call to return it below.\n        self.storage = Storage(str(config.STORE_DIR), str(config.TEMP_DIR))",
            "file": "migration_b58139cfdc8c.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with mock.patch(\"store.Storage.get_default\") as mock_storage_global:\n            mock_storage_global.return_value = self.storage\n            with self.app.app_context():\n                self.create_journalist()\n                self.create_source()\n\n                submission_id, submission_filename = self.create_submission()\n                reply_id, reply_filename = self.create_reply()\n\n                # we need to actually create files and write data to them so the\n                # RQ worker can hash them\n                for fn in [submission_filename, reply_filename]:\n                    full_path = Storage.get_default().path(self.source_filesystem_id, fn)\n\n                    dirname = path.dirname(full_path)\n                    if not path.exists(dirname):\n                        os.mkdir(dirname)\n\n                    with open(full_path, \"wb\") as f:\n                        f.write(DATA)",
            "file": "migration_b58139cfdc8c.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        \"\"\"\n        We cannot inject the `SDConfig` object provided by the fixture `config` into the alembic\n        subprocess that actually performs the migration. This is needed to get both the value of the\n        DB URL and access to the function `storage.path`. These values are passed to the `rqworker`,\n        and without being able to inject this config, the checksum function won't succeed. The above\n        `load_data` function provides data that can be manually verified by checking the `rqworker`\n        log file in `/tmp/`.\n        The other part of the migration, creating a table, cannot be tested regardless.\n        \"\"\"",
            "file": "migration_b58139cfdc8c.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester(Helper):\n    def __init__(self, config):\n        Helper.__init__(self)\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            self.create_journalist()\n            self.create_source()\n\n            # create a submission and a reply that we don't add checksums to\n            self.create_submission(checksum=False)\n            self.create_reply(checksum=False)\n\n            # create a submission and a reply that have checksums added\n            self.create_submission(checksum=True)\n            self.create_reply(checksum=True)\n\n            # add a revoked token for enable a foreign key connection\n            self.add_revoked_token()\n\n    def check_downgrade(self):\n        \"\"\"\n        Verify that the checksum column is now gone.\n        The dropping of the revoked_tokens table cannot be checked. If the migration completes,\n        then it worked correctly.\n        \"\"\"\n        with self.app.app_context():\n            sql = \"SELECT * FROM submissions\"\n            submissions = db.engine.execute(text(sql)).fetchall()\n            for submission in submissions:\n                with pytest.raises(NoSuchColumnError):\n                    submission[\"checksum\"]\n\n            sql = \"SELECT * FROM replies\"\n            replies = db.engine.execute(text(sql)).fetchall()\n            for reply in replies:\n                with pytest.raises(NoSuchColumnError):\n                    reply[\"checksum\"]\n\n    def add_revoked_token(self):\n        params = {\n            \"journalist_id\": self.journalist_id,\n            \"token\": \"abc123\",\n        }\n        sql = \"\"\"INSERT INTO revoked_tokens (journalist_id, token)\n                 VALUES (:journalist_id, :token)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_b58139cfdc8c.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        Helper.__init__(self)\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_b58139cfdc8c.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            self.create_journalist()\n            self.create_source()\n\n            # create a submission and a reply that we don't add checksums to\n            self.create_submission(checksum=False)\n            self.create_reply(checksum=False)\n\n            # create a submission and a reply that have checksums added\n            self.create_submission(checksum=True)\n            self.create_reply(checksum=True)\n\n            # add a revoked token for enable a foreign key connection\n            self.add_revoked_token()",
            "file": "migration_b58139cfdc8c.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        \"\"\"\n        Verify that the checksum column is now gone.\n        The dropping of the revoked_tokens table cannot be checked. If the migration completes,\n        then it worked correctly.\n        \"\"\"\n        with self.app.app_context():\n            sql = \"SELECT * FROM submissions\"\n            submissions = db.engine.execute(text(sql)).fetchall()\n            for submission in submissions:\n                with pytest.raises(NoSuchColumnError):\n                    submission[\"checksum\"]\n\n            sql = \"SELECT * FROM replies\"\n            replies = db.engine.execute(text(sql)).fetchall()\n            for reply in replies:\n                with pytest.raises(NoSuchColumnError):\n                    reply[\"checksum\"]",
            "file": "migration_b58139cfdc8c.py"
          },
          {
            "type": "function",
            "name": "add_revoked_token",
            "code": "def add_revoked_token(self):\n        params = {\n            \"journalist_id\": self.journalist_id,\n            \"token\": \"abc123\",\n        }\n        sql = \"\"\"INSERT INTO revoked_tokens (journalist_id, token)\n                 VALUES (:journalist_id, :token)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_b58139cfdc8c.py"
          }
        ],
        "migration_17c559a7a685.py": [
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester:\n    def __init__(self, config):\n        \"\"\"This function MUST accept an argument named `config`.\n        You will likely want to save a reference to the config in your\n        class so you can access the database later.\n        \"\"\"\n        self.config = config\n        self.app = create_app(self.config)\n        self.gpg = gnupg.GPG(\n            binary=\"gpg2\",\n            homedir=str(config.GPG_KEY_DIR),\n            options=[\"--pinentry-mode loopback\", \"--trust-model direct\"],\n        )\n        self.fingerprint = None\n        # random, chosen by fair dice roll\n        self.filesystem_id = (\n            \"HAR5WIY3C4K3MMIVLYXER7DMTYCL5PWZEPNOCR2AIBCVWXDZQDMDFUHEFJM\"\n            \"Z3JW5D6SLED3YKCBDAKNMSIYOKWEJK3ZRJT3WEFT3S5Q=\"\n        )\n\n    def load_data(self):\n        \"\"\"Create a source and GPG key pair\"\"\"\n        with self.app.app_context():\n            source = {\n                \"uuid\": str(uuid.uuid4()),\n                \"filesystem_id\": self.filesystem_id,\n                \"journalist_designation\": \"psychic webcam\",\n                \"interaction_count\": 0,\n            }\n            sql = \"\"\"\\\n                INSERT INTO sources (uuid, filesystem_id, journalist_designation,\n                    interaction_count)\n                VALUES (:uuid, :filesystem_id, :journalist_designation,\n                    :interaction_count)\"\"\"\n            db.engine.execute(text(sql), **source)\n            # Generate the GPG key pair\n            gen_key_input = self.gpg.gen_key_input(\n                passphrase=\"correct horse battery staple\",\n                name_email=self.filesystem_id,\n                key_type=\"RSA\",\n                key_length=4096,\n                name_real=\"Source Key\",\n                creation_date=\"2013-05-14\",\n                expire_date=\"0\",\n            )\n            key = self.gpg.gen_key(gen_key_input)\n            self.fingerprint = str(key.fingerprint)\n\n    def check_upgrade(self):\n        \"\"\"Verify PGP fields have been populated\"\"\"\n        with self.app.app_context():\n            query_sql = \"\"\"\\\n            SELECT pgp_fingerprint, pgp_public_key, pgp_secret_key\n            FROM sources\n            WHERE filesystem_id = :filesystem_id\"\"\"\n            source = db.engine.execute(\n                text(query_sql),\n                filesystem_id=self.filesystem_id,\n            ).fetchone()\n            assert source[0] == self.fingerprint\n            assert redwood.is_valid_public_key(source[1]) == self.fingerprint\n            assert source[2] is None",
            "file": "migration_17c559a7a685.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        \"\"\"This function MUST accept an argument named `config`.\n        You will likely want to save a reference to the config in your\n        class so you can access the database later.\n        \"\"\"\n        self.config = config\n        self.app = create_app(self.config)\n        self.gpg = gnupg.GPG(\n            binary=\"gpg2\",\n            homedir=str(config.GPG_KEY_DIR),\n            options=[\"--pinentry-mode loopback\", \"--trust-model direct\"],\n        )\n        self.fingerprint = None\n        # random, chosen by fair dice roll\n        self.filesystem_id = (\n            \"HAR5WIY3C4K3MMIVLYXER7DMTYCL5PWZEPNOCR2AIBCVWXDZQDMDFUHEFJM\"\n            \"Z3JW5D6SLED3YKCBDAKNMSIYOKWEJK3ZRJT3WEFT3S5Q=\"\n        )",
            "file": "migration_17c559a7a685.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        \"\"\"Create a source and GPG key pair\"\"\"\n        with self.app.app_context():\n            source = {\n                \"uuid\": str(uuid.uuid4()),\n                \"filesystem_id\": self.filesystem_id,\n                \"journalist_designation\": \"psychic webcam\",\n                \"interaction_count\": 0,\n            }\n            sql = \"\"\"\\\n                INSERT INTO sources (uuid, filesystem_id, journalist_designation,\n                    interaction_count)\n                VALUES (:uuid, :filesystem_id, :journalist_designation,\n                    :interaction_count)\"\"\"\n            db.engine.execute(text(sql), **source)\n            # Generate the GPG key pair\n            gen_key_input = self.gpg.gen_key_input(\n                passphrase=\"correct horse battery staple\",\n                name_email=self.filesystem_id,\n                key_type=\"RSA\",\n                key_length=4096,\n                name_real=\"Source Key\",\n                creation_date=\"2013-05-14\",\n                expire_date=\"0\",\n            )\n            key = self.gpg.gen_key(gen_key_input)\n            self.fingerprint = str(key.fingerprint)",
            "file": "migration_17c559a7a685.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        \"\"\"Verify PGP fields have been populated\"\"\"\n        with self.app.app_context():\n            query_sql = \"\"\"\\\n            SELECT pgp_fingerprint, pgp_public_key, pgp_secret_key\n            FROM sources\n            WHERE filesystem_id = :filesystem_id\"\"\"\n            source = db.engine.execute(\n                text(query_sql),\n                filesystem_id=self.filesystem_id,\n            ).fetchone()\n            assert source[0] == self.fingerprint\n            assert redwood.is_valid_public_key(source[1]) == self.fingerprint\n            assert source[2] is None",
            "file": "migration_17c559a7a685.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester:\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(self.config)\n        self.uuid = str(uuid.uuid4())\n\n    def load_data(self):\n        \"\"\"Create a source with a PGP key pair already migrated\"\"\"\n        with self.app.app_context():\n            source = {\n                \"uuid\": self.uuid,\n                \"filesystem_id\": \"1234\",\n                \"journalist_designation\": \"mucky pine\",\n                \"interaction_count\": 0,\n                \"pgp_fingerprint\": \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\n                \"pgp_public_key\": \"very public\",\n                \"pgp_secret_key\": None,\n            }\n            sql = \"\"\"\\\n                INSERT INTO sources (uuid, filesystem_id, journalist_designation,\n                    interaction_count, pgp_fingerprint, pgp_public_key, pgp_secret_key)\n                VALUES (:uuid, :filesystem_id, :journalist_designation,\n                    :interaction_count, :pgp_fingerprint, :pgp_public_key, :pgp_secret_key)\"\"\"\n            db.engine.execute(text(sql), **source)\n\n    def check_downgrade(self):\n        \"\"\"Verify the downgrade does nothing, i.e. the two PGP fields are still populated\"\"\"\n        with self.app.app_context():\n            sql = \"\"\"\\\n            SELECT pgp_fingerprint, pgp_public_key, pgp_secret_key\n            FROM sources\n            WHERE uuid = :uuid\"\"\"\n            source = db.engine.execute(\n                text(sql),\n                uuid=self.uuid,\n            ).fetchone()\n            print(source)\n            assert source == (\n                \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\n                \"very public\",\n                None,\n            )",
            "file": "migration_17c559a7a685.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(self.config)\n        self.uuid = str(uuid.uuid4())",
            "file": "migration_17c559a7a685.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        \"\"\"Create a source with a PGP key pair already migrated\"\"\"\n        with self.app.app_context():\n            source = {\n                \"uuid\": self.uuid,\n                \"filesystem_id\": \"1234\",\n                \"journalist_designation\": \"mucky pine\",\n                \"interaction_count\": 0,\n                \"pgp_fingerprint\": \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\n                \"pgp_public_key\": \"very public\",\n                \"pgp_secret_key\": None,\n            }\n            sql = \"\"\"\\\n                INSERT INTO sources (uuid, filesystem_id, journalist_designation,\n                    interaction_count, pgp_fingerprint, pgp_public_key, pgp_secret_key)\n                VALUES (:uuid, :filesystem_id, :journalist_designation,\n                    :interaction_count, :pgp_fingerprint, :pgp_public_key, :pgp_secret_key)\"\"\"\n            db.engine.execute(text(sql), **source)",
            "file": "migration_17c559a7a685.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        \"\"\"Verify the downgrade does nothing, i.e. the two PGP fields are still populated\"\"\"\n        with self.app.app_context():\n            sql = \"\"\"\\\n            SELECT pgp_fingerprint, pgp_public_key, pgp_secret_key\n            FROM sources\n            WHERE uuid = :uuid\"\"\"\n            source = db.engine.execute(\n                text(sql),\n                uuid=self.uuid,\n            ).fetchone()\n            print(source)\n            assert source == (\n                \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\n                \"very public\",\n                None,\n            )",
            "file": "migration_17c559a7a685.py"
          }
        ],
        "migration_523fff3f969c.py": [
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester:\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        pass\n\n    def check_upgrade(self):\n        with self.app.app_context():\n            db.engine.execute(text(instance_config_sql)).fetchall()",
            "file": "migration_523fff3f969c.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_523fff3f969c.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        pass",
            "file": "migration_523fff3f969c.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        with self.app.app_context():\n            db.engine.execute(text(instance_config_sql)).fetchall()",
            "file": "migration_523fff3f969c.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester:\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        pass\n\n    def check_downgrade(self):\n        with self.app.app_context():\n            try:\n                db.engine.execute(text(instance_config_sql)).fetchall()\n\n            # The SQLite driver appears to return this rather than the\n            # expected NoSuchTableError.\n            except OperationalError:\n                pass",
            "file": "migration_523fff3f969c.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_523fff3f969c.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        pass",
            "file": "migration_523fff3f969c.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        with self.app.app_context():\n            try:\n                db.engine.execute(text(instance_config_sql)).fetchall()\n\n            # The SQLite driver appears to return this rather than the\n            # expected NoSuchTableError.\n            except OperationalError:\n                pass",
            "file": "migration_523fff3f969c.py"
          }
        ],
        "migration_a9fe328b053a.py": [
          {
            "type": "class",
            "name": "Helper",
            "code": "class Helper:\n    def __init__(self):\n        self.journalist_id = None\n\n    def create_journalist(self):\n        if self.journalist_id is not None:\n            raise RuntimeError(\"Journalist already created\")\n\n        params = {\n            \"uuid\": str(uuid.uuid4()),\n            \"username\": random_chars(50),\n        }\n        sql = \"\"\"INSERT INTO journalists (uuid, username)\n                 VALUES (:uuid, :username)\n              \"\"\"\n        self.journalist_id = db.engine.execute(text(sql), **params).lastrowid\n\n    def create_journalist_after_migration(self):\n        if self.journalist_id is not None:\n            raise RuntimeError(\"Journalist already created\")\n\n        params = {\n            \"uuid\": str(uuid.uuid4()),\n            \"username\": random_chars(50),\n            \"first_name\": random_chars(50),\n            \"last_name\": random_chars(50),\n        }\n        sql = \"\"\"\n        INSERT INTO journalists (uuid, username, first_name, last_name)\n        VALUES (:uuid, :username, :first_name, :last_name)\n        \"\"\"\n        self.journalist_id = db.engine.execute(text(sql), **params).lastrowid",
            "file": "migration_a9fe328b053a.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self):\n        self.journalist_id = None",
            "file": "migration_a9fe328b053a.py"
          },
          {
            "type": "function",
            "name": "create_journalist",
            "code": "def create_journalist(self):\n        if self.journalist_id is not None:\n            raise RuntimeError(\"Journalist already created\")\n\n        params = {\n            \"uuid\": str(uuid.uuid4()),\n            \"username\": random_chars(50),\n        }\n        sql = \"\"\"INSERT INTO journalists (uuid, username)\n                 VALUES (:uuid, :username)\n              \"\"\"\n        self.journalist_id = db.engine.execute(text(sql), **params).lastrowid",
            "file": "migration_a9fe328b053a.py"
          },
          {
            "type": "function",
            "name": "create_journalist_after_migration",
            "code": "def create_journalist_after_migration(self):\n        if self.journalist_id is not None:\n            raise RuntimeError(\"Journalist already created\")\n\n        params = {\n            \"uuid\": str(uuid.uuid4()),\n            \"username\": random_chars(50),\n            \"first_name\": random_chars(50),\n            \"last_name\": random_chars(50),\n        }\n        sql = \"\"\"\n        INSERT INTO journalists (uuid, username, first_name, last_name)\n        VALUES (:uuid, :username, :first_name, :last_name)\n        \"\"\"\n        self.journalist_id = db.engine.execute(text(sql), **params).lastrowid",
            "file": "migration_a9fe328b053a.py"
          },
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester(Helper):\n    def __init__(self, config):\n        Helper.__init__(self)\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            self.create_journalist()\n\n    def check_upgrade(self):\n        \"\"\"\n        - Verify that Journalist first and last names are present after upgrade.\n        \"\"\"\n        with self.app.app_context():\n            journalists_sql = \"SELECT * FROM journalists\"\n            journalists = db.engine.execute(text(journalists_sql)).fetchall()\n            for journalist in journalists:\n                assert journalist[\"first_name\"] is None\n                assert journalist[\"last_name\"] is None",
            "file": "migration_a9fe328b053a.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        Helper.__init__(self)\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_a9fe328b053a.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            self.create_journalist()",
            "file": "migration_a9fe328b053a.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        \"\"\"\n        - Verify that Journalist first and last names are present after upgrade.\n        \"\"\"\n        with self.app.app_context():\n            journalists_sql = \"SELECT * FROM journalists\"\n            journalists = db.engine.execute(text(journalists_sql)).fetchall()\n            for journalist in journalists:\n                assert journalist[\"first_name\"] is None\n                assert journalist[\"last_name\"] is None",
            "file": "migration_a9fe328b053a.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester(Helper):\n    def __init__(self, config):\n        Helper.__init__(self)\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            self.create_journalist_after_migration()\n\n    def check_downgrade(self):\n        \"\"\"\n        - Verify that Journalist first and last names are gone after downgrade.\n        \"\"\"\n        with self.app.app_context():\n            journalists_sql = \"SELECT * FROM journalists\"\n            journalists = db.engine.execute(text(journalists_sql)).fetchall()\n            for journalist in journalists:\n                try:\n                    assert journalist[\"first_name\"]\n                except NoSuchColumnError:\n                    pass\n                try:\n                    assert journalist[\"last_name\"]\n                except NoSuchColumnError:\n                    pass",
            "file": "migration_a9fe328b053a.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        Helper.__init__(self)\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_a9fe328b053a.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            self.create_journalist_after_migration()",
            "file": "migration_a9fe328b053a.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        \"\"\"\n        - Verify that Journalist first and last names are gone after downgrade.\n        \"\"\"\n        with self.app.app_context():\n            journalists_sql = \"SELECT * FROM journalists\"\n            journalists = db.engine.execute(text(journalists_sql)).fetchall()\n            for journalist in journalists:\n                try:\n                    assert journalist[\"first_name\"]\n                except NoSuchColumnError:\n                    pass\n                try:\n                    assert journalist[\"last_name\"]\n                except NoSuchColumnError:\n                    pass",
            "file": "migration_a9fe328b053a.py"
          }
        ],
        "migration_d9d36b6f4d1e.py": [
          {
            "type": "function",
            "name": "get_schema",
            "code": "def get_schema(app):\n    with app.app_context():\n        result = list(\n            db.engine.execute(\n                sqlalchemy.text(\"SELECT type, name, tbl_name, sql FROM sqlite_master\")\n            )\n        )\n\n    return ((x[0], x[1], x[2], x[3]) for x in result)",
            "file": "migration_d9d36b6f4d1e.py"
          },
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester:\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            self.update_config()\n            db.session.commit()\n\n    @staticmethod\n    def update_config():\n        params = {\n            \"valid_until\": random_datetime(nullable=False),\n            \"allow_document_uploads\": random_bool(),\n            \"organization_name\": random_ascii_chars(secrets.randbelow(75)),\n        }\n        sql = \"\"\"\n        INSERT INTO instance_config (\n            valid_until, allow_document_uploads, organization_name\n        ) VALUES (\n            :valid_until, :allow_document_uploads, :organization_name\n        )\n        \"\"\"\n\n        db.engine.execute(sqlalchemy.text(sql), **params)\n\n    def check_upgrade(self):\n        schema = get_schema(self.app)\n        print(schema)\n        assert index_definition not in schema\n\n        with self.app.app_context():\n            for query in [\n                \"SELECT * FROM instance_config WHERE initial_message_min_len != 0\",\n                \"SELECT * FROM instance_config WHERE reject_message_with_codename != 0\",\n            ]:\n                result = db.engine.execute(sqlalchemy.text(query)).fetchall()\n                assert len(result) == 0",
            "file": "migration_d9d36b6f4d1e.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_d9d36b6f4d1e.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            self.update_config()\n            db.session.commit()",
            "file": "migration_d9d36b6f4d1e.py"
          },
          {
            "type": "function",
            "name": "update_config",
            "code": "def update_config():\n        params = {\n            \"valid_until\": random_datetime(nullable=False),\n            \"allow_document_uploads\": random_bool(),\n            \"organization_name\": random_ascii_chars(secrets.randbelow(75)),\n        }\n        sql = \"\"\"\n        INSERT INTO instance_config (\n            valid_until, allow_document_uploads, organization_name\n        ) VALUES (\n            :valid_until, :allow_document_uploads, :organization_name\n        )\n        \"\"\"\n\n        db.engine.execute(sqlalchemy.text(sql), **params)",
            "file": "migration_d9d36b6f4d1e.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        schema = get_schema(self.app)\n        print(schema)\n        assert index_definition not in schema\n\n        with self.app.app_context():\n            for query in [\n                \"SELECT * FROM instance_config WHERE initial_message_min_len != 0\",\n                \"SELECT * FROM instance_config WHERE reject_message_with_codename != 0\",\n            ]:\n                result = db.engine.execute(sqlalchemy.text(query)).fetchall()\n                assert len(result) == 0",
            "file": "migration_d9d36b6f4d1e.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester:\n    def __init__(self, config):\n        self.app = create_app(config)\n\n    def load_data(self):\n        pass\n\n    def check_downgrade(self):\n        assert index_definition in get_schema(self.app)\n\n        with self.app.app_context():\n            for query in [\n                \"SELECT * FROM instance_config WHERE initial_message_min_len IS NOT NULL\",\n                \"SELECT * FROM instance_config WHERE reject_message_with_codename IS NOT NULL\",\n            ]:\n                with pytest.raises(sqlalchemy.exc.OperationalError):\n                    db.engine.execute(sqlalchemy.text(query)).fetchall()",
            "file": "migration_d9d36b6f4d1e.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.app = create_app(config)",
            "file": "migration_d9d36b6f4d1e.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        pass",
            "file": "migration_d9d36b6f4d1e.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        assert index_definition in get_schema(self.app)\n\n        with self.app.app_context():\n            for query in [\n                \"SELECT * FROM instance_config WHERE initial_message_min_len IS NOT NULL\",\n                \"SELECT * FROM instance_config WHERE reject_message_with_codename IS NOT NULL\",\n            ]:\n                with pytest.raises(sqlalchemy.exc.OperationalError):\n                    db.engine.execute(sqlalchemy.text(query)).fetchall()",
            "file": "migration_d9d36b6f4d1e.py"
          }
        ],
        "migration_6db892e17271.py": [
          {
            "type": "function",
            "name": "add_source",
            "code": "def add_source():\n    filesystem_id = random_chars(96) if random_bool() else None\n    params = {\n        \"uuid\": str(uuid.uuid4()),\n        \"filesystem_id\": filesystem_id,\n        \"journalist_designation\": random_chars(50),\n        \"flagged\": bool_or_none(),\n        \"last_updated\": random_datetime(nullable=True),\n        \"pending\": bool_or_none(),\n        \"interaction_count\": random.randint(0, 1000),\n    }\n    sql = \"\"\"INSERT INTO sources (uuid, filesystem_id,\n                journalist_designation, flagged, last_updated, pending,\n                interaction_count)\n             VALUES (:uuid, :filesystem_id, :journalist_designation,\n                :flagged, :last_updated, :pending, :interaction_count)\n          \"\"\"\n    db.engine.execute(text(sql), **params)",
            "file": "migration_6db892e17271.py"
          },
          {
            "type": "function",
            "name": "add_journalist",
            "code": "def add_journalist():\n    if random_bool():\n        otp_secret = random_chars(16, string.ascii_uppercase + \"234567\")\n    else:\n        otp_secret = None\n\n    is_totp = random_bool()\n    if is_totp:\n        hotp_counter = 0 if random_bool() else None\n    else:\n        hotp_counter = random.randint(0, 10000) if random_bool() else None\n\n    last_token = random_chars(6, string.digits) if random_bool() else None\n\n    params = {\n        \"username\": random_username(),\n        \"pw_salt\": random_bytes(1, 64, nullable=True),\n        \"pw_hash\": random_bytes(32, 64, nullable=True),\n        \"is_admin\": bool_or_none(),\n        \"otp_secret\": otp_secret,\n        \"is_totp\": is_totp,\n        \"hotp_counter\": hotp_counter,\n        \"last_token\": last_token,\n        \"created_on\": random_datetime(nullable=True),\n        \"last_access\": random_datetime(nullable=True),\n        \"passphrase_hash\": random_bytes(32, 64, nullable=True),\n    }\n    sql = \"\"\"INSERT INTO journalists (username, pw_salt, pw_hash,\n                is_admin, otp_secret, is_totp, hotp_counter, last_token,\n                created_on, last_access, passphrase_hash)\n             VALUES (:username, :pw_salt, :pw_hash, :is_admin,\n                :otp_secret, :is_totp, :hotp_counter, :last_token,\n                :created_on, :last_access, :passphrase_hash);\n          \"\"\"\n    db.engine.execute(text(sql), **params)",
            "file": "migration_6db892e17271.py"
          },
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester:\n    \"\"\"This migration verifies that the deleted_by_source column now exists,\n    and that the data migration completed successfully.\n    \"\"\"\n\n    SOURCE_NUM = 200\n    JOURNO_NUM = 20\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                add_journalist()\n\n            add_source()\n\n            for jid in range(1, self.JOURNO_NUM):\n                self.add_reply(jid, 1)\n\n            db.session.commit()\n\n    @staticmethod\n    def add_reply(journalist_id, source_id):\n        params = {\n            \"journalist_id\": journalist_id,\n            \"source_id\": source_id,\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"deleted_by_source\": False,\n        }\n        sql = \"\"\"INSERT INTO replies (journalist_id, source_id, filename,\n                    size, deleted_by_source)\n                 VALUES (:journalist_id, :source_id, :filename, :size,\n                         :deleted_by_source)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    def check_upgrade(self):\n        with self.app.app_context():\n            replies = db.engine.execute(text(\"SELECT * FROM replies\")).fetchall()\n            assert len(replies) == self.JOURNO_NUM - 1\n\n            for reply in replies:\n                assert reply.uuid is not None",
            "file": "migration_6db892e17271.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_6db892e17271.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                add_journalist()\n\n            add_source()\n\n            for jid in range(1, self.JOURNO_NUM):\n                self.add_reply(jid, 1)\n\n            db.session.commit()",
            "file": "migration_6db892e17271.py"
          },
          {
            "type": "function",
            "name": "add_reply",
            "code": "def add_reply(journalist_id, source_id):\n        params = {\n            \"journalist_id\": journalist_id,\n            \"source_id\": source_id,\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"deleted_by_source\": False,\n        }\n        sql = \"\"\"INSERT INTO replies (journalist_id, source_id, filename,\n                    size, deleted_by_source)\n                 VALUES (:journalist_id, :source_id, :filename, :size,\n                         :deleted_by_source)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_6db892e17271.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        with self.app.app_context():\n            replies = db.engine.execute(text(\"SELECT * FROM replies\")).fetchall()\n            assert len(replies) == self.JOURNO_NUM - 1\n\n            for reply in replies:\n                assert reply.uuid is not None",
            "file": "migration_6db892e17271.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester:\n    SOURCE_NUM = 200\n    JOURNO_NUM = 20\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                add_journalist()\n\n            add_source()\n\n            for jid in range(1, self.JOURNO_NUM):\n                self.add_reply(jid, 1)\n\n            db.session.commit()\n\n    @staticmethod\n    def add_reply(journalist_id, source_id):\n        params = {\n            \"journalist_id\": journalist_id,\n            \"source_id\": source_id,\n            \"uuid\": str(uuid.uuid4()),\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"deleted_by_source\": False,\n        }\n        sql = \"\"\"INSERT INTO replies (journalist_id, source_id, uuid, filename,\n                    size, deleted_by_source)\n                 VALUES (:journalist_id, :source_id, :uuid, :filename, :size,\n                    :deleted_by_source)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    def check_downgrade(self):\n        \"\"\"Verify that the deleted_by_source column is now gone, and\n        otherwise the table has the expected number of rows.\n        \"\"\"\n        with self.app.app_context():\n            sql = \"SELECT * FROM replies\"\n            replies = db.engine.execute(text(sql)).fetchall()\n\n            for reply in replies:\n                try:\n                    # This should produce an exception, as the column (should)\n                    # be gone.\n                    assert reply[\"uuid\"] is None\n                except NoSuchColumnError:\n                    pass\n\n            assert len(replies) == self.JOURNO_NUM - 1",
            "file": "migration_6db892e17271.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_6db892e17271.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.JOURNO_NUM):\n                add_journalist()\n\n            add_source()\n\n            for jid in range(1, self.JOURNO_NUM):\n                self.add_reply(jid, 1)\n\n            db.session.commit()",
            "file": "migration_6db892e17271.py"
          },
          {
            "type": "function",
            "name": "add_reply",
            "code": "def add_reply(journalist_id, source_id):\n        params = {\n            \"journalist_id\": journalist_id,\n            \"source_id\": source_id,\n            \"uuid\": str(uuid.uuid4()),\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"deleted_by_source\": False,\n        }\n        sql = \"\"\"INSERT INTO replies (journalist_id, source_id, uuid, filename,\n                    size, deleted_by_source)\n                 VALUES (:journalist_id, :source_id, :uuid, :filename, :size,\n                    :deleted_by_source)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_6db892e17271.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        \"\"\"Verify that the deleted_by_source column is now gone, and\n        otherwise the table has the expected number of rows.\n        \"\"\"\n        with self.app.app_context():\n            sql = \"SELECT * FROM replies\"\n            replies = db.engine.execute(text(sql)).fetchall()\n\n            for reply in replies:\n                try:\n                    # This should produce an exception, as the column (should)\n                    # be gone.\n                    assert reply[\"uuid\"] is None\n                except NoSuchColumnError:\n                    pass\n\n            assert len(replies) == self.JOURNO_NUM - 1",
            "file": "migration_6db892e17271.py"
          }
        ],
        "migration_92fba0be98e9.py": [
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester:\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            self.update_config()\n\n            db.session.commit()\n\n    @staticmethod\n    def update_config():\n        params = {\n            \"valid_until\": random_datetime(nullable=True),\n            \"allow_document_uploads\": random_bool(),\n        }\n        sql = \"\"\"\n        INSERT INTO instance_config (\n            valid_until, allow_document_uploads\n        ) VALUES (\n            :valid_until, :allow_document_uploads\n        )\n        \"\"\"\n\n        db.engine.execute(sqlalchemy.text(sql), **params)\n\n    def check_upgrade(self):\n        \"\"\"\n        Check the new `organization_name` column\n\n        Querying `organization_name` shouldn't cause an error, but it should not yet be set.\n        \"\"\"\n        with self.app.app_context():\n            configs = db.engine.execute(\n                sqlalchemy.text(\"SELECT * FROM instance_config WHERE organization_name IS NOT NULL\")\n            ).fetchall()\n            assert len(configs) == 0",
            "file": "migration_92fba0be98e9.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_92fba0be98e9.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            self.update_config()\n\n            db.session.commit()",
            "file": "migration_92fba0be98e9.py"
          },
          {
            "type": "function",
            "name": "update_config",
            "code": "def update_config():\n        params = {\n            \"valid_until\": random_datetime(nullable=True),\n            \"allow_document_uploads\": random_bool(),\n        }\n        sql = \"\"\"\n        INSERT INTO instance_config (\n            valid_until, allow_document_uploads\n        ) VALUES (\n            :valid_until, :allow_document_uploads\n        )\n        \"\"\"\n\n        db.engine.execute(sqlalchemy.text(sql), **params)",
            "file": "migration_92fba0be98e9.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        \"\"\"\n        Check the new `organization_name` column\n\n        Querying `organization_name` shouldn't cause an error, but it should not yet be set.\n        \"\"\"\n        with self.app.app_context():\n            configs = db.engine.execute(\n                sqlalchemy.text(\"SELECT * FROM instance_config WHERE organization_name IS NOT NULL\")\n            ).fetchall()\n            assert len(configs) == 0",
            "file": "migration_92fba0be98e9.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester:\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        pass\n\n    def check_downgrade(self):\n        \"\"\"\n        After downgrade, using `organization_name` in a query should raise an exception\n        \"\"\"\n        with self.app.app_context(), pytest.raises(sqlalchemy.exc.OperationalError):\n            db.engine.execute(\n                sqlalchemy.text(\"SELECT * FROM instance_config WHERE organization_name IS NOT NULL\")\n            ).fetchall()",
            "file": "migration_92fba0be98e9.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_92fba0be98e9.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        pass",
            "file": "migration_92fba0be98e9.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        \"\"\"\n        After downgrade, using `organization_name` in a query should raise an exception\n        \"\"\"\n        with self.app.app_context(), pytest.raises(sqlalchemy.exc.OperationalError):\n            db.engine.execute(\n                sqlalchemy.text(\"SELECT * FROM instance_config WHERE organization_name IS NOT NULL\")\n            ).fetchall()",
            "file": "migration_92fba0be98e9.py"
          }
        ],
        "migration_b060f38c0c31.py": [
          {
            "type": "function",
            "name": "add_submission",
            "code": "def add_submission(source_id):\n    params = {\n        \"uuid\": str(uuid.uuid4()),\n        \"source_id\": source_id,\n        \"filename\": random_chars(50),\n        \"size\": random.randint(0, 1024 * 1024 * 500),\n        \"downloaded\": bool_or_none(),\n        \"checksum\": random_chars(255, chars=\"0123456789abcdef\"),\n    }\n    sql = \"\"\"\n    INSERT INTO submissions (uuid, source_id, filename, size, downloaded, checksum)\n    VALUES (:uuid, :source_id, :filename, :size, :downloaded, :checksum)\n    \"\"\"\n    db.engine.execute(text(sql), **params)",
            "file": "migration_b060f38c0c31.py"
          },
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester:\n    \"\"\"\n    Verify that the Source.flagged column no longer exists.\n    \"\"\"\n\n    source_count = 10\n    original_sources: Dict[str, Any] = {}\n    source_submissions: Dict[str, Any] = {}\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            for _i in range(self.source_count):\n                self.add_source()\n\n            self.original_sources = {\n                s.uuid: s for s in db.engine.execute(text(\"SELECT * FROM sources\")).fetchall()\n            }\n\n            for s in self.original_sources.values():\n                for _i in range(random.randint(0, 3)):\n                    add_submission(s.id)\n\n                self.source_submissions[s.id] = db.engine.execute(\n                    text(\"SELECT * FROM submissions WHERE source_id = :source_id\"),\n                    source_id=s.id,\n                ).fetchall()\n\n    def add_source(self):\n        params = {\n            \"uuid\": str(uuid.uuid4()),\n            \"filesystem_id\": random_chars(96),\n            \"journalist_designation\": random_chars(50),\n            \"flagged\": bool_or_none(),\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": bool_or_none(),\n            \"interaction_count\": random.randint(0, 1000),\n        }\n        sql = \"\"\"\n        INSERT INTO sources (uuid, filesystem_id,\n        journalist_designation, flagged, last_updated, pending,\n        interaction_count)\n        VALUES (:uuid, :filesystem_id, :journalist_designation,\n        :flagged, :last_updated, :pending, :interaction_count)\n        \"\"\"\n\n        db.engine.execute(text(sql), **params)\n\n    def check_upgrade(self):\n        with self.app.app_context():\n            # check that the flagged column is gone\n            with pytest.raises(OperationalError, match=\".*sources has no column named flagged.*\"):\n                self.add_source()\n\n            # check that the sources are otherwise unchanged\n            sources = db.engine.execute(text(\"SELECT * FROM sources\")).fetchall()\n            assert len(sources) == len(self.original_sources)\n            for source in sources:\n                assert not hasattr(source, \"flagged\")\n                original_source = self.original_sources[source.uuid]\n                assert source.id == original_source.id\n                assert source.journalist_designation == original_source.journalist_designation\n                assert source.last_updated == original_source.last_updated\n                assert source.pending == original_source.pending\n                assert source.interaction_count == original_source.interaction_count\n\n                source_submissions = db.engine.execute(\n                    text(\"SELECT * FROM submissions WHERE source_id = :source_id\"),\n                    source_id=source.id,\n                ).fetchall()\n                assert source_submissions == self.source_submissions[source.id]",
            "file": "migration_b060f38c0c31.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_b060f38c0c31.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            for _i in range(self.source_count):\n                self.add_source()\n\n            self.original_sources = {\n                s.uuid: s for s in db.engine.execute(text(\"SELECT * FROM sources\")).fetchall()\n            }\n\n            for s in self.original_sources.values():\n                for _i in range(random.randint(0, 3)):\n                    add_submission(s.id)\n\n                self.source_submissions[s.id] = db.engine.execute(\n                    text(\"SELECT * FROM submissions WHERE source_id = :source_id\"),\n                    source_id=s.id,\n                ).fetchall()",
            "file": "migration_b060f38c0c31.py"
          },
          {
            "type": "function",
            "name": "add_source",
            "code": "def add_source(self):\n        params = {\n            \"uuid\": str(uuid.uuid4()),\n            \"filesystem_id\": random_chars(96),\n            \"journalist_designation\": random_chars(50),\n            \"flagged\": bool_or_none(),\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": bool_or_none(),\n            \"interaction_count\": random.randint(0, 1000),\n        }\n        sql = \"\"\"\n        INSERT INTO sources (uuid, filesystem_id,\n        journalist_designation, flagged, last_updated, pending,\n        interaction_count)\n        VALUES (:uuid, :filesystem_id, :journalist_designation,\n        :flagged, :last_updated, :pending, :interaction_count)\n        \"\"\"\n\n        db.engine.execute(text(sql), **params)",
            "file": "migration_b060f38c0c31.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        with self.app.app_context():\n            # check that the flagged column is gone\n            with pytest.raises(OperationalError, match=\".*sources has no column named flagged.*\"):\n                self.add_source()\n\n            # check that the sources are otherwise unchanged\n            sources = db.engine.execute(text(\"SELECT * FROM sources\")).fetchall()\n            assert len(sources) == len(self.original_sources)\n            for source in sources:\n                assert not hasattr(source, \"flagged\")\n                original_source = self.original_sources[source.uuid]\n                assert source.id == original_source.id\n                assert source.journalist_designation == original_source.journalist_designation\n                assert source.last_updated == original_source.last_updated\n                assert source.pending == original_source.pending\n                assert source.interaction_count == original_source.interaction_count\n\n                source_submissions = db.engine.execute(\n                    text(\"SELECT * FROM submissions WHERE source_id = :source_id\"),\n                    source_id=source.id,\n                ).fetchall()\n                assert source_submissions == self.source_submissions[source.id]",
            "file": "migration_b060f38c0c31.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester:\n    \"\"\"\n    Verify that the Source.flagged column has been recreated properly.\n    \"\"\"\n\n    source_count = 10\n    original_sources: Dict[str, Any] = {}\n    source_submissions: Dict[str, Any] = {}\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def add_source(self):\n        params = {\n            \"uuid\": str(uuid.uuid4()),\n            \"filesystem_id\": random_chars(96),\n            \"journalist_designation\": random_chars(50),\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": bool_or_none(),\n            \"interaction_count\": random.randint(0, 1000),\n            \"deleted_at\": None,\n        }\n        sql = \"\"\"\n        INSERT INTO sources (\n        uuid, filesystem_id, journalist_designation, last_updated, pending,\n        interaction_count\n        ) VALUES (\n        :uuid, :filesystem_id, :journalist_designation, :last_updated, :pending,\n        :interaction_count\n        )\n        \"\"\"\n\n        db.engine.execute(text(sql), **params)\n\n    def load_data(self):\n        with self.app.app_context():\n            for _i in range(self.source_count):\n                self.add_source()\n\n            self.original_sources = {\n                s.uuid: s for s in db.engine.execute(text(\"SELECT * FROM sources\")).fetchall()\n            }\n\n            for s in self.original_sources.values():\n                for _i in range(random.randint(0, 3)):\n                    add_submission(s.id)\n\n                self.source_submissions[s.id] = db.engine.execute(\n                    text(\"SELECT * FROM submissions WHERE source_id = :source_id\"),\n                    source_id=s.id,\n                ).fetchall()\n\n    def check_downgrade(self):\n        with self.app.app_context():\n            # check that the sources are otherwise unchanged\n            sources = db.engine.execute(text(\"SELECT * FROM sources\")).fetchall()\n            assert len(sources) == len(self.original_sources)\n            for source in sources:\n                assert hasattr(source, \"flagged\")\n                original_source = self.original_sources[source.uuid]\n                assert source.id == original_source.id\n                assert source.journalist_designation == original_source.journalist_designation\n                assert source.last_updated == original_source.last_updated\n                assert source.pending == original_source.pending\n                assert source.interaction_count == original_source.interaction_count\n                assert not hasattr(original_source, \"flagged\")\n                assert source.flagged is None\n\n                source_submissions = db.engine.execute(\n                    text(\"SELECT * FROM submissions WHERE source_id = :source_id\"),\n                    source_id=source.id,\n                ).fetchall()\n                assert source_submissions == self.source_submissions[source.id]",
            "file": "migration_b060f38c0c31.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_b060f38c0c31.py"
          },
          {
            "type": "function",
            "name": "add_source",
            "code": "def add_source(self):\n        params = {\n            \"uuid\": str(uuid.uuid4()),\n            \"filesystem_id\": random_chars(96),\n            \"journalist_designation\": random_chars(50),\n            \"last_updated\": random_datetime(nullable=True),\n            \"pending\": bool_or_none(),\n            \"interaction_count\": random.randint(0, 1000),\n            \"deleted_at\": None,\n        }\n        sql = \"\"\"\n        INSERT INTO sources (\n        uuid, filesystem_id, journalist_designation, last_updated, pending,\n        interaction_count\n        ) VALUES (\n        :uuid, :filesystem_id, :journalist_designation, :last_updated, :pending,\n        :interaction_count\n        )\n        \"\"\"\n\n        db.engine.execute(text(sql), **params)",
            "file": "migration_b060f38c0c31.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            for _i in range(self.source_count):\n                self.add_source()\n\n            self.original_sources = {\n                s.uuid: s for s in db.engine.execute(text(\"SELECT * FROM sources\")).fetchall()\n            }\n\n            for s in self.original_sources.values():\n                for _i in range(random.randint(0, 3)):\n                    add_submission(s.id)\n\n                self.source_submissions[s.id] = db.engine.execute(\n                    text(\"SELECT * FROM submissions WHERE source_id = :source_id\"),\n                    source_id=s.id,\n                ).fetchall()",
            "file": "migration_b060f38c0c31.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        with self.app.app_context():\n            # check that the sources are otherwise unchanged\n            sources = db.engine.execute(text(\"SELECT * FROM sources\")).fetchall()\n            assert len(sources) == len(self.original_sources)\n            for source in sources:\n                assert hasattr(source, \"flagged\")\n                original_source = self.original_sources[source.uuid]\n                assert source.id == original_source.id\n                assert source.journalist_designation == original_source.journalist_designation\n                assert source.last_updated == original_source.last_updated\n                assert source.pending == original_source.pending\n                assert source.interaction_count == original_source.interaction_count\n                assert not hasattr(original_source, \"flagged\")\n                assert source.flagged is None\n\n                source_submissions = db.engine.execute(\n                    text(\"SELECT * FROM submissions WHERE source_id = :source_id\"),\n                    source_id=source.id,\n                ).fetchall()\n                assert source_submissions == self.source_submissions[source.id]",
            "file": "migration_b060f38c0c31.py"
          }
        ],
        "migration_fccf57ceef02.py": [
          {
            "type": "function",
            "name": "add_source",
            "code": "def add_source():\n    filesystem_id = random_chars(96) if random_bool() else None\n    params = {\n        \"filesystem_id\": filesystem_id,\n        \"uuid\": str(uuid.uuid4()),\n        \"journalist_designation\": random_chars(50),\n        \"flagged\": bool_or_none(),\n        \"last_updated\": random_datetime(nullable=True),\n        \"pending\": bool_or_none(),\n        \"interaction_count\": random.randint(0, 1000),\n    }\n    sql = \"\"\"INSERT INTO sources (filesystem_id, uuid,\n                journalist_designation, flagged, last_updated, pending,\n                interaction_count)\n             VALUES (:filesystem_id, :uuid, :journalist_designation,\n                :flagged, :last_updated, :pending, :interaction_count)\n          \"\"\"\n    db.engine.execute(text(sql), **params)",
            "file": "migration_fccf57ceef02.py"
          },
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester:\n    \"\"\"This migration verifies that the UUID column now exists, and that\n    the data migration completed successfully.\n    \"\"\"\n\n    SOURCE_NUM = 200\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.SOURCE_NUM):\n                add_source()\n\n            for sid in range(1, self.SOURCE_NUM, 8):\n                for _ in range(random.randint(1, 3)):\n                    self.add_submission(sid)\n\n            # create \"abandoned\" submissions (issue #1189)\n            for sid in range(self.SOURCE_NUM, self.SOURCE_NUM + 50):\n                self.add_submission(sid)\n\n            db.session.commit()\n\n    @staticmethod\n    def add_submission(source_id):\n        params = {\n            \"source_id\": source_id,\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"downloaded\": bool_or_none(),\n        }\n        sql = \"\"\"INSERT INTO submissions (source_id, filename, size,\n                    downloaded)\n                 VALUES (:source_id, :filename, :size, :downloaded)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    def check_upgrade(self):\n        with self.app.app_context():\n            submissions = db.engine.execute(text(\"SELECT * FROM submissions\")).fetchall()\n\n            for submission in submissions:\n                assert submission.uuid is not None",
            "file": "migration_fccf57ceef02.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_fccf57ceef02.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.SOURCE_NUM):\n                add_source()\n\n            for sid in range(1, self.SOURCE_NUM, 8):\n                for _ in range(random.randint(1, 3)):\n                    self.add_submission(sid)\n\n            # create \"abandoned\" submissions (issue #1189)\n            for sid in range(self.SOURCE_NUM, self.SOURCE_NUM + 50):\n                self.add_submission(sid)\n\n            db.session.commit()",
            "file": "migration_fccf57ceef02.py"
          },
          {
            "type": "function",
            "name": "add_submission",
            "code": "def add_submission(source_id):\n        params = {\n            \"source_id\": source_id,\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"downloaded\": bool_or_none(),\n        }\n        sql = \"\"\"INSERT INTO submissions (source_id, filename, size,\n                    downloaded)\n                 VALUES (:source_id, :filename, :size, :downloaded)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_fccf57ceef02.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        with self.app.app_context():\n            submissions = db.engine.execute(text(\"SELECT * FROM submissions\")).fetchall()\n\n            for submission in submissions:\n                assert submission.uuid is not None",
            "file": "migration_fccf57ceef02.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester:\n    SOURCE_NUM = 200\n\n    def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)\n\n    def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.SOURCE_NUM):\n                add_source()\n\n            for sid in range(1, self.SOURCE_NUM, 8):\n                for _ in range(random.randint(1, 3)):\n                    self.add_submission(sid)\n\n            # create \"abandoned\" submissions (issue #1189)\n            for sid in range(self.SOURCE_NUM, self.SOURCE_NUM + 50):\n                self.add_submission(sid)\n\n            db.session.commit()\n\n    @staticmethod\n    def add_submission(source_id):\n        params = {\n            \"source_id\": source_id,\n            \"uuid\": str(uuid.uuid4()),\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"downloaded\": bool_or_none(),\n        }\n        sql = \"\"\"INSERT INTO submissions (source_id, uuid, filename, size,\n                    downloaded)\n                 VALUES (:source_id, :uuid, :filename, :size, :downloaded)\n              \"\"\"\n        db.engine.execute(text(sql), **params)\n\n    def check_downgrade(self):\n        \"\"\"Verify that the UUID column is now gone, but otherwise the table\n        has the expected number of rows.\n        \"\"\"\n        with self.app.app_context():\n            sql = \"SELECT * FROM submissions\"\n            submissions = db.engine.execute(text(sql)).fetchall()\n\n            for submission in submissions:\n                try:\n                    # This should produce an exception, as the column (should)\n                    # be gone.\n                    assert submission[\"uuid\"] is None\n                except NoSuchColumnError:\n                    pass",
            "file": "migration_fccf57ceef02.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.config = config\n        self.app = create_app(config)",
            "file": "migration_fccf57ceef02.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        with self.app.app_context():\n            for _ in range(self.SOURCE_NUM):\n                add_source()\n\n            for sid in range(1, self.SOURCE_NUM, 8):\n                for _ in range(random.randint(1, 3)):\n                    self.add_submission(sid)\n\n            # create \"abandoned\" submissions (issue #1189)\n            for sid in range(self.SOURCE_NUM, self.SOURCE_NUM + 50):\n                self.add_submission(sid)\n\n            db.session.commit()",
            "file": "migration_fccf57ceef02.py"
          },
          {
            "type": "function",
            "name": "add_submission",
            "code": "def add_submission(source_id):\n        params = {\n            \"source_id\": source_id,\n            \"uuid\": str(uuid.uuid4()),\n            \"filename\": random_chars(50),\n            \"size\": random.randint(0, 1024 * 1024 * 500),\n            \"downloaded\": bool_or_none(),\n        }\n        sql = \"\"\"INSERT INTO submissions (source_id, uuid, filename, size,\n                    downloaded)\n                 VALUES (:source_id, :uuid, :filename, :size, :downloaded)\n              \"\"\"\n        db.engine.execute(text(sql), **params)",
            "file": "migration_fccf57ceef02.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        \"\"\"Verify that the UUID column is now gone, but otherwise the table\n        has the expected number of rows.\n        \"\"\"\n        with self.app.app_context():\n            sql = \"SELECT * FROM submissions\"\n            submissions = db.engine.execute(text(sql)).fetchall()\n\n            for submission in submissions:\n                try:\n                    # This should produce an exception, as the column (should)\n                    # be gone.\n                    assert submission[\"uuid\"] is None\n                except NoSuchColumnError:\n                    pass",
            "file": "migration_fccf57ceef02.py"
          }
        ],
        "migration_1ddb81fb88c2.py": [
          {
            "type": "function",
            "name": "get_schema",
            "code": "def get_schema(app):\n    with app.app_context():\n        result = list(\n            db.engine.execute(text(\"SELECT type, name, tbl_name, sql FROM sqlite_master\"))\n        )\n\n    return ((x[0], x[1], x[2], x[3]) for x in result)",
            "file": "migration_1ddb81fb88c2.py"
          },
          {
            "type": "class",
            "name": "UpgradeTester",
            "code": "class UpgradeTester:\n    \"\"\"\n    Ensure that the new index is created.\n    \"\"\"\n\n    def __init__(self, config):\n        self.app = create_app(config)\n\n    def load_data(self):\n        pass\n\n    def check_upgrade(self):\n        schema = get_schema(self.app)\n        print(schema)\n        assert index_definition in schema",
            "file": "migration_1ddb81fb88c2.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.app = create_app(config)",
            "file": "migration_1ddb81fb88c2.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        pass",
            "file": "migration_1ddb81fb88c2.py"
          },
          {
            "type": "function",
            "name": "check_upgrade",
            "code": "def check_upgrade(self):\n        schema = get_schema(self.app)\n        print(schema)\n        assert index_definition in schema",
            "file": "migration_1ddb81fb88c2.py"
          },
          {
            "type": "class",
            "name": "DowngradeTester",
            "code": "class DowngradeTester:\n    \"\"\"\n    Ensure that the new index is removed.\n    \"\"\"\n\n    def __init__(self, config):\n        self.app = create_app(config)\n\n    def load_data(self):\n        pass\n\n    def check_downgrade(self):\n        assert index_definition not in get_schema(self.app)",
            "file": "migration_1ddb81fb88c2.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self, config):\n        self.app = create_app(config)",
            "file": "migration_1ddb81fb88c2.py"
          },
          {
            "type": "function",
            "name": "load_data",
            "code": "def load_data(self):\n        pass",
            "file": "migration_1ddb81fb88c2.py"
          },
          {
            "type": "function",
            "name": "check_downgrade",
            "code": "def check_downgrade(self):\n        assert index_definition not in get_schema(self.app)",
            "file": "migration_1ddb81fb88c2.py"
          }
        ]
      }
    },
    "journalist_app": {
      "utils.py": [
        {
          "type": "function",
          "name": "commit_account_changes",
          "code": "def commit_account_changes(user: Journalist) -> None:\n    if db.session.is_modified(user):\n        try:\n            db.session.add(user)\n            db.session.commit()\n        except Exception as e:\n            flash(\n                gettext(\"An unexpected error occurred! Please \" \"inform your admin.\"),\n                \"error\",\n            )\n            current_app.logger.error(f\"Account changes for '{user}' failed: {e}\")\n            db.session.rollback()\n        else:\n            flash(gettext(\"Account updated.\"), \"success\")",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "get_source",
          "code": "def get_source(filesystem_id: str, include_deleted: bool = False) -> Source:\n    \"\"\"\n    Return the Source object with `filesystem_id`\n\n    If `include_deleted` is False, only sources with a null `deleted_at` will\n    be returned.\n    \"\"\"\n    query = Source.query.filter(Source.filesystem_id == filesystem_id)\n    if not include_deleted:\n        query = query.filter_by(deleted_at=None)\n    return get_one_or_else(query, current_app.logger, abort)",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "validate_user",
          "code": "def validate_user(\n    username: str,\n    password: Optional[str],\n    token: Optional[str],\n    error_message: Optional[str] = None,\n) -> Optional[Journalist]:\n    \"\"\"\n    Validates the user by calling the login and handling exceptions\n    :param username: Username\n    :param password: Password\n    :param token: Two-factor authentication token\n    :param error_message: Localized error message string to use on failure\n    :return: Journalist user object if successful, None otherwise.\n    \"\"\"\n    try:\n        return Journalist.login(username, password, token)\n    except (\n        InvalidUsernameException,\n        OtpSecretInvalid,\n        OtpTokenInvalid,\n        WrongPasswordException,\n        LoginThrottledException,\n        InvalidPasswordLength,\n    ) as e:\n        current_app.logger.error(f\"Login for '{username}' failed: {e}\")\n        login_flashed_msg = error_message if error_message else gettext(\"Login failed.\")\n\n        if isinstance(e, LoginThrottledException):\n            login_flashed_msg += \" \"\n            period = Journalist._LOGIN_ATTEMPT_PERIOD\n            # ngettext is needed although we always have period > 1\n            # see https://github.com/freedomofpress/securedrop/issues/2422\n            login_flashed_msg += ngettext(\n                \"Please wait at least {num} second before logging in again.\",\n                \"Please wait at least {num} seconds before logging in again.\",\n                period,\n            ).format(num=period)\n        elif isinstance(e, OtpSecretInvalid):\n            login_flashed_msg += \" \"\n            login_flashed_msg += gettext(\n                \"Your 2FA details are invalid\" \" - please contact an administrator to reset them.\"\n            )\n        else:\n            try:\n                user = Journalist.query.filter_by(username=username).one()\n                if user.is_totp:\n                    login_flashed_msg += \" \"\n                    login_flashed_msg += gettext(\n                        \"Please wait for a new code from your two-factor mobile\"\n                        \" app or security key before trying again.\"\n                    )\n            except Exception:\n                pass\n\n        flash(login_flashed_msg, \"error\")\n        return None",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "validate_hotp_secret",
          "code": "def validate_hotp_secret(user: Journalist, otp_secret: str) -> bool:\n    \"\"\"\n    Validates and sets the HOTP provided by a user\n    :param user: the change is for this instance of the User object\n    :param otp_secret: the new HOTP secret\n    :return: True if it validates, False if it does not\n    \"\"\"\n    strip_whitespace = otp_secret.replace(\" \", \"\")\n    secret_length = len(strip_whitespace)\n\n    if secret_length != HOTP.SECRET_HEX_LENGTH:\n        flash(\n            ngettext(\n                \"HOTP secrets are 40 characters long - you have entered {num}.\",\n                \"HOTP secrets are 40 characters long - you have entered {num}.\",\n                secret_length,\n            ).format(num=secret_length),\n            \"error\",\n        )\n        return False\n\n    try:\n        user.set_hotp_secret(otp_secret)\n    except (binascii.Error, TypeError) as e:\n        if \"Non-hexadecimal digit found\" in str(e):\n            flash(\n                gettext(\n                    \"Invalid HOTP secret format: \" \"please only submit letters A-F and numbers 0-9.\"\n                ),\n                \"error\",\n            )\n            return False\n        else:\n            flash(\n                gettext(\"An unexpected error occurred! \" \"Please inform your admin.\"),\n                \"error\",\n            )\n            current_app.logger.error(f\"set_hotp_secret '{otp_secret}' (id {user.id}) failed: {e}\")\n            return False\n    return True",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "mark_seen",
          "code": "def mark_seen(targets: List[Union[Submission, Reply]], user: Journalist) -> None:\n    \"\"\"\n    Marks a list of submissions or replies seen by the given journalist.\n    \"\"\"\n    for t in targets:\n        try:\n            if isinstance(t, Submission):\n                t.downloaded = True\n                if t.is_file:\n                    sf = SeenFile(file_id=t.id, journalist_id=user.id)\n                    db.session.add(sf)\n                elif t.is_message:\n                    sm = SeenMessage(message_id=t.id, journalist_id=user.id)\n                    db.session.add(sm)\n                db.session.commit()\n            elif isinstance(t, Reply):\n                sr = SeenReply(reply_id=t.id, journalist_id=user.id)\n                db.session.add(sr)\n                db.session.commit()\n        except IntegrityError as e:\n            db.session.rollback()\n            if \"UNIQUE constraint failed\" in str(e):\n                continue\n            raise",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "download",
          "code": "def download(\n    zip_basename: str,\n    submissions: List[Union[Submission, Reply]],\n    on_error_redirect: Optional[str] = None,\n) -> werkzeug.Response:\n    \"\"\"Send client contents of ZIP-file *zip_basename*-<timestamp>.zip\n    containing *submissions*. The ZIP-file, being a\n    :class:`tempfile.NamedTemporaryFile`, is stored on disk only\n    temporarily.\n\n    :param str zip_basename: The basename of the ZIP-file download.\n\n    :param list submissions: A list of :class:`models.Submission`s to\n                             include in the ZIP-file.\n    \"\"\"\n    try:\n        zf = Storage.get_default().get_bulk_archive(submissions, zip_directory=zip_basename)\n    except FileNotFoundError:\n        flash(\n            ngettext(\n                \"Your download failed because the file could not be found. An admin can find \"\n                + \"more information in the system and monitoring logs.\",\n                \"Your download failed because a file could not be found. An admin can find \"\n                + \"more information in the system and monitoring logs.\",\n                len(submissions),\n            ),\n            \"error\",\n        )\n        if on_error_redirect is None:\n            on_error_redirect = url_for(\"main.index\")\n        return redirect(on_error_redirect)\n\n    attachment_filename = \"{}--{}.zip\".format(\n        zip_basename, datetime.now(timezone.utc).strftime(\"%Y-%m-%d--%H-%M-%S\")\n    )\n\n    mark_seen(submissions, session.get_user())\n\n    return send_file(\n        zf.name,\n        mimetype=\"application/zip\",\n        download_name=attachment_filename,\n        as_attachment=True,\n    )",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "delete_file_object",
          "code": "def delete_file_object(file_object: Union[Submission, Reply]) -> None:\n    path = Storage.get_default().path(file_object.source.filesystem_id, file_object.filename)\n    try:\n        Storage.get_default().move_to_shredder(path)\n    except ValueError as e:\n        current_app.logger.error(\"could not queue file for deletion: %s\", e)\n        raise\n    finally:\n        db.session.delete(file_object)\n        db.session.commit()",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "bulk_delete",
          "code": "def bulk_delete(\n    filesystem_id: str, items_selected: List[Union[Submission, Reply]]\n) -> werkzeug.Response:\n    deletion_errors = 0\n    for item in items_selected:\n        try:\n            delete_file_object(item)\n        except ValueError:\n            deletion_errors += 1\n\n    num_selected = len(items_selected)\n    success_message = ngettext(\n        \"The item has been deleted.\", \"{num} items have been deleted.\", num_selected\n    ).format(num=num_selected)\n\n    flash(\n        Markup(\n            \"<b>{}</b> {}\".format(\n                # Translators: Precedes a message confirming the success of an operation.\n                escape(gettext(\"Success!\")),\n                escape(success_message),\n            )\n        ),\n        \"success\",\n    )\n\n    if deletion_errors > 0:\n        current_app.logger.error(\n            \"Disconnected submission entries (%d) were detected\", deletion_errors\n        )\n    return redirect(url_for(\"col.col\", filesystem_id=filesystem_id))",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "make_star_true",
          "code": "def make_star_true(filesystem_id: str) -> None:\n    source = get_source(filesystem_id)\n    if source.star:\n        source.star.starred = True\n    else:\n        source_star = SourceStar(source)\n        db.session.add(source_star)",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "make_star_false",
          "code": "def make_star_false(filesystem_id: str) -> None:\n    source = get_source(filesystem_id)\n    if not source.star:\n        source_star = SourceStar(source)\n        db.session.add(source_star)\n        db.session.commit()\n    source.star.starred = False",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "col_star",
          "code": "def col_star(cols_selected: List[str]) -> werkzeug.Response:\n    for filesystem_id in cols_selected:\n        make_star_true(filesystem_id)\n\n    db.session.commit()\n    return redirect(url_for(\"main.index\"))",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "col_un_star",
          "code": "def col_un_star(cols_selected: List[str]) -> werkzeug.Response:\n    for filesystem_id in cols_selected:\n        make_star_false(filesystem_id)\n\n    db.session.commit()\n    return redirect(url_for(\"main.index\"))",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "col_delete",
          "code": "def col_delete(cols_selected: List[str]) -> werkzeug.Response:\n    \"\"\"deleting multiple collections from the index\"\"\"\n    if len(cols_selected) < 1:\n        flash(gettext(\"No collections selected for deletion.\"), \"error\")\n    else:\n        now = datetime.now(timezone.utc)\n        sources = Source.query.filter(Source.filesystem_id.in_(cols_selected))\n        sources.update({Source.deleted_at: now}, synchronize_session=\"fetch\")\n        db.session.commit()\n\n        num = len(cols_selected)\n\n        success_message = ngettext(\n            \"The account and all data for the source have been deleted.\",\n            \"The accounts and all data for {n} sources have been deleted.\",\n            num,\n        ).format(n=num)\n\n        flash(\n            Markup(\n                \"<b>{}</b> {}\".format(\n                    # Translators: Precedes a message confirming the success of an operation.\n                    escape(gettext(\"Success!\")),\n                    escape(success_message),\n                )\n            ),\n            \"success\",\n        )\n\n    return redirect(url_for(\"main.index\"))",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "delete_source_files",
          "code": "def delete_source_files(filesystem_id: str) -> None:\n    \"\"\"deletes submissions and replies for specified source\"\"\"\n    source = get_source(filesystem_id, include_deleted=True)\n    if source is not None:\n        # queue all files for deletion and remove them from the database\n        for f in source.collection:\n            try:\n                delete_file_object(f)\n            except Exception:\n                pass",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "col_delete_data",
          "code": "def col_delete_data(cols_selected: List[str]) -> werkzeug.Response:\n    \"\"\"deletes store data for selected sources\"\"\"\n    if len(cols_selected) < 1:\n        flash(\n            Markup(\n                \"<b>{}</b> {}\".format(\n                    # Translators: Error shown when a user has not selected items to act on.\n                    escape(gettext(\"Nothing Selected\")),\n                    escape(gettext(\"You must select one or more items for deletion.\")),\n                )\n            ),\n            \"error\",\n        )\n    else:\n        for filesystem_id in cols_selected:\n            delete_source_files(filesystem_id)\n\n        flash(\n            Markup(\n                \"<b>{}</b> {}\".format(\n                    # Translators: Precedes a message confirming the success of an operation.\n                    escape(gettext(\"Success!\")),\n                    escape(gettext(\"The files and messages have been deleted.\")),\n                )\n            ),\n            \"success\",\n        )\n\n    return redirect(url_for(\"main.index\"))",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "delete_collection",
          "code": "def delete_collection(filesystem_id: str) -> None:\n    \"\"\"deletes source account including files and reply key\"\"\"\n    # Delete the source's collection of submissions\n    path = Storage.get_default().path(filesystem_id)\n    if os.path.exists(path):\n        Storage.get_default().move_to_shredder(path)\n\n    # Delete the source's reply keypair\n    EncryptionManager.get_default().delete_source_key_pair(filesystem_id)\n\n    # Delete their entry in the db\n    source = get_source(filesystem_id, include_deleted=True)\n    db.session.delete(source)\n    db.session.commit()",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "purge_deleted_sources",
          "code": "def purge_deleted_sources() -> None:\n    \"\"\"\n    Deletes all Sources with a non-null `deleted_at` attribute.\n    \"\"\"\n    sources = Source.query.filter(Source.deleted_at.isnot(None)).order_by(Source.deleted_at).all()\n    if sources:\n        current_app.logger.info(\"Purging deleted sources (%s)\", len(sources))\n    for source in sources:\n        try:\n            delete_collection(source.filesystem_id)\n        except Exception as e:\n            current_app.logger.error(\"Error deleting source %s: %s\", source.uuid, e)",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "set_name",
          "code": "def set_name(user: Journalist, first_name: Optional[str], last_name: Optional[str]) -> None:\n    try:\n        user.set_name(first_name, last_name)\n        db.session.commit()\n        flash(gettext(\"Name updated.\"), \"success\")\n    except FirstOrLastNameError as e:\n        flash(gettext(\"Name not updated: {message}\").format(message=e), \"error\")",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "set_pending_password",
          "code": "def set_pending_password(for_: Union[Journalist, Literal[\"new\"]], passphrase: str) -> None:\n    \"\"\"\n    The user has requested a password change, but hasn't confirmed it yet.\n\n    NOTE: This mutates the current session and not the database.\n\n    We keep track of the hash so we can verify they are using the password\n    we provided for them. It is expected they hit /new-password  →\n    utils.set_diceware_password()\n    \"\"\"\n    hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n    # Include the user's id in the hash to avoid possible collisions in case we're\n    # resetting someone else's password.\n    if isinstance(for_, Journalist):\n        id = str(for_.id)\n    else:  # \"new\"\n        id = for_\n    session[f\"pending_password_{id}\"] = hasher.hash(passphrase)",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "verify_pending_password",
          "code": "def verify_pending_password(for_: Union[Journalist, Literal[\"new\"]], passphrase: str) -> None:\n    if isinstance(for_, Journalist):\n        id = str(for_.id)\n    else:  # \"new\"\n        id = for_\n    pending_password_hash = session.get(f\"pending_password_{id}\")\n    if pending_password_hash is None:\n        raise PasswordError()\n    hasher = argon2.PasswordHasher(**ARGON2_PARAMS)\n    try:\n        hasher.verify(pending_password_hash, passphrase)\n    except argon2.exceptions.VerificationError:\n        raise PasswordError()",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "set_diceware_password",
          "code": "def set_diceware_password(\n    user: Journalist, password: Optional[str], admin: Optional[bool] = False\n) -> bool:\n    try:\n        if password is not None:\n            # FIXME: password being None will trigger an error in set_password(), we\n            # should turn it into a type error\n            verify_pending_password(user, password)\n        # nosemgrep: python.django.security.audit.unvalidated-password.unvalidated-password\n        user.set_password(password)\n    except PasswordError:\n        flash(\n            gettext(\"The password you submitted is invalid. Password not changed.\"),\n            \"error\",\n        )\n        return False\n\n    try:\n        db.session.commit()\n    except Exception:\n        flash(\n            gettext(\n                \"There was an error, and the new password might not have been \"\n                \"saved correctly. To prevent you from getting locked \"\n                \"out of your account, you should reset your password again.\"\n            ),\n            \"error\",\n        )\n        current_app.logger.error(\"Failed to update a valid password.\")\n        return False\n\n    # using Markup so the HTML isn't escaped\n    if not admin:\n        session.destroy(\n            (\n                \"success\",\n                Markup(\n                    \"<p>{message} <span><code>{password}</code></span></p>\".format(\n                        message=Markup.escape(\n                            gettext(\n                                \"Password updated. Don't forget to save it in your KeePassX database. \"  # noqa: E501\n                                \"New password:\"\n                            )\n                        ),\n                        password=Markup.escape(\"\" if password is None else password),\n                    )\n                ),\n            ),\n            session.get(\"locale\"),\n        )\n    else:\n        flash(\n            Markup(\n                \"<p>{message} <span><code>{password}</code></span></p>\".format(\n                    message=Markup.escape(\n                        gettext(\n                            \"Password updated. Don't forget to save it in your KeePassX database. \"\n                            \"New password:\"\n                        )\n                    ),\n                    password=Markup.escape(\"\" if password is None else password),\n                )\n            ),\n            \"success\",\n        )\n    return True",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "col_download_unread",
          "code": "def col_download_unread(cols_selected: List[str]) -> werkzeug.Response:\n    \"\"\"\n    Download all unseen submissions from all selected sources.\n    \"\"\"\n    unseen_submissions = (\n        Submission.query.join(Source)\n        .filter(Source.deleted_at.is_(None), Source.filesystem_id.in_(cols_selected))\n        .filter(~Submission.seen_files.any(), ~Submission.seen_messages.any())\n        .all()\n    )\n\n    if len(unseen_submissions) == 0:\n        flash(gettext(\"No unread submissions in selected collections.\"), \"error\")\n        return redirect(url_for(\"main.index\"))\n\n    return download(\"unread\", unseen_submissions)",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "col_download_all",
          "code": "def col_download_all(cols_selected: List[str]) -> werkzeug.Response:\n    \"\"\"Download all submissions from all selected sources.\"\"\"\n    submissions: List[Union[Source, Submission]] = []\n    for filesystem_id in cols_selected:\n        id = (\n            Source.query.filter(Source.filesystem_id == filesystem_id)\n            .filter_by(deleted_at=None)\n            .one()\n            .id\n        )\n        submissions += Submission.query.filter(Submission.source_id == id).all()\n    return download(\"all\", submissions)",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "serve_file_with_etag",
          "code": "def serve_file_with_etag(db_obj: Union[Reply, Submission]) -> flask.Response:\n    file_path = Storage.get_default().path(db_obj.source.filesystem_id, db_obj.filename)\n    add_range_headers = not current_app.config[\"USE_X_SENDFILE\"]\n    response = send_file(\n        file_path,\n        mimetype=\"application/pgp-encrypted\",\n        as_attachment=True,\n        etag=False,\n        conditional=add_range_headers,\n    )  # Disable Flask default ETag\n\n    if not db_obj.checksum:\n        add_checksum_for_file(db.session, db_obj, file_path)\n\n    response.direct_passthrough = False\n    response.headers[\"Etag\"] = db_obj.checksum\n    response.headers[\"Accept-Ranges\"] = \"bytes\"\n    return response",
          "file": "utils.py"
        }
      ],
      "account.py": [
        {
          "type": "function",
          "name": "make_blueprint",
          "code": "def make_blueprint() -> Blueprint:\n    view = Blueprint(\"account\", __name__)\n\n    @view.route(\"/account\", methods=(\"GET\",))\n    def edit() -> str:\n        password = PassphraseGenerator.get_default().generate_passphrase(\n            preferred_language=g.localeinfo.language\n        )\n        # Store password in session for future verification\n        set_pending_password(session.get_user(), password)\n        return render_template(\"edit_account.html\", password=password)\n\n    @view.route(\"/change-name\", methods=(\"POST\",))\n    def change_name() -> werkzeug.Response:\n        first_name = request.form.get(\"first_name\")\n        last_name = request.form.get(\"last_name\")\n        set_name(session.get_user(), first_name, last_name)\n        return redirect(url_for(\"account.edit\"))\n\n    @view.route(\"/new-password\", methods=(\"POST\",))\n    def new_password() -> werkzeug.Response:\n        user = session.get_user()\n        current_password = request.form.get(\"current_password\")\n        token = request.form.get(\"token\")\n        error_message = gettext(\"Incorrect password or two-factor code.\")\n        # If the user is validated, change their password\n        if validate_user(user.username, current_password, token, error_message):\n            password = request.form.get(\"password\")\n            if set_diceware_password(user, password):\n                current_app.session_interface.logout_user(user.id)  # type: ignore\n                return redirect(url_for(\"main.login\"))\n        return redirect(url_for(\"account.edit\"))\n\n    @view.route(\"/verify-2fa-totp\", methods=(\"POST\",))\n    def new_two_factor_totp() -> Union[str, werkzeug.Response]:\n        \"\"\"\n        After (re)setting a user's 2FA TOTP, allow them to verify the newly generated code.\n\n        We don't want users to be able to see their TOTP secret after generation, so it must\n        be supplied in the form body, generated by another endpoint. The provided token is\n        then verified against the supplied secret.\n        \"\"\"\n        token = request.form[\"token\"]\n        # NOTE: We only use the session for getting the user's name for the QR code\n        # and don't fetch any secrets from it.\n        username = session.get_user().username\n        otp_secret = request.form[\"otp_secret\"]\n        totp = two_factor.TOTP(otp_secret)\n        try:\n            # Note: this intentionally doesn't prevent replay attacks, since we just want\n            # to make sure they have the right token\n            totp.verify(token, datetime.utcnow())\n            flash(\n                gettext(\"Your two-factor credentials have been reset successfully.\"),\n                \"notification\",\n            )\n            return redirect(url_for(\"account.edit\"))\n\n        except two_factor.OtpTokenInvalid:\n            flash(\n                gettext(\"There was a problem verifying the two-factor code. Please try again.\"),\n                \"error\",\n            )\n\n        return render_template(\n            \"account_new_two_factor_totp.html\",\n            qrcode=Markup(totp.qrcode_svg(username).decode()),\n            otp_secret=otp_secret,\n            formatted_otp_secret=two_factor.format_secret(otp_secret),\n        )\n\n    @view.route(\"/reset-2fa-totp\", methods=[\"POST\"])\n    def reset_two_factor_totp() -> str:\n        session.get_user().is_totp = True\n        session.get_user().regenerate_totp_shared_secret()\n        db.session.commit()\n        new_otp_secret = session.get_user().otp_secret\n        return render_template(\n            \"account_new_two_factor_totp.html\",\n            qrcode=Markup(session.get_user().totp.qrcode_svg(session.get_user().username).decode()),\n            otp_secret=new_otp_secret,\n            formatted_otp_secret=two_factor.format_secret(new_otp_secret),\n        )\n\n    @view.route(\"/verify-2fa-hotp\", methods=(\"POST\",))\n    def new_two_factor_hotp() -> Union[str, werkzeug.Response]:\n        \"\"\"\n        After (re)setting a user's 2FA HOTP, allow them to verify the newly generated code.\n\n        This works differently than the analogous TOTP endpoint, as here we do verify against\n        the database secret because we need to compare with and increment the counter.\n        \"\"\"\n        user = session.get_user()\n        token = request.form[\"token\"]\n\n        error = False\n\n        if not user.is_totp:\n            try:\n                user.verify_2fa_token(token)\n                flash(\n                    gettext(\"Your two-factor credentials have been reset successfully.\"),\n                    \"notification\",\n                )\n                return redirect(url_for(\"account.edit\"))\n\n            except two_factor.OtpTokenInvalid:\n                error = True\n        else:\n            # XXX: Consider using a different error message here, or do we not want to reveal\n            # if the user is using HOTP vs TOTP\n            error = True\n\n        if error:\n            flash(\n                gettext(\"There was a problem verifying the two-factor code. Please try again.\"),\n                \"error\",\n            )\n\n        return render_template(\"account_new_two_factor_hotp.html\", user=user)\n\n    @view.route(\"/reset-2fa-hotp\", methods=[\"POST\"])\n    def reset_two_factor_hotp() -> Union[str, werkzeug.Response]:\n        otp_secret = request.form.get(\"otp_secret\", None)\n        if otp_secret:\n            if not validate_hotp_secret(session.get_user(), otp_secret):\n                return render_template(\"account_edit_hotp_secret.html\")\n            session.get_user().set_hotp_secret(otp_secret)\n            db.session.commit()\n            return render_template(\"account_new_two_factor_hotp.html\", user=session.get_user())\n        else:\n            return render_template(\"account_edit_hotp_secret.html\")\n\n    return view",
          "file": "account.py"
        },
        {
          "type": "function",
          "name": "edit",
          "code": "def edit() -> str:\n        password = PassphraseGenerator.get_default().generate_passphrase(\n            preferred_language=g.localeinfo.language\n        )\n        # Store password in session for future verification\n        set_pending_password(session.get_user(), password)\n        return render_template(\"edit_account.html\", password=password)",
          "file": "account.py"
        },
        {
          "type": "function",
          "name": "change_name",
          "code": "def change_name() -> werkzeug.Response:\n        first_name = request.form.get(\"first_name\")\n        last_name = request.form.get(\"last_name\")\n        set_name(session.get_user(), first_name, last_name)\n        return redirect(url_for(\"account.edit\"))",
          "file": "account.py"
        },
        {
          "type": "function",
          "name": "new_password",
          "code": "def new_password() -> werkzeug.Response:\n        user = session.get_user()\n        current_password = request.form.get(\"current_password\")\n        token = request.form.get(\"token\")\n        error_message = gettext(\"Incorrect password or two-factor code.\")\n        # If the user is validated, change their password\n        if validate_user(user.username, current_password, token, error_message):\n            password = request.form.get(\"password\")\n            if set_diceware_password(user, password):\n                current_app.session_interface.logout_user(user.id)  # type: ignore\n                return redirect(url_for(\"main.login\"))\n        return redirect(url_for(\"account.edit\"))",
          "file": "account.py"
        },
        {
          "type": "function",
          "name": "new_two_factor_totp",
          "code": "def new_two_factor_totp() -> Union[str, werkzeug.Response]:\n        \"\"\"\n        After (re)setting a user's 2FA TOTP, allow them to verify the newly generated code.\n\n        We don't want users to be able to see their TOTP secret after generation, so it must\n        be supplied in the form body, generated by another endpoint. The provided token is\n        then verified against the supplied secret.\n        \"\"\"\n        token = request.form[\"token\"]\n        # NOTE: We only use the session for getting the user's name for the QR code\n        # and don't fetch any secrets from it.\n        username = session.get_user().username\n        otp_secret = request.form[\"otp_secret\"]\n        totp = two_factor.TOTP(otp_secret)\n        try:\n            # Note: this intentionally doesn't prevent replay attacks, since we just want\n            # to make sure they have the right token\n            totp.verify(token, datetime.utcnow())\n            flash(\n                gettext(\"Your two-factor credentials have been reset successfully.\"),\n                \"notification\",\n            )\n            return redirect(url_for(\"account.edit\"))\n\n        except two_factor.OtpTokenInvalid:\n            flash(\n                gettext(\"There was a problem verifying the two-factor code. Please try again.\"),\n                \"error\",\n            )\n\n        return render_template(\n            \"account_new_two_factor_totp.html\",\n            qrcode=Markup(totp.qrcode_svg(username).decode()),\n            otp_secret=otp_secret,\n            formatted_otp_secret=two_factor.format_secret(otp_secret),\n        )",
          "file": "account.py"
        },
        {
          "type": "function",
          "name": "reset_two_factor_totp",
          "code": "def reset_two_factor_totp() -> str:\n        session.get_user().is_totp = True\n        session.get_user().regenerate_totp_shared_secret()\n        db.session.commit()\n        new_otp_secret = session.get_user().otp_secret\n        return render_template(\n            \"account_new_two_factor_totp.html\",\n            qrcode=Markup(session.get_user().totp.qrcode_svg(session.get_user().username).decode()),\n            otp_secret=new_otp_secret,\n            formatted_otp_secret=two_factor.format_secret(new_otp_secret),\n        )",
          "file": "account.py"
        },
        {
          "type": "function",
          "name": "new_two_factor_hotp",
          "code": "def new_two_factor_hotp() -> Union[str, werkzeug.Response]:\n        \"\"\"\n        After (re)setting a user's 2FA HOTP, allow them to verify the newly generated code.\n\n        This works differently than the analogous TOTP endpoint, as here we do verify against\n        the database secret because we need to compare with and increment the counter.\n        \"\"\"\n        user = session.get_user()\n        token = request.form[\"token\"]\n\n        error = False\n\n        if not user.is_totp:\n            try:\n                user.verify_2fa_token(token)\n                flash(\n                    gettext(\"Your two-factor credentials have been reset successfully.\"),\n                    \"notification\",\n                )\n                return redirect(url_for(\"account.edit\"))\n\n            except two_factor.OtpTokenInvalid:\n                error = True\n        else:\n            # XXX: Consider using a different error message here, or do we not want to reveal\n            # if the user is using HOTP vs TOTP\n            error = True\n\n        if error:\n            flash(\n                gettext(\"There was a problem verifying the two-factor code. Please try again.\"),\n                \"error\",\n            )\n\n        return render_template(\"account_new_two_factor_hotp.html\", user=user)",
          "file": "account.py"
        },
        {
          "type": "function",
          "name": "reset_two_factor_hotp",
          "code": "def reset_two_factor_hotp() -> Union[str, werkzeug.Response]:\n        otp_secret = request.form.get(\"otp_secret\", None)\n        if otp_secret:\n            if not validate_hotp_secret(session.get_user(), otp_secret):\n                return render_template(\"account_edit_hotp_secret.html\")\n            session.get_user().set_hotp_secret(otp_secret)\n            db.session.commit()\n            return render_template(\"account_new_two_factor_hotp.html\", user=session.get_user())\n        else:\n            return render_template(\"account_edit_hotp_secret.html\")",
          "file": "account.py"
        }
      ],
      "api.py": [
        {
          "type": "function",
          "name": "get_or_404",
          "code": "def get_or_404(model: db.Model, object_id: str, column: Column) -> db.Model:\n    result = model.query.filter(column == object_id).one_or_none()\n    if result is None:\n        abort(404)\n    return result",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "make_blueprint",
          "code": "def make_blueprint() -> Blueprint:\n    api = Blueprint(\"api\", __name__)\n\n    @api.route(\"/\")\n    def get_endpoints() -> Tuple[flask.Response, int]:\n        endpoints = {\n            \"sources_url\": \"/api/v1/sources\",\n            \"current_user_url\": \"/api/v1/user\",\n            \"all_users_url\": \"/api/v1/users\",\n            \"submissions_url\": \"/api/v1/submissions\",\n            \"replies_url\": \"/api/v1/replies\",\n            \"seen_url\": \"/api/v1/seen\",\n            \"auth_token_url\": \"/api/v1/token\",\n        }\n        return jsonify(endpoints), 200\n\n    # Before every post, we validate the payload before processing the request\n    @api.before_request\n    def validate_data() -> None:\n        if request.method == \"POST\":\n            # flag, star, and logout can have empty payloads\n            if not request.data:\n                dataless_endpoints = [\n                    \"add_star\",\n                    \"remove_star\",\n                    \"flag\",\n                    \"logout\",\n                ]\n                for endpoint in dataless_endpoints:\n                    if request.endpoint == \"api.\" + endpoint:\n                        return\n                abort(400, \"malformed request\")\n            # other requests must have valid JSON payload\n            else:\n                try:\n                    json.loads(request.data.decode(\"utf-8\"))\n                except ValueError:\n                    abort(400, \"malformed request\")\n\n    @api.route(\"/token\", methods=[\"POST\"])\n    def get_token() -> Tuple[flask.Response, int]:\n        creds = json.loads(request.data.decode(\"utf-8\"))\n\n        username = creds.get(\"username\", None)\n        passphrase = creds.get(\"passphrase\", None)\n        one_time_code = creds.get(\"one_time_code\", None)\n\n        if username is None:\n            abort(400, \"username field is missing\")\n        if passphrase is None:\n            abort(400, \"passphrase field is missing\")\n        if one_time_code is None:\n            abort(400, \"one_time_code field is missing\")\n\n        try:\n            journalist = Journalist.login(username, passphrase, one_time_code)\n\n            response = jsonify(\n                {\n                    \"token\": session.get_token(),\n                    \"expiration\": session.get_lifetime(),\n                    \"journalist_uuid\": journalist.uuid,\n                    \"journalist_first_name\": journalist.first_name,\n                    \"journalist_last_name\": journalist.last_name,\n                }\n            )\n\n            # Update access metadata\n            journalist.last_access = datetime.now(timezone.utc)\n            db.session.add(journalist)\n            db.session.commit()\n\n            session[\"uid\"] = journalist.id\n\n            return response, 200\n        except (\n            LoginThrottledException,\n            InvalidUsernameException,\n            OtpSecretInvalid,\n            OtpTokenInvalid,\n            WrongPasswordException,\n        ):\n            return abort(403, \"Token authentication failed.\")\n\n    @api.route(\"/sources\", methods=[\"GET\"])\n    def get_all_sources() -> Tuple[flask.Response, int]:\n        sources = Source.query.filter_by(pending=False, deleted_at=None).all()\n        return jsonify({\"sources\": [source.to_json() for source in sources]}), 200\n\n    @api.route(\"/sources/<source_uuid>\", methods=[\"GET\", \"DELETE\"])\n    def single_source(source_uuid: str) -> Tuple[flask.Response, int]:\n        if request.method == \"GET\":\n            source = get_or_404(Source, source_uuid, column=Source.uuid)\n            return jsonify(source.to_json()), 200\n        elif request.method == \"DELETE\":\n            source = get_or_404(Source, source_uuid, column=Source.uuid)\n            utils.delete_collection(source.filesystem_id)\n            return jsonify({\"message\": \"Source and submissions deleted\"}), 200\n        else:\n            abort(405)\n\n    @api.route(\"/sources/<source_uuid>/add_star\", methods=[\"POST\"])\n    def add_star(source_uuid: str) -> Tuple[flask.Response, int]:\n        source = get_or_404(Source, source_uuid, column=Source.uuid)\n        utils.make_star_true(source.filesystem_id)\n        db.session.commit()\n        return jsonify({\"message\": \"Star added\"}), 201\n\n    @api.route(\"/sources/<source_uuid>/remove_star\", methods=[\"DELETE\"])\n    def remove_star(source_uuid: str) -> Tuple[flask.Response, int]:\n        source = get_or_404(Source, source_uuid, column=Source.uuid)\n        utils.make_star_false(source.filesystem_id)\n        db.session.commit()\n        return jsonify({\"message\": \"Star removed\"}), 200\n\n    @api.route(\"/sources/<source_uuid>/flag\", methods=[\"POST\"])\n    def flag(source_uuid: str) -> Tuple[flask.Response, int]:\n        return (\n            jsonify({\"message\": \"Sources no longer need to be flagged for reply\"}),\n            200,\n        )\n\n    @api.route(\"/sources/<source_uuid>/conversation\", methods=[\"DELETE\"])\n    def source_conversation(source_uuid: str) -> Tuple[flask.Response, int]:\n        if request.method == \"DELETE\":\n            source = get_or_404(Source, source_uuid, column=Source.uuid)\n            utils.delete_source_files(source.filesystem_id)\n            return jsonify({\"message\": \"Source data deleted\"}), 200\n        else:\n            abort(405)\n\n    @api.route(\"/sources/<source_uuid>/submissions\", methods=[\"GET\"])\n    def all_source_submissions(source_uuid: str) -> Tuple[flask.Response, int]:\n        source = get_or_404(Source, source_uuid, column=Source.uuid)\n        return (\n            jsonify({\"submissions\": [submission.to_json() for submission in source.submissions]}),\n            200,\n        )\n\n    @api.route(\"/sources/<source_uuid>/submissions/<submission_uuid>/download\", methods=[\"GET\"])\n    def download_submission(source_uuid: str, submission_uuid: str) -> flask.Response:\n        get_or_404(Source, source_uuid, column=Source.uuid)\n        submission = get_or_404(Submission, submission_uuid, column=Submission.uuid)\n        return utils.serve_file_with_etag(submission)\n\n    @api.route(\"/sources/<source_uuid>/replies/<reply_uuid>/download\", methods=[\"GET\"])\n    def download_reply(source_uuid: str, reply_uuid: str) -> flask.Response:\n        get_or_404(Source, source_uuid, column=Source.uuid)\n        reply = get_or_404(Reply, reply_uuid, column=Reply.uuid)\n\n        return utils.serve_file_with_etag(reply)\n\n    @api.route(\n        \"/sources/<source_uuid>/submissions/<submission_uuid>\",\n        methods=[\"GET\", \"DELETE\"],\n    )\n    def single_submission(source_uuid: str, submission_uuid: str) -> Tuple[flask.Response, int]:\n        if request.method == \"GET\":\n            get_or_404(Source, source_uuid, column=Source.uuid)\n            submission = get_or_404(Submission, submission_uuid, column=Submission.uuid)\n            return jsonify(submission.to_json()), 200\n        elif request.method == \"DELETE\":\n            get_or_404(Source, source_uuid, column=Source.uuid)\n            submission = get_or_404(Submission, submission_uuid, column=Submission.uuid)\n            utils.delete_file_object(submission)\n            return jsonify({\"message\": \"Submission deleted\"}), 200\n        else:\n            abort(405)\n\n    @api.route(\"/sources/<source_uuid>/replies\", methods=[\"GET\", \"POST\"])\n    def all_source_replies(source_uuid: str) -> Tuple[flask.Response, int]:\n        if request.method == \"GET\":\n            source = get_or_404(Source, source_uuid, column=Source.uuid)\n            return (\n                jsonify({\"replies\": [reply.to_json() for reply in source.replies]}),\n                200,\n            )\n        elif request.method == \"POST\":\n            source = get_or_404(Source, source_uuid, column=Source.uuid)\n            if request.json is None:\n                abort(400, \"please send requests in valid JSON\")\n\n            if \"reply\" not in request.json:\n                abort(400, \"reply not found in request body\")\n\n            data = request.json\n            if not data[\"reply\"]:\n                abort(400, \"reply should not be empty\")\n\n            source.interaction_count += 1\n            try:\n                filename = Storage.get_default().save_pre_encrypted_reply(\n                    source.filesystem_id,\n                    source.interaction_count,\n                    source.journalist_filename,\n                    data[\"reply\"],\n                )\n            except NotEncrypted:\n                return jsonify({\"message\": \"You must encrypt replies client side\"}), 400\n\n            # issue #3918\n            filename = path.basename(filename)\n\n            reply = Reply(session.get_user(), source, filename, Storage.get_default())\n\n            reply_uuid = data.get(\"uuid\", None)\n            if reply_uuid is not None:\n                # check that is is parseable\n                try:\n                    UUID(reply_uuid)\n                except ValueError:\n                    abort(400, \"'uuid' was not a valid UUID\")\n                reply.uuid = reply_uuid\n\n            try:\n                db.session.add(reply)\n                seen_reply = SeenReply(reply=reply, journalist=session.get_user())\n                db.session.add(seen_reply)\n                db.session.add(source)\n                db.session.commit()\n            except IntegrityError as e:\n                db.session.rollback()\n                if \"UNIQUE constraint failed: replies.uuid\" in str(e):\n                    abort(409, \"That UUID is already in use.\")\n                else:\n                    raise e\n\n            return (\n                jsonify(\n                    {\n                        \"message\": \"Your reply has been stored\",\n                        \"uuid\": reply.uuid,\n                        \"filename\": reply.filename,\n                    }\n                ),\n                201,\n            )\n        else:\n            abort(405)\n\n    @api.route(\"/sources/<source_uuid>/replies/<reply_uuid>\", methods=[\"GET\", \"DELETE\"])\n    def single_reply(source_uuid: str, reply_uuid: str) -> Tuple[flask.Response, int]:\n        get_or_404(Source, source_uuid, column=Source.uuid)\n        reply = get_or_404(Reply, reply_uuid, column=Reply.uuid)\n        if request.method == \"GET\":\n            return jsonify(reply.to_json()), 200\n        elif request.method == \"DELETE\":\n            utils.delete_file_object(reply)\n            return jsonify({\"message\": \"Reply deleted\"}), 200\n        else:\n            abort(405)\n\n    @api.route(\"/submissions\", methods=[\"GET\"])\n    def get_all_submissions() -> Tuple[flask.Response, int]:\n        submissions = Submission.query.all()\n        return (\n            jsonify(\n                {\n                    \"submissions\": [\n                        submission.to_json() for submission in submissions if submission.source\n                    ]\n                }\n            ),\n            200,\n        )\n\n    @api.route(\"/replies\", methods=[\"GET\"])\n    def get_all_replies() -> Tuple[flask.Response, int]:\n        replies = Reply.query.all()\n        return (\n            jsonify({\"replies\": [reply.to_json() for reply in replies if reply.source]}),\n            200,\n        )\n\n    @api.route(\"/seen\", methods=[\"POST\"])\n    def seen() -> Tuple[flask.Response, int]:\n        \"\"\"\n        Lists or marks the source conversation items that the journalist has seen.\n        \"\"\"\n\n        if request.method == \"POST\":\n            if request.json is None or not isinstance(request.json, collections.abc.Mapping):\n                abort(400, \"Please send requests in valid JSON.\")\n\n            if not any(map(request.json.get, [\"files\", \"messages\", \"replies\"])):\n                abort(400, \"Please specify the resources to mark seen.\")\n\n            # gather everything to be marked seen. if any don't exist,\n            # reject the request.\n            targets: Set[Union[Submission, Reply]] = set()\n            for file_uuid in request.json.get(\"files\", []):\n                f = Submission.query.filter(Submission.uuid == file_uuid).one_or_none()\n                if f is None or not f.is_file:\n                    abort(404, f\"file not found: {file_uuid}\")\n                targets.add(f)\n\n            for message_uuid in request.json.get(\"messages\", []):\n                m = Submission.query.filter(Submission.uuid == message_uuid).one_or_none()\n                if m is None or not m.is_message:\n                    abort(404, f\"message not found: {message_uuid}\")\n                targets.add(m)\n\n            for reply_uuid in request.json.get(\"replies\", []):\n                r = Reply.query.filter(Reply.uuid == reply_uuid).one_or_none()\n                if r is None:\n                    abort(404, f\"reply not found: {reply_uuid}\")\n                targets.add(r)\n\n            # now mark everything seen.\n            utils.mark_seen(list(targets), session.get_user())\n\n            return jsonify({\"message\": \"resources marked seen\"}), 200\n\n        abort(405)\n\n    @api.route(\"/user\", methods=[\"GET\"])\n    def get_current_user() -> Tuple[flask.Response, int]:\n        return jsonify(session.get_user().to_json()), 200\n\n    @api.route(\"/users\", methods=[\"GET\"])\n    def get_all_users() -> Tuple[flask.Response, int]:\n        users = Journalist.query.all()\n        return jsonify({\"users\": [user.to_json(all_info=False) for user in users]}), 200\n\n    @api.route(\"/logout\", methods=[\"POST\"])\n    def logout() -> Tuple[flask.Response, int]:\n        session.destroy()\n        return jsonify({\"message\": \"Your token has been revoked.\"}), 200\n\n    def _handle_api_http_exception(\n        error: werkzeug.exceptions.HTTPException,\n    ) -> Tuple[flask.Response, int]:\n        # Workaround for no blueprint-level 404/5 error handlers, see:\n        # https://github.com/pallets/flask/issues/503#issuecomment-71383286\n        response = jsonify({\"error\": error.name, \"message\": error.description})\n\n        return response, error.code  # type: ignore\n\n    for code in default_exceptions:\n        api.errorhandler(code)(_handle_api_http_exception)\n\n    return api",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "get_endpoints",
          "code": "def get_endpoints() -> Tuple[flask.Response, int]:\n        endpoints = {\n            \"sources_url\": \"/api/v1/sources\",\n            \"current_user_url\": \"/api/v1/user\",\n            \"all_users_url\": \"/api/v1/users\",\n            \"submissions_url\": \"/api/v1/submissions\",\n            \"replies_url\": \"/api/v1/replies\",\n            \"seen_url\": \"/api/v1/seen\",\n            \"auth_token_url\": \"/api/v1/token\",\n        }\n        return jsonify(endpoints), 200",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "validate_data",
          "code": "def validate_data() -> None:\n        if request.method == \"POST\":\n            # flag, star, and logout can have empty payloads\n            if not request.data:\n                dataless_endpoints = [\n                    \"add_star\",\n                    \"remove_star\",\n                    \"flag\",\n                    \"logout\",\n                ]\n                for endpoint in dataless_endpoints:\n                    if request.endpoint == \"api.\" + endpoint:\n                        return\n                abort(400, \"malformed request\")\n            # other requests must have valid JSON payload\n            else:\n                try:\n                    json.loads(request.data.decode(\"utf-8\"))\n                except ValueError:\n                    abort(400, \"malformed request\")",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "get_token",
          "code": "def get_token() -> Tuple[flask.Response, int]:\n        creds = json.loads(request.data.decode(\"utf-8\"))\n\n        username = creds.get(\"username\", None)\n        passphrase = creds.get(\"passphrase\", None)\n        one_time_code = creds.get(\"one_time_code\", None)\n\n        if username is None:\n            abort(400, \"username field is missing\")\n        if passphrase is None:\n            abort(400, \"passphrase field is missing\")\n        if one_time_code is None:\n            abort(400, \"one_time_code field is missing\")\n\n        try:\n            journalist = Journalist.login(username, passphrase, one_time_code)\n\n            response = jsonify(\n                {\n                    \"token\": session.get_token(),\n                    \"expiration\": session.get_lifetime(),\n                    \"journalist_uuid\": journalist.uuid,\n                    \"journalist_first_name\": journalist.first_name,\n                    \"journalist_last_name\": journalist.last_name,\n                }\n            )\n\n            # Update access metadata\n            journalist.last_access = datetime.now(timezone.utc)\n            db.session.add(journalist)\n            db.session.commit()\n\n            session[\"uid\"] = journalist.id\n\n            return response, 200\n        except (\n            LoginThrottledException,\n            InvalidUsernameException,\n            OtpSecretInvalid,\n            OtpTokenInvalid,\n            WrongPasswordException,\n        ):\n            return abort(403, \"Token authentication failed.\")",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "get_all_sources",
          "code": "def get_all_sources() -> Tuple[flask.Response, int]:\n        sources = Source.query.filter_by(pending=False, deleted_at=None).all()\n        return jsonify({\"sources\": [source.to_json() for source in sources]}), 200",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "single_source",
          "code": "def single_source(source_uuid: str) -> Tuple[flask.Response, int]:\n        if request.method == \"GET\":\n            source = get_or_404(Source, source_uuid, column=Source.uuid)\n            return jsonify(source.to_json()), 200\n        elif request.method == \"DELETE\":\n            source = get_or_404(Source, source_uuid, column=Source.uuid)\n            utils.delete_collection(source.filesystem_id)\n            return jsonify({\"message\": \"Source and submissions deleted\"}), 200\n        else:\n            abort(405)",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "add_star",
          "code": "def add_star(source_uuid: str) -> Tuple[flask.Response, int]:\n        source = get_or_404(Source, source_uuid, column=Source.uuid)\n        utils.make_star_true(source.filesystem_id)\n        db.session.commit()\n        return jsonify({\"message\": \"Star added\"}), 201",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "remove_star",
          "code": "def remove_star(source_uuid: str) -> Tuple[flask.Response, int]:\n        source = get_or_404(Source, source_uuid, column=Source.uuid)\n        utils.make_star_false(source.filesystem_id)\n        db.session.commit()\n        return jsonify({\"message\": \"Star removed\"}), 200",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "flag",
          "code": "def flag(source_uuid: str) -> Tuple[flask.Response, int]:\n        return (\n            jsonify({\"message\": \"Sources no longer need to be flagged for reply\"}),\n            200,\n        )",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "source_conversation",
          "code": "def source_conversation(source_uuid: str) -> Tuple[flask.Response, int]:\n        if request.method == \"DELETE\":\n            source = get_or_404(Source, source_uuid, column=Source.uuid)\n            utils.delete_source_files(source.filesystem_id)\n            return jsonify({\"message\": \"Source data deleted\"}), 200\n        else:\n            abort(405)",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "all_source_submissions",
          "code": "def all_source_submissions(source_uuid: str) -> Tuple[flask.Response, int]:\n        source = get_or_404(Source, source_uuid, column=Source.uuid)\n        return (\n            jsonify({\"submissions\": [submission.to_json() for submission in source.submissions]}),\n            200,\n        )",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "download_submission",
          "code": "def download_submission(source_uuid: str, submission_uuid: str) -> flask.Response:\n        get_or_404(Source, source_uuid, column=Source.uuid)\n        submission = get_or_404(Submission, submission_uuid, column=Submission.uuid)\n        return utils.serve_file_with_etag(submission)",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "download_reply",
          "code": "def download_reply(source_uuid: str, reply_uuid: str) -> flask.Response:\n        get_or_404(Source, source_uuid, column=Source.uuid)\n        reply = get_or_404(Reply, reply_uuid, column=Reply.uuid)\n\n        return utils.serve_file_with_etag(reply)",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "single_submission",
          "code": "def single_submission(source_uuid: str, submission_uuid: str) -> Tuple[flask.Response, int]:\n        if request.method == \"GET\":\n            get_or_404(Source, source_uuid, column=Source.uuid)\n            submission = get_or_404(Submission, submission_uuid, column=Submission.uuid)\n            return jsonify(submission.to_json()), 200\n        elif request.method == \"DELETE\":\n            get_or_404(Source, source_uuid, column=Source.uuid)\n            submission = get_or_404(Submission, submission_uuid, column=Submission.uuid)\n            utils.delete_file_object(submission)\n            return jsonify({\"message\": \"Submission deleted\"}), 200\n        else:\n            abort(405)",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "all_source_replies",
          "code": "def all_source_replies(source_uuid: str) -> Tuple[flask.Response, int]:\n        if request.method == \"GET\":\n            source = get_or_404(Source, source_uuid, column=Source.uuid)\n            return (\n                jsonify({\"replies\": [reply.to_json() for reply in source.replies]}),\n                200,\n            )\n        elif request.method == \"POST\":\n            source = get_or_404(Source, source_uuid, column=Source.uuid)\n            if request.json is None:\n                abort(400, \"please send requests in valid JSON\")\n\n            if \"reply\" not in request.json:\n                abort(400, \"reply not found in request body\")\n\n            data = request.json\n            if not data[\"reply\"]:\n                abort(400, \"reply should not be empty\")\n\n            source.interaction_count += 1\n            try:\n                filename = Storage.get_default().save_pre_encrypted_reply(\n                    source.filesystem_id,\n                    source.interaction_count,\n                    source.journalist_filename,\n                    data[\"reply\"],\n                )\n            except NotEncrypted:\n                return jsonify({\"message\": \"You must encrypt replies client side\"}), 400\n\n            # issue #3918\n            filename = path.basename(filename)\n\n            reply = Reply(session.get_user(), source, filename, Storage.get_default())\n\n            reply_uuid = data.get(\"uuid\", None)\n            if reply_uuid is not None:\n                # check that is is parseable\n                try:\n                    UUID(reply_uuid)\n                except ValueError:\n                    abort(400, \"'uuid' was not a valid UUID\")\n                reply.uuid = reply_uuid\n\n            try:\n                db.session.add(reply)\n                seen_reply = SeenReply(reply=reply, journalist=session.get_user())\n                db.session.add(seen_reply)\n                db.session.add(source)\n                db.session.commit()\n            except IntegrityError as e:\n                db.session.rollback()\n                if \"UNIQUE constraint failed: replies.uuid\" in str(e):\n                    abort(409, \"That UUID is already in use.\")\n                else:\n                    raise e\n\n            return (\n                jsonify(\n                    {\n                        \"message\": \"Your reply has been stored\",\n                        \"uuid\": reply.uuid,\n                        \"filename\": reply.filename,\n                    }\n                ),\n                201,\n            )\n        else:\n            abort(405)",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "single_reply",
          "code": "def single_reply(source_uuid: str, reply_uuid: str) -> Tuple[flask.Response, int]:\n        get_or_404(Source, source_uuid, column=Source.uuid)\n        reply = get_or_404(Reply, reply_uuid, column=Reply.uuid)\n        if request.method == \"GET\":\n            return jsonify(reply.to_json()), 200\n        elif request.method == \"DELETE\":\n            utils.delete_file_object(reply)\n            return jsonify({\"message\": \"Reply deleted\"}), 200\n        else:\n            abort(405)",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "get_all_submissions",
          "code": "def get_all_submissions() -> Tuple[flask.Response, int]:\n        submissions = Submission.query.all()\n        return (\n            jsonify(\n                {\n                    \"submissions\": [\n                        submission.to_json() for submission in submissions if submission.source\n                    ]\n                }\n            ),\n            200,\n        )",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "get_all_replies",
          "code": "def get_all_replies() -> Tuple[flask.Response, int]:\n        replies = Reply.query.all()\n        return (\n            jsonify({\"replies\": [reply.to_json() for reply in replies if reply.source]}),\n            200,\n        )",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "seen",
          "code": "def seen() -> Tuple[flask.Response, int]:\n        \"\"\"\n        Lists or marks the source conversation items that the journalist has seen.\n        \"\"\"\n\n        if request.method == \"POST\":\n            if request.json is None or not isinstance(request.json, collections.abc.Mapping):\n                abort(400, \"Please send requests in valid JSON.\")\n\n            if not any(map(request.json.get, [\"files\", \"messages\", \"replies\"])):\n                abort(400, \"Please specify the resources to mark seen.\")\n\n            # gather everything to be marked seen. if any don't exist,\n            # reject the request.\n            targets: Set[Union[Submission, Reply]] = set()\n            for file_uuid in request.json.get(\"files\", []):\n                f = Submission.query.filter(Submission.uuid == file_uuid).one_or_none()\n                if f is None or not f.is_file:\n                    abort(404, f\"file not found: {file_uuid}\")\n                targets.add(f)\n\n            for message_uuid in request.json.get(\"messages\", []):\n                m = Submission.query.filter(Submission.uuid == message_uuid).one_or_none()\n                if m is None or not m.is_message:\n                    abort(404, f\"message not found: {message_uuid}\")\n                targets.add(m)\n\n            for reply_uuid in request.json.get(\"replies\", []):\n                r = Reply.query.filter(Reply.uuid == reply_uuid).one_or_none()\n                if r is None:\n                    abort(404, f\"reply not found: {reply_uuid}\")\n                targets.add(r)\n\n            # now mark everything seen.\n            utils.mark_seen(list(targets), session.get_user())\n\n            return jsonify({\"message\": \"resources marked seen\"}), 200\n\n        abort(405)",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "get_current_user",
          "code": "def get_current_user() -> Tuple[flask.Response, int]:\n        return jsonify(session.get_user().to_json()), 200",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "get_all_users",
          "code": "def get_all_users() -> Tuple[flask.Response, int]:\n        users = Journalist.query.all()\n        return jsonify({\"users\": [user.to_json(all_info=False) for user in users]}), 200",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "logout",
          "code": "def logout() -> Tuple[flask.Response, int]:\n        session.destroy()\n        return jsonify({\"message\": \"Your token has been revoked.\"}), 200",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "_handle_api_http_exception",
          "code": "def _handle_api_http_exception(\n        error: werkzeug.exceptions.HTTPException,\n    ) -> Tuple[flask.Response, int]:\n        # Workaround for no blueprint-level 404/5 error handlers, see:\n        # https://github.com/pallets/flask/issues/503#issuecomment-71383286\n        response = jsonify({\"error\": error.name, \"message\": error.description})\n\n        return response, error.code  # type: ignore",
          "file": "api.py"
        }
      ],
      "forms.py": [
        {
          "type": "class",
          "name": "RequiredIf",
          "code": "class RequiredIf(DataRequired):\n    def __init__(\n        self,\n        other_field_name: str,\n        custom_message: typing.Optional[str] = None,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        self.other_field_name = other_field_name\n        if custom_message is not None:\n            self.custom_message = custom_message\n        else:\n            self.custom_message = \"\"\n\n    def __call__(self, form: FlaskForm, field: Field) -> None:\n        if self.other_field_name in form:\n            other_field = form[self.other_field_name]\n            if bool(other_field.data):\n                if self.custom_message != \"\":\n                    self.message = self.custom_message\n                else:\n                    self.message = gettext(\n                        'The \"{name}\" field is required when \"{other_name}\" is set.'\n                    ).format(other_name=self.other_field_name, name=field.name)\n                super().__call__(form, field)\n            else:\n                field.errors[:] = []\n                raise StopValidation()\n        else:\n            raise ValidationError(\n                gettext(\n                    'The \"{other_name}\" field was not found - it is required by \"{name}\".'\n                ).format(other_name=self.other_field_name, name=field.name)\n            )",
          "file": "forms.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(\n        self,\n        other_field_name: str,\n        custom_message: typing.Optional[str] = None,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        self.other_field_name = other_field_name\n        if custom_message is not None:\n            self.custom_message = custom_message\n        else:\n            self.custom_message = \"\"",
          "file": "forms.py"
        },
        {
          "type": "function",
          "name": "__call__",
          "code": "def __call__(self, form: FlaskForm, field: Field) -> None:\n        if self.other_field_name in form:\n            other_field = form[self.other_field_name]\n            if bool(other_field.data):\n                if self.custom_message != \"\":\n                    self.message = self.custom_message\n                else:\n                    self.message = gettext(\n                        'The \"{name}\" field is required when \"{other_name}\" is set.'\n                    ).format(other_name=self.other_field_name, name=field.name)\n                super().__call__(form, field)\n            else:\n                field.errors[:] = []\n                raise StopValidation()\n        else:\n            raise ValidationError(\n                gettext(\n                    'The \"{other_name}\" field was not found - it is required by \"{name}\".'\n                ).format(other_name=self.other_field_name, name=field.name)\n            )",
          "file": "forms.py"
        },
        {
          "type": "function",
          "name": "otp_secret_validation",
          "code": "def otp_secret_validation(form: FlaskForm, field: Field) -> None:\n    strip_whitespace = field.data.replace(\" \", \"\")\n    input_length = len(strip_whitespace)\n    if input_length != HOTP.SECRET_HEX_LENGTH:\n        raise ValidationError(\n            ngettext(\n                \"HOTP secrets are 40 characters long - you have entered {num}.\",\n                \"HOTP secrets are 40 characters long - you have entered {num}.\",\n                input_length,\n            ).format(num=input_length)\n        )",
          "file": "forms.py"
        },
        {
          "type": "function",
          "name": "minimum_length_validation",
          "code": "def minimum_length_validation(form: FlaskForm, field: Field) -> None:\n    if len(field.data) < Journalist.MIN_USERNAME_LEN:\n        raise ValidationError(\n            ngettext(\n                \"Must be at least {num} character long.\",\n                \"Must be at least {num} characters long.\",\n                Journalist.MIN_USERNAME_LEN,\n            ).format(num=Journalist.MIN_USERNAME_LEN)\n        )",
          "file": "forms.py"
        },
        {
          "type": "function",
          "name": "name_length_validation",
          "code": "def name_length_validation(form: FlaskForm, field: Field) -> None:\n    if len(field.data) > Journalist.MAX_NAME_LEN:\n        raise ValidationError(\n            ngettext(\n                \"Cannot be longer than {num} character.\",\n                \"Cannot be longer than {num} characters.\",\n                Journalist.MAX_NAME_LEN,\n            ).format(num=Journalist.MAX_NAME_LEN)\n        )",
          "file": "forms.py"
        },
        {
          "type": "function",
          "name": "check_orgname",
          "code": "def check_orgname(form: FlaskForm, field: Field) -> None:\n    if len(field.data) > InstanceConfig.MAX_ORG_NAME_LEN:\n        raise ValidationError(\n            ngettext(\n                \"Cannot be longer than {num} character.\",\n                \"Cannot be longer than {num} characters.\",\n                InstanceConfig.MAX_ORG_NAME_LEN,\n            ).format(num=InstanceConfig.MAX_ORG_NAME_LEN)\n        )",
          "file": "forms.py"
        },
        {
          "type": "function",
          "name": "check_invalid_usernames",
          "code": "def check_invalid_usernames(form: FlaskForm, field: Field) -> None:\n    if field.data in Journalist.INVALID_USERNAMES:\n        raise ValidationError(\n            gettext(\n                \"This username is invalid because it is reserved for internal use by the software.\"\n            )\n        )",
          "file": "forms.py"
        },
        {
          "type": "function",
          "name": "check_message_length",
          "code": "def check_message_length(form: FlaskForm, field: Field) -> None:\n    msg_len = field.data\n    if not isinstance(msg_len, int) or msg_len < 0:\n        raise ValidationError(gettext(\"Please specify an integer value greater than 0.\"))",
          "file": "forms.py"
        },
        {
          "type": "class",
          "name": "NewUserForm",
          "code": "class NewUserForm(FlaskForm):\n    username = StringField(\n        \"username\",\n        validators=[\n            InputRequired(message=gettext(\"This field is required.\")),\n            minimum_length_validation,\n            check_invalid_usernames,\n        ],\n        render_kw={\"aria-describedby\": \"username-notes\"},\n    )\n    first_name = StringField(\n        \"first_name\",\n        validators=[name_length_validation, Optional()],\n        render_kw={\"aria-describedby\": \"name-notes\"},\n    )\n    last_name = StringField(\n        \"last_name\",\n        validators=[name_length_validation, Optional()],\n        render_kw={\"aria-describedby\": \"name-notes\"},\n    )\n    password = HiddenField(\"password\")\n    is_admin = BooleanField(\"is_admin\")\n    is_hotp = BooleanField(\"is_hotp\")\n    otp_secret = StringField(\n        \"otp_secret\", validators=[RequiredIf(\"is_hotp\"), otp_secret_validation]\n    )",
          "file": "forms.py"
        },
        {
          "type": "class",
          "name": "ReplyForm",
          "code": "class ReplyForm(FlaskForm):\n    message = TextAreaField(\n        \"Message\",\n        id=\"content-area\",\n        validators=[\n            InputRequired(message=gettext(\"You cannot send an empty reply.\")),\n        ],\n    )",
          "file": "forms.py"
        },
        {
          "type": "class",
          "name": "SubmissionPreferencesForm",
          "code": "class SubmissionPreferencesForm(FlaskForm):\n    prevent_document_uploads = BooleanField(\n        \"prevent_document_uploads\", false_values=(\"false\", \"False\", \"\")\n    )\n    prevent_short_messages = BooleanField(\n        \"prevent_short_messages\", false_values=(\"false\", \"False\", \"\")\n    )\n    min_message_length = IntegerField(\n        \"min_message_length\",\n        validators=[\n            RequiredIf(\n                \"prevent_short_messages\",\n                gettext(\n                    \"To configure a minimum message length, \"\n                    \"you must set the required number of \"\n                    \"characters.\"\n                ),\n            ),\n            check_message_length,\n        ],\n        render_kw={\"aria-describedby\": \"message-length-notes\"},\n    )\n    reject_codename_messages = BooleanField(\n        \"reject_codename_messages\", false_values=(\"false\", \"False\", \"\")\n    )",
          "file": "forms.py"
        },
        {
          "type": "class",
          "name": "OrgNameForm",
          "code": "class OrgNameForm(FlaskForm):\n    organization_name = StringField(\n        \"organization_name\",\n        validators=[InputRequired(message=gettext(\"This field is required.\")), check_orgname],\n    )",
          "file": "forms.py"
        },
        {
          "type": "class",
          "name": "LogoForm",
          "code": "class LogoForm(FlaskForm):\n    logo = FileField(\n        validators=[\n            FileRequired(message=gettext(\"File required.\")),\n            FileAllowed([\"png\"], message=gettext(\"You can only upload PNG image files.\")),\n        ]\n    )",
          "file": "forms.py"
        }
      ],
      "admin.py": [
        {
          "type": "function",
          "name": "make_blueprint",
          "code": "def make_blueprint() -> Blueprint:\n    view = Blueprint(\"admin\", __name__)\n\n    @view.route(\"/\", methods=(\"GET\", \"POST\"))\n    @admin_required\n    def index() -> str:\n        users = Journalist.query.filter(Journalist.username != \"deleted\").all()\n        return render_template(\"admin.html\", users=users)\n\n    @view.route(\"/config\", methods=(\"GET\", \"POST\"))\n    @admin_required\n    def manage_config() -> Union[str, werkzeug.Response]:\n        if InstanceConfig.get_default().initial_message_min_len > 0:\n            prevent_short_messages = True\n        else:\n            prevent_short_messages = False\n\n        # The UI document upload prompt (\"prevent\") is the opposite of the setting (\"allow\")\n        submission_preferences_form = SubmissionPreferencesForm(\n            prevent_document_uploads=not InstanceConfig.get_default().allow_document_uploads,\n            prevent_short_messages=prevent_short_messages,\n            min_message_length=InstanceConfig.get_default().initial_message_min_len,\n            reject_codename_messages=InstanceConfig.get_default().reject_message_with_codename,\n        )\n        organization_name_form = OrgNameForm(\n            organization_name=InstanceConfig.get_default().organization_name\n        )\n        logo_form = LogoForm()\n        if logo_form.validate_on_submit():\n            f = logo_form.logo.data\n\n            if current_app.static_folder is None:\n                abort(500)\n            custom_logo_filepath = os.path.join(current_app.static_folder, \"i\", \"custom_logo.png\")\n            try:\n                f.save(custom_logo_filepath)\n                flash(gettext(\"Image updated.\"), \"logo-success\")\n            except Exception:\n                flash(\n                    # Translators: This error is shown when an uploaded image cannot be used.\n                    gettext(\"Unable to process the image file. Please try another one.\"),\n                    \"logo-error\",\n                )\n            return redirect(url_for(\"admin.manage_config\") + \"#config-logoimage\")\n        else:\n            for errors in logo_form.errors.values():\n                for error in errors:\n                    flash(error, \"logo-error\")\n            return render_template(\n                \"config.html\",\n                submission_preferences_form=submission_preferences_form,\n                organization_name_form=organization_name_form,\n                max_len=Submission.MAX_MESSAGE_LEN,\n                logo_form=logo_form,\n            )\n\n    @view.route(\"/update-submission-preferences\", methods=[\"POST\"])\n    @admin_required\n    def update_submission_preferences() -> Optional[werkzeug.Response]:\n        form = SubmissionPreferencesForm()\n        if form.validate_on_submit():\n            # The UI prompt (\"prevent\") is the opposite of the setting (\"allow\"):\n            allow_uploads = not form.prevent_document_uploads.data\n\n            if form.prevent_short_messages.data:\n                msg_length = form.min_message_length.data\n            else:\n                msg_length = 0\n\n            reject_codenames = form.reject_codename_messages.data\n\n            InstanceConfig.update_submission_prefs(allow_uploads, msg_length, reject_codenames)\n            flash(gettext(\"Preferences saved.\"), \"submission-preferences-success\")\n            return redirect(url_for(\"admin.manage_config\") + \"#config-preventuploads\")\n        else:\n            for errors in list(form.errors.values()):\n                for error in errors:\n                    flash(\n                        gettext(\"Preferences not updated.\") + \" \" + error,\n                        \"submission-preferences-error\",\n                    )\n        return redirect(url_for(\"admin.manage_config\") + \"#config-preventuploads\")\n\n    @view.route(\"/update-org-name\", methods=[\"POST\"])\n    @admin_required\n    def update_org_name() -> Union[str, werkzeug.Response]:\n        form = OrgNameForm()\n        if form.validate_on_submit():\n            try:\n                value = request.form[\"organization_name\"]\n                InstanceConfig.set_organization_name(value)\n                flash(gettext(\"Preferences saved.\"), \"org-name-success\")\n            except Exception:\n                flash(gettext(\"Failed to update organization name.\"), \"org-name-error\")\n            return redirect(url_for(\"admin.manage_config\") + \"#config-orgname\")\n        else:\n            for errors in list(form.errors.values()):\n                for error in errors:\n                    flash(error, \"org-name-error\")\n        return redirect(url_for(\"admin.manage_config\") + \"#config-orgname\")\n\n    @view.route(\"/add\", methods=(\"GET\", \"POST\"))\n    @admin_required\n    def add_user() -> Union[str, werkzeug.Response]:\n        form = NewUserForm()\n        if form.validate_on_submit():\n            form_valid = True\n            username = request.form[\"username\"]\n            first_name = request.form[\"first_name\"]\n            last_name = request.form[\"last_name\"]\n            password = request.form[\"password\"]\n            is_admin = bool(request.form.get(\"is_admin\"))\n\n            try:\n                otp_secret = None\n                if request.form.get(\"is_hotp\", False):\n                    otp_secret = request.form.get(\"otp_secret\", \"\")\n                verify_pending_password(for_=\"new\", passphrase=password)\n                new_user = Journalist(\n                    username=username,\n                    password=password,\n                    first_name=first_name,\n                    last_name=last_name,\n                    is_admin=is_admin,\n                    otp_secret=otp_secret,\n                )\n                db.session.add(new_user)\n                db.session.commit()\n            except PasswordError:\n                flash(\n                    gettext(\n                        \"There was an error with the autogenerated password. \"\n                        \"User not created. Please try again.\"\n                    ),\n                    \"error\",\n                )\n                form_valid = False\n            except (binascii.Error, TypeError) as e:\n                if \"Non-hexadecimal digit found\" in str(e):\n                    flash(\n                        gettext(\n                            \"Invalid HOTP secret format: \"\n                            \"please only submit letters A-F and numbers 0-9.\"\n                        ),\n                        \"error\",\n                    )\n                else:\n                    flash(\n                        gettext(\"An unexpected error occurred! \" \"Please inform your admin.\"),\n                        \"error\",\n                    )\n                form_valid = False\n            except InvalidUsernameException as e:\n                form_valid = False\n                # Translators: Here, \"{message}\" explains the problem with the username.\n                flash(gettext(\"Invalid username: {message}\").format(message=e), \"error\")\n            except IntegrityError as e:\n                db.session.rollback()\n                form_valid = False\n                if \"UNIQUE constraint failed: journalists.username\" in str(e):\n                    flash(\n                        gettext('Username \"{username}\" already taken.').format(username=username),\n                        \"error\",\n                    )\n                else:\n                    flash(\n                        gettext(\n                            \"An error occurred saving this user\"\n                            \" to the database.\"\n                            \" Please inform your admin.\"\n                        ),\n                        \"error\",\n                    )\n                    current_app.logger.error(\"Adding user \" f\"'{username}' failed: {e}\")\n\n            if form_valid:\n                if new_user.is_totp:\n                    return render_template(\n                        \"admin_new_user_two_factor_totp.html\",\n                        qrcode=Markup(new_user.totp.qrcode_svg(new_user.username).decode()),\n                        otp_secret=new_user.otp_secret,\n                        formatted_otp_secret=new_user.formatted_otp_secret,\n                        userid=str(new_user.id),\n                    )\n\n                else:\n                    return render_template(\n                        \"admin_new_user_two_factor_hotp.html\",\n                        user=new_user,\n                    )\n        password = PassphraseGenerator.get_default().generate_passphrase(\n            preferred_language=g.localeinfo.language\n        )\n        # Store password in session for future verification\n        set_pending_password(\"new\", password)\n        return render_template(\"admin_add_user.html\", password=password, form=form)\n\n    @view.route(\"/verify-2fa-totp\", methods=(\"POST\",))\n    @admin_required\n    def new_user_two_factor_totp() -> Union[str, werkzeug.Response]:\n        \"\"\"\n        After (re)setting a user's 2FA TOTP, allow the admin to verify the newly generated code.\n\n        We don't want admins to be able to look up arbitrary users' TOTP secrets, so it must\n        be supplied in the form body, generated by another endpoint. The provided token is\n        then verified against the supplied secret.\n        \"\"\"\n        token = request.form[\"token\"]\n        # NOTE: This ID comes from the user and should be only used to look up the username\n        # for embedding in the QR code and success messages. We don't load any other state\n        # from the database to prevent IDOR attacks.\n        username = Journalist.query.get(request.form[\"userid\"]).username\n        otp_secret = request.form[\"otp_secret\"]\n        totp = two_factor.TOTP(otp_secret)\n        try:\n            # Note: this intentionally doesn't prevent replay attacks, since we just want\n            # to make sure they have the right token\n            totp.verify(token, datetime.utcnow())\n            flash(\n                gettext(\n                    'The two-factor code for user \"{user}\" was verified ' \"successfully.\"\n                ).format(user=username),\n                \"notification\",\n            )\n            return redirect(url_for(\"admin.index\"))\n\n        except two_factor.OtpTokenInvalid:\n            flash(\n                gettext(\"There was a problem verifying the two-factor code. Please try again.\"),\n                \"error\",\n            )\n\n        return render_template(\n            \"admin_new_user_two_factor_totp.html\",\n            qrcode=Markup(totp.qrcode_svg(username).decode()),\n            otp_secret=otp_secret,\n            formatted_otp_secret=two_factor.format_secret(otp_secret),\n            userid=request.form[\"userid\"],\n        )\n\n    @view.route(\"/reset-2fa-totp\", methods=[\"POST\"])\n    @admin_required\n    def reset_two_factor_totp() -> str:\n        uid = request.form[\"uid\"]\n        user = Journalist.query.get(uid)\n        user.is_totp = True\n        user.regenerate_totp_shared_secret()\n        db.session.commit()\n        return render_template(\n            \"admin_new_user_two_factor_totp.html\",\n            qrcode=Markup(user.totp.qrcode_svg(user.username).decode()),\n            otp_secret=user.otp_secret,\n            formatted_otp_secret=user.formatted_otp_secret,\n            userid=str(user.id),\n        )\n\n    @view.route(\"/verify-2fa-hotp\", methods=(\"POST\",))\n    @admin_required\n    def new_user_two_factor_hotp() -> Union[str, werkzeug.Response]:\n        \"\"\"\n        After (re)setting a user's 2FA HOTP, allow the admin to verify the newly generated code.\n\n        This works differently than the analogous TOTP endpoint, as here we do verify against\n        the database secret because we need to compare with and increment the counter.\n        \"\"\"\n        user = Journalist.query.get(request.form[\"uid\"])\n        token = request.form[\"token\"]\n\n        error = False\n\n        if not user.is_totp:\n            try:\n                user.verify_2fa_token(token)\n                flash(\n                    gettext(\n                        'The two-factor code for user \"{user}\" was verified ' \"successfully.\"\n                    ).format(user=user.username),\n                    \"notification\",\n                )\n                return redirect(url_for(\"admin.index\"))\n\n            except two_factor.OtpTokenInvalid:\n                error = True\n        else:\n            # XXX: Consider using a different error message here, or do we not want to reveal\n            # if the user is using HOTP vs TOTP\n            error = True\n\n        if error:\n            flash(\n                gettext(\"There was a problem verifying the two-factor code. Please try again.\"),\n                \"error\",\n            )\n\n        return render_template(\"admin_new_user_two_factor_hotp.html\", user=user)\n\n    @view.route(\"/reset-2fa-hotp\", methods=[\"POST\"])\n    @admin_required\n    def reset_two_factor_hotp() -> Union[str, werkzeug.Response]:\n        uid = request.form[\"uid\"]\n        user = Journalist.query.get(uid)\n        otp_secret = request.form.get(\"otp_secret\", None)\n        if otp_secret:\n            if not validate_hotp_secret(user, otp_secret):\n                return render_template(\"admin_edit_hotp_secret.html\", uid=user.id)\n            db.session.commit()\n            return render_template(\"admin_new_user_two_factor_hotp.html\", user=user)\n        else:\n            return render_template(\"admin_edit_hotp_secret.html\", uid=user.id)\n\n    @view.route(\"/edit/<int:user_id>\", methods=(\"GET\", \"POST\"))\n    @admin_required\n    def edit_user(user_id: int) -> Union[str, werkzeug.Response]:\n        user = Journalist.query.get(user_id)\n\n        if request.method == \"POST\":\n            if request.form.get(\"username\", None):\n                new_username = request.form[\"username\"]\n\n                try:\n                    Journalist.check_username_acceptable(new_username)\n                except InvalidUsernameException as e:\n                    flash(\n                        gettext(\"Invalid username: {message}\").format(message=e),\n                        \"error\",\n                    )\n                    return redirect(url_for(\"admin.edit_user\", user_id=user_id))\n\n                if new_username == user.username:\n                    pass\n                elif Journalist.query.filter_by(username=new_username).one_or_none():\n                    flash(\n                        gettext('Username \"{username}\" already taken.').format(\n                            username=new_username\n                        ),\n                        \"error\",\n                    )\n                    return redirect(url_for(\"admin.edit_user\", user_id=user_id))\n                else:\n                    user.username = new_username\n\n            try:\n                first_name = request.form[\"first_name\"]\n                Journalist.check_name_acceptable(first_name)\n                user.first_name = first_name\n            except FirstOrLastNameError as e:\n                # Translators: Here, \"{message}\" explains the problem with the name.\n                flash(gettext(\"Name not updated: {message}\").format(message=e), \"error\")\n                return redirect(url_for(\"admin.edit_user\", user_id=user_id))\n\n            try:\n                last_name = request.form[\"last_name\"]\n                Journalist.check_name_acceptable(last_name)\n                user.last_name = last_name\n            except FirstOrLastNameError as e:\n                flash(gettext(\"Name not updated: {message}\").format(message=e), \"error\")\n                return redirect(url_for(\"admin.edit_user\", user_id=user_id))\n\n            user.is_admin = bool(request.form.get(\"is_admin\"))\n\n            commit_account_changes(user)\n\n        password = PassphraseGenerator.get_default().generate_passphrase(\n            preferred_language=g.localeinfo.language\n        )\n        # Store password in session for future verification\n        set_pending_password(user, password)\n        return render_template(\"edit_account.html\", user=user, password=password)\n\n    @view.route(\"/delete/<int:user_id>\", methods=(\"POST\",))\n    @admin_required\n    def delete_user(user_id: int) -> werkzeug.Response:\n        user = Journalist.query.get(user_id)\n        if user_id == session.get_uid():\n            # Do not flash because the interface already has safe guards.\n            # It can only happen by manually crafting a POST request\n            current_app.logger.error(f\"Admin {session.get_user().username} tried to delete itself\")\n            abort(403)\n        elif not user:\n            current_app.logger.error(\n                f\"Admin {session.get_user().username} tried to delete nonexistent user with \"\n                f\"pk={user_id}\"\n            )\n            abort(404)\n        elif user.is_deleted_user():\n            # Do not flash because the interface does not expose this.\n            # It can only happen by manually crafting a POST request\n            current_app.logger.error(\n                f'Admin {session.get_user().username} tried to delete \"deleted\" user'\n            )\n            abort(403)\n        else:\n            user.delete()\n            current_app.session_interface.logout_user(user.id)  # type: ignore\n            db.session.commit()\n            flash(\n                gettext(\"Deleted user '{user}'.\").format(user=user.username),\n                \"notification\",\n            )\n\n        return redirect(url_for(\"admin.index\"))\n\n    @view.route(\"/edit/<int:user_id>/new-password\", methods=(\"POST\",))\n    @admin_required\n    def new_password(user_id: int) -> werkzeug.Response:\n        try:\n            user = Journalist.query.get(user_id)\n        except NoResultFound:\n            abort(404)\n\n        if user.id == session.get_uid():\n            current_app.logger.error(\n                f\"Admin {session.get_user().username} tried to change their password without \"\n                \"validation.\"\n            )\n            abort(403)\n\n        password = request.form.get(\"password\")\n        if set_diceware_password(user, password, admin=True) is not False:\n            current_app.session_interface.logout_user(user.id)  # type: ignore\n            db.session.commit()\n        return redirect(url_for(\"admin.edit_user\", user_id=user_id))\n\n    @view.route(\"/ossec-test\", methods=(\"POST\",))\n    @admin_required\n    def ossec_test() -> werkzeug.Response:\n        current_app.logger.error(\"This is a test OSSEC alert\")\n        flash(\n            gettext(\"Test alert sent. Please check your email.\"),\n            \"testalert-notification\",\n        )\n        return redirect(url_for(\"admin.manage_config\") + \"#config-testalert\")\n\n    return view",
          "file": "admin.py"
        },
        {
          "type": "function",
          "name": "index",
          "code": "def index() -> str:\n        users = Journalist.query.filter(Journalist.username != \"deleted\").all()\n        return render_template(\"admin.html\", users=users)",
          "file": "admin.py"
        },
        {
          "type": "function",
          "name": "manage_config",
          "code": "def manage_config() -> Union[str, werkzeug.Response]:\n        if InstanceConfig.get_default().initial_message_min_len > 0:\n            prevent_short_messages = True\n        else:\n            prevent_short_messages = False\n\n        # The UI document upload prompt (\"prevent\") is the opposite of the setting (\"allow\")\n        submission_preferences_form = SubmissionPreferencesForm(\n            prevent_document_uploads=not InstanceConfig.get_default().allow_document_uploads,\n            prevent_short_messages=prevent_short_messages,\n            min_message_length=InstanceConfig.get_default().initial_message_min_len,\n            reject_codename_messages=InstanceConfig.get_default().reject_message_with_codename,\n        )\n        organization_name_form = OrgNameForm(\n            organization_name=InstanceConfig.get_default().organization_name\n        )\n        logo_form = LogoForm()\n        if logo_form.validate_on_submit():\n            f = logo_form.logo.data\n\n            if current_app.static_folder is None:\n                abort(500)\n            custom_logo_filepath = os.path.join(current_app.static_folder, \"i\", \"custom_logo.png\")\n            try:\n                f.save(custom_logo_filepath)\n                flash(gettext(\"Image updated.\"), \"logo-success\")\n            except Exception:\n                flash(\n                    # Translators: This error is shown when an uploaded image cannot be used.\n                    gettext(\"Unable to process the image file. Please try another one.\"),\n                    \"logo-error\",\n                )\n            return redirect(url_for(\"admin.manage_config\") + \"#config-logoimage\")\n        else:\n            for errors in logo_form.errors.values():\n                for error in errors:\n                    flash(error, \"logo-error\")\n            return render_template(\n                \"config.html\",\n                submission_preferences_form=submission_preferences_form,\n                organization_name_form=organization_name_form,\n                max_len=Submission.MAX_MESSAGE_LEN,\n                logo_form=logo_form,\n            )",
          "file": "admin.py"
        },
        {
          "type": "function",
          "name": "update_submission_preferences",
          "code": "def update_submission_preferences() -> Optional[werkzeug.Response]:\n        form = SubmissionPreferencesForm()\n        if form.validate_on_submit():\n            # The UI prompt (\"prevent\") is the opposite of the setting (\"allow\"):\n            allow_uploads = not form.prevent_document_uploads.data\n\n            if form.prevent_short_messages.data:\n                msg_length = form.min_message_length.data\n            else:\n                msg_length = 0\n\n            reject_codenames = form.reject_codename_messages.data\n\n            InstanceConfig.update_submission_prefs(allow_uploads, msg_length, reject_codenames)\n            flash(gettext(\"Preferences saved.\"), \"submission-preferences-success\")\n            return redirect(url_for(\"admin.manage_config\") + \"#config-preventuploads\")\n        else:\n            for errors in list(form.errors.values()):\n                for error in errors:\n                    flash(\n                        gettext(\"Preferences not updated.\") + \" \" + error,\n                        \"submission-preferences-error\",\n                    )\n        return redirect(url_for(\"admin.manage_config\") + \"#config-preventuploads\")",
          "file": "admin.py"
        },
        {
          "type": "function",
          "name": "update_org_name",
          "code": "def update_org_name() -> Union[str, werkzeug.Response]:\n        form = OrgNameForm()\n        if form.validate_on_submit():\n            try:\n                value = request.form[\"organization_name\"]\n                InstanceConfig.set_organization_name(value)\n                flash(gettext(\"Preferences saved.\"), \"org-name-success\")\n            except Exception:\n                flash(gettext(\"Failed to update organization name.\"), \"org-name-error\")\n            return redirect(url_for(\"admin.manage_config\") + \"#config-orgname\")\n        else:\n            for errors in list(form.errors.values()):\n                for error in errors:\n                    flash(error, \"org-name-error\")\n        return redirect(url_for(\"admin.manage_config\") + \"#config-orgname\")",
          "file": "admin.py"
        },
        {
          "type": "function",
          "name": "add_user",
          "code": "def add_user() -> Union[str, werkzeug.Response]:\n        form = NewUserForm()\n        if form.validate_on_submit():\n            form_valid = True\n            username = request.form[\"username\"]\n            first_name = request.form[\"first_name\"]\n            last_name = request.form[\"last_name\"]\n            password = request.form[\"password\"]\n            is_admin = bool(request.form.get(\"is_admin\"))\n\n            try:\n                otp_secret = None\n                if request.form.get(\"is_hotp\", False):\n                    otp_secret = request.form.get(\"otp_secret\", \"\")\n                verify_pending_password(for_=\"new\", passphrase=password)\n                new_user = Journalist(\n                    username=username,\n                    password=password,\n                    first_name=first_name,\n                    last_name=last_name,\n                    is_admin=is_admin,\n                    otp_secret=otp_secret,\n                )\n                db.session.add(new_user)\n                db.session.commit()\n            except PasswordError:\n                flash(\n                    gettext(\n                        \"There was an error with the autogenerated password. \"\n                        \"User not created. Please try again.\"\n                    ),\n                    \"error\",\n                )\n                form_valid = False\n            except (binascii.Error, TypeError) as e:\n                if \"Non-hexadecimal digit found\" in str(e):\n                    flash(\n                        gettext(\n                            \"Invalid HOTP secret format: \"\n                            \"please only submit letters A-F and numbers 0-9.\"\n                        ),\n                        \"error\",\n                    )\n                else:\n                    flash(\n                        gettext(\"An unexpected error occurred! \" \"Please inform your admin.\"),\n                        \"error\",\n                    )\n                form_valid = False\n            except InvalidUsernameException as e:\n                form_valid = False\n                # Translators: Here, \"{message}\" explains the problem with the username.\n                flash(gettext(\"Invalid username: {message}\").format(message=e), \"error\")\n            except IntegrityError as e:\n                db.session.rollback()\n                form_valid = False\n                if \"UNIQUE constraint failed: journalists.username\" in str(e):\n                    flash(\n                        gettext('Username \"{username}\" already taken.').format(username=username),\n                        \"error\",\n                    )\n                else:\n                    flash(\n                        gettext(\n                            \"An error occurred saving this user\"\n                            \" to the database.\"\n                            \" Please inform your admin.\"\n                        ),\n                        \"error\",\n                    )\n                    current_app.logger.error(\"Adding user \" f\"'{username}' failed: {e}\")\n\n            if form_valid:\n                if new_user.is_totp:\n                    return render_template(\n                        \"admin_new_user_two_factor_totp.html\",\n                        qrcode=Markup(new_user.totp.qrcode_svg(new_user.username).decode()),\n                        otp_secret=new_user.otp_secret,\n                        formatted_otp_secret=new_user.formatted_otp_secret,\n                        userid=str(new_user.id),\n                    )\n\n                else:\n                    return render_template(\n                        \"admin_new_user_two_factor_hotp.html\",\n                        user=new_user,\n                    )\n        password = PassphraseGenerator.get_default().generate_passphrase(\n            preferred_language=g.localeinfo.language\n        )\n        # Store password in session for future verification\n        set_pending_password(\"new\", password)\n        return render_template(\"admin_add_user.html\", password=password, form=form)",
          "file": "admin.py"
        },
        {
          "type": "function",
          "name": "new_user_two_factor_totp",
          "code": "def new_user_two_factor_totp() -> Union[str, werkzeug.Response]:\n        \"\"\"\n        After (re)setting a user's 2FA TOTP, allow the admin to verify the newly generated code.\n\n        We don't want admins to be able to look up arbitrary users' TOTP secrets, so it must\n        be supplied in the form body, generated by another endpoint. The provided token is\n        then verified against the supplied secret.\n        \"\"\"\n        token = request.form[\"token\"]\n        # NOTE: This ID comes from the user and should be only used to look up the username\n        # for embedding in the QR code and success messages. We don't load any other state\n        # from the database to prevent IDOR attacks.\n        username = Journalist.query.get(request.form[\"userid\"]).username\n        otp_secret = request.form[\"otp_secret\"]\n        totp = two_factor.TOTP(otp_secret)\n        try:\n            # Note: this intentionally doesn't prevent replay attacks, since we just want\n            # to make sure they have the right token\n            totp.verify(token, datetime.utcnow())\n            flash(\n                gettext(\n                    'The two-factor code for user \"{user}\" was verified ' \"successfully.\"\n                ).format(user=username),\n                \"notification\",\n            )\n            return redirect(url_for(\"admin.index\"))\n\n        except two_factor.OtpTokenInvalid:\n            flash(\n                gettext(\"There was a problem verifying the two-factor code. Please try again.\"),\n                \"error\",\n            )\n\n        return render_template(\n            \"admin_new_user_two_factor_totp.html\",\n            qrcode=Markup(totp.qrcode_svg(username).decode()),\n            otp_secret=otp_secret,\n            formatted_otp_secret=two_factor.format_secret(otp_secret),\n            userid=request.form[\"userid\"],\n        )",
          "file": "admin.py"
        },
        {
          "type": "function",
          "name": "reset_two_factor_totp",
          "code": "def reset_two_factor_totp() -> str:\n        uid = request.form[\"uid\"]\n        user = Journalist.query.get(uid)\n        user.is_totp = True\n        user.regenerate_totp_shared_secret()\n        db.session.commit()\n        return render_template(\n            \"admin_new_user_two_factor_totp.html\",\n            qrcode=Markup(user.totp.qrcode_svg(user.username).decode()),\n            otp_secret=user.otp_secret,\n            formatted_otp_secret=user.formatted_otp_secret,\n            userid=str(user.id),\n        )",
          "file": "admin.py"
        },
        {
          "type": "function",
          "name": "new_user_two_factor_hotp",
          "code": "def new_user_two_factor_hotp() -> Union[str, werkzeug.Response]:\n        \"\"\"\n        After (re)setting a user's 2FA HOTP, allow the admin to verify the newly generated code.\n\n        This works differently than the analogous TOTP endpoint, as here we do verify against\n        the database secret because we need to compare with and increment the counter.\n        \"\"\"\n        user = Journalist.query.get(request.form[\"uid\"])\n        token = request.form[\"token\"]\n\n        error = False\n\n        if not user.is_totp:\n            try:\n                user.verify_2fa_token(token)\n                flash(\n                    gettext(\n                        'The two-factor code for user \"{user}\" was verified ' \"successfully.\"\n                    ).format(user=user.username),\n                    \"notification\",\n                )\n                return redirect(url_for(\"admin.index\"))\n\n            except two_factor.OtpTokenInvalid:\n                error = True\n        else:\n            # XXX: Consider using a different error message here, or do we not want to reveal\n            # if the user is using HOTP vs TOTP\n            error = True\n\n        if error:\n            flash(\n                gettext(\"There was a problem verifying the two-factor code. Please try again.\"),\n                \"error\",\n            )\n\n        return render_template(\"admin_new_user_two_factor_hotp.html\", user=user)",
          "file": "admin.py"
        },
        {
          "type": "function",
          "name": "reset_two_factor_hotp",
          "code": "def reset_two_factor_hotp() -> Union[str, werkzeug.Response]:\n        uid = request.form[\"uid\"]\n        user = Journalist.query.get(uid)\n        otp_secret = request.form.get(\"otp_secret\", None)\n        if otp_secret:\n            if not validate_hotp_secret(user, otp_secret):\n                return render_template(\"admin_edit_hotp_secret.html\", uid=user.id)\n            db.session.commit()\n            return render_template(\"admin_new_user_two_factor_hotp.html\", user=user)\n        else:\n            return render_template(\"admin_edit_hotp_secret.html\", uid=user.id)",
          "file": "admin.py"
        },
        {
          "type": "function",
          "name": "edit_user",
          "code": "def edit_user(user_id: int) -> Union[str, werkzeug.Response]:\n        user = Journalist.query.get(user_id)\n\n        if request.method == \"POST\":\n            if request.form.get(\"username\", None):\n                new_username = request.form[\"username\"]\n\n                try:\n                    Journalist.check_username_acceptable(new_username)\n                except InvalidUsernameException as e:\n                    flash(\n                        gettext(\"Invalid username: {message}\").format(message=e),\n                        \"error\",\n                    )\n                    return redirect(url_for(\"admin.edit_user\", user_id=user_id))\n\n                if new_username == user.username:\n                    pass\n                elif Journalist.query.filter_by(username=new_username).one_or_none():\n                    flash(\n                        gettext('Username \"{username}\" already taken.').format(\n                            username=new_username\n                        ),\n                        \"error\",\n                    )\n                    return redirect(url_for(\"admin.edit_user\", user_id=user_id))\n                else:\n                    user.username = new_username\n\n            try:\n                first_name = request.form[\"first_name\"]\n                Journalist.check_name_acceptable(first_name)\n                user.first_name = first_name\n            except FirstOrLastNameError as e:\n                # Translators: Here, \"{message}\" explains the problem with the name.\n                flash(gettext(\"Name not updated: {message}\").format(message=e), \"error\")\n                return redirect(url_for(\"admin.edit_user\", user_id=user_id))\n\n            try:\n                last_name = request.form[\"last_name\"]\n                Journalist.check_name_acceptable(last_name)\n                user.last_name = last_name\n            except FirstOrLastNameError as e:\n                flash(gettext(\"Name not updated: {message}\").format(message=e), \"error\")\n                return redirect(url_for(\"admin.edit_user\", user_id=user_id))\n\n            user.is_admin = bool(request.form.get(\"is_admin\"))\n\n            commit_account_changes(user)\n\n        password = PassphraseGenerator.get_default().generate_passphrase(\n            preferred_language=g.localeinfo.language\n        )\n        # Store password in session for future verification\n        set_pending_password(user, password)\n        return render_template(\"edit_account.html\", user=user, password=password)",
          "file": "admin.py"
        },
        {
          "type": "function",
          "name": "delete_user",
          "code": "def delete_user(user_id: int) -> werkzeug.Response:\n        user = Journalist.query.get(user_id)\n        if user_id == session.get_uid():\n            # Do not flash because the interface already has safe guards.\n            # It can only happen by manually crafting a POST request\n            current_app.logger.error(f\"Admin {session.get_user().username} tried to delete itself\")\n            abort(403)\n        elif not user:\n            current_app.logger.error(\n                f\"Admin {session.get_user().username} tried to delete nonexistent user with \"\n                f\"pk={user_id}\"\n            )\n            abort(404)\n        elif user.is_deleted_user():\n            # Do not flash because the interface does not expose this.\n            # It can only happen by manually crafting a POST request\n            current_app.logger.error(\n                f'Admin {session.get_user().username} tried to delete \"deleted\" user'\n            )\n            abort(403)\n        else:\n            user.delete()\n            current_app.session_interface.logout_user(user.id)  # type: ignore\n            db.session.commit()\n            flash(\n                gettext(\"Deleted user '{user}'.\").format(user=user.username),\n                \"notification\",\n            )\n\n        return redirect(url_for(\"admin.index\"))",
          "file": "admin.py"
        },
        {
          "type": "function",
          "name": "new_password",
          "code": "def new_password(user_id: int) -> werkzeug.Response:\n        try:\n            user = Journalist.query.get(user_id)\n        except NoResultFound:\n            abort(404)\n\n        if user.id == session.get_uid():\n            current_app.logger.error(\n                f\"Admin {session.get_user().username} tried to change their password without \"\n                \"validation.\"\n            )\n            abort(403)\n\n        password = request.form.get(\"password\")\n        if set_diceware_password(user, password, admin=True) is not False:\n            current_app.session_interface.logout_user(user.id)  # type: ignore\n            db.session.commit()\n        return redirect(url_for(\"admin.edit_user\", user_id=user_id))",
          "file": "admin.py"
        },
        {
          "type": "function",
          "name": "ossec_test",
          "code": "def ossec_test() -> werkzeug.Response:\n        current_app.logger.error(\"This is a test OSSEC alert\")\n        flash(\n            gettext(\"Test alert sent. Please check your email.\"),\n            \"testalert-notification\",\n        )\n        return redirect(url_for(\"admin.manage_config\") + \"#config-testalert\")",
          "file": "admin.py"
        }
      ],
      "sessions.py": [
        {
          "type": "class",
          "name": "ServerSideSession",
          "code": "class ServerSideSession(CallbackDict, SessionMixin):\n    \"\"\"Baseclass for server-side based sessions.\"\"\"\n\n    def __init__(self, sid: str, token: str, lifetime: int = 0, initial: Any = None) -> None:\n        def on_update(self: ServerSideSession) -> None:\n            self.modified = True\n\n        if initial and \"uid\" in initial:\n            self.set_uid(initial[\"uid\"])\n            self.set_user()\n        else:\n            self.uid: Optional[int] = None\n            self.user = None\n        CallbackDict.__init__(self, initial, on_update)\n        self.sid = sid\n        self.token: str = token\n        self.lifetime = lifetime\n        self.is_api = False\n        self.to_destroy = False\n        self.to_regenerate = False\n        self.modified = False\n        self.flash: Optional[Tuple[str, str]] = None\n        self.locale: Optional[str] = None\n\n    def get_token(self) -> Optional[str]:\n        return self.token\n\n    def get_lifetime(self) -> datetime:\n        return datetime.now(timezone.utc) + timedelta(seconds=self.lifetime)\n\n    def set_user(self) -> None:\n        if self.uid is not None:\n            self.user = Journalist.query.get(self.uid)\n        if self.user is None:\n            # The uid has no match in the database, and this should really not happen\n            self.uid = None\n            self.to_destroy = True\n\n    def get_user(self) -> Optional[Journalist]:\n        return self.user\n\n    def get_uid(self) -> Optional[int]:\n        return self.uid\n\n    def set_uid(self, uid: int) -> None:\n        self.user = None\n        self.uid = uid\n\n    def logged_in(self) -> bool:\n        return self.uid is not None\n\n    def destroy(\n        self, flash: Optional[Tuple[str, str]] = None, locale: Optional[str] = None\n    ) -> None:\n        # The parameters are needed to pass the information to the new session\n        self.locale = locale\n        self.flash = flash\n        self.uid = None\n        self.user = None\n        self.to_destroy = True\n\n    def regenerate(self) -> None:\n        self.to_regenerate = True",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, sid: str, token: str, lifetime: int = 0, initial: Any = None) -> None:\n        def on_update(self: ServerSideSession) -> None:\n            self.modified = True\n\n        if initial and \"uid\" in initial:\n            self.set_uid(initial[\"uid\"])\n            self.set_user()\n        else:\n            self.uid: Optional[int] = None\n            self.user = None\n        CallbackDict.__init__(self, initial, on_update)\n        self.sid = sid\n        self.token: str = token\n        self.lifetime = lifetime\n        self.is_api = False\n        self.to_destroy = False\n        self.to_regenerate = False\n        self.modified = False\n        self.flash: Optional[Tuple[str, str]] = None\n        self.locale: Optional[str] = None",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "on_update",
          "code": "def on_update(self: ServerSideSession) -> None:\n            self.modified = True",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "get_token",
          "code": "def get_token(self) -> Optional[str]:\n        return self.token",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "get_lifetime",
          "code": "def get_lifetime(self) -> datetime:\n        return datetime.now(timezone.utc) + timedelta(seconds=self.lifetime)",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "set_user",
          "code": "def set_user(self) -> None:\n        if self.uid is not None:\n            self.user = Journalist.query.get(self.uid)\n        if self.user is None:\n            # The uid has no match in the database, and this should really not happen\n            self.uid = None\n            self.to_destroy = True",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "get_user",
          "code": "def get_user(self) -> Optional[Journalist]:\n        return self.user",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "get_uid",
          "code": "def get_uid(self) -> Optional[int]:\n        return self.uid",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "set_uid",
          "code": "def set_uid(self, uid: int) -> None:\n        self.user = None\n        self.uid = uid",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "logged_in",
          "code": "def logged_in(self) -> bool:\n        return self.uid is not None",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "destroy",
          "code": "def destroy(\n        self, flash: Optional[Tuple[str, str]] = None, locale: Optional[str] = None\n    ) -> None:\n        # The parameters are needed to pass the information to the new session\n        self.locale = locale\n        self.flash = flash\n        self.uid = None\n        self.user = None\n        self.to_destroy = True",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "regenerate",
          "code": "def regenerate(self) -> None:\n        self.to_regenerate = True",
          "file": "sessions.py"
        },
        {
          "type": "class",
          "name": "SessionInterface",
          "code": "class SessionInterface(FlaskSessionInterface):\n    def _generate_sid(self) -> str:\n        return token_urlsafe(32)\n\n    def _get_signer(self, app: Flask) -> URLSafeTimedSerializer:\n        if not app.secret_key:\n            raise RuntimeError(\"No secret key set\")\n        return URLSafeTimedSerializer(app.secret_key, salt=self.salt)\n\n    \"\"\"Uses the Redis key-value store as a session backend.\n\n    :param redis: A ``redis.Redis`` instance.\n    :param key_prefix: A prefix that is added to all Redis store keys.\n    :param salt: Allows to set the signer salt from the calling interface\n    :param header_name: if use_header, set the header name to parse\n    \"\"\"\n\n    def __init__(\n        self,\n        lifetime: int,\n        renew_count: int,\n        redis: Redis,\n        key_prefix: str,\n        salt: str,\n        header_name: str,\n    ) -> None:\n        self.serializer = session_json_serializer\n        self.redis = redis\n        self.lifetime = lifetime\n        self.renew_count = renew_count\n        self.key_prefix = key_prefix\n        self.api_key_prefix = \"api_\" + key_prefix\n        self.salt = salt\n        self.api_salt = \"api_\" + salt\n        self.header_name = header_name\n        self.new = False\n\n    def _new_session(self, is_api: bool = False, initial: Any = None) -> ServerSideSession:\n        sid = self._generate_sid()\n        token: str = self._get_signer(app).dumps(sid)  # type: ignore\n        session = ServerSideSession(sid=sid, token=token, lifetime=self.lifetime, initial=initial)\n        session.new = True\n        session.is_api = is_api\n        return session\n\n    def open_session(self, app: Flask, request: Request) -> Optional[ServerSideSession]:\n        \"\"\"This function is called by the flask session interface at the\n        beginning of each request.\n        \"\"\"\n        is_api = request.path.split(\"/\")[1] == \"api\"\n\n        if is_api:\n            self.key_prefix = self.api_key_prefix\n            self.salt = self.api_salt\n            auth_header = request.headers.get(self.header_name)\n            if auth_header:\n                split = auth_header.split(\" \")\n                if len(split) != 2 or split[0] != \"Token\":\n                    return self._new_session(is_api)\n                sid: Optional[str] = split[1]\n            else:\n                return self._new_session(is_api)\n        else:\n            sid = request.cookies.get(app.session_cookie_name)\n        if sid:\n            try:\n                sid = self._get_signer(app).loads(sid)\n            except BadSignature:\n                sid = None\n        if not sid:\n            return self._new_session(is_api)\n\n        val = self.redis.get(self.key_prefix + sid)\n        if val is not None:\n            try:\n                data = self.serializer.loads(val.decode(\"utf-8\"))\n                token: str = self._get_signer(app).dumps(sid)  # type: ignore\n                return ServerSideSession(sid=sid, token=token, initial=data)\n            except (JSONDecodeError, NotImplementedError):\n                return self._new_session(is_api)\n        # signed session_id provided in cookie is valid, but the session is not on the server\n        # anymore so maybe here is the code path for a meaningful error message\n        msg = gettext(\"You have been logged out due to inactivity.\")\n        return self._new_session(is_api, initial={\"_flashes\": [(\"error\", msg)]})\n\n    def save_session(  # type: ignore[override]\n        self, app: Flask, session: ServerSideSession, response: Response\n    ) -> None:\n        \"\"\"This is called at the end of each request, just\n        before sending the response.\n        \"\"\"\n        domain = self.get_cookie_domain(app)\n        path = self.get_cookie_path(app)\n        if session.to_destroy:\n            initial: Dict[str, Any] = {\"locale\": session.locale}\n            if session.flash:\n                initial[\"_flashes\"] = [session.flash]\n            self.redis.delete(self.key_prefix + session.sid)\n            if not session.is_api:\n                # Instead of deleting the cookie and send a new sid with the next request\n                # create the new session already, so we can pass along messages and locale\n                session = self._new_session(False, initial=initial)\n        expires = self.redis.ttl(name=self.key_prefix + session.sid)\n        if session.new:\n            session[\"renew_count\"] = self.renew_count\n            expires = self.lifetime\n        elif expires < (30 * 60) and session[\"renew_count\"] > 0:\n            session[\"renew_count\"] -= 1\n            expires += self.lifetime\n            session.modified = True\n        httponly = self.get_cookie_httponly(app)\n        secure = self.get_cookie_secure(app)\n        samesite = self.get_cookie_samesite(app)\n        val = self.serializer.dumps(dict(session))\n        if session.to_regenerate:\n            self.redis.delete(self.key_prefix + session.sid)\n            session.sid = self._generate_sid()\n            session.token = self._get_signer(app).dumps(session.sid)  # type: ignore\n        if session.new or session.to_regenerate:\n            self.redis.setex(name=self.key_prefix + session.sid, value=val, time=expires)\n        elif session.modified:\n            # To prevent race conditions where session is delete by an admin in the middle of a req\n            # accept to save the session object if and only if already exists using the xx flag\n            self.redis.set(name=self.key_prefix + session.sid, value=val, ex=expires, xx=True)\n        if not session.is_api and (session.new or session.to_regenerate):\n            response.headers.add(\"Vary\", \"Cookie\")\n            response.set_cookie(\n                app.session_cookie_name,\n                session.token,\n                httponly=httponly,\n                domain=domain,\n                path=path,\n                secure=secure,\n                samesite=samesite,\n            )\n\n    def logout_user(self, uid: int) -> None:\n        for key in self.redis.keys(self.key_prefix + \"*\") + self.redis.keys(\n            \"api_\" + self.key_prefix + \"*\"\n        ):\n            found = self.redis.get(key)\n            if found:\n                sess = session_json_serializer.loads(found.decode(\"utf-8\"))\n                if \"uid\" in sess and sess[\"uid\"] == uid:\n                    self.redis.delete(key)",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "_generate_sid",
          "code": "def _generate_sid(self) -> str:\n        return token_urlsafe(32)",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "_get_signer",
          "code": "def _get_signer(self, app: Flask) -> URLSafeTimedSerializer:\n        if not app.secret_key:\n            raise RuntimeError(\"No secret key set\")\n        return URLSafeTimedSerializer(app.secret_key, salt=self.salt)",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(\n        self,\n        lifetime: int,\n        renew_count: int,\n        redis: Redis,\n        key_prefix: str,\n        salt: str,\n        header_name: str,\n    ) -> None:\n        self.serializer = session_json_serializer\n        self.redis = redis\n        self.lifetime = lifetime\n        self.renew_count = renew_count\n        self.key_prefix = key_prefix\n        self.api_key_prefix = \"api_\" + key_prefix\n        self.salt = salt\n        self.api_salt = \"api_\" + salt\n        self.header_name = header_name\n        self.new = False",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "_new_session",
          "code": "def _new_session(self, is_api: bool = False, initial: Any = None) -> ServerSideSession:\n        sid = self._generate_sid()\n        token: str = self._get_signer(app).dumps(sid)  # type: ignore\n        session = ServerSideSession(sid=sid, token=token, lifetime=self.lifetime, initial=initial)\n        session.new = True\n        session.is_api = is_api\n        return session",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "open_session",
          "code": "def open_session(self, app: Flask, request: Request) -> Optional[ServerSideSession]:\n        \"\"\"This function is called by the flask session interface at the\n        beginning of each request.\n        \"\"\"\n        is_api = request.path.split(\"/\")[1] == \"api\"\n\n        if is_api:\n            self.key_prefix = self.api_key_prefix\n            self.salt = self.api_salt\n            auth_header = request.headers.get(self.header_name)\n            if auth_header:\n                split = auth_header.split(\" \")\n                if len(split) != 2 or split[0] != \"Token\":\n                    return self._new_session(is_api)\n                sid: Optional[str] = split[1]\n            else:\n                return self._new_session(is_api)\n        else:\n            sid = request.cookies.get(app.session_cookie_name)\n        if sid:\n            try:\n                sid = self._get_signer(app).loads(sid)\n            except BadSignature:\n                sid = None\n        if not sid:\n            return self._new_session(is_api)\n\n        val = self.redis.get(self.key_prefix + sid)\n        if val is not None:\n            try:\n                data = self.serializer.loads(val.decode(\"utf-8\"))\n                token: str = self._get_signer(app).dumps(sid)  # type: ignore\n                return ServerSideSession(sid=sid, token=token, initial=data)\n            except (JSONDecodeError, NotImplementedError):\n                return self._new_session(is_api)\n        # signed session_id provided in cookie is valid, but the session is not on the server\n        # anymore so maybe here is the code path for a meaningful error message\n        msg = gettext(\"You have been logged out due to inactivity.\")\n        return self._new_session(is_api, initial={\"_flashes\": [(\"error\", msg)]})",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "save_session",
          "code": "def save_session(  # type: ignore[override]\n        self, app: Flask, session: ServerSideSession, response: Response\n    ) -> None:\n        \"\"\"This is called at the end of each request, just\n        before sending the response.\n        \"\"\"\n        domain = self.get_cookie_domain(app)\n        path = self.get_cookie_path(app)\n        if session.to_destroy:\n            initial: Dict[str, Any] = {\"locale\": session.locale}\n            if session.flash:\n                initial[\"_flashes\"] = [session.flash]\n            self.redis.delete(self.key_prefix + session.sid)\n            if not session.is_api:\n                # Instead of deleting the cookie and send a new sid with the next request\n                # create the new session already, so we can pass along messages and locale\n                session = self._new_session(False, initial=initial)\n        expires = self.redis.ttl(name=self.key_prefix + session.sid)\n        if session.new:\n            session[\"renew_count\"] = self.renew_count\n            expires = self.lifetime\n        elif expires < (30 * 60) and session[\"renew_count\"] > 0:\n            session[\"renew_count\"] -= 1\n            expires += self.lifetime\n            session.modified = True\n        httponly = self.get_cookie_httponly(app)\n        secure = self.get_cookie_secure(app)\n        samesite = self.get_cookie_samesite(app)\n        val = self.serializer.dumps(dict(session))\n        if session.to_regenerate:\n            self.redis.delete(self.key_prefix + session.sid)\n            session.sid = self._generate_sid()\n            session.token = self._get_signer(app).dumps(session.sid)  # type: ignore\n        if session.new or session.to_regenerate:\n            self.redis.setex(name=self.key_prefix + session.sid, value=val, time=expires)\n        elif session.modified:\n            # To prevent race conditions where session is delete by an admin in the middle of a req\n            # accept to save the session object if and only if already exists using the xx flag\n            self.redis.set(name=self.key_prefix + session.sid, value=val, ex=expires, xx=True)\n        if not session.is_api and (session.new or session.to_regenerate):\n            response.headers.add(\"Vary\", \"Cookie\")\n            response.set_cookie(\n                app.session_cookie_name,\n                session.token,\n                httponly=httponly,\n                domain=domain,\n                path=path,\n                secure=secure,\n                samesite=samesite,\n            )",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "logout_user",
          "code": "def logout_user(self, uid: int) -> None:\n        for key in self.redis.keys(self.key_prefix + \"*\") + self.redis.keys(\n            \"api_\" + self.key_prefix + \"*\"\n        ):\n            found = self.redis.get(key)\n            if found:\n                sess = session_json_serializer.loads(found.decode(\"utf-8\"))\n                if \"uid\" in sess and sess[\"uid\"] == uid:\n                    self.redis.delete(key)",
          "file": "sessions.py"
        },
        {
          "type": "class",
          "name": "Session",
          "code": "class Session:\n    def __init__(self, app: Flask, sdconfig: SecureDropConfig) -> None:\n        self.app = app\n        if app is not None:\n            self.init_app(app, sdconfig)\n\n    def init_app(self, app: Flask, sdconfig: SecureDropConfig) -> \"None\":\n        \"\"\"This is used to set up session for your app object.\n        :param app: the Flask app object with proper configuration.\n        \"\"\"\n        app.session_interface = self._get_interface(app, sdconfig)  # type: ignore\n\n    def _get_interface(self, app: Flask, sdconfig: SecureDropConfig) -> SessionInterface:\n        config = app.config.copy()\n        config.setdefault(\"SESSION_REDIS\", Redis(**sdconfig.REDIS_KWARGS))\n        config.setdefault(\"SESSION_LIFETIME\", 2 * 60 * 60)\n        config.setdefault(\"SESSION_RENEW_COUNT\", 5)\n        config.setdefault(\"SESSION_SIGNER_SALT\", \"session\")\n        config.setdefault(\"SESSION_KEY_PREFIX\", \"session:\")\n        config.setdefault(\"SESSION_HEADER_NAME\", \"authorization\")\n\n        return SessionInterface(\n            config[\"SESSION_LIFETIME\"],\n            config[\"SESSION_RENEW_COUNT\"],\n            config[\"SESSION_REDIS\"],\n            config[\"SESSION_KEY_PREFIX\"],\n            config[\"SESSION_SIGNER_SALT\"],\n            config[\"SESSION_HEADER_NAME\"],\n        )",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, app: Flask, sdconfig: SecureDropConfig) -> None:\n        self.app = app\n        if app is not None:\n            self.init_app(app, sdconfig)",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "init_app",
          "code": "def init_app(self, app: Flask, sdconfig: SecureDropConfig) -> \"None\":\n        \"\"\"This is used to set up session for your app object.\n        :param app: the Flask app object with proper configuration.\n        \"\"\"\n        app.session_interface = self._get_interface(app, sdconfig)  # type: ignore",
          "file": "sessions.py"
        },
        {
          "type": "function",
          "name": "_get_interface",
          "code": "def _get_interface(self, app: Flask, sdconfig: SecureDropConfig) -> SessionInterface:\n        config = app.config.copy()\n        config.setdefault(\"SESSION_REDIS\", Redis(**sdconfig.REDIS_KWARGS))\n        config.setdefault(\"SESSION_LIFETIME\", 2 * 60 * 60)\n        config.setdefault(\"SESSION_RENEW_COUNT\", 5)\n        config.setdefault(\"SESSION_SIGNER_SALT\", \"session\")\n        config.setdefault(\"SESSION_KEY_PREFIX\", \"session:\")\n        config.setdefault(\"SESSION_HEADER_NAME\", \"authorization\")\n\n        return SessionInterface(\n            config[\"SESSION_LIFETIME\"],\n            config[\"SESSION_RENEW_COUNT\"],\n            config[\"SESSION_REDIS\"],\n            config[\"SESSION_KEY_PREFIX\"],\n            config[\"SESSION_SIGNER_SALT\"],\n            config[\"SESSION_HEADER_NAME\"],\n        )",
          "file": "sessions.py"
        }
      ],
      "decorators.py": [
        {
          "type": "function",
          "name": "admin_required",
          "code": "def admin_required(func: Callable) -> Callable:\n    @wraps(func)\n    def wrapper(*args: Any, **kwargs: Any) -> Any:\n        if session.logged_in() and session.get_user().is_admin:\n            return func(*args, **kwargs)\n        flash(gettext(\"Only admins can access this page.\"), \"notification\")\n        return redirect(url_for(\"main.index\"))\n\n    return wrapper",
          "file": "decorators.py"
        },
        {
          "type": "function",
          "name": "wrapper",
          "code": "def wrapper(*args: Any, **kwargs: Any) -> Any:\n        if session.logged_in() and session.get_user().is_admin:\n            return func(*args, **kwargs)\n        flash(gettext(\"Only admins can access this page.\"), \"notification\")\n        return redirect(url_for(\"main.index\"))",
          "file": "decorators.py"
        }
      ],
      "__init__.py": [
        {
          "type": "function",
          "name": "get_logo_url",
          "code": "def get_logo_url(app: Flask) -> str:\n    if not app.static_folder:\n        raise FileNotFoundError\n\n    custom_logo_filename = \"i/custom_logo.png\"\n    default_logo_filename = \"i/logo.png\"\n    custom_logo_path = Path(app.static_folder) / custom_logo_filename\n    default_logo_path = Path(app.static_folder) / default_logo_filename\n    if custom_logo_path.is_file():\n        return url_for(\"static\", filename=custom_logo_filename)\n    elif default_logo_path.is_file():\n        return url_for(\"static\", filename=default_logo_filename)\n\n    raise FileNotFoundError",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "create_app",
          "code": "def create_app(config: SecureDropConfig) -> Flask:\n    app = Flask(\n        __name__,\n        template_folder=str(config.JOURNALIST_TEMPLATES_DIR.absolute()),\n        static_folder=config.STATIC_DIR.absolute(),\n    )\n\n    app.config.from_object(config.JOURNALIST_APP_FLASK_CONFIG_CLS)\n\n    Session(app, config)\n    csrf = CSRFProtect(app)\n    app.config[\"SESSION_COOKIE_SAMESITE\"] = \"Strict\"\n\n    app.config[\"SQLALCHEMY_TRACK_MODIFICATIONS\"] = False\n    app.config[\"SQLALCHEMY_DATABASE_URI\"] = config.DATABASE_URI\n\n    # Check if the server OS is past EOL; if so, we'll display banners\n    app.config[\"OS_PAST_EOL\"] = server_os.is_os_past_eol()\n    app.config[\"OS_NEEDS_MIGRATION_FIXES\"] = server_os.needs_migration_fixes()\n\n    db.init_app(app)\n\n    class JSONEncoder(json.JSONEncoder):\n        \"\"\"Custom JSON encoder to use our preferred timestamp format\"\"\"\n\n        def default(self, obj: \"Any\") -> \"Any\":\n            if isinstance(obj, datetime):\n                return obj.isoformat()\n            super().default(obj)\n\n    app.json_encoder = JSONEncoder\n\n    @app.errorhandler(CSRFError)\n    def handle_csrf_error(e: CSRFError) -> \"Response\":\n        app.logger.error(\"The CSRF token is invalid.\")\n        msg = gettext(\"You have been logged out due to inactivity or a problem with your session.\")\n        session.destroy((\"error\", msg), session.get(\"locale\"))\n        return redirect(url_for(\"main.login\"))\n\n    def _handle_http_exception(\n        error: HTTPException,\n    ) -> Tuple[Union[Response, str], Optional[int]]:\n        # Workaround for no blueprint-level 404/5 error handlers, see:\n        # https://github.com/pallets/flask/issues/503#issuecomment-71383286\n        # TODO: clean up API error handling such that all except 404/5s are\n        # registered in the blueprint and 404/5s are handled at the application\n        # level.\n        if request.path.startswith(\"/api/\"):\n            handler = list(app.error_handler_spec[\"api\"][error.code].values())[0]\n            return handler(error)  # type: ignore\n\n        return render_template(\"error.html\", error=error), error.code\n\n    for code in default_exceptions:\n        app.errorhandler(code)(_handle_http_exception)\n\n    i18n.configure(config, app)\n\n    app.jinja_env.trim_blocks = True\n    app.jinja_env.lstrip_blocks = True\n    app.jinja_env.globals[\"version\"] = version.__version__\n    app.jinja_env.filters[\"rel_datetime_format\"] = template_filters.rel_datetime_format\n    app.jinja_env.filters[\"filesizeformat\"] = template_filters.filesizeformat\n    app.jinja_env.filters[\"html_datetime_format\"] = template_filters.html_datetime_format\n    app.jinja_env.add_extension(\"jinja2.ext.do\")\n\n    @app.before_request\n    def update_instance_config() -> None:\n        InstanceConfig.get_default(refresh=True)\n\n    @app.before_request\n    def setup_g() -> Optional[Response]:\n        \"\"\"Store commonly used values in Flask's special g object\"\"\"\n\n        i18n.set_locale(config)\n        g.show_os_past_eol_warning = app.config[\"OS_PAST_EOL\"]\n        g.show_os_needs_migration_fixes = app.config[\"OS_NEEDS_MIGRATION_FIXES\"]\n\n        if InstanceConfig.get_default().organization_name:\n            g.organization_name = (  # pylint: disable=assigning-non-slot\n                InstanceConfig.get_default().organization_name\n            )\n        else:\n            g.organization_name = gettext(\"SecureDrop\")  # pylint: disable=assigning-non-slot\n\n        try:\n            g.logo = get_logo_url(app)  # pylint: disable=assigning-non-slot\n        except FileNotFoundError:\n            app.logger.error(\"Site logo not found.\")\n\n        if request.path.split(\"/\")[1] == \"api\":\n            if request.endpoint not in _insecure_api_views and not session.logged_in():\n                abort(403)\n        elif request.endpoint not in _insecure_views and not session.logged_in():\n            return redirect(url_for(\"main.login\"))\n\n        if request.method == \"POST\":\n            filesystem_id = request.form.get(\"filesystem_id\")\n            if filesystem_id:\n                g.filesystem_id = filesystem_id  # pylint: disable=assigning-non-slot\n                g.source = get_source(filesystem_id)  # pylint: disable=assigning-non-slot\n\n        return None\n\n    app.register_blueprint(main.make_blueprint())\n    app.register_blueprint(account.make_blueprint(), url_prefix=\"/account\")\n    app.register_blueprint(admin.make_blueprint(), url_prefix=\"/admin\")\n    app.register_blueprint(col.make_blueprint(), url_prefix=\"/col\")\n    api_blueprint = api.make_blueprint()\n    app.register_blueprint(api_blueprint, url_prefix=\"/api/v1\")\n    csrf.exempt(api_blueprint)\n\n    return app",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "JSONEncoder",
          "code": "class JSONEncoder(json.JSONEncoder):\n        \"\"\"Custom JSON encoder to use our preferred timestamp format\"\"\"\n\n        def default(self, obj: \"Any\") -> \"Any\":\n            if isinstance(obj, datetime):\n                return obj.isoformat()\n            super().default(obj)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "default",
          "code": "def default(self, obj: \"Any\") -> \"Any\":\n            if isinstance(obj, datetime):\n                return obj.isoformat()\n            super().default(obj)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "handle_csrf_error",
          "code": "def handle_csrf_error(e: CSRFError) -> \"Response\":\n        app.logger.error(\"The CSRF token is invalid.\")\n        msg = gettext(\"You have been logged out due to inactivity or a problem with your session.\")\n        session.destroy((\"error\", msg), session.get(\"locale\"))\n        return redirect(url_for(\"main.login\"))",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "_handle_http_exception",
          "code": "def _handle_http_exception(\n        error: HTTPException,\n    ) -> Tuple[Union[Response, str], Optional[int]]:\n        # Workaround for no blueprint-level 404/5 error handlers, see:\n        # https://github.com/pallets/flask/issues/503#issuecomment-71383286\n        # TODO: clean up API error handling such that all except 404/5s are\n        # registered in the blueprint and 404/5s are handled at the application\n        # level.\n        if request.path.startswith(\"/api/\"):\n            handler = list(app.error_handler_spec[\"api\"][error.code].values())[0]\n            return handler(error)  # type: ignore\n\n        return render_template(\"error.html\", error=error), error.code",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "update_instance_config",
          "code": "def update_instance_config() -> None:\n        InstanceConfig.get_default(refresh=True)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "setup_g",
          "code": "def setup_g() -> Optional[Response]:\n        \"\"\"Store commonly used values in Flask's special g object\"\"\"\n\n        i18n.set_locale(config)\n        g.show_os_past_eol_warning = app.config[\"OS_PAST_EOL\"]\n        g.show_os_needs_migration_fixes = app.config[\"OS_NEEDS_MIGRATION_FIXES\"]\n\n        if InstanceConfig.get_default().organization_name:\n            g.organization_name = (  # pylint: disable=assigning-non-slot\n                InstanceConfig.get_default().organization_name\n            )\n        else:\n            g.organization_name = gettext(\"SecureDrop\")  # pylint: disable=assigning-non-slot\n\n        try:\n            g.logo = get_logo_url(app)  # pylint: disable=assigning-non-slot\n        except FileNotFoundError:\n            app.logger.error(\"Site logo not found.\")\n\n        if request.path.split(\"/\")[1] == \"api\":\n            if request.endpoint not in _insecure_api_views and not session.logged_in():\n                abort(403)\n        elif request.endpoint not in _insecure_views and not session.logged_in():\n            return redirect(url_for(\"main.login\"))\n\n        if request.method == \"POST\":\n            filesystem_id = request.form.get(\"filesystem_id\")\n            if filesystem_id:\n                g.filesystem_id = filesystem_id  # pylint: disable=assigning-non-slot\n                g.source = get_source(filesystem_id)  # pylint: disable=assigning-non-slot\n\n        return None",
          "file": "__init__.py"
        }
      ],
      "main.py": [
        {
          "type": "function",
          "name": "make_blueprint",
          "code": "def make_blueprint() -> Blueprint:\n    view = Blueprint(\"main\", __name__)\n\n    @view.route(\"/login\", methods=(\"GET\", \"POST\"))\n    def login() -> Union[str, werkzeug.Response]:\n        if request.method == \"POST\":\n            user = validate_user(\n                request.form[\"username\"],\n                request.form[\"password\"],\n                request.form[\"token\"],\n            )\n            if user:\n                current_app.logger.info(\n                    \"'{}' logged in with the two-factor code {}\".format(\n                        request.form[\"username\"], request.form[\"token\"]\n                    )\n                )\n\n                # Update access metadata\n                user.last_access = datetime.now(timezone.utc)\n                db.session.add(user)\n                db.session.commit()\n\n                session[\"uid\"] = user.id\n                session.regenerate()\n                return redirect(url_for(\"main.index\"))\n\n        return render_template(\"login.html\")\n\n    @view.route(\"/logout\", methods=(\"POST\",))\n    def logout() -> werkzeug.Response:\n        session.destroy()\n        return redirect(url_for(\"main.index\"))\n\n    @view.route(\"/\")\n    def index() -> str:\n        # Gather the count of unread submissions for each source\n        # ID. This query will be joined in the queries for starred and\n        # unstarred sources below, and the unread counts added to\n        # their result sets as an extra column.\n        unread_stmt = (\n            db.session.query(Submission.source_id, func.count(\"*\").label(\"num_unread\"))\n            .filter_by(seen_files=None, seen_messages=None)\n            .group_by(Submission.source_id)\n            .subquery()\n        )\n\n        # Query for starred sources, along with their unread\n        # submission counts.\n        starred = (\n            db.session.query(Source, unread_stmt.c.num_unread)\n            .filter_by(pending=False, deleted_at=None)\n            .filter(Source.last_updated.isnot(None))\n            .filter(SourceStar.starred.is_(True))\n            .outerjoin(SourceStar)\n            .options(joinedload(Source.submissions))\n            .options(joinedload(Source.star))\n            .outerjoin(unread_stmt, Source.id == unread_stmt.c.source_id)\n            .order_by(Source.last_updated.desc())\n            .all()\n        )\n\n        # Now, add \"num_unread\" attributes to the source entities.\n        for source, num_unread in starred:\n            source.num_unread = num_unread or 0\n        starred = [source for source, num_unread in starred]\n\n        # Query for sources without stars, along with their unread\n        # submission counts.\n        unstarred = (\n            db.session.query(Source, unread_stmt.c.num_unread)\n            .filter_by(pending=False, deleted_at=None)\n            .filter(Source.last_updated.isnot(None))\n            .filter(~Source.star.has(SourceStar.starred.is_(True)))\n            .options(joinedload(Source.submissions))\n            .options(joinedload(Source.star))\n            .outerjoin(unread_stmt, Source.id == unread_stmt.c.source_id)\n            .order_by(Source.last_updated.desc())\n            .all()\n        )\n\n        # Again, add \"num_unread\" attributes to the source entities.\n        for source, num_unread in unstarred:\n            source.num_unread = num_unread or 0\n        unstarred = [source for source, num_unread in unstarred]\n\n        return render_template(\"index.html\", unstarred=unstarred, starred=starred)\n\n    @view.route(\"/reply\", methods=(\"POST\",))\n    def reply() -> werkzeug.Response:\n        \"\"\"Attempt to send a Reply from a Journalist to a Source. Empty\n        messages are rejected, and an informative error message is flashed\n        on the client. In the case of unexpected errors involving database\n        transactions (potentially caused by racing request threads that\n        modify the same the database object) logging is done in such a way\n        so as not to write potentially sensitive information to disk, and a\n        generic error message is flashed on the client.\n\n        Returns:\n           flask.Response: The user is redirected to the same Source\n               collection view, regardless if the Reply is created\n               successfully.\n        \"\"\"\n        form = ReplyForm()\n        if not form.validate_on_submit():\n            for error in form.message.errors:\n                flash(error, \"error\")\n            return redirect(url_for(\"col.col\", filesystem_id=g.filesystem_id))\n\n        g.source.interaction_count += 1\n        filename = f\"{g.source.interaction_count}-{g.source.journalist_filename}-reply.gpg\"\n        EncryptionManager.get_default().encrypt_journalist_reply(\n            for_source=g.source,\n            reply_in=form.message.data,\n            encrypted_reply_path_out=Path(Storage.get_default().path(g.filesystem_id, filename)),\n        )\n\n        try:\n            reply = Reply(session.get_user(), g.source, filename, Storage.get_default())\n            db.session.add(reply)\n            seen_reply = SeenReply(reply=reply, journalist=session.get_user())\n            db.session.add(seen_reply)\n            db.session.commit()\n            store.async_add_checksum_for_file(reply, Storage.get_default())\n        except Exception as exc:\n            flash(\n                gettext(\"An unexpected error occurred! Please \" \"inform your admin.\"),\n                \"error\",\n            )\n            # We take a cautious approach to logging here because we're dealing\n            # with responses to sources. It's possible the exception message\n            # could contain information we don't want to write to disk.\n            current_app.logger.error(\n                f\"Reply from '{session.get_user().username}' (ID {session.get_uid()}) \"\n                f\"failed: {exc.__class__}!\"\n            )\n        else:\n            flash(\n                Markup(\n                    \"<b>{}</b> {}\".format(\n                        # Translators: Precedes a message confirming the success of an operation.\n                        escape(gettext(\"Success!\")),\n                        escape(\n                            gettext(\"The source will receive your reply \" \"next time they log in.\")\n                        ),\n                    )\n                ),\n                \"success\",\n            )\n\n        return redirect(url_for(\"col.col\", filesystem_id=g.filesystem_id))\n\n    @view.route(\"/bulk\", methods=(\"POST\",))\n    def bulk() -> Union[str, werkzeug.Response]:\n        action = request.form[\"action\"]\n        error_redirect = url_for(\"col.col\", filesystem_id=g.filesystem_id)\n        doc_names_selected = request.form.getlist(\"doc_names_selected\")\n        selected_docs = [doc for doc in g.source.collection if doc.filename in doc_names_selected]\n        if selected_docs == []:\n            if action == \"download\":\n                flash(\n                    Markup(\n                        \"<b>{}</b> {}\".format(\n                            # Translators: Error shown when a user has not selected items to act on.\n                            escape(gettext(\"Nothing Selected\")),\n                            escape(gettext(\"You must select one or more items for download\")),\n                        )\n                    ),\n                    \"error\",\n                )\n            elif action == \"delete\":\n                flash(\n                    Markup(\n                        \"<b>{}</b> {}\".format(\n                            # Translators: Error shown when a user has not selected items to act on.\n                            escape(gettext(\"Nothing Selected\")),\n                            escape(gettext(\"You must select one or more items for deletion\")),\n                        )\n                    ),\n                    \"error\",\n                )\n            else:\n                abort(400)\n\n            return redirect(error_redirect)\n\n        if action == \"download\":\n            source = get_source(g.filesystem_id)\n            return download(\n                source.journalist_filename,\n                selected_docs,\n                on_error_redirect=error_redirect,\n            )\n        elif action == \"delete\":\n            return bulk_delete(g.filesystem_id, selected_docs)\n        else:\n            abort(400)\n\n    @view.route(\"/download_unread/<filesystem_id>\")\n    def download_unread_filesystem_id(filesystem_id: str) -> werkzeug.Response:\n        unseen_submissions = (\n            Submission.query.join(Source)\n            .filter(Source.deleted_at.is_(None), Source.filesystem_id == filesystem_id)\n            .filter(~Submission.seen_files.any(), ~Submission.seen_messages.any())\n            .all()\n        )\n        if len(unseen_submissions) == 0:\n            flash(gettext(\"No unread submissions for this source.\"), \"error\")\n            return redirect(url_for(\"col.col\", filesystem_id=filesystem_id))\n        source = get_source(filesystem_id)\n        return download(source.journalist_filename, unseen_submissions)\n\n    return view",
          "file": "main.py"
        },
        {
          "type": "function",
          "name": "login",
          "code": "def login() -> Union[str, werkzeug.Response]:\n        if request.method == \"POST\":\n            user = validate_user(\n                request.form[\"username\"],\n                request.form[\"password\"],\n                request.form[\"token\"],\n            )\n            if user:\n                current_app.logger.info(\n                    \"'{}' logged in with the two-factor code {}\".format(\n                        request.form[\"username\"], request.form[\"token\"]\n                    )\n                )\n\n                # Update access metadata\n                user.last_access = datetime.now(timezone.utc)\n                db.session.add(user)\n                db.session.commit()\n\n                session[\"uid\"] = user.id\n                session.regenerate()\n                return redirect(url_for(\"main.index\"))\n\n        return render_template(\"login.html\")",
          "file": "main.py"
        },
        {
          "type": "function",
          "name": "logout",
          "code": "def logout() -> werkzeug.Response:\n        session.destroy()\n        return redirect(url_for(\"main.index\"))",
          "file": "main.py"
        },
        {
          "type": "function",
          "name": "index",
          "code": "def index() -> str:\n        # Gather the count of unread submissions for each source\n        # ID. This query will be joined in the queries for starred and\n        # unstarred sources below, and the unread counts added to\n        # their result sets as an extra column.\n        unread_stmt = (\n            db.session.query(Submission.source_id, func.count(\"*\").label(\"num_unread\"))\n            .filter_by(seen_files=None, seen_messages=None)\n            .group_by(Submission.source_id)\n            .subquery()\n        )\n\n        # Query for starred sources, along with their unread\n        # submission counts.\n        starred = (\n            db.session.query(Source, unread_stmt.c.num_unread)\n            .filter_by(pending=False, deleted_at=None)\n            .filter(Source.last_updated.isnot(None))\n            .filter(SourceStar.starred.is_(True))\n            .outerjoin(SourceStar)\n            .options(joinedload(Source.submissions))\n            .options(joinedload(Source.star))\n            .outerjoin(unread_stmt, Source.id == unread_stmt.c.source_id)\n            .order_by(Source.last_updated.desc())\n            .all()\n        )\n\n        # Now, add \"num_unread\" attributes to the source entities.\n        for source, num_unread in starred:\n            source.num_unread = num_unread or 0\n        starred = [source for source, num_unread in starred]\n\n        # Query for sources without stars, along with their unread\n        # submission counts.\n        unstarred = (\n            db.session.query(Source, unread_stmt.c.num_unread)\n            .filter_by(pending=False, deleted_at=None)\n            .filter(Source.last_updated.isnot(None))\n            .filter(~Source.star.has(SourceStar.starred.is_(True)))\n            .options(joinedload(Source.submissions))\n            .options(joinedload(Source.star))\n            .outerjoin(unread_stmt, Source.id == unread_stmt.c.source_id)\n            .order_by(Source.last_updated.desc())\n            .all()\n        )\n\n        # Again, add \"num_unread\" attributes to the source entities.\n        for source, num_unread in unstarred:\n            source.num_unread = num_unread or 0\n        unstarred = [source for source, num_unread in unstarred]\n\n        return render_template(\"index.html\", unstarred=unstarred, starred=starred)",
          "file": "main.py"
        },
        {
          "type": "function",
          "name": "reply",
          "code": "def reply() -> werkzeug.Response:\n        \"\"\"Attempt to send a Reply from a Journalist to a Source. Empty\n        messages are rejected, and an informative error message is flashed\n        on the client. In the case of unexpected errors involving database\n        transactions (potentially caused by racing request threads that\n        modify the same the database object) logging is done in such a way\n        so as not to write potentially sensitive information to disk, and a\n        generic error message is flashed on the client.\n\n        Returns:\n           flask.Response: The user is redirected to the same Source\n               collection view, regardless if the Reply is created\n               successfully.\n        \"\"\"\n        form = ReplyForm()\n        if not form.validate_on_submit():\n            for error in form.message.errors:\n                flash(error, \"error\")\n            return redirect(url_for(\"col.col\", filesystem_id=g.filesystem_id))\n\n        g.source.interaction_count += 1\n        filename = f\"{g.source.interaction_count}-{g.source.journalist_filename}-reply.gpg\"\n        EncryptionManager.get_default().encrypt_journalist_reply(\n            for_source=g.source,\n            reply_in=form.message.data,\n            encrypted_reply_path_out=Path(Storage.get_default().path(g.filesystem_id, filename)),\n        )\n\n        try:\n            reply = Reply(session.get_user(), g.source, filename, Storage.get_default())\n            db.session.add(reply)\n            seen_reply = SeenReply(reply=reply, journalist=session.get_user())\n            db.session.add(seen_reply)\n            db.session.commit()\n            store.async_add_checksum_for_file(reply, Storage.get_default())\n        except Exception as exc:\n            flash(\n                gettext(\"An unexpected error occurred! Please \" \"inform your admin.\"),\n                \"error\",\n            )\n            # We take a cautious approach to logging here because we're dealing\n            # with responses to sources. It's possible the exception message\n            # could contain information we don't want to write to disk.\n            current_app.logger.error(\n                f\"Reply from '{session.get_user().username}' (ID {session.get_uid()}) \"\n                f\"failed: {exc.__class__}!\"\n            )\n        else:\n            flash(\n                Markup(\n                    \"<b>{}</b> {}\".format(\n                        # Translators: Precedes a message confirming the success of an operation.\n                        escape(gettext(\"Success!\")),\n                        escape(\n                            gettext(\"The source will receive your reply \" \"next time they log in.\")\n                        ),\n                    )\n                ),\n                \"success\",\n            )\n\n        return redirect(url_for(\"col.col\", filesystem_id=g.filesystem_id))",
          "file": "main.py"
        },
        {
          "type": "function",
          "name": "bulk",
          "code": "def bulk() -> Union[str, werkzeug.Response]:\n        action = request.form[\"action\"]\n        error_redirect = url_for(\"col.col\", filesystem_id=g.filesystem_id)\n        doc_names_selected = request.form.getlist(\"doc_names_selected\")\n        selected_docs = [doc for doc in g.source.collection if doc.filename in doc_names_selected]\n        if selected_docs == []:\n            if action == \"download\":\n                flash(\n                    Markup(\n                        \"<b>{}</b> {}\".format(\n                            # Translators: Error shown when a user has not selected items to act on.\n                            escape(gettext(\"Nothing Selected\")),\n                            escape(gettext(\"You must select one or more items for download\")),\n                        )\n                    ),\n                    \"error\",\n                )\n            elif action == \"delete\":\n                flash(\n                    Markup(\n                        \"<b>{}</b> {}\".format(\n                            # Translators: Error shown when a user has not selected items to act on.\n                            escape(gettext(\"Nothing Selected\")),\n                            escape(gettext(\"You must select one or more items for deletion\")),\n                        )\n                    ),\n                    \"error\",\n                )\n            else:\n                abort(400)\n\n            return redirect(error_redirect)\n\n        if action == \"download\":\n            source = get_source(g.filesystem_id)\n            return download(\n                source.journalist_filename,\n                selected_docs,\n                on_error_redirect=error_redirect,\n            )\n        elif action == \"delete\":\n            return bulk_delete(g.filesystem_id, selected_docs)\n        else:\n            abort(400)",
          "file": "main.py"
        },
        {
          "type": "function",
          "name": "download_unread_filesystem_id",
          "code": "def download_unread_filesystem_id(filesystem_id: str) -> werkzeug.Response:\n        unseen_submissions = (\n            Submission.query.join(Source)\n            .filter(Source.deleted_at.is_(None), Source.filesystem_id == filesystem_id)\n            .filter(~Submission.seen_files.any(), ~Submission.seen_messages.any())\n            .all()\n        )\n        if len(unseen_submissions) == 0:\n            flash(gettext(\"No unread submissions for this source.\"), \"error\")\n            return redirect(url_for(\"col.col\", filesystem_id=filesystem_id))\n        source = get_source(filesystem_id)\n        return download(source.journalist_filename, unseen_submissions)",
          "file": "main.py"
        }
      ],
      "col.py": [
        {
          "type": "function",
          "name": "make_blueprint",
          "code": "def make_blueprint() -> Blueprint:\n    view = Blueprint(\"col\", __name__)\n\n    @view.route(\"/add_star/<filesystem_id>\", methods=(\"POST\",))\n    def add_star(filesystem_id: str) -> werkzeug.Response:\n        make_star_true(filesystem_id)\n        db.session.commit()\n        return redirect(url_for(\"main.index\"))\n\n    @view.route(\"/remove_star/<filesystem_id>\", methods=(\"POST\",))\n    def remove_star(filesystem_id: str) -> werkzeug.Response:\n        make_star_false(filesystem_id)\n        db.session.commit()\n        return redirect(url_for(\"main.index\"))\n\n    @view.route(\"/<filesystem_id>\")\n    def col(filesystem_id: str) -> str:\n        form = ReplyForm()\n        source = get_source(filesystem_id)\n        has_key = source.public_key is not None\n\n        return render_template(\n            \"col.html\", filesystem_id=filesystem_id, source=source, has_key=has_key, form=form\n        )\n\n    @view.route(\"/delete/<filesystem_id>\", methods=(\"POST\",))\n    def delete_single(filesystem_id: str) -> werkzeug.Response:\n        \"\"\"deleting a single collection from its /col page\"\"\"\n        source = get_source(filesystem_id)\n        try:\n            delete_collection(filesystem_id)\n        except GpgKeyNotFoundError as e:\n            current_app.logger.error(\"error deleting collection: %s\", e)\n            abort(500)\n\n        flash(\n            Markup(\n                \"<b>{}</b> {}\".format(\n                    # Translators: Precedes a message confirming the success of an operation.\n                    escape(gettext(\"Success!\")),\n                    escape(\n                        gettext(\"The account and data for the source {} have been deleted.\").format(\n                            source.journalist_designation\n                        )\n                    ),\n                )\n            ),\n            \"success\",\n        )\n\n        return redirect(url_for(\"main.index\"))\n\n    @view.route(\"/process\", methods=(\"POST\",))\n    def process() -> werkzeug.Response:\n        actions = {\n            \"download-unread\": col_download_unread,\n            \"download-all\": col_download_all,\n            \"star\": col_star,\n            \"un-star\": col_un_star,\n            \"delete\": col_delete,\n            \"delete-data\": col_delete_data,\n        }\n        if \"cols_selected\" not in request.form:\n            flash(\n                Markup(\n                    \"<b>{}</b> {}\".format(\n                        # Translators: Error shown when a user has not selected items to act on.\n                        escape(gettext(\"Nothing Selected\")),\n                        escape(gettext(\"You must select one or more items.\")),\n                    )\n                ),\n                \"error\",\n            )\n            return redirect(url_for(\"main.index\"))\n\n        # getlist is cgi.FieldStorage.getlist\n        cols_selected = request.form.getlist(\"cols_selected\")\n        action = request.form[\"action\"]\n\n        if action not in actions:\n            return abort(500)\n\n        method = actions[action]\n        return method(cols_selected)\n\n    @view.route(\"/<filesystem_id>/<fn>\")\n    def download_single_file(filesystem_id: str, fn: str) -> werkzeug.Response:\n        \"\"\"\n        Marks the file being download (the file being downloaded is either a submission message,\n        submission file attachment, or journalist reply) as seen by the current logged-in user and\n        send the file to a client to be saved or opened.\n        \"\"\"\n        if \"..\" in fn or fn.startswith(\"/\"):\n            abort(404)\n\n        file = Storage.get_default().path(filesystem_id, fn)\n        if not Path(file).is_file():\n            flash(\n                gettext(\n                    \"Your download failed because the file could not be found. An admin can find \"\n                    + \"more information in the system and monitoring logs.\"\n                ),\n                \"error\",\n            )\n            current_app.logger.error(f\"File {file} not found\")\n            return redirect(url_for(\"col.col\", filesystem_id=filesystem_id))\n\n        # mark as seen by the current user\n        try:\n            journalist = session.get_user()\n            if fn.endswith(\"reply.gpg\"):\n                reply = Reply.query.filter(Reply.filename == fn).one()\n                mark_seen([reply], journalist)\n            elif fn.endswith((\"-doc.gz.gpg\", \"doc.zip.gpg\")):\n                submitted_file = Submission.query.filter(Submission.filename == fn).one()\n                mark_seen([submitted_file], journalist)\n            else:\n                message = Submission.query.filter(Submission.filename == fn).one()\n                mark_seen([message], journalist)\n        except NoResultFound as e:\n            current_app.logger.error(f\"Could not mark {fn} as seen: {e}\")\n\n        return send_file(\n            Storage.get_default().path(filesystem_id, fn),\n            mimetype=\"application/pgp-encrypted\",\n        )\n\n    return view",
          "file": "col.py"
        },
        {
          "type": "function",
          "name": "add_star",
          "code": "def add_star(filesystem_id: str) -> werkzeug.Response:\n        make_star_true(filesystem_id)\n        db.session.commit()\n        return redirect(url_for(\"main.index\"))",
          "file": "col.py"
        },
        {
          "type": "function",
          "name": "remove_star",
          "code": "def remove_star(filesystem_id: str) -> werkzeug.Response:\n        make_star_false(filesystem_id)\n        db.session.commit()\n        return redirect(url_for(\"main.index\"))",
          "file": "col.py"
        },
        {
          "type": "function",
          "name": "col",
          "code": "def col(filesystem_id: str) -> str:\n        form = ReplyForm()\n        source = get_source(filesystem_id)\n        has_key = source.public_key is not None\n\n        return render_template(\n            \"col.html\", filesystem_id=filesystem_id, source=source, has_key=has_key, form=form\n        )",
          "file": "col.py"
        },
        {
          "type": "function",
          "name": "delete_single",
          "code": "def delete_single(filesystem_id: str) -> werkzeug.Response:\n        \"\"\"deleting a single collection from its /col page\"\"\"\n        source = get_source(filesystem_id)\n        try:\n            delete_collection(filesystem_id)\n        except GpgKeyNotFoundError as e:\n            current_app.logger.error(\"error deleting collection: %s\", e)\n            abort(500)\n\n        flash(\n            Markup(\n                \"<b>{}</b> {}\".format(\n                    # Translators: Precedes a message confirming the success of an operation.\n                    escape(gettext(\"Success!\")),\n                    escape(\n                        gettext(\"The account and data for the source {} have been deleted.\").format(\n                            source.journalist_designation\n                        )\n                    ),\n                )\n            ),\n            \"success\",\n        )\n\n        return redirect(url_for(\"main.index\"))",
          "file": "col.py"
        },
        {
          "type": "function",
          "name": "process",
          "code": "def process() -> werkzeug.Response:\n        actions = {\n            \"download-unread\": col_download_unread,\n            \"download-all\": col_download_all,\n            \"star\": col_star,\n            \"un-star\": col_un_star,\n            \"delete\": col_delete,\n            \"delete-data\": col_delete_data,\n        }\n        if \"cols_selected\" not in request.form:\n            flash(\n                Markup(\n                    \"<b>{}</b> {}\".format(\n                        # Translators: Error shown when a user has not selected items to act on.\n                        escape(gettext(\"Nothing Selected\")),\n                        escape(gettext(\"You must select one or more items.\")),\n                    )\n                ),\n                \"error\",\n            )\n            return redirect(url_for(\"main.index\"))\n\n        # getlist is cgi.FieldStorage.getlist\n        cols_selected = request.form.getlist(\"cols_selected\")\n        action = request.form[\"action\"]\n\n        if action not in actions:\n            return abort(500)\n\n        method = actions[action]\n        return method(cols_selected)",
          "file": "col.py"
        },
        {
          "type": "function",
          "name": "download_single_file",
          "code": "def download_single_file(filesystem_id: str, fn: str) -> werkzeug.Response:\n        \"\"\"\n        Marks the file being download (the file being downloaded is either a submission message,\n        submission file attachment, or journalist reply) as seen by the current logged-in user and\n        send the file to a client to be saved or opened.\n        \"\"\"\n        if \"..\" in fn or fn.startswith(\"/\"):\n            abort(404)\n\n        file = Storage.get_default().path(filesystem_id, fn)\n        if not Path(file).is_file():\n            flash(\n                gettext(\n                    \"Your download failed because the file could not be found. An admin can find \"\n                    + \"more information in the system and monitoring logs.\"\n                ),\n                \"error\",\n            )\n            current_app.logger.error(f\"File {file} not found\")\n            return redirect(url_for(\"col.col\", filesystem_id=filesystem_id))\n\n        # mark as seen by the current user\n        try:\n            journalist = session.get_user()\n            if fn.endswith(\"reply.gpg\"):\n                reply = Reply.query.filter(Reply.filename == fn).one()\n                mark_seen([reply], journalist)\n            elif fn.endswith((\"-doc.gz.gpg\", \"doc.zip.gpg\")):\n                submitted_file = Submission.query.filter(Submission.filename == fn).one()\n                mark_seen([submitted_file], journalist)\n            else:\n                message = Submission.query.filter(Submission.filename == fn).one()\n                mark_seen([message], journalist)\n        except NoResultFound as e:\n            current_app.logger.error(f\"Could not mark {fn} as seen: {e}\")\n\n        return send_file(\n            Storage.get_default().path(filesystem_id, fn),\n            mimetype=\"application/pgp-encrypted\",\n        )",
          "file": "col.py"
        }
      ]
    },
    "source_app": {
      "utils.py": [
        {
          "type": "function",
          "name": "codename_detected",
          "code": "def codename_detected(message: str, codename: str) -> bool:\n    \"\"\"\n    Check for codenames in incoming messages. including case where user copy/pasted\n    from /generate or the codename widget on the same page\n    \"\"\"\n    message = message.strip()\n\n    return compare_digest(message.strip().encode(\"utf-8\"), codename.encode(\"utf-8\"))",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "flash_msg",
          "code": "def flash_msg(\n    category: str,\n    declarative: \"Optional[str]\",\n    *msg_contents: \"str\",\n) -> None:\n    \"\"\"\n    Render flash message with a (currently) optional declarative heading.\n    \"\"\"\n    contents = Markup(\"<br>\".join([escape(part) for part in msg_contents]))\n\n    msg = render_template(\n        \"flash_message.html\",\n        declarative=declarative,\n        msg_contents=contents,\n    )\n    flash(Markup(msg), category)",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "clear_session_and_redirect_to_logged_out_page",
          "code": "def clear_session_and_redirect_to_logged_out_page(flask_session: SessionMixin) -> werkzeug.Response:\n    msg = render_template(\n        \"flash_message.html\",\n        declarative=gettext(\"Important\"),\n        msg_contents=Markup(\n            gettext(\n                \"You have been logged out due to inactivity or a problem with \"\n                'your session. Click the <img src={icon} alt=\"\" width=\"16\" '\n                'height=\"16\">&nbsp;<b>New Identity</b> button in your Tor '\n                \"Browser's toolbar before moving on. This will clear your Tor \"\n                \"Browser activity data on this device.\"\n            ).format(icon=url_for(\"static\", filename=\"i/torbroom.png\"))\n        ),\n    )\n\n    # Clear the session after we render the message so it's localized\n    flask_session.clear()\n\n    flash(Markup(msg), \"error\")\n    return redirect(url_for(\"main.index\"))",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "normalize_timestamps",
          "code": "def normalize_timestamps(logged_in_source: SourceUser) -> None:\n    \"\"\"\n    Update the timestamps on all of the source's submissions. This\n    minimizes metadata that could be useful to investigators. See\n    #301.\n    \"\"\"\n    source_in_db = logged_in_source.get_db_record()\n    sub_paths = [\n        Storage.get_default().path(logged_in_source.filesystem_id, submission.filename)\n        for submission in source_in_db.submissions\n    ]\n    if len(sub_paths) > 1:\n        args = [\"touch\", \"--no-create\"]\n        args.extend(sub_paths)\n        rc = subprocess.call(args)\n        if rc != 0:\n            current_app.logger.warning(\n                f\"Couldn't normalize submission timestamps (touch exited with {rc})\"\n            )",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "check_url_file",
          "code": "def check_url_file(path: str, regexp: str) -> \"Optional[str]\":\n    \"\"\"\n    Check that a file exists at the path given and contains a single line\n    matching the regexp. Used for checking the source interface address\n    files in /var/lib/securedrop (as the Apache user can't read Tor config)\n    \"\"\"\n    try:\n        f = open(path)\n        contents = f.readline().strip()\n        f.close()\n        if re.match(regexp, contents):\n            return contents\n        else:\n            return None\n    except OSError:\n        return None",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "get_sourcev3_url",
          "code": "def get_sourcev3_url() -> \"Optional[str]\":\n    return check_url_file(\"/var/lib/securedrop/source_v3_url\", r\"^[a-z0-9]{56}\\.onion$\")",
          "file": "utils.py"
        },
        {
          "type": "function",
          "name": "fit_codenames_into_cookie",
          "code": "def fit_codenames_into_cookie(codenames: dict) -> dict:\n    \"\"\"\n    If `codenames` will approach `werkzeug.Response.max_cookie_size` once\n    serialized, incrementally pop off the oldest codename until the remaining\n    (newer) ones will fit.\n    \"\"\"\n\n    serialized = json.dumps(codenames).encode()\n    if len(codenames) > 1 and len(serialized) > 4000:  # werkzeug.Response.max_cookie_size = 4093\n        if current_app:\n            current_app.logger.warn(\n                f\"Popping oldest of {len(codenames)} \"\n                f\"codenames ({len(serialized)} bytes) to \"\n                f\"fit within maximum cookie size\"\n            )\n        del codenames[list(codenames)[0]]  # FIFO\n\n        return fit_codenames_into_cookie(codenames)\n\n    return codenames",
          "file": "utils.py"
        }
      ],
      "api.py": [
        {
          "type": "function",
          "name": "make_blueprint",
          "code": "def make_blueprint(config: SecureDropConfig) -> Blueprint:\n    view = Blueprint(\"api\", __name__)\n\n    @view.route(\"/metadata\")\n    def metadata() -> flask.Response:\n        meta = {\n            \"organization_name\": InstanceConfig.get_default().organization_name,\n            \"allow_document_uploads\": InstanceConfig.get_default().allow_document_uploads,\n            \"gpg_fpr\": config.JOURNALIST_KEY,\n            \"sd_version\": version.__version__,\n            \"server_os\": server_os.get_os_release(),\n            \"supported_languages\": config.SUPPORTED_LOCALES,\n            \"v3_source_url\": get_sourcev3_url(),\n        }\n        resp = make_response(json.dumps(meta))\n        resp.headers[\"Content-Type\"] = \"application/json\"\n        return resp\n\n    return view",
          "file": "api.py"
        },
        {
          "type": "function",
          "name": "metadata",
          "code": "def metadata() -> flask.Response:\n        meta = {\n            \"organization_name\": InstanceConfig.get_default().organization_name,\n            \"allow_document_uploads\": InstanceConfig.get_default().allow_document_uploads,\n            \"gpg_fpr\": config.JOURNALIST_KEY,\n            \"sd_version\": version.__version__,\n            \"server_os\": server_os.get_os_release(),\n            \"supported_languages\": config.SUPPORTED_LOCALES,\n            \"v3_source_url\": get_sourcev3_url(),\n        }\n        resp = make_response(json.dumps(meta))\n        resp.headers[\"Content-Type\"] = \"application/json\"\n        return resp",
          "file": "api.py"
        }
      ],
      "forms.py": [
        {
          "type": "class",
          "name": "LoginForm",
          "code": "class LoginForm(FlaskForm):\n    codename = PasswordField(\n        \"codename\",\n        validators=[\n            InputRequired(message=gettext(\"This field is required.\")),\n            Length(\n                1,\n                PassphraseGenerator.MAX_PASSPHRASE_LENGTH,\n                message=ngettext(\n                    \"Field must be 1 character long.\",\n                    \"Field must be between 1 and {max_codename_len} characters long.\",\n                    PassphraseGenerator.MAX_PASSPHRASE_LENGTH,\n                ).format(max_codename_len=PassphraseGenerator.MAX_PASSPHRASE_LENGTH),\n            ),\n            # Make sure to allow dashes since some words in the wordlist have them\n            Regexp(r\"[\\sA-Za-z0-9-]+$\", message=gettext(\"Invalid input.\")),\n        ],\n    )",
          "file": "forms.py"
        },
        {
          "type": "class",
          "name": "SubmissionForm",
          "code": "class SubmissionForm(FlaskForm):\n    msg = TextAreaField(\n        \"msg\",\n        render_kw={\n            \"placeholder\": gettext(\"Write a message.\"),\n            \"aria-label\": gettext(\"Write a message.\"),\n        },\n    )\n    fh = FileField(\"fh\", render_kw={\"aria-label\": gettext(\"Select a file to upload.\")})\n    antispam = StringField(id=\"text\", name=\"text\")\n\n    def validate_msg(self, field: wtforms.Field) -> None:\n        if len(field.data) > Submission.MAX_MESSAGE_LEN:\n            message = gettext(\"Message text too long.\")\n            if InstanceConfig.get_default().allow_document_uploads:\n                message = \"{} {}\".format(\n                    message,\n                    gettext(\n                        \"Large blocks of text must be uploaded as a file, not copied and pasted.\"\n                    ),\n                )\n            raise ValidationError(message)\n\n    def validate_antispam(self, field: wtforms.Field) -> None:\n        \"\"\"If the antispam field has any contents, abort with a 403\"\"\"\n        if field.data:\n            abort(403)",
          "file": "forms.py"
        },
        {
          "type": "function",
          "name": "validate_msg",
          "code": "def validate_msg(self, field: wtforms.Field) -> None:\n        if len(field.data) > Submission.MAX_MESSAGE_LEN:\n            message = gettext(\"Message text too long.\")\n            if InstanceConfig.get_default().allow_document_uploads:\n                message = \"{} {}\".format(\n                    message,\n                    gettext(\n                        \"Large blocks of text must be uploaded as a file, not copied and pasted.\"\n                    ),\n                )\n            raise ValidationError(message)",
          "file": "forms.py"
        },
        {
          "type": "function",
          "name": "validate_antispam",
          "code": "def validate_antispam(self, field: wtforms.Field) -> None:\n        \"\"\"If the antispam field has any contents, abort with a 403\"\"\"\n        if field.data:\n            abort(403)",
          "file": "forms.py"
        }
      ],
      "decorators.py": [
        {
          "type": "function",
          "name": "login_required",
          "code": "def login_required(f: Callable) -> Callable:\n    @wraps(f)\n    def decorated_function(*args: Any, **kwargs: Any) -> Any:\n        try:\n            logged_in_source = SessionManager.get_logged_in_user(db_session=db.session)\n\n        except (UserSessionExpired, UserHasBeenDeleted):\n            return clear_session_and_redirect_to_logged_out_page(flask_session=session)\n\n        except UserNotLoggedIn:\n            return redirect(url_for(\"main.login\"))\n\n        return f(*args, **kwargs, logged_in_source=logged_in_source)\n\n    return decorated_function",
          "file": "decorators.py"
        },
        {
          "type": "function",
          "name": "decorated_function",
          "code": "def decorated_function(*args: Any, **kwargs: Any) -> Any:\n        try:\n            logged_in_source = SessionManager.get_logged_in_user(db_session=db.session)\n\n        except (UserSessionExpired, UserHasBeenDeleted):\n            return clear_session_and_redirect_to_logged_out_page(flask_session=session)\n\n        except UserNotLoggedIn:\n            return redirect(url_for(\"main.login\"))\n\n        return f(*args, **kwargs, logged_in_source=logged_in_source)",
          "file": "decorators.py"
        },
        {
          "type": "function",
          "name": "ignore_static",
          "code": "def ignore_static(f: Callable) -> Callable:\n    \"\"\"Only executes the wrapped function if we're not loading\n    a static resource.\"\"\"\n\n    @wraps(f)\n    def decorated_function(*args: Any, **kwargs: Any) -> Any:\n        if request.path.startswith(\"/static\"):\n            return None  # don't execute the decorated function\n        return f(*args, **kwargs)\n\n    return decorated_function",
          "file": "decorators.py"
        },
        {
          "type": "function",
          "name": "decorated_function",
          "code": "def decorated_function(*args: Any, **kwargs: Any) -> Any:\n        if request.path.startswith(\"/static\"):\n            return None  # don't execute the decorated function\n        return f(*args, **kwargs)",
          "file": "decorators.py"
        }
      ],
      "info.py": [
        {
          "type": "function",
          "name": "make_blueprint",
          "code": "def make_blueprint(config: SecureDropConfig) -> Blueprint:\n    view = Blueprint(\"info\", __name__)\n\n    @view.route(\"/tor2web-warning\")\n    def tor2web_warning() -> flask.Response:\n        flash_msg(\"error\", None, gettext(\"Your connection is not anonymous right now!\"))\n        return flask.Response(\n            render_template(\"tor2web-warning.html\", source_url=get_sourcev3_url()), 403\n        )\n\n    @view.route(\"/use-tor\")\n    def recommend_tor_browser() -> str:\n        return render_template(\"use-tor-browser.html\")\n\n    @view.route(\"/public-key\")\n    def download_public_key() -> flask.Response:\n        journalist_pubkey = EncryptionManager.get_default().get_journalist_public_key()\n        data = BytesIO(journalist_pubkey.encode(\"utf-8\"))\n        return send_file(\n            data,\n            mimetype=\"application/pgp-keys\",\n            attachment_filename=config.JOURNALIST_KEY + \".asc\",\n            as_attachment=True,\n        )\n\n    @view.route(\"/journalist-key\")\n    def download_journalist_key() -> werkzeug.wrappers.Response:\n        return redirect(url_for(\".download_public_key\"), code=301)\n\n    @view.route(\"/why-public-key\")\n    def why_download_public_key() -> str:\n        return render_template(\"why-public-key.html\")\n\n    return view",
          "file": "info.py"
        },
        {
          "type": "function",
          "name": "tor2web_warning",
          "code": "def tor2web_warning() -> flask.Response:\n        flash_msg(\"error\", None, gettext(\"Your connection is not anonymous right now!\"))\n        return flask.Response(\n            render_template(\"tor2web-warning.html\", source_url=get_sourcev3_url()), 403\n        )",
          "file": "info.py"
        },
        {
          "type": "function",
          "name": "recommend_tor_browser",
          "code": "def recommend_tor_browser() -> str:\n        return render_template(\"use-tor-browser.html\")",
          "file": "info.py"
        },
        {
          "type": "function",
          "name": "download_public_key",
          "code": "def download_public_key() -> flask.Response:\n        journalist_pubkey = EncryptionManager.get_default().get_journalist_public_key()\n        data = BytesIO(journalist_pubkey.encode(\"utf-8\"))\n        return send_file(\n            data,\n            mimetype=\"application/pgp-keys\",\n            attachment_filename=config.JOURNALIST_KEY + \".asc\",\n            as_attachment=True,\n        )",
          "file": "info.py"
        },
        {
          "type": "function",
          "name": "download_journalist_key",
          "code": "def download_journalist_key() -> werkzeug.wrappers.Response:\n        return redirect(url_for(\".download_public_key\"), code=301)",
          "file": "info.py"
        },
        {
          "type": "function",
          "name": "why_download_public_key",
          "code": "def why_download_public_key() -> str:\n        return render_template(\"why-public-key.html\")",
          "file": "info.py"
        }
      ],
      "__init__.py": [
        {
          "type": "function",
          "name": "get_logo_url",
          "code": "def get_logo_url(app: Flask) -> str:\n    if not app.static_folder:\n        raise FileNotFoundError\n\n    custom_logo_filename = \"i/custom_logo.png\"\n    default_logo_filename = \"i/logo.png\"\n    custom_logo_path = Path(app.static_folder) / custom_logo_filename\n    default_logo_path = Path(app.static_folder) / default_logo_filename\n    if custom_logo_path.is_file():\n        return url_for(\"static\", filename=custom_logo_filename)\n    elif default_logo_path.is_file():\n        return url_for(\"static\", filename=default_logo_filename)\n\n    raise FileNotFoundError",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "create_app",
          "code": "def create_app(config: SecureDropConfig) -> Flask:\n    app = Flask(\n        __name__,\n        template_folder=str(config.SOURCE_TEMPLATES_DIR.absolute()),\n        static_folder=config.STATIC_DIR.absolute(),\n    )\n    app.request_class = RequestThatSecuresFileUploads\n    app.config.from_object(config.SOURCE_APP_FLASK_CONFIG_CLS)\n\n    i18n.configure(config, app)\n\n    @app.before_request\n    @ignore_static\n    def setup_i18n() -> None:\n        \"\"\"Store i18n-related values in Flask's special g object\"\"\"\n        i18n.set_locale(config)\n\n    # The default CSRF token expiration is 1 hour. Since large uploads can\n    # take longer than an hour over Tor, we increase the valid window to 24h.\n    app.config[\"WTF_CSRF_TIME_LIMIT\"] = 60 * 60 * 24\n    CSRFProtect(app)\n    app.config[\"SESSION_COOKIE_SAMESITE\"] = \"Strict\"\n\n    app.config[\"SQLALCHEMY_TRACK_MODIFICATIONS\"] = False\n    app.config[\"SQLALCHEMY_DATABASE_URI\"] = config.DATABASE_URI\n    db.init_app(app)\n\n    # Check if the Submission Key is valid; if not, we'll disable the UI\n    app.config[\"SUBMISSION_KEY_VALID\"] = validate_journalist_key()\n    # Check if the server OS is past EOL; if so, we'll disable the UI\n    app.config[\"OS_PAST_EOL\"] = server_os.is_os_past_eol()\n\n    @app.errorhandler(CSRFError)\n    def handle_csrf_error(e: CSRFError) -> werkzeug.Response:\n        return clear_session_and_redirect_to_logged_out_page(flask_session=session)\n\n    app.jinja_env.trim_blocks = True\n    app.jinja_env.lstrip_blocks = True\n    app.jinja_env.globals[\"version\"] = version.__version__\n    # Exported to source templates for being included in instructions\n    app.jinja_env.globals[\"submission_key_fpr\"] = config.JOURNALIST_KEY\n    app.jinja_env.filters[\"rel_datetime_format\"] = template_filters.rel_datetime_format\n    app.jinja_env.filters[\"nl2br\"] = template_filters.nl2br\n    app.jinja_env.filters[\"filesizeformat\"] = template_filters.filesizeformat\n    app.jinja_env.filters[\"html_datetime_format\"] = template_filters.html_datetime_format\n    app.jinja_env.add_extension(\"jinja2.ext.do\")\n\n    for module in [main, info, api]:\n        app.register_blueprint(module.make_blueprint(config))  # type: ignore\n\n    # before_request hooks are executed in order of declaration, so set up g object\n    # before the potential tor2web 403 response.\n    @app.before_request\n    @ignore_static\n    def setup_g() -> Optional[werkzeug.Response]:\n        if InstanceConfig.get_default(refresh=True).organization_name:\n            g.organization_name = (  # pylint: disable=assigning-non-slot\n                InstanceConfig.get_default().organization_name\n            )\n        else:\n            g.organization_name = gettext(\"SecureDrop\")  # pylint: disable=assigning-non-slot\n\n        try:\n            g.logo = get_logo_url(app)  # pylint: disable=assigning-non-slot\n        except FileNotFoundError:\n            app.logger.error(\"Site logo not found.\")\n\n        return None\n\n    @app.before_request\n    @ignore_static\n    def check_tor2web() -> Optional[werkzeug.Response]:\n        # TODO: expand header checking logic to catch modern tor2web proxies\n        if \"X-tor2web\" in request.headers and request.path != url_for(\"info.tor2web_warning\"):\n            return redirect(url_for(\"info.tor2web_warning\"))\n        return None\n\n    @app.before_request\n    @ignore_static\n    def check_offline() -> Optional[werkzeug.Response]:\n        if not app.config[\"SUBMISSION_KEY_VALID\"] or app.config[\"OS_PAST_EOL\"]:\n            session.clear()\n            g.show_offline_message = True\n            return make_response(render_template(\"offline.html\"), 503)\n        return None\n\n    @app.errorhandler(404)\n    def page_not_found(error: werkzeug.exceptions.HTTPException) -> Tuple[str, int]:\n        return render_template(\"notfound.html\"), 404\n\n    @app.errorhandler(500)\n    def internal_error(error: werkzeug.exceptions.HTTPException) -> Tuple[str, int]:\n        return render_template(\"error.html\"), 500\n\n    # Obscure the creation time of source private keys by touching them all\n    # on startup.\n    private_keys = config.GPG_KEY_DIR / \"private-keys-v1.d\"\n    # The folder may not exist yet in some dev/testing setups,\n    # and if it doesn't exist there's no mtime to obscure.\n    if private_keys.is_dir():\n        now = time.time()\n        for entry in os.scandir(private_keys):\n            if not entry.is_file() or not entry.name.endswith(\".key\"):\n                continue\n            os.utime(entry.path, times=(now, now))\n            # So the ctime is also updated\n            os.chmod(entry.path, entry.stat().st_mode)\n\n    return app",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "setup_i18n",
          "code": "def setup_i18n() -> None:\n        \"\"\"Store i18n-related values in Flask's special g object\"\"\"\n        i18n.set_locale(config)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "handle_csrf_error",
          "code": "def handle_csrf_error(e: CSRFError) -> werkzeug.Response:\n        return clear_session_and_redirect_to_logged_out_page(flask_session=session)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "setup_g",
          "code": "def setup_g() -> Optional[werkzeug.Response]:\n        if InstanceConfig.get_default(refresh=True).organization_name:\n            g.organization_name = (  # pylint: disable=assigning-non-slot\n                InstanceConfig.get_default().organization_name\n            )\n        else:\n            g.organization_name = gettext(\"SecureDrop\")  # pylint: disable=assigning-non-slot\n\n        try:\n            g.logo = get_logo_url(app)  # pylint: disable=assigning-non-slot\n        except FileNotFoundError:\n            app.logger.error(\"Site logo not found.\")\n\n        return None",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "check_tor2web",
          "code": "def check_tor2web() -> Optional[werkzeug.Response]:\n        # TODO: expand header checking logic to catch modern tor2web proxies\n        if \"X-tor2web\" in request.headers and request.path != url_for(\"info.tor2web_warning\"):\n            return redirect(url_for(\"info.tor2web_warning\"))\n        return None",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "check_offline",
          "code": "def check_offline() -> Optional[werkzeug.Response]:\n        if not app.config[\"SUBMISSION_KEY_VALID\"] or app.config[\"OS_PAST_EOL\"]:\n            session.clear()\n            g.show_offline_message = True\n            return make_response(render_template(\"offline.html\"), 503)\n        return None",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "page_not_found",
          "code": "def page_not_found(error: werkzeug.exceptions.HTTPException) -> Tuple[str, int]:\n        return render_template(\"notfound.html\"), 404",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "internal_error",
          "code": "def internal_error(error: werkzeug.exceptions.HTTPException) -> Tuple[str, int]:\n        return render_template(\"error.html\"), 500",
          "file": "__init__.py"
        }
      ],
      "main.py": [
        {
          "type": "function",
          "name": "make_blueprint",
          "code": "def make_blueprint(config: SecureDropConfig) -> Blueprint:\n    view = Blueprint(\"main\", __name__)\n\n    @view.route(\"/\")\n    def index() -> str:\n        return render_template(\"index.html\")\n\n    @view.route(\"/generate\", methods=(\"POST\", \"GET\"))\n    def generate() -> Union[str, werkzeug.Response]:\n        if request.method == \"POST\":\n            # Try to detect Tor2Web usage by looking to see if tor2web_check got mangled\n            tor2web_check = request.form.get(\"tor2web_check\")\n            if tor2web_check is None:\n                # Missing form field\n                abort(403)\n            elif tor2web_check != 'href=\"fake.onion\"':\n                return redirect(url_for(\"info.tor2web_warning\"))\n\n        if SessionManager.is_user_logged_in(db_session=db.session):\n            flash_msg(\n                \"notification\",\n                None,\n                gettext(\n                    \"You were redirected because you are already logged in. \"\n                    \"If you want to create a new account, you should log out first.\"\n                ),\n            )\n            return redirect(url_for(\".lookup\"))\n        codename = PassphraseGenerator.get_default().generate_passphrase(\n            preferred_language=g.localeinfo.language\n        )\n\n        # Generate a unique id for each browser tab and associate the codename with this id.\n        # This will allow retrieval of the codename displayed in the tab from which the source has\n        # clicked to proceed to /generate (ref. issue #4458)\n        tab_id = urlsafe_b64encode(os.urandom(64)).decode()\n        codenames = session.get(\"codenames\", {})\n        codenames[tab_id] = codename\n        session[\"codenames\"] = fit_codenames_into_cookie(codenames)\n        session[\"codenames_expire\"] = datetime.now(timezone.utc) + timedelta(\n            minutes=config.SESSION_EXPIRATION_MINUTES\n        )\n        return render_template(\"generate.html\", codename=codename, tab_id=tab_id)\n\n    @view.route(\"/create\", methods=[\"POST\"])\n    def create() -> werkzeug.Response:\n        if SessionManager.is_user_logged_in(db_session=db.session):\n            flash_msg(\n                \"notification\",\n                None,\n                gettext(\n                    \"You are already logged in. Please verify your codename as it \"\n                    \"may differ from the one displayed on the previous page.\"\n                ),\n            )\n        else:\n            # Ensure the codenames have not expired\n            date_codenames_expire = session.get(\"codenames_expire\")\n            if not date_codenames_expire or datetime.now(timezone.utc) >= date_codenames_expire:\n                return clear_session_and_redirect_to_logged_out_page(flask_session=session)\n\n            tab_id = request.form[\"tab_id\"]\n            codename = session[\"codenames\"][tab_id]\n            del session[\"codenames\"]\n\n            try:\n                current_app.logger.info(\"Creating new source user...\")\n                create_source_user(\n                    db_session=db.session,\n                    source_passphrase=codename,\n                    source_app_storage=Storage.get_default(),\n                )\n            except (SourcePassphraseCollisionError, SourceDesignationCollisionError) as e:\n                current_app.logger.error(f\"Could not create a source: {e}\")\n                flash_msg(\n                    \"error\",\n                    None,\n                    gettext(\n                        \"There was a temporary problem creating your account. Please try again.\"\n                    ),\n                )\n                return redirect(url_for(\".index\"))\n\n            # All done - source user was successfully created\n            current_app.logger.info(\"New source user created\")\n            session[\"new_user_codename\"] = codename\n            SessionManager.log_user_in(\n                db_session=db.session, supplied_passphrase=DicewarePassphrase(codename)\n            )\n\n        return redirect(url_for(\".lookup\"))\n\n    @view.route(\"/lookup\", methods=(\"GET\",))\n    @login_required\n    def lookup(logged_in_source: SourceUser) -> str:\n        replies = []\n        logged_in_source_in_db = logged_in_source.get_db_record()\n        source_inbox = Reply.query.filter_by(\n            source_id=logged_in_source_in_db.id, deleted_by_source=False\n        ).all()\n\n        first_submission = logged_in_source_in_db.interaction_count == 0\n\n        if first_submission:\n            min_message_length = InstanceConfig.get_default().initial_message_min_len\n        else:\n            min_message_length = 0\n\n        for reply in source_inbox:\n            reply_path = Storage.get_default().path(\n                logged_in_source.filesystem_id,\n                reply.filename,\n            )\n            try:\n                with open(reply_path, \"rb\") as f:\n                    contents = f.read()\n                decrypted_reply = EncryptionManager.get_default().decrypt_journalist_reply(\n                    for_source_user=logged_in_source,\n                    ciphertext_in=contents,\n                )\n                reply.decrypted = decrypted_reply\n            except UnicodeDecodeError:\n                current_app.logger.error(f\"Could not decode reply {reply.filename}\")\n            except FileNotFoundError:\n                current_app.logger.error(f\"Reply file missing: {reply.filename}\")\n            except (GpgDecryptError, RedwoodError) as e:\n                current_app.logger.error(f\"Could not decrypt reply {reply.filename}: {str(e)}\")\n            else:\n                reply.date = datetime.utcfromtimestamp(os.stat(reply_path).st_mtime)\n                replies.append(reply)\n\n        # Sort the replies by date\n        replies.sort(key=operator.attrgetter(\"date\"), reverse=True)\n\n        return render_template(\n            \"lookup.html\",\n            is_user_logged_in=True,\n            allow_document_uploads=InstanceConfig.get_default().allow_document_uploads,\n            replies=replies,\n            min_len=min_message_length,\n            new_user_codename=session.get(\"new_user_codename\", None),\n            form=SubmissionForm(),\n        )\n\n    @view.route(\"/submit\", methods=(\"POST\",))\n    @login_required\n    def submit(logged_in_source: SourceUser) -> werkzeug.Response:\n        allow_document_uploads = InstanceConfig.get_default().allow_document_uploads\n        form = SubmissionForm()\n        if not form.validate():\n            for errors in form.errors.values():\n                for error in errors:\n                    flash_msg(\"error\", None, error)\n            return redirect(url_for(\"main.lookup\"))\n\n        msg = request.form[\"msg\"]\n        fh = None\n        if allow_document_uploads and \"fh\" in request.files:\n            fh = request.files[\"fh\"]\n\n        # Don't submit anything if it was an \"empty\" submission. #878\n        if not (msg or fh):\n            if allow_document_uploads:\n                html_contents = gettext(\"You must enter a message or choose a file to submit.\")\n            else:\n                html_contents = gettext(\"You must enter a message.\")\n\n            flash_msg(\"error\", None, html_contents)\n            return redirect(url_for(\"main.lookup\"))\n\n        fnames = []\n        logged_in_source_in_db = logged_in_source.get_db_record()\n        first_submission = logged_in_source_in_db.interaction_count == 0\n\n        if first_submission:\n            min_len = InstanceConfig.get_default().initial_message_min_len\n            if (min_len > 0) and (msg and not fh) and (len(msg) < min_len):\n                flash_msg(\n                    \"error\",\n                    None,\n                    gettext(\"Your first message must be at least {} characters long.\").format(\n                        min_len\n                    ),\n                )\n                return redirect(url_for(\"main.lookup\"))\n\n            # if the new_user_codename key is not present in the session, this is\n            # not a first session\n            new_codename = session.get(\"new_user_codename\", None)\n\n            codenames_rejected = InstanceConfig.get_default().reject_message_with_codename\n            if new_codename is not None:\n                if codenames_rejected and codename_detected(msg, new_codename):\n                    flash_msg(\n                        \"error\",\n                        None,\n                        gettext(\"Please do not submit your codename!\"),\n                        gettext(\n                            \"Keep your codename secret, and use it to log in later to \"\n                            \"check for replies.\"\n                        ),\n                    )\n                    return redirect(url_for(\"main.lookup\"))\n\n        if not os.path.exists(Storage.get_default().path(logged_in_source.filesystem_id)):\n            current_app.logger.debug(\n                f\"Store directory not found for source \"\n                f\"'{logged_in_source_in_db.journalist_designation}', creating one.\"\n            )\n            os.mkdir(Storage.get_default().path(logged_in_source.filesystem_id))\n\n        if msg:\n            logged_in_source_in_db.interaction_count += 1\n            fnames.append(\n                Storage.get_default().save_message_submission(\n                    logged_in_source_in_db.filesystem_id,\n                    logged_in_source_in_db.interaction_count,\n                    logged_in_source_in_db.journalist_filename,\n                    msg,\n                )\n            )\n        if fh:\n            logged_in_source_in_db.interaction_count += 1\n            fnames.append(\n                Storage.get_default().save_file_submission(\n                    logged_in_source_in_db.filesystem_id,\n                    logged_in_source_in_db.interaction_count,\n                    logged_in_source_in_db.journalist_filename,\n                    fh.filename,\n                    fh.stream,\n                )\n            )\n\n        if first_submission or msg or fh:\n            if first_submission:\n                html_contents = gettext(\n                    \"Thank you for sending this information to us. Please \"\n                    \"check back later for replies.\"\n                )\n            elif msg and not fh:\n                html_contents = gettext(\"Thanks! We received your message.\")\n            elif fh and not msg:\n                html_contents = gettext(\"Thanks! We received your file.\")\n            else:\n                html_contents = gettext(\"Thanks! We received your file and message.\")\n\n            flash_msg(\"success\", gettext(\"Success!\"), html_contents)\n\n        new_submissions = []\n        for fname in fnames:\n            submission = Submission(logged_in_source_in_db, fname, Storage.get_default())\n            db.session.add(submission)\n            new_submissions.append(submission)\n\n        logged_in_source_in_db.pending = False\n        logged_in_source_in_db.last_updated = datetime.now(timezone.utc)\n        db.session.commit()\n\n        for sub in new_submissions:\n            store.async_add_checksum_for_file(sub, Storage.get_default())\n\n        normalize_timestamps(logged_in_source)\n\n        return redirect(url_for(\"main.lookup\"))\n\n    @view.route(\"/delete\", methods=(\"POST\",))\n    @login_required\n    def delete(logged_in_source: SourceUser) -> werkzeug.Response:\n        \"\"\"This deletes the reply from the source's inbox, but preserves\n        the history for journalists such that they can view conversation\n        history.\n        \"\"\"\n\n        query = Reply.query.filter_by(\n            filename=request.form[\"reply_filename\"], source_id=logged_in_source.db_record_id\n        )\n        reply = get_one_or_else(query, current_app.logger, abort)\n        reply.deleted_by_source = True\n        db.session.add(reply)\n        db.session.commit()\n\n        flash_msg(\"success\", gettext(\"Success!\"), gettext(\"Reply deleted\"))\n        return redirect(url_for(\".lookup\"))\n\n    @view.route(\"/delete-all\", methods=(\"POST\",))\n    @login_required\n    def batch_delete(logged_in_source: SourceUser) -> werkzeug.Response:\n        replies = (\n            Reply.query.filter(Reply.source_id == logged_in_source.db_record_id)\n            .filter(Reply.deleted_by_source == False)\n            .all()\n        )\n        if len(replies) == 0:\n            current_app.logger.error(\"Found no replies when at least one was \" \"expected\")\n            return redirect(url_for(\".lookup\"))\n\n        for reply in replies:\n            reply.deleted_by_source = True\n            db.session.add(reply)\n        db.session.commit()\n\n        flash_msg(\"success\", gettext(\"Success!\"), gettext(\"All replies have been deleted\"))\n        return redirect(url_for(\".lookup\"))\n\n    @view.route(\"/login\", methods=(\"GET\", \"POST\"))\n    def login() -> Union[str, werkzeug.Response]:\n        form = LoginForm()\n        if not form.validate_on_submit():\n            return render_template(\"login.html\", form=form)\n        try:\n            source_user = SessionManager.log_user_in(\n                db_session=db.session,\n                supplied_passphrase=DicewarePassphrase(request.form[\"codename\"].strip()),\n            )\n        except InvalidPassphraseError:\n            current_app.logger.info(\"Login failed for invalid codename\")\n            flash_msg(\"error\", None, gettext(\"Sorry, that is not a recognized codename.\"))\n            return render_template(\"login.html\", form=form)\n        # Success: a valid passphrase was supplied and the source was logged-in\n        source = source_user.get_db_record()\n        if source.fingerprint is None:\n            # This legacy source didn't have a PGP keypair generated yet,\n            # do it now.\n            public_key, secret_key, fingerprint = redwood.generate_source_key_pair(\n                source_user.gpg_secret, source_user.filesystem_id\n            )\n            source.pgp_public_key = public_key\n            source.pgp_secret_key = secret_key\n            source.pgp_fingerprint = fingerprint\n            db.session.add(source)\n            db.session.commit()\n        elif source.pgp_secret_key is None:\n            # Need to migrate the secret key out of GPG\n            encryption_mgr = EncryptionManager.get_default()\n            try:\n                secret_key = encryption_mgr.get_source_secret_key_from_gpg(\n                    source.fingerprint, source_user.gpg_secret\n                )\n            except GpgKeyNotFoundError:\n                # Don't fail here, but it's likely decryption of journalist\n                # messages will fail.\n                secret_key = None\n            if secret_key:\n                source.pgp_secret_key = secret_key\n                db.session.add(source)\n                db.session.commit()\n                # Let's optimistically delete the GPG key from the keyring\n                # if *everything* has been migrated. If this fails it's OK,\n                # since we still try to delete from the keyring on source\n                # deletion too.\n                if source.pgp_fingerprint and source.pgp_public_key:\n                    try:\n                        encryption_mgr.delete_source_key_pair(source.filesystem_id)\n                    except:  # noqa: E722\n                        pass\n\n        return redirect(url_for(\".lookup\", from_login=\"1\"))\n\n    @view.route(\"/logout\", methods=(\"POST\",))\n    @login_required\n    def logout(logged_in_source: SourceUser) -> Union[str, werkzeug.Response]:\n        \"\"\"\n        If a user is logged in, show them a logout page that prompts them to\n        click the New Identity button in Tor Browser to complete their session.\n        Otherwise redirect to the main Source Interface page.\n        \"\"\"\n        if SessionManager.is_user_logged_in(db_session=db.session):\n            SessionManager.log_user_out()\n\n            # Clear the session after we render the message so it's localized\n            # If a user specified a locale, save it and restore it\n            session.clear()\n            session[\"locale\"] = g.localeinfo.id\n\n            return render_template(\"logout.html\")\n        else:\n            return redirect(url_for(\".index\"))\n\n    @view.route(\"/robots.txt\")\n    def robots_txt() -> werkzeug.Response:\n        \"\"\"Tell robots we don't want them\"\"\"\n        resp = make_response(\"User-agent: *\\nDisallow: /\")\n        resp.headers[\"content-type\"] = \"text/plain\"\n        return resp\n\n    return view",
          "file": "main.py"
        },
        {
          "type": "function",
          "name": "index",
          "code": "def index() -> str:\n        return render_template(\"index.html\")",
          "file": "main.py"
        },
        {
          "type": "function",
          "name": "generate",
          "code": "def generate() -> Union[str, werkzeug.Response]:\n        if request.method == \"POST\":\n            # Try to detect Tor2Web usage by looking to see if tor2web_check got mangled\n            tor2web_check = request.form.get(\"tor2web_check\")\n            if tor2web_check is None:\n                # Missing form field\n                abort(403)\n            elif tor2web_check != 'href=\"fake.onion\"':\n                return redirect(url_for(\"info.tor2web_warning\"))\n\n        if SessionManager.is_user_logged_in(db_session=db.session):\n            flash_msg(\n                \"notification\",\n                None,\n                gettext(\n                    \"You were redirected because you are already logged in. \"\n                    \"If you want to create a new account, you should log out first.\"\n                ),\n            )\n            return redirect(url_for(\".lookup\"))\n        codename = PassphraseGenerator.get_default().generate_passphrase(\n            preferred_language=g.localeinfo.language\n        )\n\n        # Generate a unique id for each browser tab and associate the codename with this id.\n        # This will allow retrieval of the codename displayed in the tab from which the source has\n        # clicked to proceed to /generate (ref. issue #4458)\n        tab_id = urlsafe_b64encode(os.urandom(64)).decode()\n        codenames = session.get(\"codenames\", {})\n        codenames[tab_id] = codename\n        session[\"codenames\"] = fit_codenames_into_cookie(codenames)\n        session[\"codenames_expire\"] = datetime.now(timezone.utc) + timedelta(\n            minutes=config.SESSION_EXPIRATION_MINUTES\n        )\n        return render_template(\"generate.html\", codename=codename, tab_id=tab_id)",
          "file": "main.py"
        },
        {
          "type": "function",
          "name": "create",
          "code": "def create() -> werkzeug.Response:\n        if SessionManager.is_user_logged_in(db_session=db.session):\n            flash_msg(\n                \"notification\",\n                None,\n                gettext(\n                    \"You are already logged in. Please verify your codename as it \"\n                    \"may differ from the one displayed on the previous page.\"\n                ),\n            )\n        else:\n            # Ensure the codenames have not expired\n            date_codenames_expire = session.get(\"codenames_expire\")\n            if not date_codenames_expire or datetime.now(timezone.utc) >= date_codenames_expire:\n                return clear_session_and_redirect_to_logged_out_page(flask_session=session)\n\n            tab_id = request.form[\"tab_id\"]\n            codename = session[\"codenames\"][tab_id]\n            del session[\"codenames\"]\n\n            try:\n                current_app.logger.info(\"Creating new source user...\")\n                create_source_user(\n                    db_session=db.session,\n                    source_passphrase=codename,\n                    source_app_storage=Storage.get_default(),\n                )\n            except (SourcePassphraseCollisionError, SourceDesignationCollisionError) as e:\n                current_app.logger.error(f\"Could not create a source: {e}\")\n                flash_msg(\n                    \"error\",\n                    None,\n                    gettext(\n                        \"There was a temporary problem creating your account. Please try again.\"\n                    ),\n                )\n                return redirect(url_for(\".index\"))\n\n            # All done - source user was successfully created\n            current_app.logger.info(\"New source user created\")\n            session[\"new_user_codename\"] = codename\n            SessionManager.log_user_in(\n                db_session=db.session, supplied_passphrase=DicewarePassphrase(codename)\n            )\n\n        return redirect(url_for(\".lookup\"))",
          "file": "main.py"
        },
        {
          "type": "function",
          "name": "lookup",
          "code": "def lookup(logged_in_source: SourceUser) -> str:\n        replies = []\n        logged_in_source_in_db = logged_in_source.get_db_record()\n        source_inbox = Reply.query.filter_by(\n            source_id=logged_in_source_in_db.id, deleted_by_source=False\n        ).all()\n\n        first_submission = logged_in_source_in_db.interaction_count == 0\n\n        if first_submission:\n            min_message_length = InstanceConfig.get_default().initial_message_min_len\n        else:\n            min_message_length = 0\n\n        for reply in source_inbox:\n            reply_path = Storage.get_default().path(\n                logged_in_source.filesystem_id,\n                reply.filename,\n            )\n            try:\n                with open(reply_path, \"rb\") as f:\n                    contents = f.read()\n                decrypted_reply = EncryptionManager.get_default().decrypt_journalist_reply(\n                    for_source_user=logged_in_source,\n                    ciphertext_in=contents,\n                )\n                reply.decrypted = decrypted_reply\n            except UnicodeDecodeError:\n                current_app.logger.error(f\"Could not decode reply {reply.filename}\")\n            except FileNotFoundError:\n                current_app.logger.error(f\"Reply file missing: {reply.filename}\")\n            except (GpgDecryptError, RedwoodError) as e:\n                current_app.logger.error(f\"Could not decrypt reply {reply.filename}: {str(e)}\")\n            else:\n                reply.date = datetime.utcfromtimestamp(os.stat(reply_path).st_mtime)\n                replies.append(reply)\n\n        # Sort the replies by date\n        replies.sort(key=operator.attrgetter(\"date\"), reverse=True)\n\n        return render_template(\n            \"lookup.html\",\n            is_user_logged_in=True,\n            allow_document_uploads=InstanceConfig.get_default().allow_document_uploads,\n            replies=replies,\n            min_len=min_message_length,\n            new_user_codename=session.get(\"new_user_codename\", None),\n            form=SubmissionForm(),\n        )",
          "file": "main.py"
        },
        {
          "type": "function",
          "name": "submit",
          "code": "def submit(logged_in_source: SourceUser) -> werkzeug.Response:\n        allow_document_uploads = InstanceConfig.get_default().allow_document_uploads\n        form = SubmissionForm()\n        if not form.validate():\n            for errors in form.errors.values():\n                for error in errors:\n                    flash_msg(\"error\", None, error)\n            return redirect(url_for(\"main.lookup\"))\n\n        msg = request.form[\"msg\"]\n        fh = None\n        if allow_document_uploads and \"fh\" in request.files:\n            fh = request.files[\"fh\"]\n\n        # Don't submit anything if it was an \"empty\" submission. #878\n        if not (msg or fh):\n            if allow_document_uploads:\n                html_contents = gettext(\"You must enter a message or choose a file to submit.\")\n            else:\n                html_contents = gettext(\"You must enter a message.\")\n\n            flash_msg(\"error\", None, html_contents)\n            return redirect(url_for(\"main.lookup\"))\n\n        fnames = []\n        logged_in_source_in_db = logged_in_source.get_db_record()\n        first_submission = logged_in_source_in_db.interaction_count == 0\n\n        if first_submission:\n            min_len = InstanceConfig.get_default().initial_message_min_len\n            if (min_len > 0) and (msg and not fh) and (len(msg) < min_len):\n                flash_msg(\n                    \"error\",\n                    None,\n                    gettext(\"Your first message must be at least {} characters long.\").format(\n                        min_len\n                    ),\n                )\n                return redirect(url_for(\"main.lookup\"))\n\n            # if the new_user_codename key is not present in the session, this is\n            # not a first session\n            new_codename = session.get(\"new_user_codename\", None)\n\n            codenames_rejected = InstanceConfig.get_default().reject_message_with_codename\n            if new_codename is not None:\n                if codenames_rejected and codename_detected(msg, new_codename):\n                    flash_msg(\n                        \"error\",\n                        None,\n                        gettext(\"Please do not submit your codename!\"),\n                        gettext(\n                            \"Keep your codename secret, and use it to log in later to \"\n                            \"check for replies.\"\n                        ),\n                    )\n                    return redirect(url_for(\"main.lookup\"))\n\n        if not os.path.exists(Storage.get_default().path(logged_in_source.filesystem_id)):\n            current_app.logger.debug(\n                f\"Store directory not found for source \"\n                f\"'{logged_in_source_in_db.journalist_designation}', creating one.\"\n            )\n            os.mkdir(Storage.get_default().path(logged_in_source.filesystem_id))\n\n        if msg:\n            logged_in_source_in_db.interaction_count += 1\n            fnames.append(\n                Storage.get_default().save_message_submission(\n                    logged_in_source_in_db.filesystem_id,\n                    logged_in_source_in_db.interaction_count,\n                    logged_in_source_in_db.journalist_filename,\n                    msg,\n                )\n            )\n        if fh:\n            logged_in_source_in_db.interaction_count += 1\n            fnames.append(\n                Storage.get_default().save_file_submission(\n                    logged_in_source_in_db.filesystem_id,\n                    logged_in_source_in_db.interaction_count,\n                    logged_in_source_in_db.journalist_filename,\n                    fh.filename,\n                    fh.stream,\n                )\n            )\n\n        if first_submission or msg or fh:\n            if first_submission:\n                html_contents = gettext(\n                    \"Thank you for sending this information to us. Please \"\n                    \"check back later for replies.\"\n                )\n            elif msg and not fh:\n                html_contents = gettext(\"Thanks! We received your message.\")\n            elif fh and not msg:\n                html_contents = gettext(\"Thanks! We received your file.\")\n            else:\n                html_contents = gettext(\"Thanks! We received your file and message.\")\n\n            flash_msg(\"success\", gettext(\"Success!\"), html_contents)\n\n        new_submissions = []\n        for fname in fnames:\n            submission = Submission(logged_in_source_in_db, fname, Storage.get_default())\n            db.session.add(submission)\n            new_submissions.append(submission)\n\n        logged_in_source_in_db.pending = False\n        logged_in_source_in_db.last_updated = datetime.now(timezone.utc)\n        db.session.commit()\n\n        for sub in new_submissions:\n            store.async_add_checksum_for_file(sub, Storage.get_default())\n\n        normalize_timestamps(logged_in_source)\n\n        return redirect(url_for(\"main.lookup\"))",
          "file": "main.py"
        },
        {
          "type": "function",
          "name": "delete",
          "code": "def delete(logged_in_source: SourceUser) -> werkzeug.Response:\n        \"\"\"This deletes the reply from the source's inbox, but preserves\n        the history for journalists such that they can view conversation\n        history.\n        \"\"\"\n\n        query = Reply.query.filter_by(\n            filename=request.form[\"reply_filename\"], source_id=logged_in_source.db_record_id\n        )\n        reply = get_one_or_else(query, current_app.logger, abort)\n        reply.deleted_by_source = True\n        db.session.add(reply)\n        db.session.commit()\n\n        flash_msg(\"success\", gettext(\"Success!\"), gettext(\"Reply deleted\"))\n        return redirect(url_for(\".lookup\"))",
          "file": "main.py"
        },
        {
          "type": "function",
          "name": "batch_delete",
          "code": "def batch_delete(logged_in_source: SourceUser) -> werkzeug.Response:\n        replies = (\n            Reply.query.filter(Reply.source_id == logged_in_source.db_record_id)\n            .filter(Reply.deleted_by_source == False)\n            .all()\n        )\n        if len(replies) == 0:\n            current_app.logger.error(\"Found no replies when at least one was \" \"expected\")\n            return redirect(url_for(\".lookup\"))\n\n        for reply in replies:\n            reply.deleted_by_source = True\n            db.session.add(reply)\n        db.session.commit()\n\n        flash_msg(\"success\", gettext(\"Success!\"), gettext(\"All replies have been deleted\"))\n        return redirect(url_for(\".lookup\"))",
          "file": "main.py"
        },
        {
          "type": "function",
          "name": "login",
          "code": "def login() -> Union[str, werkzeug.Response]:\n        form = LoginForm()\n        if not form.validate_on_submit():\n            return render_template(\"login.html\", form=form)\n        try:\n            source_user = SessionManager.log_user_in(\n                db_session=db.session,\n                supplied_passphrase=DicewarePassphrase(request.form[\"codename\"].strip()),\n            )\n        except InvalidPassphraseError:\n            current_app.logger.info(\"Login failed for invalid codename\")\n            flash_msg(\"error\", None, gettext(\"Sorry, that is not a recognized codename.\"))\n            return render_template(\"login.html\", form=form)\n        # Success: a valid passphrase was supplied and the source was logged-in\n        source = source_user.get_db_record()\n        if source.fingerprint is None:\n            # This legacy source didn't have a PGP keypair generated yet,\n            # do it now.\n            public_key, secret_key, fingerprint = redwood.generate_source_key_pair(\n                source_user.gpg_secret, source_user.filesystem_id\n            )\n            source.pgp_public_key = public_key\n            source.pgp_secret_key = secret_key\n            source.pgp_fingerprint = fingerprint\n            db.session.add(source)\n            db.session.commit()\n        elif source.pgp_secret_key is None:\n            # Need to migrate the secret key out of GPG\n            encryption_mgr = EncryptionManager.get_default()\n            try:\n                secret_key = encryption_mgr.get_source_secret_key_from_gpg(\n                    source.fingerprint, source_user.gpg_secret\n                )\n            except GpgKeyNotFoundError:\n                # Don't fail here, but it's likely decryption of journalist\n                # messages will fail.\n                secret_key = None\n            if secret_key:\n                source.pgp_secret_key = secret_key\n                db.session.add(source)\n                db.session.commit()\n                # Let's optimistically delete the GPG key from the keyring\n                # if *everything* has been migrated. If this fails it's OK,\n                # since we still try to delete from the keyring on source\n                # deletion too.\n                if source.pgp_fingerprint and source.pgp_public_key:\n                    try:\n                        encryption_mgr.delete_source_key_pair(source.filesystem_id)\n                    except:  # noqa: E722\n                        pass\n\n        return redirect(url_for(\".lookup\", from_login=\"1\"))",
          "file": "main.py"
        },
        {
          "type": "function",
          "name": "logout",
          "code": "def logout(logged_in_source: SourceUser) -> Union[str, werkzeug.Response]:\n        \"\"\"\n        If a user is logged in, show them a logout page that prompts them to\n        click the New Identity button in Tor Browser to complete their session.\n        Otherwise redirect to the main Source Interface page.\n        \"\"\"\n        if SessionManager.is_user_logged_in(db_session=db.session):\n            SessionManager.log_user_out()\n\n            # Clear the session after we render the message so it's localized\n            # If a user specified a locale, save it and restore it\n            session.clear()\n            session[\"locale\"] = g.localeinfo.id\n\n            return render_template(\"logout.html\")\n        else:\n            return redirect(url_for(\".index\"))",
          "file": "main.py"
        },
        {
          "type": "function",
          "name": "robots_txt",
          "code": "def robots_txt() -> werkzeug.Response:\n        \"\"\"Tell robots we don't want them\"\"\"\n        resp = make_response(\"User-agent: *\\nDisallow: /\")\n        resp.headers[\"content-type\"] = \"text/plain\"\n        return resp",
          "file": "main.py"
        }
      ],
      "session_manager.py": [
        {
          "type": "class",
          "name": "_InvalidUserSession",
          "code": "class _InvalidUserSession(Exception):\n    pass",
          "file": "session_manager.py"
        },
        {
          "type": "class",
          "name": "UserNotLoggedIn",
          "code": "class UserNotLoggedIn(_InvalidUserSession):\n    pass",
          "file": "session_manager.py"
        },
        {
          "type": "class",
          "name": "UserSessionExpired",
          "code": "class UserSessionExpired(_InvalidUserSession):\n    pass",
          "file": "session_manager.py"
        },
        {
          "type": "class",
          "name": "UserHasBeenDeleted",
          "code": "class UserHasBeenDeleted(_InvalidUserSession):\n    pass",
          "file": "session_manager.py"
        },
        {
          "type": "class",
          "name": "SessionManager",
          "code": "class SessionManager:\n    \"\"\"Helper to manage the user's session cookie accessible via flask.session.\"\"\"\n\n    # The keys in flask.session for the user's passphrase and expiration date\n    _SESSION_COOKIE_KEY_FOR_CODENAME = \"codename\"\n    _SESSION_COOKIE_KEY_FOR_EXPIRATION_DATE = \"expires\"\n\n    @classmethod\n    def log_user_in(\n        cls, db_session: sqlalchemy.orm.Session, supplied_passphrase: \"DicewarePassphrase\"\n    ) -> SourceUser:\n        # Validate the passphrase; will raise an exception if it is not valid\n        source_user = authenticate_source_user(\n            db_session=db_session, supplied_passphrase=supplied_passphrase\n        )\n\n        # Save the passphrase in the user's session cookie\n        session[cls._SESSION_COOKIE_KEY_FOR_CODENAME] = supplied_passphrase\n\n        # Save the session expiration date in the user's session cookie\n        config = SecureDropConfig.get_current()\n        session_duration = timedelta(minutes=config.SESSION_EXPIRATION_MINUTES)\n        session[cls._SESSION_COOKIE_KEY_FOR_EXPIRATION_DATE] = (\n            datetime.now(timezone.utc) + session_duration\n        )\n\n        return source_user\n\n    @classmethod\n    def log_user_out(cls) -> None:\n        # Remove session data from the session cookie\n        try:\n            del session[cls._SESSION_COOKIE_KEY_FOR_CODENAME]\n        except KeyError:\n            pass\n\n        try:\n            del session[cls._SESSION_COOKIE_KEY_FOR_EXPIRATION_DATE]\n        except KeyError:\n            pass\n\n    @classmethod\n    def get_logged_in_user(cls, db_session: sqlalchemy.orm.Session) -> SourceUser:\n        # Retrieve the user's passphrase from the Flask session cookie\n        try:\n            user_passphrase = session[cls._SESSION_COOKIE_KEY_FOR_CODENAME]\n            date_session_expires = session[cls._SESSION_COOKIE_KEY_FOR_EXPIRATION_DATE]\n        except KeyError:\n            cls.log_user_out()\n            raise UserNotLoggedIn()\n\n        if datetime.now(timezone.utc) >= date_session_expires:\n            cls.log_user_out()\n            raise UserSessionExpired()\n\n        # Fetch the user's info\n        try:\n            source_user = authenticate_source_user(\n                db_session=db_session, supplied_passphrase=user_passphrase\n            )\n        except InvalidPassphraseError:\n            # The cookie contains a passphrase that is invalid: happens if the user was deleted\n            cls.log_user_out()\n            raise UserHasBeenDeleted()\n\n        return source_user\n\n    @classmethod\n    def is_user_logged_in(cls, db_session: sqlalchemy.orm.Session) -> bool:\n        try:\n            cls.get_logged_in_user(db_session)\n        except _InvalidUserSession:\n            return False\n\n        return True",
          "file": "session_manager.py"
        },
        {
          "type": "function",
          "name": "log_user_in",
          "code": "def log_user_in(\n        cls, db_session: sqlalchemy.orm.Session, supplied_passphrase: \"DicewarePassphrase\"\n    ) -> SourceUser:\n        # Validate the passphrase; will raise an exception if it is not valid\n        source_user = authenticate_source_user(\n            db_session=db_session, supplied_passphrase=supplied_passphrase\n        )\n\n        # Save the passphrase in the user's session cookie\n        session[cls._SESSION_COOKIE_KEY_FOR_CODENAME] = supplied_passphrase\n\n        # Save the session expiration date in the user's session cookie\n        config = SecureDropConfig.get_current()\n        session_duration = timedelta(minutes=config.SESSION_EXPIRATION_MINUTES)\n        session[cls._SESSION_COOKIE_KEY_FOR_EXPIRATION_DATE] = (\n            datetime.now(timezone.utc) + session_duration\n        )\n\n        return source_user",
          "file": "session_manager.py"
        },
        {
          "type": "function",
          "name": "log_user_out",
          "code": "def log_user_out(cls) -> None:\n        # Remove session data from the session cookie\n        try:\n            del session[cls._SESSION_COOKIE_KEY_FOR_CODENAME]\n        except KeyError:\n            pass\n\n        try:\n            del session[cls._SESSION_COOKIE_KEY_FOR_EXPIRATION_DATE]\n        except KeyError:\n            pass",
          "file": "session_manager.py"
        },
        {
          "type": "function",
          "name": "get_logged_in_user",
          "code": "def get_logged_in_user(cls, db_session: sqlalchemy.orm.Session) -> SourceUser:\n        # Retrieve the user's passphrase from the Flask session cookie\n        try:\n            user_passphrase = session[cls._SESSION_COOKIE_KEY_FOR_CODENAME]\n            date_session_expires = session[cls._SESSION_COOKIE_KEY_FOR_EXPIRATION_DATE]\n        except KeyError:\n            cls.log_user_out()\n            raise UserNotLoggedIn()\n\n        if datetime.now(timezone.utc) >= date_session_expires:\n            cls.log_user_out()\n            raise UserSessionExpired()\n\n        # Fetch the user's info\n        try:\n            source_user = authenticate_source_user(\n                db_session=db_session, supplied_passphrase=user_passphrase\n            )\n        except InvalidPassphraseError:\n            # The cookie contains a passphrase that is invalid: happens if the user was deleted\n            cls.log_user_out()\n            raise UserHasBeenDeleted()\n\n        return source_user",
          "file": "session_manager.py"
        },
        {
          "type": "function",
          "name": "is_user_logged_in",
          "code": "def is_user_logged_in(cls, db_session: sqlalchemy.orm.Session) -> bool:\n        try:\n            cls.get_logged_in_user(db_session)\n        except _InvalidUserSession:\n            return False\n\n        return True",
          "file": "session_manager.py"
        }
      ]
    },
    "management": {
      "run.py": [
        {
          "type": "function",
          "name": "colorize",
          "code": "def colorize(s: str, color: str, bold: bool = False) -> str:\n    \"\"\"\n    Returns the string s surrounded by shell metacharacters to display\n    it with the given color and optionally bolded.\n    \"\"\"\n    # List of shell colors from https://www.siafoo.net/snippet/88\n    shell_colors = {\n        \"gray\": \"30\",\n        \"red\": \"31\",\n        \"green\": \"32\",\n        \"yellow\": \"33\",\n        \"blue\": \"34\",\n        \"magenta\": \"35\",\n        \"cyan\": \"36\",\n        \"white\": \"37\",\n        \"crimson\": \"38\",\n        \"highlighted_red\": \"41\",\n        \"highlighted_green\": \"42\",\n        \"highlighted_brown\": \"43\",\n        \"highlighted_blue\": \"44\",\n        \"highlighted_magenta\": \"45\",\n        \"highlighted_cyan\": \"46\",\n        \"highlighted_gray\": \"47\",\n        \"highlighted_crimson\": \"48\",\n    }\n\n    # Based on http://stackoverflow.com/a/2330297/1093000\n    attrs = []\n    attrs.append(shell_colors[color])\n    if bold:\n        attrs.append(\"1\")\n\n    return \"\\x1b[{}m{}\\x1b[0m\".format(\";\".join(attrs), s)",
          "file": "run.py"
        },
        {
          "type": "class",
          "name": "DevServerProcess",
          "code": "class DevServerProcess(subprocess.Popen):  # pragma: no cover\n    def __init__(self, label: str, cmd: List[str], color: str) -> None:\n        self.label = label\n        self.cmd = cmd\n        self.color = color\n\n        super().__init__(  # type: ignore\n            self.cmd,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            preexec_fn=os.setsid,\n        )\n\n    def print_label(self, to: TextIO) -> None:\n        label = f\"\\n => {self.label} <= \\n\\n\"\n        if to.isatty():\n            label = colorize(label, self.color, True)\n        to.write(label)\n\n    def fileno(self) -> int:\n        \"\"\"Implement fileno() in order to use DevServerProcesses with\n        select.select directly.\n\n        Note this method assumes we only want to select this process'\n        stdout. This is a reasonable assumption for a DevServerProcess\n        because the __init__ redirects stderr to stdout, so all output is\n        available on stdout.\n\n        \"\"\"\n        if not self.stdout:\n            raise RuntimeError()\n        return self.stdout.fileno()",
          "file": "run.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, label: str, cmd: List[str], color: str) -> None:\n        self.label = label\n        self.cmd = cmd\n        self.color = color\n\n        super().__init__(  # type: ignore\n            self.cmd,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            preexec_fn=os.setsid,\n        )",
          "file": "run.py"
        },
        {
          "type": "function",
          "name": "print_label",
          "code": "def print_label(self, to: TextIO) -> None:\n        label = f\"\\n => {self.label} <= \\n\\n\"\n        if to.isatty():\n            label = colorize(label, self.color, True)\n        to.write(label)",
          "file": "run.py"
        },
        {
          "type": "function",
          "name": "fileno",
          "code": "def fileno(self) -> int:\n        \"\"\"Implement fileno() in order to use DevServerProcesses with\n        select.select directly.\n\n        Note this method assumes we only want to select this process'\n        stdout. This is a reasonable assumption for a DevServerProcess\n        because the __init__ redirects stderr to stdout, so all output is\n        available on stdout.\n\n        \"\"\"\n        if not self.stdout:\n            raise RuntimeError()\n        return self.stdout.fileno()",
          "file": "run.py"
        },
        {
          "type": "class",
          "name": "DevServerProcessMonitor",
          "code": "class DevServerProcessMonitor:  # pragma: no cover\n    def __init__(self, proc_funcs: List[Callable]) -> None:\n        self.procs = []\n        self.last_proc = None\n        atexit.register(self.cleanup)\n\n        for pf in proc_funcs:\n            self.procs.append(pf())\n\n    def monitor(self) -> None:\n        while True:\n            rprocs, _, _ = select.select(self.procs, [], [])\n\n            for proc in rprocs:\n                # To keep track of which process output what, print a\n                # helpful label every time the process sending output\n                # changes.\n                if proc != self.last_proc:\n                    proc.print_label(sys.stdout)\n                    self.last_proc = proc\n\n                line = proc.stdout.readline()\n                sys.stdout.write(line.decode(\"utf-8\"))\n                sys.stdout.flush()\n\n            if any(proc.poll() is not None for proc in self.procs):\n                # If any of the processes terminates (for example, due to\n                # a syntax error causing a reload to fail), kill them all\n                # so we don't get stuck.\n                sys.stdout.write(\n                    colorize(\n                        \"\\nOne of the development servers exited unexpectedly. \"\n                        \"See the traceback above for details.\\n\"\n                        \"Once you have resolved the issue, you can re-run \"\n                        \"'./manage.py run' to continue developing.\\n\\n\",\n                        \"red\",\n                        True,\n                    )\n                )\n                self.cleanup()\n                break\n\n        for proc in self.procs:\n            proc.wait()\n\n    def cleanup(self) -> None:\n        for proc in self.procs:\n            if proc.poll() is None:\n                # When the development servers use automatic reloading, they\n                # spawn new subprocesses frequently. In order to make sure we\n                # kill all of the subprocesses, we need to send SIGTERM to\n                # the process group and not just the process we initially\n                # created. See http://stackoverflow.com/a/4791612/1093000\n                os.killpg(proc.pid, signal.SIGTERM)\n                proc.terminate()",
          "file": "run.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, proc_funcs: List[Callable]) -> None:\n        self.procs = []\n        self.last_proc = None\n        atexit.register(self.cleanup)\n\n        for pf in proc_funcs:\n            self.procs.append(pf())",
          "file": "run.py"
        },
        {
          "type": "function",
          "name": "monitor",
          "code": "def monitor(self) -> None:\n        while True:\n            rprocs, _, _ = select.select(self.procs, [], [])\n\n            for proc in rprocs:\n                # To keep track of which process output what, print a\n                # helpful label every time the process sending output\n                # changes.\n                if proc != self.last_proc:\n                    proc.print_label(sys.stdout)\n                    self.last_proc = proc\n\n                line = proc.stdout.readline()\n                sys.stdout.write(line.decode(\"utf-8\"))\n                sys.stdout.flush()\n\n            if any(proc.poll() is not None for proc in self.procs):\n                # If any of the processes terminates (for example, due to\n                # a syntax error causing a reload to fail), kill them all\n                # so we don't get stuck.\n                sys.stdout.write(\n                    colorize(\n                        \"\\nOne of the development servers exited unexpectedly. \"\n                        \"See the traceback above for details.\\n\"\n                        \"Once you have resolved the issue, you can re-run \"\n                        \"'./manage.py run' to continue developing.\\n\\n\",\n                        \"red\",\n                        True,\n                    )\n                )\n                self.cleanup()\n                break\n\n        for proc in self.procs:\n            proc.wait()",
          "file": "run.py"
        },
        {
          "type": "function",
          "name": "cleanup",
          "code": "def cleanup(self) -> None:\n        for proc in self.procs:\n            if proc.poll() is None:\n                # When the development servers use automatic reloading, they\n                # spawn new subprocesses frequently. In order to make sure we\n                # kill all of the subprocesses, we need to send SIGTERM to\n                # the process group and not just the process we initially\n                # created. See http://stackoverflow.com/a/4791612/1093000\n                os.killpg(proc.pid, signal.SIGTERM)\n                proc.terminate()",
          "file": "run.py"
        },
        {
          "type": "function",
          "name": "run",
          "code": "def run(args: Any) -> None:  # pragma: no cover\n    \"\"\"\n    Starts development servers for both the Source Interface and the\n    Journalist Interface concurrently. Their output is collected,\n    labeled, and sent to stdout to present a unified view to the\n    developer.\n\n    Ctrl-C will kill the servers and return you to the terminal.\n\n    Useful resources:\n    * https://stackoverflow.com/q/22565606/837471\n\n    \"\"\"\n    print(\n        r\"\"\"\n ____                                       _____\n/\\  _`\\                                    /\\  _ `\\\n\\ \\,\\L\\_\\    __    ___   __  __  _ __    __\\ \\ \\/\\ \\  _ __   ___   _____\n \\/_\\__ \\  /'__`\\ /'___\\/\\ \\/\\ \\/\\`'__\\/'__`\\ \\ \\ \\ \\/\\`'__\\/ __`\\/\\ '__`\\\n  /\\ \\L\\ \\/\\  __//\\ \\__/\\ \\ \\_\\ \\ \\ \\//\\  __/\\ \\ \\_\\ \\ \\ \\//\\ \\L\\ \\ \\ \\L\\ \\\n  \\ `\\____\\ \\____\\ \\____\\\\ \\____/\\ \\_\\\\ \\____\\\\ \\____/\\ \\_\\\\ \\____/\\ \\ ,__/\n   \\/_____/\\/____/\\/____/ \\/___/  \\/_/ \\/____/ \\/___/  \\/_/ \\/___/  \\ \\ \\/\n                                                                     \\ \\_\\\n                                                                      \\/_/\n\"\"\"\n    )\n\n    procs = [\n        lambda: DevServerProcess(\"Source Interface\", [\"python\", \"source.py\"], \"blue\"),\n        lambda: DevServerProcess(\"Journalist Interface\", [\"python\", \"journalist.py\"], \"cyan\"),\n    ]\n\n    monitor = DevServerProcessMonitor(procs)\n    monitor.monitor()",
          "file": "run.py"
        }
      ],
      "sources.py": [
        {
          "type": "function",
          "name": "remove_pending_sources",
          "code": "def remove_pending_sources(args: argparse.Namespace) -> int:\n    \"\"\"\n    Removes pending source accounts, with the option of keeping\n    the `args.keep_most_recent` newest source accounts.\n    \"\"\"\n    sources = find_pending_sources(args.keep_most_recent)\n    print(f\"Found {len(sources)} pending sources\")\n\n    for source in sources:\n        try:\n            EncryptionManager.get_default().delete_source_key_pair(source.filesystem_id)\n        except GpgKeyNotFoundError:\n            pass\n        delete_pending_source(source)\n\n    print(f\"Deleted {len(sources)} pending sources\")\n    return 0",
          "file": "sources.py"
        },
        {
          "type": "function",
          "name": "find_pending_sources",
          "code": "def find_pending_sources(keep_most_recent: int) -> List[Source]:\n    \"\"\"\n    Finds all sources that are marked as pending\n    \"\"\"\n    with app_context():\n        return (\n            Source.query.filter_by(pending=True)\n            .order_by(Source.id.desc())\n            .offset(keep_most_recent)\n            .all()\n        )",
          "file": "sources.py"
        },
        {
          "type": "function",
          "name": "delete_pending_source",
          "code": "def delete_pending_source(source: Source) -> None:\n    \"\"\"\n    Delete a pending source from the database\n    \"\"\"\n    if source.pending:\n        with app_context():\n            try:\n                db.session.delete(source)\n                db.session.commit()\n            except Exception as exc:\n                db.session.rollback()\n                print(f\"ERROR: Could not remove pending source: {exc}.\")",
          "file": "sources.py"
        }
      ],
      "__init__.py": [
        {
          "type": "function",
          "name": "app_context",
          "code": "def app_context() -> Generator:\n    config = SecureDropConfig.get_current()\n    with journalist_app.create_app(config).app_context():\n        yield",
          "file": "__init__.py"
        }
      ],
      "submissions.py": [
        {
          "type": "function",
          "name": "find_disconnected_db_submissions",
          "code": "def find_disconnected_db_submissions(path: str) -> List[Submission]:\n    \"\"\"\n    Finds Submission records whose file does not exist.\n    \"\"\"\n    submissions = db.session.query(Submission).order_by(Submission.id, Submission.filename).all()\n\n    files_in_fs = {}\n    for directory, _subdirs, files in os.walk(path):\n        for f in files:\n            files_in_fs[f] = os.path.abspath(os.path.join(directory, f))\n\n    return [s for s in submissions if s.filename not in files_in_fs]",
          "file": "submissions.py"
        },
        {
          "type": "function",
          "name": "check_for_disconnected_db_submissions",
          "code": "def check_for_disconnected_db_submissions(args: argparse.Namespace) -> None:\n    \"\"\"\n    Check for Submission records whose files are missing.\n    \"\"\"\n    with app_context():\n        disconnected = find_disconnected_db_submissions(args.store_dir)\n        if disconnected:\n            print(\n                \"There are submissions in the database with no corresponding files. \"\n                'Run \"manage.py list-disconnected-db-submissions\" for details.'\n            )\n        else:\n            print(\"No problems were found. All submissions' files are present.\")",
          "file": "submissions.py"
        },
        {
          "type": "function",
          "name": "list_disconnected_db_submissions",
          "code": "def list_disconnected_db_submissions(args: argparse.Namespace) -> None:\n    \"\"\"\n    List the IDs of Submission records whose files are missing.\n    \"\"\"\n    with app_context():\n        disconnected_submissions = find_disconnected_db_submissions(args.store_dir)\n        if disconnected_submissions:\n            print(\n                'Run \"manage.py delete-disconnected-db-submissions\" to delete these records.',\n                file=sys.stderr,\n            )\n        for s in disconnected_submissions:\n            print(s.id)",
          "file": "submissions.py"
        },
        {
          "type": "function",
          "name": "delete_disconnected_db_submissions",
          "code": "def delete_disconnected_db_submissions(args: argparse.Namespace) -> None:\n    \"\"\"\n    Delete Submission records whose files are missing.\n    \"\"\"\n    with app_context():\n        disconnected_submissions = find_disconnected_db_submissions(args.store_dir)\n        ids = [s.id for s in disconnected_submissions]\n\n        remove = args.force\n        if not args.force:\n            remove = input(\"Enter 'y' to delete all submissions missing files: \") == \"y\"\n        if remove:\n            print(f\"Removing submission IDs {ids}...\")\n            db.session.query(Submission).filter(Submission.id.in_(ids)).delete(\n                synchronize_session=\"fetch\"\n            )\n            db.session.commit()\n        else:\n            print(\"Not removing disconnected submissions in database.\")",
          "file": "submissions.py"
        },
        {
          "type": "function",
          "name": "find_disconnected_fs_submissions",
          "code": "def find_disconnected_fs_submissions(path: str) -> List[str]:\n    \"\"\"\n    Finds files in the store that lack a Submission or Reply record.\n    \"\"\"\n    submissions = Submission.query.order_by(Submission.id, Submission.filename).all()\n    files_in_db = {s.filename: True for s in submissions}\n\n    replies = Reply.query.order_by(Reply.id, Reply.filename).all()\n    files_in_db.update({r.filename: True for r in replies})\n\n    files_in_fs = {}\n    for directory, _subdirs, files in os.walk(path):\n        for f in files:\n            files_in_fs[f] = os.path.abspath(os.path.join(directory, f))\n\n    disconnected_files_and_sizes = []\n    for f, p in files_in_fs.items():\n        if f not in files_in_db:\n            filesize = os.stat(p).st_size\n            disconnected_files_and_sizes.append((p, filesize))\n\n    return [file for (file, size) in sorted(disconnected_files_and_sizes, key=lambda t: t[1])]",
          "file": "submissions.py"
        },
        {
          "type": "function",
          "name": "check_for_disconnected_fs_submissions",
          "code": "def check_for_disconnected_fs_submissions(args: argparse.Namespace) -> None:\n    \"\"\"\n    Check for files without a corresponding Submission or Reply record in the database.\n    \"\"\"\n    with app_context():\n        disconnected = find_disconnected_fs_submissions(args.store_dir)\n        if disconnected:\n            print(\n                \"There are files in the submission area with no corresponding records in the \"\n                'database. Run \"manage.py list-disconnected-fs-submissions\" for details.'\n            )\n        else:\n            print(\"No unexpected files were found in the store.\")",
          "file": "submissions.py"
        },
        {
          "type": "function",
          "name": "list_disconnected_fs_submissions",
          "code": "def list_disconnected_fs_submissions(args: argparse.Namespace) -> None:\n    \"\"\"\n    List files without a corresponding Submission or Reply record in the database.\n    \"\"\"\n    with app_context():\n        disconnected_files = find_disconnected_fs_submissions(args.store_dir)\n        if disconnected_files:\n            print(\n                'Run \"manage.py delete-disconnected-fs-submissions\" to delete these files.',\n                file=sys.stderr,\n            )\n        for f in disconnected_files:\n            print(f)",
          "file": "submissions.py"
        },
        {
          "type": "function",
          "name": "delete_disconnected_fs_submissions",
          "code": "def delete_disconnected_fs_submissions(args: argparse.Namespace) -> None:\n    \"\"\"\n    Delete files without a corresponding Submission record in the database.\n    \"\"\"\n    with app_context():\n        disconnected_files = find_disconnected_fs_submissions(args.store_dir)\n        bytes_deleted = 0\n        time_elapsed = 0.0\n        rate = 1.0\n        filecount = len(disconnected_files)\n        eta_msg = \"\"\n        for i, f in enumerate(disconnected_files, 1):\n            remove = args.force\n            if not args.force:\n                remove = input(f\"Enter 'y' to delete {f}: \") == \"y\"\n            if remove:\n                filesize = os.stat(f).st_size\n                if i > 1:\n                    eta = filesize / rate\n                    eta_msg = f\" (ETA to remove {filesize:d} bytes: {eta:.0f}s )\"\n                print(f\"Securely removing file {i}/{filecount} {f}{eta_msg}...\")\n                start = time.time()\n                secure_delete(f)\n                file_elapsed = time.time() - start\n                bytes_deleted += filesize\n                time_elapsed += file_elapsed\n                rate = bytes_deleted / time_elapsed\n                print(\n                    f\"elapsed: {file_elapsed:.2f}s rate: {filesize / 1048576 / file_elapsed:.1f} \"\n                    f\"MB/s overall rate: {rate / 1048576:.1f} MB/s\"\n                )\n            else:\n                print(f\"Not removing {f}.\")",
          "file": "submissions.py"
        },
        {
          "type": "function",
          "name": "were_there_submissions_today",
          "code": "def were_there_submissions_today(\n    args: argparse.Namespace, context: Optional[AppContext] = None\n) -> None:\n    with context or app_context():\n        something = (\n            db.session.query(Source)\n            .filter(Source.last_updated > datetime.datetime.utcnow() - datetime.timedelta(hours=24))\n            .count()\n            > 0\n        )\n        count_file = os.path.join(args.data_root, \"submissions_today.txt\")\n        open(count_file, \"w\").write(something and \"1\" or \"0\")",
          "file": "submissions.py"
        },
        {
          "type": "function",
          "name": "add_check_db_disconnect_parser",
          "code": "def add_check_db_disconnect_parser(subps: _SubParsersAction) -> None:\n    check_db_disconnect_subp = subps.add_parser(\n        \"check-disconnected-db-submissions\",\n        help=\"Check for submissions that exist in the database but not the filesystem.\",\n    )\n    check_db_disconnect_subp.set_defaults(func=check_for_disconnected_db_submissions)",
          "file": "submissions.py"
        },
        {
          "type": "function",
          "name": "add_check_fs_disconnect_parser",
          "code": "def add_check_fs_disconnect_parser(subps: _SubParsersAction) -> None:\n    check_fs_disconnect_subp = subps.add_parser(\n        \"check-disconnected-fs-submissions\",\n        help=\"Check for submissions that exist in the filesystem but not in the database.\",\n    )\n    check_fs_disconnect_subp.set_defaults(func=check_for_disconnected_fs_submissions)",
          "file": "submissions.py"
        },
        {
          "type": "function",
          "name": "add_delete_db_disconnect_parser",
          "code": "def add_delete_db_disconnect_parser(subps: _SubParsersAction) -> None:\n    delete_db_disconnect_subp = subps.add_parser(\n        \"delete-disconnected-db-submissions\",\n        help=\"Delete submissions that exist in the database but not the filesystem.\",\n    )\n    delete_db_disconnect_subp.set_defaults(func=delete_disconnected_db_submissions)\n    delete_db_disconnect_subp.add_argument(\n        \"--force\", action=\"store_true\", help=\"Do not ask for confirmation.\"\n    )",
          "file": "submissions.py"
        },
        {
          "type": "function",
          "name": "add_delete_fs_disconnect_parser",
          "code": "def add_delete_fs_disconnect_parser(subps: _SubParsersAction) -> None:\n    delete_fs_disconnect_subp = subps.add_parser(\n        \"delete-disconnected-fs-submissions\",\n        help=\"Delete submissions that exist in the filesystem but not the database.\",\n    )\n    delete_fs_disconnect_subp.set_defaults(func=delete_disconnected_fs_submissions)\n    delete_fs_disconnect_subp.add_argument(\n        \"--force\", action=\"store_true\", help=\"Do not ask for confirmation.\"\n    )",
          "file": "submissions.py"
        },
        {
          "type": "function",
          "name": "add_list_db_disconnect_parser",
          "code": "def add_list_db_disconnect_parser(subps: _SubParsersAction) -> None:\n    list_db_disconnect_subp = subps.add_parser(\n        \"list-disconnected-db-submissions\",\n        help=\"List submissions that exist in the database but not the filesystem.\",\n    )\n    list_db_disconnect_subp.set_defaults(func=list_disconnected_db_submissions)",
          "file": "submissions.py"
        },
        {
          "type": "function",
          "name": "add_list_fs_disconnect_parser",
          "code": "def add_list_fs_disconnect_parser(subps: _SubParsersAction) -> None:\n    list_fs_disconnect_subp = subps.add_parser(\n        \"list-disconnected-fs-submissions\",\n        help=\"List submissions that exist in the filesystem but not the database.\",\n    )\n    list_fs_disconnect_subp.set_defaults(func=list_disconnected_fs_submissions)",
          "file": "submissions.py"
        },
        {
          "type": "function",
          "name": "add_were_there_submissions_today",
          "code": "def add_were_there_submissions_today(subps: _SubParsersAction) -> None:\n    parser = subps.add_parser(\n        \"were-there-submissions-today\",\n        help=(\"Update the file indicating \" \"whether submissions were received in the past 24h.\"),\n    )\n    parser.set_defaults(func=were_there_submissions_today)",
          "file": "submissions.py"
        }
      ]
    },
    "static": {
      "js": {
        "source.js": [
          {
            "type": "function",
            "name": "display",
            "code": "function display(selector, displayStyle = \"block\") {\n  let nodelist = document.querySelectorAll(selector);\n  Array.prototype.forEach.call(nodelist, function(element) {\n    element.style.display = displayStyle;\n  });\n}",
            "file": "source.js"
          },
          {
            "type": "function",
            "name": "looksLikeTorBrowser",
            "code": "function looksLikeTorBrowser() {\n  return window.navigator.userAgent.match(TBB_UA_REGEX) &&\n    new Date().getTimezoneOffset() == 0 &&\n    window.screen.width == window.innerWidth &&\n    window.screen.height == window.innerHeight;\n}",
            "file": "source.js"
          },
          {
            "type": "function",
            "name": "looksLikeTorBrowserAndroid",
            "code": "function looksLikeTorBrowserAndroid() {\n    return (\n        window.navigator.userAgent.match(TB4A_UA_REGEX) &&\n        new Date().getTimezoneOffset() == 0\n    );\n}",
            "file": "source.js"
          },
          {
            "type": "function",
            "name": "addClose",
            "code": "function addClose(id, elementToClose) {\n  document.getElementById(id).addEventListener(\"click\", function() {\n    display(elementToClose, \"none\");\n  });\n}",
            "file": "source.js"
          },
          {
            "type": "function",
            "name": "showSecurityLevelSuggestions",
            "code": "function showSecurityLevelSuggestions() {\n  display(\"#browser-security-level\");\n\n  // show the instruction popup when the link is clicked\n  document.getElementById(\"disable-js\").addEventListener(\n    \"click\",\n    function(e) {\n      e.preventDefault();\n      e.stopPropagation();\n      display(\"#security-level-info\");\n    }\n  );\n}",
            "file": "source.js"
          },
          {
            "type": "function",
            "name": "checkClearnet",
            "code": "function checkClearnet() {\n  let url = new URL(location.href);\n  // Allow localhost for development\n  let localhost = [\"127.0.0.1\", \"localhost\"];\n  if (localhost.indexOf(url.hostname) === -1 && !url.host.endsWith(\".onion\")) {\n    location.href = \"/tor2web-warning\"\n  }\n}",
            "file": "source.js"
          },
          {
            "type": "function",
            "name": "ready",
            "code": "function ready(fn) {\n  if (document.readyState != \"loading\"){\n    fn();\n  } else {\n    document.addEventListener(\"DOMContentLoaded\", fn);\n  }\n}",
            "file": "source.js"
          }
        ],
        "journalist.js": [
          {
            "type": "function",
            "name": "closest",
            "code": "function closest(element, selector) {\n  let parent = element.parentNode;\n  let closest = null;\n  while (parent.parentNode) {\n    if (parent.matches(selector)) {\n      closest = parent;\n    }\n    parent = parent.parentNode;\n  }\n  return closest;\n}",
            "file": "journalist.js"
          },
          {
            "type": "function",
            "name": "hide",
            "code": "function hide(selector) {\n  let nodelist = document.querySelectorAll(selector);\n  Array.prototype.forEach.call(nodelist, function(element) {\n    element.style.display = \"none\";\n    element.classList.add(\"hidden\");\n  });\n}",
            "file": "journalist.js"
          },
          {
            "type": "function",
            "name": "show",
            "code": "function show(selector, displayStyle = \"revert\") {\n  let nodelist = document.querySelectorAll(selector);\n  Array.prototype.forEach.call(nodelist, function(element) {\n    element.style.display = displayStyle;\n    element.classList.remove(\"hidden\");\n  });\n}",
            "file": "journalist.js"
          },
          {
            "type": "function",
            "name": "enhance_ui",
            "code": "function enhance_ui() {\n  // Add the \"quick filter\" box for the list of sources\n  let filterContainer = document.getElementById(\"filter-container\");\n  if (filterContainer) {\n    filterContainer.innerHTML = '<input id=\"filter\" type=\"text\" placeholder=\"' +\n      get_string(\"filter-by-codename-placeholder-string\") +\n      '\" aria-label=\"' +\n      get_string(\"filter-by-codename-placeholder-string\") +\n      '\" autofocus >';\n  }\n\n  // Add the \"select {all,none}\" buttons for the list of sources\n  let indexSelectContainer = document.getElementById(\"index-select-container\");\n  if (indexSelectContainer) {\n    indexSelectContainer.outerHTML =\n      '<button id=\"select_all\" type=\"button\" class=\"small\"> ' +\n      get_string(\"select-all-string\") +\n      '</button> <button id=\"select_none\" type=\"button\" class=\"small\"> ' +\n      get_string(\"select-none-string\") +\n      '</button>';\n  }\n\n  // Add the \"select {all,unread,none}\" buttons for the source collection\n  let selectContainer = document.getElementById(\"select-container\");\n  if (selectContainer) {\n    selectContainer.innerHTML =\n      '<button id=\"select_all\" type=\"button\" class=\"small\"> ' +\n      get_string(\"select-all-string\") +\n      '</button> <button id=\"select_unread\" type=\"button\" class=\"small\"> ' +\n      get_string(\"select-unread-string\") +\n      '</button> <button id=\"select_none\" type=\"button\" class=\"small\"> ' +\n      get_string(\"select-none-string\") +\n      '</button>';\n  }\n\n}",
            "file": "journalist.js"
          },
          {
            "type": "function",
            "name": "get_string",
            "code": "function get_string(string_id) {\n  let stringContainer = document.querySelector(\"#js-strings > #\" + string_id);\n  return stringContainer ? stringContainer.innerHTML : \"\";\n}",
            "file": "journalist.js"
          },
          {
            "type": "function",
            "name": "filter_codenames",
            "code": "function filter_codenames(value) {\n  if(value == \"\"){\n    show(ROW_SELECTOR_PREFIX);\n  } else {\n    hide(ROW_SELECTOR_PREFIX);\n    show(\n      ROW_SELECTOR_PREFIX + '[data-source-designation*=\"' + value.replace(/\"/g, \"\").toLowerCase() + '\"]'\n    );\n  }\n}",
            "file": "journalist.js"
          },
          {
            "type": "function",
            "name": "ready",
            "code": "function ready(fn) {\n  if (document.readyState != 'loading'){\n    fn();\n  } else {\n    document.addEventListener('DOMContentLoaded', fn);\n  }\n}",
            "file": "journalist.js"
          }
        ]
      }
    }
  },
  "molecule": {
    "testinfra": {
      "conftest.py": [
        {
          "type": "function",
          "name": "securedrop_import_testinfra_vars",
          "code": "def securedrop_import_testinfra_vars(hostname, with_header=False):\n    \"\"\"\n    Import vars from a YAML file to populate tests with host-specific\n    values used in checks. For instance, the SecureDrop docroot will\n    be under /vagrant in development, but /var/www/securedrop in staging.\n\n    Vars must be stored in `testinfra/vars/<hostname>.yml`.\n    \"\"\"\n    filepath = os.path.join(os.path.dirname(__file__), \"vars\", hostname + \".yml\")\n    with open(filepath) as f:\n        hostvars = yaml.safe_load(f)\n\n    hostvars[\"securedrop_venv_site_packages\"] = hostvars[\"securedrop_venv_site_packages\"].format(\n        \"3.8\"\n    )\n\n    # If the tests are run against a production environment, check local config\n    # and override as necessary.\n    prod_filepath = os.path.join(\n        os.path.dirname(__file__), \"../../install_files/ansible-base/group_vars/all/site-specific\"\n    )\n    if os.path.isfile(prod_filepath):\n        with open(prod_filepath) as f:\n            prodvars = yaml.safe_load(f)\n\n        def _prod_override(vars_key, prod_key):\n            if prod_key in prodvars:\n                hostvars[vars_key] = prodvars[prod_key]\n\n        _prod_override(\"app_ip\", \"app_ip\")\n        _prod_override(\"app_hostname\", \"app_hostname\")\n        _prod_override(\"mon_ip\", \"monitor_ip\")\n        _prod_override(\"monitor_hostname\", \"monitor_hostname\")\n        _prod_override(\"sasl_domain\", \"sasl_domain\")\n        _prod_override(\"sasl_username\", \"sasl_username\")\n        _prod_override(\"sasl_password\", \"sasl_password\")\n        _prod_override(\"daily_reboot_time\", \"daily_reboot_time\")\n\n        # Check repo targeting, and update vars\n        repo_filepath = os.path.join(\n            os.path.dirname(__file__),\n            \"../../install_files/ansible-base/roles/install-fpf-repo/defaults/main.yml\",\n        )\n        if os.path.isfile(repo_filepath):\n            with open(repo_filepath) as f:\n                repovars = yaml.safe_load(f)\n                if \"apt_repo_url\" in repovars:\n                    hostvars[\"fpf_apt_repo_url\"] = repovars[\"apt_repo_url\"]\n\n    if with_header:\n        hostvars = dict(securedrop_test_vars=hostvars)\n\n    return hostvars",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "_prod_override",
          "code": "def _prod_override(vars_key, prod_key):\n            if prod_key in prodvars:\n                hostvars[vars_key] = prodvars[prod_key]",
          "file": "conftest.py"
        },
        {
          "type": "class",
          "name": "TestVars",
          "code": "class TestVars(dict):\n    managed_attrs: Dict[str, Any] = {}\n\n    def __init__(self, initial: Dict[str, Any]) -> None:\n        self.securedrop_target_distribution = os.environ.get(\"SECUREDROP_TARGET_DISTRIBUTION\")\n        self.managed_attrs.update(initial)\n\n    def __getattr__(self, name: str) -> Any:\n        \"\"\"\n        If the requested attribute names a dict in managed_attrs and that\n        contains a key with the name of the target distribution,\n        e.g. \"focal\", return that. Otherwise return the entire item\n        under the requested name.\n        \"\"\"\n        try:\n            attr = self.managed_attrs[name]\n            if isinstance(attr, dict) and self.securedrop_target_distribution in attr:\n                return attr[self.securedrop_target_distribution]\n            return attr\n        except KeyError:\n            raise AttributeError(name)\n\n    def __str__(self) -> str:\n        return str(self.managed_attrs)",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, initial: Dict[str, Any]) -> None:\n        self.securedrop_target_distribution = os.environ.get(\"SECUREDROP_TARGET_DISTRIBUTION\")\n        self.managed_attrs.update(initial)",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "__getattr__",
          "code": "def __getattr__(self, name: str) -> Any:\n        \"\"\"\n        If the requested attribute names a dict in managed_attrs and that\n        contains a key with the name of the target distribution,\n        e.g. \"focal\", return that. Otherwise return the entire item\n        under the requested name.\n        \"\"\"\n        try:\n            attr = self.managed_attrs[name]\n            if isinstance(attr, dict) and self.securedrop_target_distribution in attr:\n                return attr[self.securedrop_target_distribution]\n            return attr\n        except KeyError:\n            raise AttributeError(name)",
          "file": "conftest.py"
        },
        {
          "type": "function",
          "name": "__str__",
          "code": "def __str__(self) -> str:\n        return str(self.managed_attrs)",
          "file": "conftest.py"
        }
      ],
      "ossec": {
        "test_journalist_mail.py": [
          {
            "type": "class",
            "name": "TestBase",
            "code": "class TestBase:\n    @pytest.fixture(autouse=True)\n    def only_mon_staging_sudo(self, host):\n        if host.backend.host != \"mon-staging\":\n            pytest.skip()\n\n        with host.sudo():\n            yield\n\n    def ansible(self, host, module, parameters):\n        r = host.ansible(module, parameters, check=False)\n        assert \"exception\" not in r\n\n    def run(self, host, cmd):\n        print(host.backend.host + \" running: \" + cmd)\n        r = host.run(cmd)\n        print(r.stdout)\n        print(r.stderr)\n        return r.rc == 0\n\n    def wait_for(self, fun):\n        success = False\n        for d in (1, 2, 4, 8, 16, 32, 64):\n            if fun():\n                success = True\n                break\n            time.sleep(d)\n        return success\n\n    def wait_for_command(self, host, cmd):\n        return self.wait_for(lambda: self.run(host, cmd))\n\n    #\n    # implementation note: we do not use host.ansible(\"service\", ...\n    # because it only works for services in /etc/init and not those\n    # legacy only found in /etc/init.d such as postfix\n    #\n    def service_started(self, host, name):\n        assert self.run(host, f\"service {name} start\")\n        assert self.wait_for_command(host, f\"service {name} status | grep -q 'is running'\")\n\n    def service_restarted(self, host, name):\n        assert self.run(host, f\"service {name} restart\")\n        assert self.wait_for_command(host, f\"service {name} status | grep -q 'is running'\")\n\n    def service_stopped(self, host, name):\n        assert self.run(host, f\"service {name} stop\")\n        assert self.wait_for_command(host, f\"service {name} status | grep -q 'not running'\")",
            "file": "test_journalist_mail.py"
          },
          {
            "type": "function",
            "name": "only_mon_staging_sudo",
            "code": "def only_mon_staging_sudo(self, host):\n        if host.backend.host != \"mon-staging\":\n            pytest.skip()\n\n        with host.sudo():\n            yield",
            "file": "test_journalist_mail.py"
          },
          {
            "type": "function",
            "name": "ansible",
            "code": "def ansible(self, host, module, parameters):\n        r = host.ansible(module, parameters, check=False)\n        assert \"exception\" not in r",
            "file": "test_journalist_mail.py"
          },
          {
            "type": "function",
            "name": "run",
            "code": "def run(self, host, cmd):\n        print(host.backend.host + \" running: \" + cmd)\n        r = host.run(cmd)\n        print(r.stdout)\n        print(r.stderr)\n        return r.rc == 0",
            "file": "test_journalist_mail.py"
          },
          {
            "type": "function",
            "name": "wait_for",
            "code": "def wait_for(self, fun):\n        success = False\n        for d in (1, 2, 4, 8, 16, 32, 64):\n            if fun():\n                success = True\n                break\n            time.sleep(d)\n        return success",
            "file": "test_journalist_mail.py"
          },
          {
            "type": "function",
            "name": "wait_for_command",
            "code": "def wait_for_command(self, host, cmd):\n        return self.wait_for(lambda: self.run(host, cmd))",
            "file": "test_journalist_mail.py"
          },
          {
            "type": "function",
            "name": "service_started",
            "code": "def service_started(self, host, name):\n        assert self.run(host, f\"service {name} start\")\n        assert self.wait_for_command(host, f\"service {name} status | grep -q 'is running'\")",
            "file": "test_journalist_mail.py"
          },
          {
            "type": "function",
            "name": "service_restarted",
            "code": "def service_restarted(self, host, name):\n        assert self.run(host, f\"service {name} restart\")\n        assert self.wait_for_command(host, f\"service {name} status | grep -q 'is running'\")",
            "file": "test_journalist_mail.py"
          },
          {
            "type": "function",
            "name": "service_stopped",
            "code": "def service_stopped(self, host, name):\n        assert self.run(host, f\"service {name} stop\")\n        assert self.wait_for_command(host, f\"service {name} status | grep -q 'not running'\")",
            "file": "test_journalist_mail.py"
          },
          {
            "type": "class",
            "name": "TestJournalistMail",
            "code": "class TestJournalistMail(TestBase):\n    @pytest.mark.skip(reason=SKIP_REASON)\n    def test_procmail(self, host):\n        self.service_started(host, \"postfix\")\n        for destination, f in (\n            (\"journalist\", \"alert-journalist-one.txt\"),\n            (\"journalist\", \"alert-journalist-two.txt\"),\n            (\"ossec\", \"alert-ossec.txt\"),\n        ):\n            # Look up CWD, in case tests move in the future\n            current_dir = os.path.dirname(os.path.abspath(__file__))\n            self.ansible(host, \"copy\", f\"dest=/tmp/{f} src={current_dir}/{f}\")\n            assert self.run(host, \"/var/ossec/process_submissions_today.sh forget\")\n            assert self.run(host, \"postsuper -d ALL\")\n            assert self.run(host, f\"cat /tmp/{f} | mail -s 'abc' root@localhost\")\n            assert self.wait_for_command(host, f\"mailq | grep -q {destination}@ossec.test\")\n        self.service_stopped(host, \"postfix\")\n\n    @pytest.mark.skip(reason=SKIP_REASON)\n    def test_process_submissions_today(self, host):\n        assert self.run(host, \"/var/ossec/process_submissions_today.sh \" \"test_handle_notification\")\n        assert self.run(\n            host, \"/var/ossec/process_submissions_today.sh \" \"test_modified_in_the_past_24h\"\n        )\n\n    @pytest.mark.skip(reason=SKIP_REASON)\n    def test_send_encrypted_alert(self, host):\n        self.service_started(host, \"postfix\")\n        src = \"../../install_files/ansible-base/roles/ossec/files/\" \"test_admin_key.sec\"\n        self.ansible(host, \"copy\", f\"dest=/tmp/test_admin_key.sec src={src}\")\n\n        self.run(host, \"gpg  --homedir /var/ossec/.gnupg\" \" --import /tmp/test_admin_key.sec\")\n\n        def trigger(who, payload):\n            assert self.run(host, f\"! mailq | grep -q {who}@ossec.test\")\n            assert self.run(\n                host,\n                f\"\"\"\n                ( echo 'Subject: TEST' ; echo ; echo -e '{payload}' ) | \\\n                /var/ossec/send_encrypted_alarm.sh {who}\n                \"\"\",\n            )\n            assert self.wait_for_command(host, f\"mailq | grep -q {who}@ossec.test\")\n\n        #\n        # encrypted mail to journalist or ossec contact\n        #\n        for who, payload, expected in (\n            (\"journalist\", \"JOURNALISTPAYLOAD\", \"JOURNALISTPAYLOAD\"),\n            (\"ossec\", \"OSSECPAYLOAD\", \"OSSECPAYLOAD\"),\n        ):\n            assert self.run(host, \"postsuper -d ALL\")\n            trigger(who, payload)\n            assert self.run(\n                host,\n                f\"\"\"\n                job=$(mailq | sed -n -e '2p' | cut -f1 -d ' ')\n                postcat -q $job | tee /dev/stderr | \\\n                   gpg --homedir /var/ossec/.gnupg --decrypt 2>&1 | \\\n                   grep -q {expected}\n                \"\"\",\n            )\n        #\n        # failure to encrypt must trigger an emergency mail to ossec contact\n        #\n        try:\n            assert self.run(host, \"postsuper -d ALL\")\n            assert self.run(host, \"mv /usr/bin/gpg /usr/bin/gpg.save\")\n            trigger(who, \"MYGREATPAYLOAD\")\n            assert self.run(\n                host,\n                \"\"\"\n                job=$(mailq | sed -n -e '2p' | cut -f1 -d ' ')\n                postcat -q $job | grep -q 'Failed to encrypt OSSEC alert'\n                \"\"\",\n            )\n        finally:\n            assert self.run(host, \"mv /usr/bin/gpg.save /usr/bin/gpg\")\n        self.service_stopped(host, \"postfix\")\n\n    @pytest.mark.skip(reason=SKIP_REASON)\n    def test_missing_journalist_alert(self, host):\n        #\n        # missing journalist mail does nothing\n        #\n        assert self.run(\n            host,\n            \"\"\"\n            JOURNALIST_EMAIL= \\\n               bash -x /var/ossec/send_encrypted_alarm.sh journalist | \\\n               tee /dev/stderr | \\\n               grep -q 'no notification sent'\n            \"\"\",\n        )\n\n    # https://ossec-docs.readthedocs.io/en/latest/manual/rules-decoders/testing.html\n    @pytest.mark.skip(reason=SKIP_REASON)\n    def test_ossec_rule_journalist(self, host):\n        assert self.run(\n            host,\n            \"\"\"\n        set -ex\n        l=\"ossec: output: 'head -1 /var/lib/securedrop/submissions_today.txt\"\n        echo \"$l\" | /var/ossec/bin/ossec-logtest\n        echo \"$l\" | /var/ossec/bin/ossec-logtest -U '400600:1:ossec'\n        \"\"\",\n        )\n\n    @pytest.mark.skip(reason=SKIP_REASON)\n    def test_journalist_mail_notification(self, host):\n        mon = host\n        app = testinfra.host.Host.get_host(\n            \"ansible://app-staging\", ansible_inventory=host.backend.ansible_inventory\n        )\n        #\n        # run ossec & postfix on mon\n        #\n        self.service_started(mon, \"postfix\")\n        self.service_started(mon, \"ossec\")\n\n        #\n        # ensure the submission_today.txt file exists\n        #\n        with app.sudo():\n            assert self.run(\n                app,\n                \"\"\"\n            cd /var/www/securedrop\n            ./manage.py were-there-submissions-today\n            test -f /var/lib/securedrop/submissions_today.txt\n            \"\"\",\n            )\n\n        #\n        # empty the mailq on mon in case there were leftovers\n        #\n        assert self.run(mon, \"postsuper -d ALL\")\n        #\n        # forget about past notifications in case there were leftovers\n        #\n        assert self.run(mon, \"/var/ossec/process_submissions_today.sh forget\")\n\n        #\n        # the command fires every time ossec starts,\n        # regardless of the frequency\n        # https://github.com/ossec/ossec-hids/issues/1415\n        #\n        with app.sudo():\n            self.service_restarted(app, \"ossec\")\n\n        #\n        # wait until at exactly one notification is sent\n        #\n        assert self.wait_for_command(mon, \"mailq | grep -q journalist@ossec.test\")\n        assert self.run(mon, \"test 1 = $(mailq | grep journalist@ossec.test | wc -l)\")\n\n        assert self.run(\n            mon, \"grep --count 'notification suppressed' /var/log/syslog \" \"> /tmp/before\"\n        )\n\n        #\n        # The second notification within less than 24h is suppressed\n        #\n        with app.sudo():\n            self.service_restarted(app, \"ossec\")\n        assert self.wait_for_command(\n            mon,\n            \"\"\"\n        grep --count 'notification suppressed' /var/log/syslog > /tmp/after\n        test $(cat /tmp/before) -lt $(cat /tmp/after)\n        \"\"\",\n        )\n\n        #\n        # teardown the ossec and postfix on mon and app\n        #\n        self.service_stopped(mon, \"postfix\")\n        self.service_stopped(mon, \"ossec\")\n        with app.sudo():\n            self.service_stopped(app, \"ossec\")",
            "file": "test_journalist_mail.py"
          },
          {
            "type": "function",
            "name": "test_procmail",
            "code": "def test_procmail(self, host):\n        self.service_started(host, \"postfix\")\n        for destination, f in (\n            (\"journalist\", \"alert-journalist-one.txt\"),\n            (\"journalist\", \"alert-journalist-two.txt\"),\n            (\"ossec\", \"alert-ossec.txt\"),\n        ):\n            # Look up CWD, in case tests move in the future\n            current_dir = os.path.dirname(os.path.abspath(__file__))\n            self.ansible(host, \"copy\", f\"dest=/tmp/{f} src={current_dir}/{f}\")\n            assert self.run(host, \"/var/ossec/process_submissions_today.sh forget\")\n            assert self.run(host, \"postsuper -d ALL\")\n            assert self.run(host, f\"cat /tmp/{f} | mail -s 'abc' root@localhost\")\n            assert self.wait_for_command(host, f\"mailq | grep -q {destination}@ossec.test\")\n        self.service_stopped(host, \"postfix\")",
            "file": "test_journalist_mail.py"
          },
          {
            "type": "function",
            "name": "test_process_submissions_today",
            "code": "def test_process_submissions_today(self, host):\n        assert self.run(host, \"/var/ossec/process_submissions_today.sh \" \"test_handle_notification\")\n        assert self.run(\n            host, \"/var/ossec/process_submissions_today.sh \" \"test_modified_in_the_past_24h\"\n        )",
            "file": "test_journalist_mail.py"
          },
          {
            "type": "function",
            "name": "test_send_encrypted_alert",
            "code": "def test_send_encrypted_alert(self, host):\n        self.service_started(host, \"postfix\")\n        src = \"../../install_files/ansible-base/roles/ossec/files/\" \"test_admin_key.sec\"\n        self.ansible(host, \"copy\", f\"dest=/tmp/test_admin_key.sec src={src}\")\n\n        self.run(host, \"gpg  --homedir /var/ossec/.gnupg\" \" --import /tmp/test_admin_key.sec\")\n\n        def trigger(who, payload):\n            assert self.run(host, f\"! mailq | grep -q {who}@ossec.test\")\n            assert self.run(\n                host,\n                f\"\"\"\n                ( echo 'Subject: TEST' ; echo ; echo -e '{payload}' ) | \\\n                /var/ossec/send_encrypted_alarm.sh {who}\n                \"\"\",\n            )\n            assert self.wait_for_command(host, f\"mailq | grep -q {who}@ossec.test\")\n\n        #\n        # encrypted mail to journalist or ossec contact\n        #\n        for who, payload, expected in (\n            (\"journalist\", \"JOURNALISTPAYLOAD\", \"JOURNALISTPAYLOAD\"),\n            (\"ossec\", \"OSSECPAYLOAD\", \"OSSECPAYLOAD\"),\n        ):\n            assert self.run(host, \"postsuper -d ALL\")\n            trigger(who, payload)\n            assert self.run(\n                host,\n                f\"\"\"\n                job=$(mailq | sed -n -e '2p' | cut -f1 -d ' ')\n                postcat -q $job | tee /dev/stderr | \\\n                   gpg --homedir /var/ossec/.gnupg --decrypt 2>&1 | \\\n                   grep -q {expected}\n                \"\"\",\n            )\n        #\n        # failure to encrypt must trigger an emergency mail to ossec contact\n        #\n        try:\n            assert self.run(host, \"postsuper -d ALL\")\n            assert self.run(host, \"mv /usr/bin/gpg /usr/bin/gpg.save\")\n            trigger(who, \"MYGREATPAYLOAD\")\n            assert self.run(\n                host,\n                \"\"\"\n                job=$(mailq | sed -n -e '2p' | cut -f1 -d ' ')\n                postcat -q $job | grep -q 'Failed to encrypt OSSEC alert'\n                \"\"\",\n            )\n        finally:\n            assert self.run(host, \"mv /usr/bin/gpg.save /usr/bin/gpg\")\n        self.service_stopped(host, \"postfix\")",
            "file": "test_journalist_mail.py"
          },
          {
            "type": "function",
            "name": "trigger",
            "code": "def trigger(who, payload):\n            assert self.run(host, f\"! mailq | grep -q {who}@ossec.test\")\n            assert self.run(\n                host,\n                f\"\"\"\n                ( echo 'Subject: TEST' ; echo ; echo -e '{payload}' ) | \\\n                /var/ossec/send_encrypted_alarm.sh {who}\n                \"\"\",\n            )\n            assert self.wait_for_command(host, f\"mailq | grep -q {who}@ossec.test\")",
            "file": "test_journalist_mail.py"
          },
          {
            "type": "function",
            "name": "test_missing_journalist_alert",
            "code": "def test_missing_journalist_alert(self, host):\n        #\n        # missing journalist mail does nothing\n        #\n        assert self.run(\n            host,\n            \"\"\"\n            JOURNALIST_EMAIL= \\\n               bash -x /var/ossec/send_encrypted_alarm.sh journalist | \\\n               tee /dev/stderr | \\\n               grep -q 'no notification sent'\n            \"\"\",\n        )",
            "file": "test_journalist_mail.py"
          },
          {
            "type": "function",
            "name": "test_ossec_rule_journalist",
            "code": "def test_ossec_rule_journalist(self, host):\n        assert self.run(\n            host,\n            \"\"\"\n        set -ex\n        l=\"ossec: output: 'head -1 /var/lib/securedrop/submissions_today.txt\"\n        echo \"$l\" | /var/ossec/bin/ossec-logtest\n        echo \"$l\" | /var/ossec/bin/ossec-logtest -U '400600:1:ossec'\n        \"\"\",\n        )",
            "file": "test_journalist_mail.py"
          },
          {
            "type": "function",
            "name": "test_journalist_mail_notification",
            "code": "def test_journalist_mail_notification(self, host):\n        mon = host\n        app = testinfra.host.Host.get_host(\n            \"ansible://app-staging\", ansible_inventory=host.backend.ansible_inventory\n        )\n        #\n        # run ossec & postfix on mon\n        #\n        self.service_started(mon, \"postfix\")\n        self.service_started(mon, \"ossec\")\n\n        #\n        # ensure the submission_today.txt file exists\n        #\n        with app.sudo():\n            assert self.run(\n                app,\n                \"\"\"\n            cd /var/www/securedrop\n            ./manage.py were-there-submissions-today\n            test -f /var/lib/securedrop/submissions_today.txt\n            \"\"\",\n            )\n\n        #\n        # empty the mailq on mon in case there were leftovers\n        #\n        assert self.run(mon, \"postsuper -d ALL\")\n        #\n        # forget about past notifications in case there were leftovers\n        #\n        assert self.run(mon, \"/var/ossec/process_submissions_today.sh forget\")\n\n        #\n        # the command fires every time ossec starts,\n        # regardless of the frequency\n        # https://github.com/ossec/ossec-hids/issues/1415\n        #\n        with app.sudo():\n            self.service_restarted(app, \"ossec\")\n\n        #\n        # wait until at exactly one notification is sent\n        #\n        assert self.wait_for_command(mon, \"mailq | grep -q journalist@ossec.test\")\n        assert self.run(mon, \"test 1 = $(mailq | grep journalist@ossec.test | wc -l)\")\n\n        assert self.run(\n            mon, \"grep --count 'notification suppressed' /var/log/syslog \" \"> /tmp/before\"\n        )\n\n        #\n        # The second notification within less than 24h is suppressed\n        #\n        with app.sudo():\n            self.service_restarted(app, \"ossec\")\n        assert self.wait_for_command(\n            mon,\n            \"\"\"\n        grep --count 'notification suppressed' /var/log/syslog > /tmp/after\n        test $(cat /tmp/before) -lt $(cat /tmp/after)\n        \"\"\",\n        )\n\n        #\n        # teardown the ossec and postfix on mon and app\n        #\n        self.service_stopped(mon, \"postfix\")\n        self.service_stopped(mon, \"ossec\")\n        with app.sudo():\n            self.service_stopped(app, \"ossec\")",
            "file": "test_journalist_mail.py"
          }
        ]
      },
      "common": {
        "test_basic_configuration.py": [
          {
            "type": "function",
            "name": "test_system_time",
            "code": "def test_system_time(host: Host) -> None:\n    assert not host.package(\"ntp\").is_installed\n    assert not host.package(\"ntpdate\").is_installed\n\n    s = host.service(\"systemd-timesyncd\")\n    assert s.is_running\n    assert s.is_enabled\n    assert not s.is_masked\n\n    # File will be touched on every successful synchronization,\n    # see 'man systemd-timesyncd'`\n    assert host.file(\"/run/systemd/timesync/synchronized\").exists\n\n    c = host.run(\"timedatectl show\")\n    assert \"NTP=yes\" in c.stdout\n    assert \"NTPSynchronized=yes\" in c.stdout",
            "file": "test_basic_configuration.py"
          },
          {
            "type": "function",
            "name": "test_ossec_cleanup",
            "code": "def test_ossec_cleanup(host: Host) -> None:\n    with host.sudo():\n        c = host.run(\"mkdir -p /var/ossec/queue/diff/local/boot/appinfra-test\")\n        assert c.rc == 0\n        c = host.run(\"echo 'test' > /var/ossec/queue/diff/local/boot/appinfra-test/state.123456789\")\n        assert c.rc == 0\n        # change the mtime on the file to be 2 years ago\n        c = host.run(\n            \"touch -d '2 years ago' /var/ossec/queue/diff/local/boot/appinfra-test/state.123456789\"\n        )\n        assert c.rc == 0\n        c = host.run(\"systemctl start securedrop-cleanup-ossec\")\n        assert c.rc == 0\n        while host.service(\"securedrop-cleanup-ossec\").is_running:\n            time.sleep(1)\n        assert not host.file(\n            \"/var/ossec/queue/diff/local/boot/appinfra-test/state.123456789\"\n        ).exists\n        # cleanup\n        c = host.run(\"rm -r /var/ossec/queue/diff/local/boot/appinfra-test\")\n        assert c.rc == 0",
            "file": "test_basic_configuration.py"
          }
        ],
        "test_automatic_updates.py": [
          {
            "type": "function",
            "name": "test_automatic_updates_dependencies",
            "code": "def test_automatic_updates_dependencies(host):\n    \"\"\"\n    Ensure critical packages are installed. If any of these are missing,\n    the system will fail to receive automatic updates.\n    In Focal, the apt config uses unattended-upgrades.\n    \"\"\"\n    assert host.package(\"unattended-upgrades\").is_installed\n    assert not host.package(\"cron-apt\").is_installed\n    assert not host.package(\"ntp\").is_installed",
            "file": "test_automatic_updates.py"
          },
          {
            "type": "function",
            "name": "test_cron_apt_config",
            "code": "def test_cron_apt_config(host):\n    \"\"\"\n    Ensure custom cron-apt config is absent, as of Focal\n    \"\"\"\n    assert not host.file(\"/etc/cron-apt/config\").exists\n    assert not host.file(\"/etc/cron-apt/action.d/0-update\").exists\n    assert not host.file(\"/etc/cron-apt/action.d/5-security\").exists\n    assert not host.file(\"/etc/cron-apt/action.d/9-remove\").exists\n    assert not host.file(\"/etc/cron.d/cron-apt\").exists\n    assert not host.file(\"/etc/apt/security.list\").exists\n    assert not host.file(\"/etc/cron-apt/action.d/3-download\").exists",
            "file": "test_automatic_updates.py"
          },
          {
            "type": "function",
            "name": "test_sources_list",
            "code": "def test_sources_list(host, repo):\n    \"\"\"\n    Ensure the correct apt repositories are specified\n    in the sources.list for apt.\n    \"\"\"\n    if host.system_info.codename != \"focal\":\n        pytest.skip(\"sources.list is only provisioned on focal\")\n    repo_config = repo.format(securedrop_target_platform=host.system_info.codename)\n    f = host.file(\"/etc/apt/sources.list\")\n    assert f.is_file\n    assert f.user == \"root\"\n    assert f.mode == 0o644\n    repo_regex = f\"^{re.escape(repo_config)}$\"\n    assert f.contains(repo_regex)",
            "file": "test_automatic_updates.py"
          },
          {
            "type": "function",
            "name": "test_ubuntu_sources",
            "code": "def test_ubuntu_sources(host):\n    \"\"\"\n    Ensure the correct apt repositories are specified\n    in the ubuntu.sources for apt.\n    \"\"\"\n    distro = host.system_info.codename\n    if distro == \"focal\":\n        pytest.skip(\"sources.list is only provisioned on noble\")\n    f = host.file(\"/etc/apt/sources.list.d/ubuntu.sources\")\n    assert f.is_file\n    assert f.user == \"root\"\n    assert f.mode == 0o644\n    expected = f\"\"\"\\\nURIs: http://archive.ubuntu.com/ubuntu/\nSuites: {distro} {distro}-updates\nComponents: main universe restricted multiverse\n\"\"\"\n    assert f.contains(expected)\n    expected_security = f\"\"\"\\\nURIs: http://security.ubuntu.com/ubuntu/\nSuites: {distro}-security\nComponents: main universe restricted multiverse\n\"\"\"\n    assert f.contains(expected_security)",
            "file": "test_automatic_updates.py"
          },
          {
            "type": "function",
            "name": "test_unattended_upgrades_config",
            "code": "def test_unattended_upgrades_config(host, k, v):\n    \"\"\"\n    Ensures the apt and unattended-upgrades config is correct only under Ubuntu Focal\n    \"\"\"\n    # Dump apt config to inspect end state, apt will build config\n    # from all conf files on disk, e.g. 80securedrop.\n    c = host.run(f\"apt-config dump --format '%v%n' {k}\")\n    assert c.rc == 0\n    # Some values are lists, so support that in the params\n    if hasattr(v, \"__getitem__\"):\n        for i in v:\n            assert i in c.stdout\n    else:\n        assert v in c.stdout",
            "file": "test_automatic_updates.py"
          },
          {
            "type": "function",
            "name": "test_unattended_securedrop_specific",
            "code": "def test_unattended_securedrop_specific(host):\n    \"\"\"\n    Ensures the 80securedrop config is correct. Under Ubuntu Focal,\n    it will include unattended-upgrade settings. Under all hosts,\n    it will disable installing 'recommended' packages.\n    \"\"\"\n    f = host.file(\"/etc/apt/apt.conf.d/80securedrop\")\n    assert f.is_file\n    assert f.user == \"root\"\n    assert f.mode == 0o644\n    assert f.contains('APT::Install-Recommends \"false\";')\n    assert f.contains(\"Automatic-Reboot-Time\")",
            "file": "test_automatic_updates.py"
          },
          {
            "type": "function",
            "name": "test_unattended_upgrades_functional",
            "code": "def test_unattended_upgrades_functional(host):\n    \"\"\"\n    Ensure unattended-upgrades completes successfully and ensures all packages\n    are up-to-date.\n    \"\"\"\n    c = host.run(\"sudo unattended-upgrades --dry-run --debug\")\n    assert c.rc == 0\n    distro = host.system_info.codename\n    expected_origins = (\n        f\"Allowed origins are:\"\n        f\" origin=Ubuntu,archive={distro}, origin=Ubuntu,archive={distro}-security\"\n        f\", origin=Ubuntu,archive={distro}-updates, origin=SecureDrop,codename={distro}\"\n    )\n\n    all_good = \"No packages found that can be upgraded unattended and no pending auto-removals\"\n    assert expected_origins in c.stdout\n    if distro == \"focal\":\n        assert all_good in c.stdout\n    else:  # noqa: PLR5501\n        if all_good in c.stdout:\n            assert all_good in c.stdout\n        else:\n            # noble+ uses phased updates, so there may be packages that can be\n            # upgraded that won't be upgraded; look for a different message in that case\n            assert \"left to upgrade set()\\nAll upgrades installed\" in c.stdout",
            "file": "test_automatic_updates.py"
          },
          {
            "type": "function",
            "name": "test_fixed_phasing",
            "code": "def test_fixed_phasing(host):\n    \"\"\"Verify APT's machine-id is set to a fixed value for consistent phasing\"\"\"\n    cmd = host.run(\"apt-config dump APT::Machine-ID\")\n    assert cmd.rc == 0\n    assert cmd.stdout.startswith('APT::Machine-ID \"1ebf')",
            "file": "test_automatic_updates.py"
          },
          {
            "type": "function",
            "name": "test_apt_daily_services_and_timers_enabled",
            "code": "def test_apt_daily_services_and_timers_enabled(host, service):\n    \"\"\"\n    Ensure the services and timers used for unattended upgrades are enabled\n    in Ubuntu 20.04 Focal.\n    \"\"\"\n    with host.sudo():\n        # The services are started only when the upgrades are being performed.\n        s = host.service(service)\n        assert s.is_enabled",
            "file": "test_automatic_updates.py"
          },
          {
            "type": "function",
            "name": "test_apt_daily_timer_schedule",
            "code": "def test_apt_daily_timer_schedule(host):\n    \"\"\"\n    Timer for running apt-daily, i.e. 'apt-get update', should be OFFSET_UPDATE hrs\n    before the daily_reboot_time.\n    \"\"\"\n    t = (int(test_vars.daily_reboot_time) - OFFSET_UPDATE) % 24\n    c = host.run(\"systemctl show apt-daily.timer\")\n    assert \"TimersCalendar={ OnCalendar=*-*-* \" + f\"{t:02d}\" + \":00:00 ;\" in c.stdout\n    assert \"RandomizedDelayUSec=20m\" in c.stdout",
            "file": "test_automatic_updates.py"
          },
          {
            "type": "function",
            "name": "test_apt_daily_upgrade_timer_schedule",
            "code": "def test_apt_daily_upgrade_timer_schedule(host):\n    \"\"\"\n    Timer for running apt-daily-upgrade, i.e. 'apt-get upgrade', should be OFFSET_UPGRADE hrs\n    before the daily_reboot_time, and 1h after the apt-daily time.\n    \"\"\"\n    t = (int(test_vars.daily_reboot_time) - OFFSET_UPGRADE) % 24\n    c = host.run(\"systemctl show apt-daily-upgrade.timer\")\n    assert \"TimersCalendar={ OnCalendar=*-*-* \" + f\"{t:02d}\" + \":00:00 ;\" in c.stdout\n    assert \"RandomizedDelayUSec=20m\" in c.stdout",
            "file": "test_automatic_updates.py"
          },
          {
            "type": "function",
            "name": "test_reboot_required_timer",
            "code": "def test_reboot_required_timer(host):\n    \"\"\"\n    Unattended-upgrades does not reboot the system if the updates don't require it.\n    However, we use daily reboots for SecureDrop to ensure memory is cleared periodically.\n    Here, we ensure that reboot-required flag is dropped twice daily to ensure the system\n    is rebooted every day at the scheduled time.\n    \"\"\"\n    f = host.service(\"securedrop-reboot-required.timer\")\n    assert f.is_enabled\n    assert f.is_running\n    # Run the service itself to verify the file is created\n    with host.sudo():\n        if host.file(\"/var/run/reboot-required\").exists:\n            cmd = host.run(\"rm /var/run/reboot-required\")\n            assert cmd.rc == 0\n        cmd = host.run(\"systemctl start securedrop-reboot-required\")\n        assert cmd.rc == 0\n        while host.service(\"securedrop-reboot-required\").is_running:\n            time.sleep(1)\n        assert host.file(\"/var/run/reboot-required\").exists",
            "file": "test_automatic_updates.py"
          },
          {
            "type": "function",
            "name": "test_all_packages_updated",
            "code": "def test_all_packages_updated(host):\n    \"\"\"\n    Ensure a safe-upgrade has already been run, by checking that no\n    packages are eligible for upgrade currently.\n\n    The Ansible config installs a specific, out-of-date version of Firefox\n    for use with Selenium. Therefore apt will report it's possible to upgrade\n    Firefox, which we'll need to mark as \"OK\" in terms of the tests.\n    \"\"\"\n    c = host.run(\"apt-get dist-upgrade --simulate\")\n    assert c.rc == 0\n    # Staging hosts will have locally built deb packages, marked as held.\n    # Staging and development will have a version-locked Firefox pinned for\n    # Selenium compatibility; if the holds are working, they shouldn't be\n    # upgraded.\n    # Example output:\n    #   0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n    # Don't test for the \"not upgraded\" because those map to held packages.\n    assert \"0 upgraded, 0 newly installed, 0 to remove\" in c.stdout",
            "file": "test_automatic_updates.py"
          }
        ],
        "test_system_hardening.py": [
          {
            "type": "function",
            "name": "test_sysctl_options",
            "code": "def test_sysctl_options(host, sysctl_opt):\n    \"\"\"\n    Ensure sysctl flags are set correctly. Most of these checks\n    are hardening IPv4, which is appropriate due to the heavy use of Tor.\n\n    These are all set via securedrop-grsec (in kernel-builder).\n    \"\"\"\n    with host.sudo():\n        assert host.sysctl(sysctl_opt[0]) == sysctl_opt[1]",
            "file": "test_system_hardening.py"
          },
          {
            "type": "function",
            "name": "test_dns_setting",
            "code": "def test_dns_setting(host):\n    \"\"\"\n    Ensure DNS service is hard-coded in resolv.conf config.\n    \"\"\"\n    f = host.file(\"/etc/resolv.conf\")\n    assert f.is_file\n    assert f.user == \"root\"\n    assert f.group == \"root\"\n    assert f.mode == 0o644\n    assert f.contains(r\"^nameserver 8\\.8\\.8\\.8$\")",
            "file": "test_system_hardening.py"
          },
          {
            "type": "function",
            "name": "test_blacklisted_kernel_modules",
            "code": "def test_blacklisted_kernel_modules(host, kernel_module):\n    \"\"\"\n    Test that unwanted kernel modules are blacklisted on the system.\n    Mostly these checks are defense-in-depth approaches to ensuring\n    that wireless interfaces will not work.\n    \"\"\"\n    with host.sudo():\n        c = host.run(\"lsmod\")\n        assert kernel_module not in c.stdout\n\n    f = host.file(\"/etc/modprobe.d/blacklist.conf\")\n    assert f.contains(f\"^blacklist {kernel_module}$\")",
            "file": "test_system_hardening.py"
          },
          {
            "type": "function",
            "name": "test_swap_disabled",
            "code": "def test_swap_disabled(host):\n    \"\"\"\n    Ensure swap space is disabled. Prohibit writing memory to swapfiles\n    to reduce the threat of forensic analysis leaking any sensitive info.\n    \"\"\"\n    hostname = host.check_output(\"hostname\")\n\n    if hostname.startswith(\"mon\"):\n        pytest.skip(\"mon doesn't have swap disabled yet\")\n\n    c = host.check_output(\"swapon --summary\")\n    # A leading slash will indicate full path to a swapfile.\n    assert not re.search(\"^/\", c, re.M)\n\n    # Since swapon 2.27.1, the summary shows blank output, with no headers,\n    # when no swap is configured, check for empty output as confirmation disabled.\n    rgx = re.compile(\"^$\")\n\n    assert re.search(rgx, c)",
            "file": "test_system_hardening.py"
          },
          {
            "type": "function",
            "name": "test_twofactor_disabled_on_tty",
            "code": "def test_twofactor_disabled_on_tty(host):\n    \"\"\"\n    Having 2FA on TTY logins is cumbersome on systems without encrypted drives.\n    Let's make sure this option is disabled!\n    \"\"\"\n\n    pam_auth_file = host.file(\"/etc/pam.d/common-auth\").content_string\n\n    assert \"auth required pam_google_authenticator.so\" not in pam_auth_file\n    assert \"pam_ecryptfs.so unwrap\" not in pam_auth_file",
            "file": "test_system_hardening.py"
          },
          {
            "type": "function",
            "name": "test_sshd_config",
            "code": "def test_sshd_config(host, sshd_opts):\n    \"\"\"\n    Let's ensure sshd does not fall back to password-based authentication\n    \"\"\"\n\n    sshd_config_file = host.file(\"/etc/ssh/sshd_config\").content_string\n\n    line = f\"{sshd_opts[0]} {sshd_opts[1]}\"\n    assert line in sshd_config_file",
            "file": "test_system_hardening.py"
          },
          {
            "type": "function",
            "name": "test_no_ecrypt_messages_in_logs",
            "code": "def test_no_ecrypt_messages_in_logs(host, logfile):\n    \"\"\"\n    Ensure pam_ecryptfs is removed from /etc/pam.d/common-auth : not only is\n    no longer needed, it causes error messages (see issue #3963)\n    \"\"\"\n    # re.escape will encode the . as `\\.` so the regex will not match the log entry itself\n    error_message = re.escape(\"pam_ecryptfs.so: cannot open shared object file\")\n    with host.sudo():\n        cmd = host.run(f\"grep '{error_message}' {logfile}\")\n        assert cmd.rc == 1, f\"error message found in {logfile}: {cmd.stdout}\"",
            "file": "test_system_hardening.py"
          },
          {
            "type": "function",
            "name": "test_unused_packages_are_removed",
            "code": "def test_unused_packages_are_removed(host, package):\n    \"\"\"Check if unused package is present\"\"\"\n    assert not host.package(package).is_installed",
            "file": "test_system_hardening.py"
          },
          {
            "type": "function",
            "name": "test_iptables_packages",
            "code": "def test_iptables_packages(host):\n    \"\"\"\n    Focal hosts should use iptables-persistent for enforcing\n    firewall config across reboots.\n    \"\"\"\n    assert host.package(\"iptables-persistent\").is_installed\n    assert not host.package(\"ufw\").is_installed",
            "file": "test_system_hardening.py"
          },
          {
            "type": "function",
            "name": "test_package_removal",
            "code": "def test_package_removal(host):\n    \"\"\"Test the securedrop-remove-packages service\"\"\"\n    if host.system_info.codename != \"focal\":\n        # ufw is uninstallable in noble because of the conflict\n        # with iptables-persistent\n        pytest.skip(\"only applicable/testable on focal\")\n\n    with host.sudo():\n        if not host.package(\"ufw\").is_installed:\n            cmd = host.run(\"apt-get install ufw --yes\")\n            assert cmd.rc == 0\n        assert host.file(\"/usr/sbin/ufw\").exists\n        # Trigger the service manually\n        cmd = host.run(\"systemctl start securedrop-remove-packages\")\n        assert cmd.rc == 0\n        # Wait for the unit to run\n        while host.service(\"securedrop-remove-packages\").is_running:\n            time.sleep(1)\n\n    assert not host.package(\"ufw\").is_installed",
            "file": "test_system_hardening.py"
          },
          {
            "type": "function",
            "name": "test_snapd_absent",
            "code": "def test_snapd_absent(host):\n    assert not host.file(\"/lib/systemd/system/snapd.service\").exists\n    assert not host.file(\"/etc/apparmor.d/usr.lib.snapd.snap-confine.real\").exists\n    assert not host.file(\"/usr/bin/snap\").exists\n    assert not host.file(\"/var/lib/snapd/snaps\").exists",
            "file": "test_system_hardening.py"
          },
          {
            "type": "function",
            "name": "test_ubuntu_pro_disabled",
            "code": "def test_ubuntu_pro_disabled(host):\n    with host.sudo():\n        cmd = host.run(\"systemctl status esm-cache\")\n        assert \"Loaded: masked\" in cmd.stdout\n        cmd = host.run(\"systemctl is-enabled ua-timer.timer\")\n        assert cmd.stdout.strip() == \"disabled\"",
            "file": "test_system_hardening.py"
          }
        ],
        "test_platform.py": [
          {
            "type": "function",
            "name": "test_ansible_version",
            "code": "def test_ansible_version(host):\n    \"\"\"\n    Check that a supported version of Ansible is being used.\n    \"\"\"\n    localhost = host.get_host(\"local://\")\n    c = localhost.check_output(\"ansible --version\")\n    assert c.startswith(\"ansible [core 2.\")",
            "file": "test_platform.py"
          },
          {
            "type": "function",
            "name": "test_platform",
            "code": "def test_platform(host):\n    \"\"\"\n    SecureDrop requires Ubuntu 20.04 (focal) or 24.04 (noble)\n    \"\"\"\n    assert host.system_info.type == \"linux\"\n    assert host.system_info.distribution == \"ubuntu\"\n    version = (host.system_info.codename, host.system_info.release)\n    assert version in {(\"focal\", \"20.04\"), (\"noble\", \"24.04\")}",
            "file": "test_platform.py"
          }
        ],
        "test_tor_mirror.py": [
          {
            "type": "function",
            "name": "test_tor_mirror_absent",
            "code": "def test_tor_mirror_absent(host, repo_file):\n    \"\"\"\n    Ensure that neither the Tor Project repo, nor the FPF mirror of the\n    Tor Project repo, tor-apt.freedom.press, are configured. We've moved\n    to hosting Tor packages inside the primary FPF apt repo.\n    \"\"\"\n    f = host.file(repo_file)\n    assert not f.exists",
            "file": "test_tor_mirror.py"
          },
          {
            "type": "function",
            "name": "test_tor_keyring_absent",
            "code": "def test_tor_keyring_absent(host):\n    \"\"\"\n    Tor packages are installed via the FPF apt mirror, and signed with the\n    SecureDrop Release Signing Key. As such, the official Tor public key\n    should *not* be present, since we don't want to install packages\n    from that source.\n    \"\"\"\n    # Can't use the TestInfra Package module to check state=absent,\n    # so let's check by shelling out to `dpkg -l`. Dpkg will automatically\n    # honor simple regex in package names.\n    package = \"deb.torproject.org-keyring\"\n    c = host.run(f\"dpkg -l {package}\")\n    assert c.rc == 1\n    error_text = f\"dpkg-query: no packages found matching {package}\"\n    assert error_text in c.stderr.strip()",
            "file": "test_tor_mirror.py"
          },
          {
            "type": "function",
            "name": "test_tor_mirror_fingerprint",
            "code": "def test_tor_mirror_fingerprint(host, tor_key_info):\n    \"\"\"\n    Legacy test. The Tor Project key was added to SecureDrop servers\n    via the `deb.torproject.org-keyring` package. Since FPF started mirroring\n    the official Tor apt repository, we no longer need the key to be present.\n\n    Since the `deb.torproject.org-keyring` package is installed on already\n    running instances, the public key will still be present. We'll need\n    to remove those packages separately.\n    \"\"\"\n    c = host.run(\"apt-key finger\")\n    assert c.rc == 0\n    assert tor_key_info not in c.stdout",
            "file": "test_tor_mirror.py"
          },
          {
            "type": "function",
            "name": "test_tor_repo_absent",
            "code": "def test_tor_repo_absent(host, repo_pattern):\n    \"\"\"\n    Ensure that no apt source list files contain the entry for\n    the official Tor apt repo, since we don't control issuing updates\n    in that repo. We're mirroring it to avoid breakage caused by\n    untested updates (which has broken prod twice to date).\n    \"\"\"\n    cmd = f\"grep -rF '{repo_pattern}' /etc/apt/\"\n    c = host.run(cmd)\n    # Grep returns non-zero when no matches, and we want no matches.\n    assert c.rc != 0\n    assert c.stdout == \"\"",
            "file": "test_tor_mirror.py"
          }
        ],
        "test_fpf_apt_repo.py": [
          {
            "type": "function",
            "name": "test_fpf_apt_repo_present",
            "code": "def test_fpf_apt_repo_present(host):\n    \"\"\"\n    Ensure the FPF apt repo, apt.freedom.press, is configured.\n    This repository is necessary for the SecureDrop Debian packages,\n    including:\n\n      * securedrop-app-code\n      * securedrop-keyring\n      * securedrop-grsec\n\n    Depending on the host, additional FPF-maintained packages will be\n    installed, e.g. for OSSEC. Install state for those packages\n    is tested separately.\n    \"\"\"\n\n    # If the var fpf_apt_repo_url test var is apt-test, validate that the\n    # apt repository is configured on the host\n    if test_vars.fpf_apt_repo_url == \"https://apt-test.freedom.press\":\n        f = host.file(\"/etc/apt/sources.list.d/apt_test_freedom_press.list\")\n    else:\n        f = host.file(\"/etc/apt/sources.list.d/apt_freedom_press.list\")\n    repo_regex = rf\"^deb \\[arch=amd64\\] {re.escape(test_vars.fpf_apt_repo_url)} \"\n    rf\"{re.escape(host.system_info.codename)} main$\"\n    assert f.contains(repo_regex)",
            "file": "test_fpf_apt_repo.py"
          },
          {
            "type": "function",
            "name": "test_fpf_apt_repo_fingerprint",
            "code": "def test_fpf_apt_repo_fingerprint(host):\n    \"\"\"\n    Ensure the FPF apt repo has the correct fingerprint on the associated\n    signing pubkey. Recent key rotations have taken place in:\n\n      * 2016-10\n      * 2021-06\n\n    The old key has been removed, so only the new key's fingerprint should be\n    returned.\n    \"\"\"\n\n    c = host.run(\"apt-key finger\")\n\n    fpf_gpg_pub_key_info_old = \"2224 5C81 E3BA EB41 38B3  6061 310F 5612 00F4 AD77\"\n    fpf_gpg_pub_key_info_new = \"2359 E653 8C06 13E6 5295  5E6C 188E DD3B 7B22 E6A3\"\n\n    assert c.rc == 0\n    assert fpf_gpg_pub_key_info_old not in c.stdout\n    assert fpf_gpg_pub_key_info_new in c.stdout",
            "file": "test_fpf_apt_repo.py"
          },
          {
            "type": "function",
            "name": "test_fpf_apt_repo_old_pubkeys_absent",
            "code": "def test_fpf_apt_repo_old_pubkeys_absent(host, old_pubkey):\n    \"\"\"\n    Ensure that expired (or about-to-expire) public keys for the FPF\n    apt repo are NOT present. Updates to the securedrop-keyring package\n    should enforce clobbering of old pubkeys, and this check will confirm\n    absence.\n    \"\"\"\n    c = host.run(\"apt-key finger\")\n    assert old_pubkey not in c.stdout",
            "file": "test_fpf_apt_repo.py"
          }
        ],
        "test_release_upgrades.py": [
          {
            "type": "function",
            "name": "test_release_manager_installed",
            "code": "def test_release_manager_installed(host):\n    \"\"\"\n    The securedrop-config package munges `do-release-upgrade` settings\n    that assume the release-upgrader logic is installed. On hardware\n    installs of Ubuntu, it is, but the VM images we use in CI may\n    remove it to make the boxes leaner.\n    \"\"\"\n    assert host.package(\"ubuntu-release-upgrader-core\").is_installed\n    assert host.exists(\"do-release-upgrade\")",
            "file": "test_release_upgrades.py"
          },
          {
            "type": "function",
            "name": "test_release_manager_upgrade_channel",
            "code": "def test_release_manager_upgrade_channel(host):\n    \"\"\"\n    Ensures that the `do-release-upgrade` command will not\n    suggest upgrades to a future LTS, until we test it and provide support.\n    \"\"\"\n    config_path = \"/etc/update-manager/release-upgrades\"\n    assert host.file(config_path).is_file\n\n    raw_output = host.check_output(f\"grep '^Prompt' {config_path}\")\n    _, channel = raw_output.split(\"=\")\n\n    assert channel == \"never\"",
            "file": "test_release_upgrades.py"
          },
          {
            "type": "function",
            "name": "test_migration_check",
            "code": "def test_migration_check(host):\n    \"\"\"Verify our migration check script works\"\"\"\n    if host.system_info.codename != \"focal\":\n        pytest.skip(\"only applicable/testable on focal\")\n\n    with host.sudo():\n        # remove state file so we can see if it works\n        if host.file(\"/etc/securedrop-noble-migration.json\").exists:\n            host.run(\"rm /etc/securedrop-noble-migration.json\")\n        cmd = host.run(\"systemctl start securedrop-noble-migration-check\")\n        assert cmd.rc == 0\n        while host.service(\"securedrop-noble-migration-check\").is_running:\n            time.sleep(1)\n\n        # JSON state file was created\n        assert host.file(\"/etc/securedrop-noble-migration.json\").exists\n\n        cmd = host.run(\"cat /etc/securedrop-noble-migration.json\")\n        assert cmd.rc == 0\n\n        contents = json.loads(cmd.stdout)\n        print(contents)\n        # The script did not error out\n        if \"error\" in contents:\n            # Run the script manually to get the error message\n            cmd = host.run(\"securedrop-noble-migration-check\")\n            print(cmd.stdout)\n            # We'll fail in the next line after this\n        assert \"error\" not in contents\n        # All the values should be True\n        assert all(contents.values())",
            "file": "test_release_upgrades.py"
          }
        ],
        "test_user_config.py": [
          {
            "type": "function",
            "name": "test_sudoers_config",
            "code": "def test_sudoers_config(host):\n    \"\"\"\n    Check sudoers config for passwordless sudo via group membership,\n    as well as environment-related hardening.\n    \"\"\"\n    f = host.file(\"/etc/sudoers\")\n    assert f.is_file\n    assert f.user == \"root\"\n    assert f.group == \"root\"\n    assert f.mode == 0o440\n\n    # Restrictive file mode requires sudo for reading, so let's\n    # read once and store the content in a var.\n    with host.sudo():\n        sudoers_config = f.content_string\n\n    # Using re.search rather than `f.contains` since the basic grep\n    # matching doesn't support PCRE, so `\\s` won't work.\n    assert re.search(r\"^Defaults\\s+env_reset$\", sudoers_config, re.M)\n    assert re.search(r\"^Defaults\\s+env_reset$\", sudoers_config, re.M)\n    assert re.search(r\"^Defaults\\s+mail_badpass$\", sudoers_config, re.M)\n    assert re.search(\n        r'Defaults\\s+secure_path=\"/usr/local/sbin:'\n        r'/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"',\n        sudoers_config,\n        re.M,\n    )\n    assert re.search(r\"^%sudo\\s+ALL=\\(ALL\\)\\s+NOPASSWD:\\s+ALL$\", sudoers_config, re.M)\n    assert re.search(r\"Defaults:%sudo\\s+!requiretty\", sudoers_config, re.M)",
            "file": "test_user_config.py"
          },
          {
            "type": "function",
            "name": "test_sudoers_tmux_env",
            "code": "def test_sudoers_tmux_env(host):\n    \"\"\"\n    Ensure SecureDrop-specific bashrc additions are present.\n    This checks for automatic tmux start on interactive shells.\n    If we switch to byobu, we can set `byobu-enabled` and check\n    the corresponding settings there.\n    \"\"\"\n\n    host_file = host.file(\"/etc/profile.d/securedrop_additions.sh\")\n    expected_content = textwrap.dedent(\n        \"\"\"\\\n        [[ $- != *i* ]] && return\n\n        which tmux >/dev/null 2>&1 || return\n\n        tmux_attach_via_proc() {\n            # If the tmux package is upgraded during the lifetime of a\n            # session, attaching with the new binary can fail due to different\n            # protocol versions. This function attaches using the reference to\n            # the old executable found in the /proc tree of an existing\n            # session.\n            pid=$(pgrep --newest tmux)\n            if test -n \"$pid\"\n            then\n                /proc/$pid/exe -u attach\n            fi\n            return 1\n        }\n\n        if test -z \"$TMUX\"\n        then\n            (tmux -u attach || tmux_attach_via_proc || tmux -u new-session)\n        fi\"\"\"\n    )\n\n    assert host_file.content_string.strip() == expected_content",
            "file": "test_user_config.py"
          },
          {
            "type": "function",
            "name": "test_tmux_installed",
            "code": "def test_tmux_installed(host):\n    \"\"\"\n    Ensure the `tmux` package is present, since it's required for the user env.\n    When running an interactive SSH session over Tor, tmux should be started\n    automatically, to prevent problems if the connection is broken\n    unexpectedly, as sometimes happens over Tor. The Admin will be able to\n    reconnect to the running tmux session and review command output.\n    \"\"\"\n    assert host.package(\"tmux\").is_installed",
            "file": "test_user_config.py"
          },
          {
            "type": "function",
            "name": "test_sudoers_tmux_env_deprecated",
            "code": "def test_sudoers_tmux_env_deprecated(host):\n    \"\"\"\n    Previous version of the Ansible config set the tmux config\n    in per-user ~/.bashrc, which was redundant. The config has\n    since moved to /etc/profile.d, to provide a single point of\n    update that applies to all users. Let's make sure that the\n    old setting isn't still active.\n    \"\"\"\n\n    f = host.file(f\"/home/{sdvars.admin_user}/.bashrc\")\n    assert not f.contains(r\"^. \\/etc\\/bashrc\\.securedrop_additions$\")",
            "file": "test_user_config.py"
          }
        ],
        "test_ip6tables.py": [
          {
            "type": "function",
            "name": "test_ip6tables_drop_everything_focal",
            "code": "def test_ip6tables_drop_everything_focal(host):\n    \"\"\"\n    Ensures that IPv6 firewall settings are inaccessible,\n    due to fully disabling IPv6 functionality at boot-time,\n    via boot options.\n    \"\"\"\n    if host.system_info.codename != \"focal\":\n        # On noble, ip6tables works despite IPv6 being disabled\n        pytest.skip(\"ip6tables behavior changed post-focal\")\n    with host.sudo():\n        c = host.run(\"ip6tables -S\")\n        assert c.rc != 0\n        assert c.stdout == \"\"",
            "file": "test_ip6tables.py"
          },
          {
            "type": "function",
            "name": "test_ipv6_addresses_absent",
            "code": "def test_ipv6_addresses_absent(host):\n    \"\"\"\n    Ensure that no IPv6 addresses are assigned to interfaces.\n    \"\"\"\n    with host.sudo():\n        c = host.check_output(\"ip -6 addr\")\n        assert c == \"\"",
            "file": "test_ip6tables.py"
          }
        ],
        "test_grsecurity.py": [
          {
            "type": "function",
            "name": "test_ssh_motd_disabled",
            "code": "def test_ssh_motd_disabled(host):\n    \"\"\"\n    Ensure the SSH MOTD (Message of the Day) is disabled.\n    Grsecurity balks at Ubuntu's default MOTD.\n    \"\"\"\n    f = host.file(\"/etc/pam.d/sshd\")\n    assert f.is_file\n    assert not f.contains(r\"pam\\.motd\")",
            "file": "test_grsecurity.py"
          },
          {
            "type": "function",
            "name": "test_grsecurity_apt_packages",
            "code": "def test_grsecurity_apt_packages(host):\n    \"\"\"\n    Ensure the grsecurity-related apt packages are present on the system.\n    Includes the FPF-maintained metapackage, as well as paxctl, for managing\n    PaX flags on binaries.\n    \"\"\"\n    assert host.package(\"securedrop-grsec\").is_installed",
            "file": "test_grsecurity.py"
          },
          {
            "type": "function",
            "name": "test_generic_kernels_absent",
            "code": "def test_generic_kernels_absent(host, package):\n    \"\"\"\n    Ensure the default Ubuntu-provided kernel packages are absent.\n    In the past, conflicting version numbers have caused machines\n    to reboot into a non-grsec kernel due to poor handling of\n    GRUB_DEFAULT logic. Removing the vendor-provided kernel packages\n    prevents accidental boots into non-grsec kernels.\n    \"\"\"\n    # Can't use the TestInfra Package module to check state=absent,\n    # so let's check by shelling out to `dpkg -l`. Dpkg will automatically\n    # honor simple regex in package names.\n    c = host.run(f\"dpkg -l {package}\")\n    assert c.rc == 1\n    error_text = f\"dpkg-query: no packages found matching {package}\"\n    assert error_text in c.stderr.strip()",
            "file": "test_grsecurity.py"
          },
          {
            "type": "function",
            "name": "test_grsecurity_lock_file",
            "code": "def test_grsecurity_lock_file(host):\n    \"\"\"\n    Ensure system is rerunning a grsecurity kernel by testing for the\n    `grsec_lock` file, which is automatically created by grsecurity.\n    \"\"\"\n    with host.sudo():\n        f = host.file(\"/proc/sys/kernel/grsecurity/grsec_lock\")\n        assert f.mode == 0o600\n        assert f.user == \"root\"\n        assert f.size == 0",
            "file": "test_grsecurity.py"
          },
          {
            "type": "function",
            "name": "test_grsecurity_kernel_is_running",
            "code": "def test_grsecurity_kernel_is_running(host):\n    \"\"\"\n    Make sure the currently running kernel is our grsec kernel.\n    \"\"\"\n    c = host.run(\"uname -r\")\n    assert c.stdout.strip().endswith(\"-grsec-securedrop\")",
            "file": "test_grsecurity.py"
          },
          {
            "type": "function",
            "name": "test_grsecurity_sysctl_options",
            "code": "def test_grsecurity_sysctl_options(host, sysctl_opt):\n    \"\"\"\n    Check that the grsecurity-related sysctl options are set correctly.\n    In production the RWX logging is disabled, to reduce log noise.\n    \"\"\"\n    with host.sudo():\n        assert host.sysctl(sysctl_opt[0]) == sysctl_opt[1]",
            "file": "test_grsecurity.py"
          },
          {
            "type": "function",
            "name": "test_grsecurity_paxtest",
            "code": "def test_grsecurity_paxtest(host):\n    \"\"\"\n    Check that paxtest reports the expected mitigations. These are\n    \"Killed\" for most of the checks, with the notable exception of the\n    memcpy ones. Only newer versions of paxtest will fail the latter,\n    regardless of kernel.\n    \"\"\"\n    if host.system_info.codename == \"noble\":\n        pytest.skip(\"FIXME: paxtest is returning unclear output on noble\")\n    if not host.exists(\"/usr/bin/paxtest\"):\n        warnings.warn(\"Installing paxtest to run kernel tests\", stacklevel=1)\n        with host.sudo():\n            # Stop u-u if it's running\n            host.run(\"systemctl stop unattended-upgrades\")\n            assert host.run(\"apt-get update\").rc == 0\n            tries = 0\n            while not host.exists(\"/usr/bin/paxtest\"):\n                cmd = host.run(\"apt-get install --yes paxtest\")\n                if cmd.rc == 0:\n                    continue\n\n                if \"Could not get lock /var/lib/dpkg/lock-frontend\" in cmd.stderr:\n                    tries += 1\n                    if tries > 5:\n                        # Give up, this assertion will cause the test to fail\n                        assert cmd.rc == 0\n                    warnings.warn(\n                        f\"Installing paxtest failed, retrying in 10 seconds (retry #{tries})\",\n                        stacklevel=1,\n                    )\n                    time.sleep(10)\n    try:\n        with host.sudo():\n            # Log to /tmp to avoid cluttering up /root.\n            paxtest_results = host.check_output(\"paxtest blackhat /tmp/paxtest.log\")\n\n        # Select only predictably formatted lines; omit\n        # the guesses, since the number of bits can vary\n        paxtest_results = (\n            \"\\n\".join(\n                line\n                for line in paxtest_results.split(\"\\n\")\n                if line.startswith((\"Executable\", \"Return\"))\n            )\n            + \"\\n\"\n        )\n        print(\"paxtest results:\\n\" + paxtest_results)\n\n        if host.system_info.codename == \"focal\":\n            paxtest_expected = PAXTEST_FOCAL\n        elif host.system_info.codename == \"noble\":\n            paxtest_expected = PAXTEST_NOBLE\n        else:\n            pytest.fail(f\"Unexpected codename {host.system_info.codename}\")\n\n        assert paxtest_results == paxtest_expected\n    finally:\n        with host.sudo():\n            host.run(\"apt-get remove -y paxtest\")",
            "file": "test_grsecurity.py"
          },
          {
            "type": "function",
            "name": "test_apt_autoremove",
            "code": "def test_apt_autoremove(host):\n    \"\"\"\n    Ensure old packages have been autoremoved.\n    \"\"\"\n    c = host.run(\"apt-get --dry-run autoremove\")\n    assert c.rc == 0\n    assert \"The following packages will be REMOVED\" not in c.stdout",
            "file": "test_grsecurity.py"
          },
          {
            "type": "function",
            "name": "test_paxctl",
            "code": "def test_paxctl(host):\n    \"\"\"\n    As of Focal, paxctl is not used, and shouldn't be installed.\n    \"\"\"\n    p = host.package(\"paxctl\")\n    assert not p.is_installed",
            "file": "test_grsecurity.py"
          },
          {
            "type": "function",
            "name": "test_paxctld_focal",
            "code": "def test_paxctld_focal(host):\n    \"\"\"\n    Focal-specific paxctld config checks.\n    Ensures paxctld is running and enabled, and relevant\n    exemptions are present in the config file.\n    \"\"\"\n    assert host.package(\"paxctld\").is_installed\n    f = host.file(\"/etc/paxctld.conf\")\n    assert f.is_file\n\n    s = host.service(\"paxctld\")\n    assert s.is_enabled\n    assert s.is_running\n\n    # The securedrop-grsec metapackage will copy the config\n    # out of /opt/ to ensure the file is always clobbered on changes.\n    assert host.file(\"/opt/securedrop/paxctld.conf\").is_file\n\n    hostname = host.check_output(\"hostname -s\")\n    assert (\"app\" in hostname) or (\"mon\" in hostname)\n\n    # Under Focal, apache2 pax flags managed by securedrop-grsec metapackage.\n    # Both hosts, app & mon, should have the same exemptions. Check precedence\n    # between install-local-packages & apt-test repo for securedrop-grsec.\n    if \"app\" in hostname:\n        assert f.contains(\"^/usr/sbin/apache2\\tm\")",
            "file": "test_grsecurity.py"
          },
          {
            "type": "function",
            "name": "test_wireless_disabled_in_kernel_config",
            "code": "def test_wireless_disabled_in_kernel_config(host, kernel_opts):\n    \"\"\"\n    Kernel modules for wireless are blacklisted, but we go one step further and\n    remove wireless support from the kernel. Let's make sure wireless is\n    disabled in the running kernel config!\n    \"\"\"\n    kernel_version = host.run(\"uname -r\").stdout.strip()\n    with host.sudo():\n        kernel_config_path = f\"/boot/config-{kernel_version}\"\n        kernel_config = host.file(kernel_config_path).content_string\n\n        line = f\"# CONFIG_{kernel_opts} is not set\"\n        assert line in kernel_config or kernel_opts not in kernel_config",
            "file": "test_grsecurity.py"
          },
          {
            "type": "function",
            "name": "test_kernel_options_enabled_config",
            "code": "def test_kernel_options_enabled_config(host, kernel_opts):\n    \"\"\"\n    Tests kernel config for options that should be enabled\n    \"\"\"\n\n    kernel_version = host.run(\"uname -r\").stdout.strip()\n    with host.sudo():\n        kernel_config_path = f\"/boot/config-{kernel_version}\"\n        kernel_config = host.file(kernel_config_path).content_string\n\n        line = f\"{kernel_opts}=y\"\n        assert line in kernel_config",
            "file": "test_grsecurity.py"
          },
          {
            "type": "function",
            "name": "test_mds_mitigations_and_smt_disabled",
            "code": "def test_mds_mitigations_and_smt_disabled(host):\n    \"\"\"\n    Ensure that full mitigations are in place for MDS\n    see https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html\n    \"\"\"\n\n    with host.sudo():\n        grub_config_path = \"/boot/grub/grub.cfg\"\n        grub_config = host.file(grub_config_path)\n\n        assert grub_config.contains(\"mds=full,nosmt\")",
            "file": "test_grsecurity.py"
          },
          {
            "type": "function",
            "name": "test_kernel_boot_options",
            "code": "def test_kernel_boot_options(host):\n    \"\"\"\n    Ensure command-line options for currently booted kernel are set.\n    \"\"\"\n    with host.sudo():\n        f = host.file(\"/proc/cmdline\")\n        boot_opts = f.content_string.split()\n    assert \"noefi\" in boot_opts\n    if host.system_info.codename == \"focal\":\n        assert \"ipv6.disable=1\" in boot_opts",
            "file": "test_grsecurity.py"
          },
          {
            "type": "function",
            "name": "test_ipv6_disabled",
            "code": "def test_ipv6_disabled(host):\n    \"\"\"\n    Verify that IPv6 is disabled at the kernel level\n    \"\"\"\n    assert not host.file(\"/proc/sys/net/ipv6\").exists",
            "file": "test_grsecurity.py"
          }
        ]
      },
      "mon": {
        "test_ossec_ruleset.py": [
          {
            "type": "function",
            "name": "test_ossec_false_positives_suppressed",
            "code": "def test_ossec_false_positives_suppressed(host, log_event):\n    with host.sudo():\n        c = host.run('echo \"{}\" | /var/ossec/bin/ossec-logtest'.format(log_event[\"alert\"]))\n        assert \"Alert to be generated\" not in c.stderr",
            "file": "test_ossec_ruleset.py"
          },
          {
            "type": "function",
            "name": "test_ossec_expected_alerts_are_present",
            "code": "def test_ossec_expected_alerts_are_present(host, log_event):\n    with host.sudo():\n        c = host.run('echo \"{}\" | /var/ossec/bin/ossec-logtest'.format(log_event[\"alert\"]))\n        assert \"Alert to be generated\" in c.stderr\n        alert_level = alert_level_regex.findall(c.stderr)[0]\n        assert alert_level == log_event[\"level\"]\n        rule_id = rule_id_regex.findall(c.stderr)[0]\n        assert rule_id == log_event[\"rule_id\"]",
            "file": "test_ossec_ruleset.py"
          },
          {
            "type": "function",
            "name": "test_noble_migration_check",
            "code": "def test_noble_migration_check(host):\n    \"\"\"\n    Verify the noble migration check does not generate OSSEC notifications\n\n    Regression check for <https://github.com/freedomofpress/securedrop/issues/7393>;\n    we are assuming that no checks will fail; otherwise\n    test_release_upgrades.py::test_migration_check would've already failed\n    \"\"\"\n    if host.system_info.codename != \"focal\":\n        pytest.skip(\"only applicable/testable on focal\")\n\n    with host.sudo():\n        cmd = host.run(\"securedrop-noble-migration-check | /var/ossec/bin/ossec-logtest\")\n        assert \"Alert to be generated\" not in cmd.stderr",
            "file": "test_ossec_ruleset.py"
          }
        ],
        "test_ossec_server.py": [
          {
            "type": "function",
            "name": "test_ossec_connectivity",
            "code": "def test_ossec_connectivity(host):\n    \"\"\"\n    Ensure ossec-server machine has active connection to the ossec-agent.\n    The ossec service will report all available agents, and we can inspect\n    that list to make sure it's the host we expect.\n    \"\"\"\n    desired_output = \"{}-{} is available.\".format(\n        securedrop_test_vars.app_hostname, os.environ.get(\"APP_IP\", securedrop_test_vars.app_ip)\n    )\n    with host.sudo():\n        c = host.check_output(\"/var/ossec/bin/list_agents -a\")\n        assert c == desired_output",
            "file": "test_ossec_server.py"
          },
          {
            "type": "function",
            "name": "test_ossec_service_start_style",
            "code": "def test_ossec_service_start_style(host):\n    \"\"\"\n    Ensure that the OSSEC services are managed by systemd.\n    \"\"\"\n    with host.sudo():\n        c = host.check_output(\"systemctl status ossec\")\n        assert \"/etc/systemd/system/ossec.service\" in c",
            "file": "test_ossec_server.py"
          },
          {
            "type": "function",
            "name": "test_ossec_keyfiles",
            "code": "def test_ossec_keyfiles(host, keyfile):\n    \"\"\"\n    Ensure that the OSSEC transport key pair exists. These keys are used\n    to protect the connection between the ossec-server and ossec-agent.\n\n    All this check does in confirm they're present, it doesn't perform any\n    matching checks to validate the configuration.\n    \"\"\"\n    with host.sudo():\n        f = host.file(keyfile)\n        assert f.is_file\n        # The postinst scripts in the OSSEC deb packages set 440 on the\n        # keyfiles; the Ansible config should be updated to do the same.\n        assert f.mode == 0o440\n        assert f.user == \"root\"\n        assert f.group == \"ossec\"",
            "file": "test_ossec_server.py"
          },
          {
            "type": "function",
            "name": "test_procmail_log",
            "code": "def test_procmail_log(host):\n    \"\"\"\n    Ensure procmail log file exist with proper ownership.\n    Only the ossec user should have read/write permissions.\n    \"\"\"\n    with host.sudo():\n        f = host.file(\"/var/log/procmail.log\")\n        assert f.is_file\n        assert f.user == \"ossec\"\n        assert f.group == \"root\"\n        assert f.mode == 0o660",
            "file": "test_ossec_server.py"
          },
          {
            "type": "function",
            "name": "test_ossec_authd",
            "code": "def test_ossec_authd(host):\n    \"\"\"Ensure that authd is not running\"\"\"\n    with host.sudo():\n        c = host.run(\"pgrep ossec-authd\")\n        assert c.stdout == \"\"\n        assert c.rc != 0",
            "file": "test_ossec_server.py"
          },
          {
            "type": "function",
            "name": "test_hosts_files",
            "code": "def test_hosts_files(host):\n    \"\"\"Ensure host files mapping are in place\"\"\"\n    f = host.file(\"/etc/hosts\")\n\n    app_ip = os.environ.get(\"APP_IP\", securedrop_test_vars.app_ip)\n    app_host = securedrop_test_vars.app_hostname\n\n    assert f.contains(\"^127.0.0.1.*localhost\")\n    assert f.contains(rf\"^{app_ip}\\s*{app_host}$\")",
            "file": "test_ossec_server.py"
          },
          {
            "type": "function",
            "name": "test_ossec_log_contains_no_malformed_events",
            "code": "def test_ossec_log_contains_no_malformed_events(host):\n    \"\"\"\n    Ensure the OSSEC log reports no errors for incorrectly formatted\n    messages. These events indicate that the OSSEC server failed to decrypt\n    the event sent by the OSSEC agent, which implies a misconfiguration,\n    likely the IPv4 address or keypair differing from what's declared.\n\n    Documentation regarding this error message can be found at:\n    http://ossec-docs.readthedocs.io/en/latest/faq/unexpected.html#id4\n    \"\"\"\n    with host.sudo():\n        f = host.file(\"/var/ossec/logs/ossec.log\")\n        assert not f.contains(\"ERROR: Incorrectly formated message from\")",
            "file": "test_ossec_server.py"
          },
          {
            "type": "function",
            "name": "test_regression_hosts",
            "code": "def test_regression_hosts(host):\n    \"\"\"Regression test to check for duplicate entries.\"\"\"\n    assert host.check_output(\"uniq --repeated /etc/hosts\") == \"\"",
            "file": "test_ossec_server.py"
          }
        ],
        "test_mon_network.py": [
          {
            "type": "function",
            "name": "test_mon_iptables_rules",
            "code": "def test_mon_iptables_rules(host):\n    local = host.get_host(\"local://\")\n\n    # Build a dict of variables to pass to jinja for iptables comparison\n    kwargs = dict(\n        app_ip=os.environ.get(\"APP_IP\", securedrop_test_vars.app_ip),\n        default_interface=host.check_output(\"ip r | head -n 1 | awk '{ print $5 }'\"),\n        tor_user_id=host.check_output(\"id -u debian-tor\"),\n        time_service_user=host.check_output(\"id -u systemd-timesync\"),\n        ssh_group_gid=host.check_output(\"getent group sdssh | cut -d: -f3\"),\n        postfix_user_id=host.check_output(\"id -u postfix\"),\n        dns_server=securedrop_test_vars.dns_server,\n    )\n\n    # Required for testing under Qubes.\n    if local.interface(\"eth0\").exists:\n        kwargs[\"ssh_ip\"] = local.interface(\"eth0\").addresses[0]\n\n    # Build iptables scrape cmd, purge comments + counters\n    iptables = r\"iptables-save | sed 's/ \\[[0-9]*\\:[0-9]*\\]//g' | egrep -v '^#'\"\n    environment = os.environ.get(\"SECUREDROP_TESTINFRA_TARGET_HOST\", \"staging\")\n    iptables_file = f\"{os.path.dirname(os.path.abspath(__file__))}/iptables-mon-{environment}.j2\"\n\n    # template out a local iptables jinja file\n    jinja_iptables = Template(open(iptables_file).read())\n    iptables_expected = jinja_iptables.render(**kwargs)\n\n    with host.sudo():\n        # Actually run the iptables scrape command\n        iptables = host.check_output(iptables)\n        # print diff comparison (only shows up in pytests if test fails or\n        # verbosity turned way up)\n        for iptablesdiff in difflib.context_diff(\n            iptables_expected.split(\"\\n\"), iptables.split(\"\\n\")\n        ):\n            print(iptablesdiff)\n        # Conduct the string comparison of the expected and actual iptables\n        # ruleset\n        assert iptables_expected == iptables",
            "file": "test_mon_network.py"
          },
          {
            "type": "function",
            "name": "test_listening_ports",
            "code": "def test_listening_ports(host, ossec_service):\n    \"\"\"\n    Ensure the OSSEC-related services are listening on the\n    expected sockets. Services to check include ossec-remoted\n    and ossec-authd. Helper services such as postfix are checked\n    separately.\n\n    Note that the SSH check will fail if run against a prod host, due\n    to the SSH-over-Tor strategy. We can port the parametrized values\n    to config test YAML vars at that point.\n    \"\"\"\n    socket = \"{proto}://{host}:{port}\".format(**ossec_service)\n    with host.sudo():\n        assert host.socket(socket).is_listening == ossec_service[\"listening\"]",
            "file": "test_mon_network.py"
          }
        ],
        "test_postfix.py": [
          {
            "type": "function",
            "name": "test_postfix_headers",
            "code": "def test_postfix_headers(host, header):\n    \"\"\"\n    Ensure postfix header filters are set correctly. Common mail headers\n    are stripped by default to avoid leaking metadata about the instance.\n    Message body is always encrypted prior to sending.\n    \"\"\"\n    f = host.file(\"/etc/postfix/header_checks\")\n    assert f.is_file\n    assert f.mode == 0o644\n    regex = f\"^{re.escape(header)}$\"\n    assert re.search(regex, f.content_string, re.M)",
            "file": "test_postfix.py"
          },
          {
            "type": "function",
            "name": "test_postfix_generic_maps",
            "code": "def test_postfix_generic_maps(host):\n    \"\"\"\n    Check configuration of Postfix generic map when sasl_domain is set\n    and ossec_from_address is not specified.\n    \"\"\"\n    assert host.file(\"/etc/postfix/generic\").exists\n    assert host.file(\"/etc/postfix/generic\").contains(\n        f\"^ossec@{securedrop_test_vars.monitor_hostname} {securedrop_test_vars.sasl_username}@\"\n        f\"{securedrop_test_vars.sasl_domain}\"\n    )\n    assert host.file(\"/etc/postfix/main.cf\").contains(\"^smtp_generic_maps\")\n    assert host.file(\"/etc/postfix/main.cf\").contains(\n        \"^smtpd_recipient_restrictions = reject_unauth_destination\"\n    )",
            "file": "test_postfix.py"
          },
          {
            "type": "function",
            "name": "test_postfix_service",
            "code": "def test_postfix_service(host):\n    \"\"\"\n    Check Postfix service. Postfix is used to deliver OSSEC alerts via\n    encrypted email. On staging hosts, Postfix is disabled, due to lack\n    of SASL authentication credentials, but on prod hosts it should run.\n    \"\"\"\n    # Elevated privileges are required to read Postfix service info,\n    # specifically `/var/spool/postfix/pid/master.pid`.\n    with host.sudo():\n        postfix = host.service(\"postfix\")\n        assert postfix.is_running == securedrop_test_vars.postfix_enabled\n        assert postfix.is_enabled == securedrop_test_vars.postfix_enabled\n\n        socket = host.socket(\"tcp://127.0.0.1:25\")\n        assert socket.is_listening == securedrop_test_vars.postfix_enabled",
            "file": "test_postfix.py"
          }
        ]
      },
      "app": {
        "test_smoke.py": [
          {
            "type": "function",
            "name": "test_interface_up",
            "code": "def test_interface_up(host, name, url, curl_flags, expected):\n    \"\"\"\n    Ensure the respective interface is up with HTTP 200 if not, we try our\n    best to grab the error log and print it via an intentionally failed\n    assertion.\n    \"\"\"\n    response = host.run(f\"curl -{curl_flags}i {url}\").stdout\n    if \"200 OK\" not in response:\n        # Try to grab the log and print it via a failed assertion\n        with host.sudo():\n            f = host.file(f\"/var/log/apache2/{name}-error.log\")\n            if f.exists:\n                assert \"nopenopenope\" in f.content_string\n    assert \"200 OK\" in response\n    assert expected in response",
            "file": "test_smoke.py"
          },
          {
            "type": "function",
            "name": "test_redwood",
            "code": "def test_redwood(host):\n    \"\"\"\n    Verify the redwood wheel was built and installed properly and basic\n    functionality works\n    \"\"\"\n    response = host.run(\n        \"/opt/venvs/securedrop-app-code/bin/python3 -c \"\n        \"'import redwood; import json; print(\"\n        'json.dumps(redwood.generate_source_key_pair(\"abcde\", \"test@invalid\")))\\''\n    )\n    parsed = json.loads(response.stdout)\n    assert \"-----BEGIN PGP PUBLIC KEY BLOCK-----\" in parsed[0]\n    assert \"-----BEGIN PGP PRIVATE KEY BLOCK-----\" in parsed[1]\n    assert len(parsed[2]) == 40",
            "file": "test_smoke.py"
          },
          {
            "type": "function",
            "name": "test_weak_submission_key",
            "code": "def test_weak_submission_key(host):\n    \"\"\"\n    If the Submission Key is weak (e.g. has a SHA-1 signature),\n    the JI should be down (500) and SI will return a 503.\n    \"\"\"\n    with host.sudo():\n        old_public_key = host.file(JOURNALIST_PUB).content_string\n        try:\n            # Install a weak key\n            set_public_key(host, WEAK_KEY_CONTENTS)\n            assert host.run(\"systemctl restart apache2\").rc == 0\n            # Now try to hit the JI\n            response = host.run(\"curl -Li http://localhost:8080/\").stdout\n            assert \"HTTP/1.1 500 Internal Server Error\" in response\n            # Now hit the SI\n            response = host.run(\"curl -i http://localhost:80/\").stdout\n            assert \"HTTP/1.1 503 SERVICE UNAVAILABLE\" in response  # Flask shouts\n            assert \"We're sorry, our SecureDrop is currently offline.\" in response\n\n        finally:\n            set_public_key(host, old_public_key)\n            assert host.run(\"systemctl restart apache2\").rc == 0",
            "file": "test_smoke.py"
          },
          {
            "type": "function",
            "name": "set_public_key",
            "code": "def set_public_key(host, pubkey: str) -> None:\n    \"\"\"apparently testinfra doesn't provide a function to write a file?\"\"\"\n    res = host.run(\n        f\"/usr/bin/python3 -c \"\n        f'\\'import pathlib; pathlib.Path(\"{JOURNALIST_PUB}\")'\n        f'.write_text(\"\"\"{pubkey}\"\"\".strip())\\''\n    )\n    print(res.stderr)\n    assert res.rc == 0",
            "file": "test_smoke.py"
          }
        ],
        "test_backup.py": [
          {
            "type": "function",
            "name": "test_backup",
            "code": "def test_backup(host):\n    \"\"\"Create a backup and verify it contains expected files\"\"\"\n\n    with host.sudo():\n        result = host.run(\"securedrop-app-backup.py --dest /tmp\")\n        assert result.rc == 0\n        tarball = result.stdout.strip()\n        # looks like a file path\n        assert tarball.endswith(\".tar.gz\")\n        assert host.file(f\"/tmp/{tarball}\").exists\n        # list files in the tarball\n        contains = host.run(f\"tar -tzf /tmp/{tarball}\")\n        assert contains.rc == 0\n        contains_list = contains.stdout.splitlines()\n        assert \"var/www/securedrop/config.py\" in contains_list\n        assert \"etc/tor/torrc\" in contains_list\n        assert \"var/lib/tor/services/\" in contains_list\n        # cleanup\n        cleanup = host.run(f\"rm /tmp/{tarball}\")\n        assert cleanup.rc == 0",
            "file": "test_backup.py"
          }
        ],
        "test_tor_config.py": [
          {
            "type": "function",
            "name": "test_tor_packages",
            "code": "def test_tor_packages(host, package):\n    \"\"\"\n    Ensure Tor packages are installed. Does not include the Tor keyring\n    package, since we want only the SecureDrop Release Signing Key\n    to be used even for Tor packages.\n    \"\"\"\n    assert host.package(package).is_installed",
            "file": "test_tor_config.py"
          },
          {
            "type": "function",
            "name": "test_tor_service_running",
            "code": "def test_tor_service_running(host):\n    \"\"\"\n    Ensure Tor is running and enabled. Tor is required for SSH access,\n    so it must be enabled to start on boot.\n    \"\"\"\n    s = host.service(\"tor\")\n    assert s.is_running\n    assert s.is_enabled",
            "file": "test_tor_config.py"
          },
          {
            "type": "function",
            "name": "test_tor_torrc_options",
            "code": "def test_tor_torrc_options(host, torrc_option):\n    \"\"\"\n    Check for required options in the system Tor config file.\n    These options should be present regardless of machine role,\n    meaning both Application and Monitor server will have them.\n\n    Separate tests will check for specific onion services.\n    \"\"\"\n    f = host.file(\"/etc/tor/torrc\")\n    assert f.is_file\n    assert f.user == \"debian-tor\"\n    assert f.mode == 0o644\n    assert f.contains(f\"^{torrc_option}$\")",
            "file": "test_tor_config.py"
          },
          {
            "type": "function",
            "name": "test_tor_torrc_sandbox",
            "code": "def test_tor_torrc_sandbox(host):\n    \"\"\"\n    Check that the `Sandbox 1` declaration is not present in the torrc.\n    The torrc manpage states this option is experimental, and although we\n    use it already on Tails workstations, further testing is required\n    before we push it out to servers. See issues #944 and #1969.\n    \"\"\"\n    f = host.file(\"/etc/tor/torrc\")\n    # Only `Sandbox 1` will enable, but make sure there are zero occurrences\n    # of \"Sandbox\", otherwise we may have a regression somewhere.\n    assert not f.contains(\"^.*Sandbox.*$\")",
            "file": "test_tor_config.py"
          },
          {
            "type": "function",
            "name": "test_tor_v2_onion_url_file_absent",
            "code": "def test_tor_v2_onion_url_file_absent(host):\n    v2_url_filepath = \"/var/lib/securedrop/source_v2_url\"\n    with host.sudo():\n        f = host.file(v2_url_filepath)\n        assert not f.exists",
            "file": "test_tor_config.py"
          },
          {
            "type": "function",
            "name": "test_tor_v3_onion_url_readable_by_app",
            "code": "def test_tor_v3_onion_url_readable_by_app(host):\n    v3_url_filepath = \"/var/lib/securedrop/source_v3_url\"\n    with host.sudo():\n        f = host.file(v3_url_filepath)\n        assert f.is_file\n        assert f.user == \"www-data\"\n        assert f.mode == 0o644\n        assert re.search(r\"^[a-z0-9]{56}\\.onion$\", f.content_string)",
            "file": "test_tor_config.py"
          }
        ],
        "test_appenv.py": [
          {
            "type": "function",
            "name": "test_app_pip_deps",
            "code": "def test_app_pip_deps(host, exp_pip_pkg):\n    \"\"\"Ensure expected package versions are installed\"\"\"\n    cmd = (\n        \"{}/bin/python3 -c \\\"from importlib.metadata import version; print(version('{}'))\\\"\".format(\n            sdvars.securedrop_venv, exp_pip_pkg[\"name\"]\n        )\n    )\n    result = host.run(cmd)\n    assert result.stdout.strip() == exp_pip_pkg[\"version\"]",
            "file": "test_appenv.py"
          },
          {
            "type": "function",
            "name": "test_app_wsgi",
            "code": "def test_app_wsgi(host):\n    \"\"\"ensure logging is enabled for source interface in staging\"\"\"\n    f = host.file(\"/var/www/source.wsgi\")\n    with host.sudo():\n        assert f.is_file\n        assert f.mode == 0o644\n        assert f.user == \"root\"\n        assert f.group == \"root\"\n        assert f.contains(\"^import logging$\")\n        assert f.contains(r\"^logging\\.basicConfig(stream=sys\\.stderr)$\")",
            "file": "test_appenv.py"
          },
          {
            "type": "function",
            "name": "test_pidfile",
            "code": "def test_pidfile(host):\n    \"\"\"ensure there are no pid files\"\"\"\n    assert not host.file(\"/tmp/journalist.pid\").exists\n    assert not host.file(\"/tmp/source.pid\").exists",
            "file": "test_appenv.py"
          },
          {
            "type": "function",
            "name": "test_app_directories",
            "code": "def test_app_directories(host, app_dir, owner):\n    \"\"\"ensure securedrop app directories exist with correct permissions\"\"\"\n    f = host.file(app_dir)\n    mode = 0o755 if owner == \"root\" else 0o700\n    with host.sudo():\n        assert f.is_directory\n        assert f.user == owner\n        assert f.group == owner\n        assert f.mode == mode",
            "file": "test_appenv.py"
          },
          {
            "type": "function",
            "name": "test_config_permissions",
            "code": "def test_config_permissions(host):\n    \"\"\"ensure config.py has correct permissions\"\"\"\n    f = host.file(\"/var/www/securedrop/config.py\")\n    with host.sudo():\n        assert f.is_file\n        assert f.user == \"root\"\n        assert f.group == \"www-data\"\n        assert f.mode == 0o640",
            "file": "test_appenv.py"
          },
          {
            "type": "function",
            "name": "test_app_code_pkg",
            "code": "def test_app_code_pkg(host):\n    \"\"\"ensure securedrop-app-code package is installed\"\"\"\n    assert host.package(\"securedrop-app-code\").is_installed",
            "file": "test_appenv.py"
          },
          {
            "type": "function",
            "name": "test_app_code_venv",
            "code": "def test_app_code_venv(host):\n    \"\"\"\n    Ensure the securedrop-app-code virtualenv is correct.\n    \"\"\"\n    cmd = (\n        f\"test -z $VIRTUAL_ENV && . {sdvars.securedrop_venv}/bin/activate && \"\n        + f'test \"$VIRTUAL_ENV\" = \"{sdvars.securedrop_venv}\" '\n    )\n\n    result = host.run(cmd)\n    assert result.rc == 0",
            "file": "test_appenv.py"
          },
          {
            "type": "function",
            "name": "test_supervisor_not_installed",
            "code": "def test_supervisor_not_installed(host):\n    \"\"\"ensure supervisor package is not installed\"\"\"\n    assert host.package(\"supervisor\").is_installed is False",
            "file": "test_appenv.py"
          },
          {
            "type": "function",
            "name": "test_gpg_key_in_keyring",
            "code": "def test_gpg_key_in_keyring(host):\n    \"\"\"ensure test gpg key is present in app keyring\"\"\"\n    with host.sudo(sdvars.securedrop_user):\n        c = host.run(\"gpg --homedir /var/lib/securedrop/keys \" \"--list-keys 28271441\")\n        assert \"2013-10-12\" in c.stdout\n        assert \"28271441\" in c.stdout",
            "file": "test_appenv.py"
          },
          {
            "type": "function",
            "name": "test_ensure_logo",
            "code": "def test_ensure_logo(host):\n    \"\"\"ensure default logo header file exists\"\"\"\n    f = host.file(f\"{sdvars.securedrop_code}/static/i/logo.png\")\n    with host.sudo():\n        assert f.mode == 0o644\n        assert f.user == \"root\"\n        assert f.group == \"root\"",
            "file": "test_appenv.py"
          },
          {
            "type": "function",
            "name": "test_empty_crontabs",
            "code": "def test_empty_crontabs(host, user):\n    \"\"\"Ensure root + www-data crontabs are empty\"\"\"\n    with host.sudo():\n        # Returns exit code 1 when it's empty\n        host.run_expect([1], f\"crontab -u {user} -l\")",
            "file": "test_appenv.py"
          }
        ],
        "test_apparmor.py": [
          {
            "type": "function",
            "name": "test_apparmor_pkg",
            "code": "def test_apparmor_pkg(host, pkg):\n    \"\"\"Apparmor package dependencies\"\"\"\n    assert host.package(pkg).is_installed",
            "file": "test_apparmor.py"
          },
          {
            "type": "function",
            "name": "test_apparmor_enabled",
            "code": "def test_apparmor_enabled(host):\n    \"\"\"Check that apparmor is enabled\"\"\"\n    with host.sudo():\n        assert host.run(\"aa-status --enabled\").rc == 0",
            "file": "test_apparmor.py"
          },
          {
            "type": "function",
            "name": "test_apparmor_apache_capabilities",
            "code": "def test_apparmor_apache_capabilities(host, cap):\n    \"\"\"check for exact list of expected app-armor capabilities for apache2\"\"\"\n    c = host.run(\n        r\"perl -nE '/^\\s+capability\\s+(\\w+),$/ && say $1' /etc/apparmor.d/usr.sbin.apache2\"\n    )\n    assert cap in c.stdout",
            "file": "test_apparmor.py"
          },
          {
            "type": "function",
            "name": "test_apparmor_apache_exact_capabilities",
            "code": "def test_apparmor_apache_exact_capabilities(host):\n    \"\"\"ensure no extra capabilities are defined for apache2\"\"\"\n    c = host.check_output(\"grep -ic capability /etc/apparmor.d/usr.sbin.apache2\")\n    assert str(len(apache2_capabilities)) == c",
            "file": "test_apparmor.py"
          },
          {
            "type": "function",
            "name": "test_apparmor_tor_capabilities",
            "code": "def test_apparmor_tor_capabilities(host, cap):\n    \"\"\"check for exact list of expected app-armor capabilities for Tor\"\"\"\n    c = host.run(r\"perl -nE '/^\\s+capability\\s+(\\w+),$/ && say $1' /etc/apparmor.d/usr.sbin.tor\")\n    assert cap in c.stdout",
            "file": "test_apparmor.py"
          },
          {
            "type": "function",
            "name": "test_apparmor_tor_exact_capabilities",
            "code": "def test_apparmor_tor_exact_capabilities(host):\n    \"\"\"ensure no extra capabilities are defined for Tor\"\"\"\n    c = host.check_output(\"grep -ic capability \" \"/etc/apparmor.d/usr.sbin.tor\")\n    assert str(len(tor_capabilities)) == c",
            "file": "test_apparmor.py"
          },
          {
            "type": "function",
            "name": "test_apparmor_ensure_not_disabled",
            "code": "def test_apparmor_ensure_not_disabled(host):\n    \"\"\"\n    Explicitly check that there are no profiles in /etc/apparmor.d/disabled\n    \"\"\"\n    with host.sudo():\n        # Check that there are no apparmor profiles disabled because the folder is missing\n        folder = host.file(\"/etc/apparmor.d/disabled\")\n        assert not folder.exists",
            "file": "test_apparmor.py"
          },
          {
            "type": "function",
            "name": "test_apparmor_enforced",
            "code": "def test_apparmor_enforced(host, aa_enforced):\n    # FIXME: don't use awk, post-process it in Python\n    awk = \"awk '/[0-9]+ profiles.*enforce./\" \"{flag=1;next}/^[0-9]+.*/{flag=0}flag'\"\n    with host.sudo():\n        c = host.check_output(f\"aa-status | {awk}\")\n        assert aa_enforced in c",
            "file": "test_apparmor.py"
          },
          {
            "type": "function",
            "name": "test_aastatus_unconfined",
            "code": "def test_aastatus_unconfined(host):\n    \"\"\"Ensure that there are no processes that are unconfined but have\n    a profile\"\"\"\n\n    # There should be 0 unconfined processes.\n    expected_unconfined = 0\n\n    unconfined_chk = str(\n        f\"{expected_unconfined} processes are unconfined but have\" \" a profile defined\"\n    )\n    with host.sudo():\n        aa_status_output = host.check_output(\"aa-status\")\n        assert unconfined_chk in aa_status_output",
            "file": "test_apparmor.py"
          },
          {
            "type": "function",
            "name": "test_aa_no_denies_in_syslog",
            "code": "def test_aa_no_denies_in_syslog(host):\n    \"\"\"Ensure that there are no apparmor denials in syslog\"\"\"\n    with host.sudo():\n        f = host.file(\"/var/log/syslog\")\n        lines = f.content_string.splitlines()\n    # syslog is very big, just print the denial lines\n    found = []\n    for line in lines:\n        if 'apparmor=\"DENIED\"' in line:\n            if 'profile=\"ubuntu_pro_apt_news\"' in line:\n                # This failure is a known bug in Ubuntu that happens before SD\n                # is installed and disables ubuntu-pro stuff. See\n                # <https://github.com/freedomofpress/securedrop/issues/7385>.\n                continue\n            found.append(line)\n    assert found == []",
            "file": "test_apparmor.py"
          }
        ],
        "test_ossec_agent.py": [
          {
            "type": "function",
            "name": "test_hosts_files",
            "code": "def test_hosts_files(host):\n    \"\"\"Ensure host files mapping are in place\"\"\"\n    f = host.file(\"/etc/hosts\")\n\n    mon_ip = os.environ.get(\"MON_IP\", sdvars.mon_ip)\n    mon_host = sdvars.monitor_hostname\n\n    assert f.contains(r\"^127.0.0.1\\s*localhost\")\n    assert f.contains(rf\"^{mon_ip}\\s*{mon_host}\\s*securedrop-monitor-server-alias$\")",
            "file": "test_ossec_agent.py"
          },
          {
            "type": "function",
            "name": "test_ossec_service_start_style",
            "code": "def test_ossec_service_start_style(host):\n    \"\"\"\n    Ensure that the OSSEC services are managed by systemd.\n    \"\"\"\n    with host.sudo():\n        c = host.check_output(\"systemctl status ossec\")\n        assert \"/etc/systemd/system/ossec.service\" in c",
            "file": "test_ossec_agent.py"
          },
          {
            "type": "function",
            "name": "test_hosts_duplicate",
            "code": "def test_hosts_duplicate(host):\n    \"\"\"Regression test for duplicate entries\"\"\"\n    assert host.check_output(\"uniq --repeated /etc/hosts\") == \"\"",
            "file": "test_ossec_agent.py"
          },
          {
            "type": "function",
            "name": "test_ossec_agent_installed",
            "code": "def test_ossec_agent_installed(host):\n    \"\"\"Check that ossec-agent package is present\"\"\"\n    assert host.package(\"securedrop-ossec-agent\").is_installed",
            "file": "test_ossec_agent.py"
          },
          {
            "type": "function",
            "name": "test_ossec_keyfile_present",
            "code": "def test_ossec_keyfile_present(host):\n    \"\"\"ensure client keyfile for ossec-agent is present\"\"\"\n    pattern = \"^1024 {} {} [0-9a-f]{{64}}$\".format(\n        sdvars.app_hostname, os.environ.get(\"APP_IP\", sdvars.app_ip)\n    )\n    regex = re.compile(pattern)\n\n    with host.sudo():\n        f = host.file(\"/var/ossec/etc/client.keys\")\n        assert f.exists\n        assert f.mode == 0o644\n        assert f.user == \"root\"\n        assert f.group == \"ossec\"\n        assert f.content_string\n        assert bool(re.search(regex, f.content))",
            "file": "test_ossec_agent.py"
          }
        ],
        "test_tor_hidden_services.py": [
          {
            "type": "function",
            "name": "test_tor_service_directories",
            "code": "def test_tor_service_directories(host, tor_service):\n    \"\"\"\n    Check mode and ownership on Tor service directories.\n    \"\"\"\n    with host.sudo():\n        f = host.file(\"/var/lib/tor/services/{}\".format(tor_service[\"name\"]))\n        assert f.is_directory\n        assert f.mode == 0o700\n        assert f.user == \"debian-tor\"\n        assert f.group == \"debian-tor\"",
            "file": "test_tor_hidden_services.py"
          },
          {
            "type": "function",
            "name": "test_tor_service_hostnames",
            "code": "def test_tor_service_hostnames(host, tor_service):\n    \"\"\"\n    Check contents of Tor service hostname file. For v3 onion services,\n    the file should contain only hostname (.onion URL).\n    \"\"\"\n    # Declare regex only for THS; we'll build regex for ATHS only if\n    # necessary, since we won't have the required values otherwise.\n    ths_hostname_regex = r\"[a-z0-9]{16}\\.onion\"\n    ths_hostname_regex_v3 = r\"[a-z0-9]{56}\\.onion\"\n\n    with host.sudo():\n        f = host.file(\"/var/lib/tor/services/{}/hostname\".format(tor_service[\"name\"]))\n        assert f.is_file\n        assert f.mode == 0o600\n        assert f.user == \"debian-tor\"\n        assert f.group == \"debian-tor\"\n\n        # All hostnames should contain at *least* the hostname.\n        assert re.search(ths_hostname_regex, f.content_string)\n\n        if tor_service[\"authenticated\"] and tor_service[\"version\"] == 3:\n            # For authenticated version 3 onion services, the authorized_client\n            # directory will exist and contain a file called client.auth.\n            client_auth = host.file(\n                \"/var/lib/tor/services/{}/authorized_clients/client.auth\".format(\n                    tor_service[\"name\"]\n                )\n            )\n            assert client_auth.is_file\n        else:\n            assert re.search(f\"^{ths_hostname_regex_v3}$\", f.content_string)",
            "file": "test_tor_hidden_services.py"
          },
          {
            "type": "function",
            "name": "test_tor_services_config",
            "code": "def test_tor_services_config(host, tor_service):\n    \"\"\"\n    Ensure torrc file contains relevant lines for onion service declarations.\n    All onion services must include:\n\n      * HiddenServiceDir\n      * HiddenServicePort\n    \"\"\"\n    f = host.file(\"/etc/tor/torrc\")\n    dir_regex = \"HiddenServiceDir /var/lib/tor/services/{}\".format(tor_service[\"name\"])\n    # We need at least one port, but it may be used for both config values.\n    # On the Journalist Interface, we reuse the \"80\" remote port but map it to\n    # a different local port, so Apache can listen on several sockets.\n    remote_port = tor_service[\"ports\"][0]\n    try:\n        local_port = tor_service[\"ports\"][1]\n    except IndexError:\n        local_port = remote_port\n\n    port_regex = f\"HiddenServicePort {remote_port} 127.0.0.1:{local_port}\"\n\n    assert f.contains(f\"^{dir_regex}$\")\n    assert f.contains(f\"^{port_regex}$\")\n\n    # Check for block in file, to ensure declaration order\n    service_regex = \"\\n\".join([dir_regex, port_regex])\n    assert service_regex in f.content_string",
            "file": "test_tor_hidden_services.py"
          }
        ],
        "test_app_network.py": [
          {
            "type": "function",
            "name": "test_app_iptables_rules",
            "code": "def test_app_iptables_rules(host):\n    local = host.get_host(\"local://\")\n\n    # Build a dict of variables to pass to jinja for iptables comparison\n    kwargs = dict(\n        mon_ip=os.environ.get(\"MON_IP\", securedrop_test_vars.mon_ip),\n        default_interface=host.check_output(\"ip r | head -n 1 | \" \"awk '{ print $5 }'\"),\n        tor_user_id=host.check_output(\"id -u debian-tor\"),\n        time_service_user=host.check_output(\"id -u systemd-timesync\"),\n        securedrop_user_id=host.check_output(\"id -u www-data\"),\n        ssh_group_gid=host.check_output(\"getent group sdssh | cut -d: -f3\"),\n        dns_server=securedrop_test_vars.dns_server,\n    )\n\n    # Required for testing under Qubes.\n    if local.interface(\"eth0\").exists:\n        kwargs[\"ssh_ip\"] = local.interface(\"eth0\").addresses[0]\n\n    # Build iptables scrape cmd, purge comments + counters\n    iptables = r\"iptables-save | sed 's/ \\[[0-9]*\\:[0-9]*\\]//g' | egrep -v '^#'\"\n    environment = os.environ.get(\"SECUREDROP_TESTINFRA_TARGET_HOST\", \"staging\")\n    iptables_file = f\"{os.path.dirname(os.path.abspath(__file__))}/iptables-app-{environment}.j2\"\n\n    # template out a local iptables jinja file\n    jinja_iptables = Template(open(iptables_file).read())\n    iptables_expected = jinja_iptables.render(**kwargs)\n\n    with host.sudo():\n        # Actually run the iptables scrape command\n        iptables = host.check_output(iptables)\n        # print diff comparison (only shows up in pytests if test fails or\n        # verbosity turned way up)\n        for iptablesdiff in difflib.context_diff(\n            iptables_expected.split(\"\\n\"), iptables.split(\"\\n\")\n        ):\n            print(iptablesdiff)\n        # Conduct the string comparison of the expected and actual iptables\n        # ruleset\n        assert iptables_expected == iptables",
            "file": "test_app_network.py"
          }
        ],
        "test_redis.py": [
          {
            "type": "function",
            "name": "extract_password",
            "code": "def extract_password(host) -> str:\n    f = host.file(\"/var/www/securedrop/rq_config.py\")\n    with host.sudo():\n        contents = f.content_string\n    print(contents)\n    return re.search(r\"^REDIS_PASSWORD = ['\\\"](.*?)['\\\"]$\", contents).group(1)",
            "file": "test_redis.py"
          },
          {
            "type": "function",
            "name": "assert_password_works",
            "code": "def assert_password_works(host, password):\n    # Run an authenticated PING\n    response = host.run(\n        f'bash -c \\'echo \"PING\" | REDISCLI_AUTH=\"{password}\" redis-cli\\''\n    ).stdout.strip()\n    assert response == \"PONG\"",
            "file": "test_redis.py"
          },
          {
            "type": "function",
            "name": "test_auth_required",
            "code": "def test_auth_required(host):\n    \"\"\"\n    Verify the redis server requires authentication\n    \"\"\"\n    response = host.run(\"bash -c 'echo \\\"PING\\\" | redis-cli'\").stdout.strip()\n    assert response == \"NOAUTH Authentication required.\"",
            "file": "test_redis.py"
          },
          {
            "type": "function",
            "name": "test_password_works",
            "code": "def test_password_works(host):\n    \"\"\"\n    Verify the redis password works\n    \"\"\"\n    f = host.file(\"/var/www/securedrop/rq_config.py\")\n    with host.sudo():\n        # First let's check file permissions\n        assert f.is_file\n        assert f.user == \"root\"\n        assert f.group == \"www-data\"\n        assert f.mode == 0o640\n    # Get the password\n    password = extract_password(host)\n    assert_password_works(host, password)",
            "file": "test_redis.py"
          },
          {
            "type": "function",
            "name": "test_check",
            "code": "def test_check(host):\n    \"\"\"All the redis passwords should be in sync\"\"\"\n    with host.sudo():\n        assert host.run(\"securedrop-set-redis-auth.py check\").rc == 0",
            "file": "test_redis.py"
          },
          {
            "type": "function",
            "name": "test_check_fail",
            "code": "def test_check_fail(host):\n    with host.sudo():\n        old = extract_password(host)\n        try:\n            cmd = host.run(\"echo 'REDIS_PASSWORD = \\\"wrong\\\"' > /var/www/securedrop/rq_config.py\")\n            assert cmd.rc == 0\n            assert host.run(\"securedrop-set-redis-auth.py check\").rc == 1\n            # Verify reset-if-needed will fix it\n            assert host.run(\"securedrop-set-redis-auth.py reset-if-needed\").rc == 0\n            assert host.run(\"systemctl restart redis-server\").rc == 0\n            assert host.run(\"systemctl restart apache2\").rc == 0\n            new = extract_password(host)\n            assert old != new, \"password changed\"\n            assert_password_works(host, new)\n            with pytest.raises(AssertionError):\n                # Old password no longer works\n                assert_password_works(host, old)\n        finally:\n            # Reset to cleanup\n            assert host.run(\"securedrop-set-redis-auth.py reset\").rc == 0\n            assert host.run(\"systemctl restart redis-server\").rc == 0\n            assert host.run(\"systemctl restart apache2\").rc == 0",
            "file": "test_redis.py"
          },
          {
            "type": "function",
            "name": "test_reset",
            "code": "def test_reset(host):\n    original = extract_password(host)\n    with host.sudo():\n        assert host.run(\"securedrop-set-redis-auth.py reset\").rc == 0\n        assert host.run(\"systemctl restart redis-server\").rc == 0\n        assert host.run(\"systemctl restart apache2\").rc == 0\n\n    new = extract_password(host)\n    assert new != original, \"password changed\"\n\n    assert_password_works(host, new)\n    with pytest.raises(AssertionError):\n        # Old password no longer works\n        assert_password_works(host, original)\n\n    # Now verify that reset-if-needed does nothing\n    with host.sudo():\n        assert host.run(\"securedrop-set-redis-auth.py reset-if-needed\").rc == 0\n    current = extract_password(host)\n    assert current == new, \"password not changed since it wasn't needed\"",
            "file": "test_redis.py"
          }
        ],
        "apache": {
          "test_apache_service.py": [
            {
              "type": "function",
              "name": "test_apache_enabled_sites",
              "code": "def test_apache_enabled_sites(host, apache_site):\n    \"\"\"\n    Ensure the Source and Journalist interfaces are enabled.\n    \"\"\"\n    with host.sudo():\n        c = host.run(f\"/usr/sbin/a2query -s {apache_site}\")\n        assert f\"{apache_site} (enabled\" in c.stdout\n        assert c.rc == 0",
              "file": "test_apache_service.py"
            },
            {
              "type": "function",
              "name": "test_apache_disabled_sites",
              "code": "def test_apache_disabled_sites(host, apache_site):\n    \"\"\"\n    Ensure the default HTML document root is disabled.\n    \"\"\"\n    c = host.run(f\"a2query -s {apache_site}\")\n    assert f\"No site matches {apache_site} (disabled\" in c.stderr\n    assert c.rc == 32",
              "file": "test_apache_service.py"
            },
            {
              "type": "function",
              "name": "test_apache_service",
              "code": "def test_apache_service(host):\n    \"\"\"\n    Ensure Apache service is running.\n    \"\"\"\n    # sudo is necessary to run `service apache2 status`, otherwise\n    # the service is falsely reported as not running.\n    with host.sudo():\n        s = host.service(\"apache2\")\n        assert s.is_running\n        assert s.is_enabled",
              "file": "test_apache_service.py"
            },
            {
              "type": "function",
              "name": "test_apache_user",
              "code": "def test_apache_user(host):\n    \"\"\"\n    Ensure user account for running application code is configured correctly.\n    \"\"\"\n    u = host.user(\"www-data\")\n    assert u.exists\n    assert u.home == \"/var/www\"\n    assert u.shell == \"/usr/sbin/nologin\"",
              "file": "test_apache_service.py"
            },
            {
              "type": "function",
              "name": "test_apache_listening",
              "code": "def test_apache_listening(host, port):\n    \"\"\"\n    Ensure Apache is listening on proper ports and interfaces.\n    In staging, expect the service to be bound to 0.0.0.0,\n    but in prod, it should be restricted to 127.0.0.1.\n    \"\"\"\n    # sudo is necessary to read from /proc/net/tcp.\n    with host.sudo():\n        s = host.socket(f\"tcp://{securedrop_test_vars.apache_listening_address}:{port}\")\n        assert s.is_listening",
              "file": "test_apache_service.py"
            }
          ],
          "test_apache_system_config.py": [
            {
              "type": "function",
              "name": "test_apache_apt_packages",
              "code": "def test_apache_apt_packages(host, package):\n    \"\"\"\n    Ensure required Apache packages are installed.\n    \"\"\"\n    assert host.package(package).is_installed",
              "file": "test_apache_system_config.py"
            },
            {
              "type": "function",
              "name": "test_apache_security_config_deprecated",
              "code": "def test_apache_security_config_deprecated(host):\n    \"\"\"\n    Ensure that /etc/apache2/security is absent, since it was setting\n    redundant options already present in /etc/apache2/apache2.conf.\n    See #643 for discussion.\n    \"\"\"\n    assert not host.file(\"/etc/apache2/security\").exists",
              "file": "test_apache_system_config.py"
            },
            {
              "type": "function",
              "name": "test_apache_config_settings",
              "code": "def test_apache_config_settings(host, apache_opt):\n    \"\"\"\n    Check required Apache config settings for general server.\n    These checks do not target individual interfaces, e.g.\n    Source versus Document Interface, and instead apply to\n    Apache more generally.\n    \"\"\"\n    f = host.file(\"/etc/apache2/apache2.conf\")\n    assert f.is_file\n    assert f.user == \"root\"\n    assert f.group == \"root\"\n    assert f.mode == 0o644\n    assert re.search(f\"^{re.escape(apache_opt)}$\", f.content_string, re.M)",
              "file": "test_apache_system_config.py"
            },
            {
              "type": "function",
              "name": "test_apache_ports_config",
              "code": "def test_apache_ports_config(host, port):\n    \"\"\"\n    Ensure Apache ports config items, which specify how the\n    Source and Document Interfaces are configured to be served\n    over Tor. On staging hosts, they will listen on any interface,\n    to permit port forwarding for local testing, but in production,\n    they're restricted to localhost, for use over Tor.\n    \"\"\"\n    f = host.file(\"/etc/apache2/ports.conf\")\n    assert f.is_file\n    assert f.user == \"root\"\n    assert f.group == \"root\"\n    assert f.mode == 0o644\n\n    listening_regex = f\"^Listen {re.escape(securedrop_test_vars.apache_listening_address)}:{port}$\"\n    assert f.contains(listening_regex)",
              "file": "test_apache_system_config.py"
            },
            {
              "type": "function",
              "name": "test_apache_modules_present",
              "code": "def test_apache_modules_present(host, apache_module):\n    \"\"\"\n    Ensure presence of required Apache modules. Application will not work\n    correctly if these are missing. A separate test will check for\n    disabled modules.\n    \"\"\"\n    with host.sudo():\n        c = host.run(f\"/usr/sbin/a2query -m {apache_module}\")\n        assert f\"{apache_module} (enabled\" in c.stdout\n        assert c.rc == 0",
              "file": "test_apache_system_config.py"
            },
            {
              "type": "function",
              "name": "test_apache_modules_absent",
              "code": "def test_apache_modules_absent(host, apache_module):\n    \"\"\"\n    Ensure absence of unwanted Apache modules. Application does not require\n    these modules, so they should be disabled to reduce attack surface.\n    A separate test will check for disabled modules.\n    \"\"\"\n    with host.sudo():\n        c = host.run(f\"/usr/sbin/a2query -m {apache_module}\")\n        assert f\"No module matches {apache_module} (disabled\" in c.stderr\n        assert c.rc == 32",
              "file": "test_apache_system_config.py"
            },
            {
              "type": "function",
              "name": "test_apache_logfiles_present",
              "code": "def test_apache_logfiles_present(host, logfile):\n    \"\"\" \"\n    Ensure that whitelisted Apache log files for the Source and Journalist\n    Interfaces are present. In staging, we permit a \"source-error\" log,\n    but on prod even that is not allowed. A separate test will confirm\n    absence of unwanted logfiles by comparing the file count in the\n    Apache log directory.\n    \"\"\"\n    # We need elevated privileges to read files inside /var/log/apache2\n    with host.sudo():\n        f = host.file(logfile)\n        assert f.is_file\n        assert f.user == \"root\"",
              "file": "test_apache_system_config.py"
            },
            {
              "type": "function",
              "name": "test_apache_logfiles_no_extras",
              "code": "def test_apache_logfiles_no_extras(host):\n    \"\"\"\n    Ensure that no unwanted Apache logfiles are present. Complements the\n    `test_apache_logfiles_present` config test. Here, we confirm that the\n    total number of Apache logfiles exactly matches the number permitted\n    on the Application Server, whether staging or prod.\n    Long-running instances may have rotated and gzipped logfiles, so this\n    test should only look for files ending in '.log'.\n    \"\"\"\n    # We need elevated privileges to read files inside /var/log/apache2\n    with host.sudo():\n        c = host.run(\"find /var/log/apache2 -mindepth 1 -name '*.log' | wc -l\")\n        assert int(c.stdout) == len(securedrop_test_vars.allowed_apache_logfiles)",
              "file": "test_apache_system_config.py"
            }
          ],
          "test_apache_source_interface.py": [
            {
              "type": "function",
              "name": "test_apache_headers_source_interface",
              "code": "def test_apache_headers_source_interface(host, header, value):\n    \"\"\"\n    Test for expected headers in Source Interface vhost config.\n    \"\"\"\n    f = host.file(\"/etc/apache2/sites-available/source.conf\")\n    assert f.is_file\n    assert f.user == \"root\"\n    assert f.group == \"root\"\n    assert f.mode == 0o644\n    header_unset = f\"Header onsuccess unset {header}\"\n    assert f.contains(header_unset)\n    header_set = f'Header always set {header} \"{value}\"'\n    assert f.contains(header_set)",
              "file": "test_apache_source_interface.py"
            },
            {
              "type": "function",
              "name": "test_apache_config_source_interface",
              "code": "def test_apache_config_source_interface(host, apache_opt):\n    \"\"\"\n    Ensure the necessary Apache settings for serving the application\n    are in place. Some values will change according to the host,\n    e.g. app-staging versus app-prod will have different listening\n    addresses, depending on whether Tor connections are forced.\n\n    These checks apply only to the Source Interface, used by Sources.\n    \"\"\"\n    f = host.file(\"/etc/apache2/sites-available/source.conf\")\n    assert f.is_file\n    assert f.user == \"root\"\n    assert f.group == \"root\"\n    assert f.mode == 0o644\n    regex = f\"^{re.escape(apache_opt)}$\"\n    assert re.search(regex, f.content_string, re.M)",
              "file": "test_apache_source_interface.py"
            },
            {
              "type": "function",
              "name": "test_apache_config_source_interface_headers_per_distro",
              "code": "def test_apache_config_source_interface_headers_per_distro(host):\n    \"\"\"\n    During migration to Focal, we updated the syntax for forcing HTTP headers.\n    \"\"\"\n    f = host.file(\"/etc/apache2/sites-available/source.conf\")\n    assert f.contains(\"Header onsuccess unset X-Frame-Options\")\n    assert f.contains('Header always set X-Frame-Options \"DENY\"')\n    assert f.contains(\"Header onsuccess unset Referrer-Policy\")\n    assert f.contains('Header always set Referrer-Policy \"same-origin\"')\n    assert f.contains(\"Header edit Set-Cookie ^(.*)$ $1;HttpOnly\")",
              "file": "test_apache_source_interface.py"
            },
            {
              "type": "function",
              "name": "test_apache_config_source_interface_access_control",
              "code": "def test_apache_config_source_interface_access_control(host, apache_opt):\n    \"\"\"\n    Verifies the access control directives for the Source Interface.\n    \"\"\"\n    f = host.file(\"/etc/apache2/sites-available/source.conf\")\n    regex = f\"^{re.escape(apache_opt)}$\"\n    assert re.search(regex, f.content_string, re.M)",
              "file": "test_apache_source_interface.py"
            }
          ],
          "test_apache_journalist_interface.py": [
            {
              "type": "function",
              "name": "test_apache_headers_journalist_interface",
              "code": "def test_apache_headers_journalist_interface(host, header, value):\n    \"\"\"\n    Test for expected headers in Document Interface vhost config.\n    \"\"\"\n    f = host.file(\"/etc/apache2/sites-available/journalist.conf\")\n    assert f.is_file\n    assert f.user == \"root\"\n    assert f.group == \"root\"\n    assert f.mode == 0o644\n    header_unset = f\"Header onsuccess unset {header}\"\n    assert f.contains(header_unset)\n    header_set = f'Header always set {header} \"{value}\"'\n    assert f.contains(header_set)",
              "file": "test_apache_journalist_interface.py"
            },
            {
              "type": "function",
              "name": "test_apache_config_journalist_interface",
              "code": "def test_apache_config_journalist_interface(host, apache_opt):\n    \"\"\"\n    Ensure the necessary Apache settings for serving the application\n    are in place. Some values will change according to the host,\n    e.g. app-staging versus app-prod will have different listening\n    addresses, depending on whether Tor connections are forced.\n\n    These checks apply only to the Document Interface, used by Journalists.\n    \"\"\"\n    f = host.file(\"/etc/apache2/sites-available/journalist.conf\")\n    assert f.is_file\n    assert f.user == \"root\"\n    assert f.group == \"root\"\n    assert f.mode == 0o644\n    regex = f\"^{re.escape(apache_opt)}$\"\n    assert re.search(regex, f.content_string, re.M)",
              "file": "test_apache_journalist_interface.py"
            },
            {
              "type": "function",
              "name": "test_apache_config_journalist_interface_headers_per_distro",
              "code": "def test_apache_config_journalist_interface_headers_per_distro(host):\n    \"\"\"\n    During migration to Focal, we updated the syntax for forcing HTTP headers.\n    \"\"\"\n    f = host.file(\"/etc/apache2/sites-available/journalist.conf\")\n    assert f.contains(\"Header onsuccess unset X-Frame-Options\")\n    assert f.contains('Header always set X-Frame-Options \"DENY\"')\n    assert f.contains(\"Header onsuccess unset Referrer-Policy\")\n    assert f.contains('Header always set Referrer-Policy \"no-referrer\"')\n    assert f.contains(\"Header edit Set-Cookie ^(.*)$ $1;HttpOnly\")",
              "file": "test_apache_journalist_interface.py"
            },
            {
              "type": "function",
              "name": "test_apache_logging_journalist_interface",
              "code": "def test_apache_logging_journalist_interface(host):\n    \"\"\"\n    Check that logging is configured correctly for the Journalist Interface.\n    The actions of Journalists are logged by the system, so that an Admin can\n    investigate incidents and track access.\n\n    Logs were broken for some period of time, logging only \"combined\" to\n    the logfile, rather than the combined LogFormat intended.\n    \"\"\"\n    # sudo is necessary because /var/log/apache2 is mode 0750.\n    with host.sudo():\n        f = host.file(\"/var/log/apache2/journalist-access.log\")\n        assert f.is_file\n        if f.size == 0:\n            # If the file is empty, the Journalist Interface hasn't been used\n            # yet, so make a quick GET request local to the host so we can\n            # validate the log entry.\n            host.check_output(\"curl http://127.0.0.1:8080\")\n\n        assert f.size > 0  # Make sure something was logged.\n        # LogFormat declaration was missing, so track regressions that log\n        # just the string \"combined\" and nothing else.\n        assert not f.contains(\"^combined$\")\n        assert f.contains(\"GET\")",
              "file": "test_apache_journalist_interface.py"
            },
            {
              "type": "function",
              "name": "test_apache_config_journalist_interface_access_control",
              "code": "def test_apache_config_journalist_interface_access_control(host, apache_opt):\n    \"\"\"\n    Verifies the access control directives for the Journalist Interface.\n    \"\"\"\n    f = host.file(\"/etc/apache2/sites-available/journalist.conf\")\n    regex = f\"^{re.escape(apache_opt)}$\"\n    assert re.search(regex, f.content_string, re.M)",
              "file": "test_apache_journalist_interface.py"
            }
          ]
        }
      },
      "app-code": {
        "test_securedrop_source_deleter_configuration.py": [
          {
            "type": "function",
            "name": "test_securedrop_source_deleter_service",
            "code": "def test_securedrop_source_deleter_service(host):\n    \"\"\"\n    Verify configuration of securedrop_source_deleter systemd service.\n    \"\"\"\n    service_file = \"/lib/systemd/system/securedrop_source_deleter.service\"\n    expected_content = \"\\n\".join(\n        [\n            \"[Unit]\",\n            \"Description=SecureDrop Source deleter\",\n            \"\",\n            \"[Service]\",\n            \"Type=exec\",\n            f'Environment=PYTHONPATH=\"{securedrop_test_vars.securedrop_code}:{securedrop_test_vars.securedrop_venv_site_packages}\"',\n            f\"ExecStart={securedrop_test_vars.securedrop_venv_bin}/python /var/www/securedrop/\"\n            \"scripts/source_deleter --interval 10\",\n            \"PrivateDevices=yes\",\n            \"PrivateTmp=yes\",\n            \"ProtectSystem=full\",\n            \"ReadOnlyDirectories=/\",\n            f\"ReadWriteDirectories={securedrop_test_vars.securedrop_data}\",\n            \"Restart=always\",\n            \"RestartSec=10s\",\n            \"UMask=077\",\n            f\"User={securedrop_test_vars.securedrop_user}\",\n            f\"WorkingDirectory={securedrop_test_vars.securedrop_code}\",\n            \"\",\n            \"[Install]\",\n            \"WantedBy=multi-user.target\\n\",\n        ]\n    )\n\n    f = host.file(service_file)\n    assert f.is_file\n    assert f.mode == 0o644\n    assert f.user == \"root\"\n    assert f.group == \"root\"\n    assert f.content_string == expected_content\n\n    s = host.service(\"securedrop_source_deleter\")\n    assert s.is_enabled\n    assert s.is_running",
            "file": "test_securedrop_source_deleter_configuration.py"
          }
        ],
        "test_securedrop_rqworker.py": [
          {
            "type": "function",
            "name": "test_securedrop_rqworker_service",
            "code": "def test_securedrop_rqworker_service(host):\n    \"\"\"\n    Verify configuration of securedrop_rqworker systemd service.\n    \"\"\"\n    securedrop_test_vars = sdvars\n    service_file = \"/lib/systemd/system/securedrop_rqworker.service\"\n\n    expected_content = \"\\n\".join(\n        [\n            \"[Unit]\",\n            \"Description=SecureDrop rq worker\",\n            \"After=redis-server.service\",\n            \"Wants=redis-server.service\",\n            \"\",\n            \"[Service]\",\n            \"Type=exec\",\n            f'Environment=PYTHONPATH=\"{securedrop_test_vars.securedrop_code}:{securedrop_test_vars.securedrop_venv_site_packages}\"',\n            f\"ExecStart={securedrop_test_vars.securedrop_venv_bin}/rqworker -c rq_config\",\n            \"PrivateDevices=yes\",\n            \"PrivateTmp=yes\",\n            \"ProtectSystem=full\",\n            \"ReadOnlyDirectories=/\",\n            f\"ReadWriteDirectories={securedrop_test_vars.securedrop_data}\",\n            \"Restart=always\",\n            \"RestartSec=10s\",\n            \"UMask=077\",\n            f\"User={securedrop_test_vars.securedrop_user}\",\n            f\"WorkingDirectory={securedrop_test_vars.securedrop_code}\",\n            \"\",\n            \"[Install]\",\n            \"WantedBy=multi-user.target\\n\",\n        ]\n    )\n\n    f = host.file(service_file)\n    assert f.is_file\n    assert f.mode == 0o644\n    assert f.user == \"root\"\n    assert f.group == \"root\"\n    assert f.content_string == expected_content\n\n    s = host.service(\"securedrop_rqworker\")\n    assert s.is_enabled\n    assert s.is_running",
            "file": "test_securedrop_rqworker.py"
          }
        ],
        "test_securedrop_app_code.py": [
          {
            "type": "function",
            "name": "test_apache_default_docroot_is_absent",
            "code": "def test_apache_default_docroot_is_absent(host):\n    \"\"\"\n    Ensure that the default docroot for Apache, containing static HTML\n    under Debian, has been removed. Leaving it in place can be a privacy\n    leak, as it displays version information by default.\n    \"\"\"\n    assert not host.file(\"/var/www/html\").exists",
            "file": "test_securedrop_app_code.py"
          },
          {
            "type": "function",
            "name": "test_unwanted_packages_absent",
            "code": "def test_unwanted_packages_absent(host, package):\n    \"\"\"\n    Ensure packages that conflict with `securedrop-app-code`\n    or are otherwise unwanted are not present.\n    \"\"\"\n    assert not host.package(package).is_installed",
            "file": "test_securedrop_app_code.py"
          },
          {
            "type": "function",
            "name": "test_securedrop_application_test_locale",
            "code": "def test_securedrop_application_test_locale(host):\n    \"\"\"\n    Ensure both SecureDrop DEFAULT_LOCALE and SUPPORTED_LOCALES are present.\n    \"\"\"\n    securedrop_config = host.file(f\"{securedrop_test_vars.securedrop_code}/config.py\")\n    with host.sudo():\n        assert securedrop_config.is_file\n        assert securedrop_config.contains(\"^DEFAULT_LOCALE\")\n        assert securedrop_config.content_string.count(\"DEFAULT_LOCALE\") == 1\n        assert securedrop_config.content_string.count(\"SUPPORTED_LOCALES\") == 1\n        assert \"\\nSUPPORTED_LOCALES = ['el', 'ar', 'en_US']\\n\" in securedrop_config.content_string",
            "file": "test_securedrop_app_code.py"
          },
          {
            "type": "function",
            "name": "test_securedrop_application_test_journalist_key",
            "code": "def test_securedrop_application_test_journalist_key(host):\n    \"\"\"\n    Ensure the SecureDrop Application GPG public key file is present.\n    This is a test-only pubkey provided in the repository strictly for testing.\n    \"\"\"\n    pubkey_file = host.file(f\"{securedrop_test_vars.securedrop_data}/journalist.pub\")\n    # sudo is only necessary when testing against app hosts, since the\n    # permissions are tighter. Let's elevate privileges so we're sure\n    # we can read the correct file attributes and test them.\n    with host.sudo():\n        assert pubkey_file.is_file\n        assert pubkey_file.user == \"root\"\n        assert pubkey_file.group == \"www-data\"\n        assert pubkey_file.mode == 0o640\n\n    # Let's make sure the corresponding fingerprint is specified\n    # in the SecureDrop app configuration.\n    securedrop_config = host.file(f\"{securedrop_test_vars.securedrop_code}/config.py\")\n    with host.sudo():\n        assert securedrop_config.is_file\n        assert securedrop_config.user == securedrop_test_vars.securedrop_code_owner\n        assert securedrop_config.group == securedrop_test_vars.securedrop_user\n        assert securedrop_config.mode == 0o640\n        assert securedrop_config.contains(\n            \"^JOURNALIST_KEY = '65A1B5FF195B56353CC63DFFCC40EF1228271441'$\"\n        )",
            "file": "test_securedrop_app_code.py"
          },
          {
            "type": "function",
            "name": "test_securedrop_application_sqlite_db",
            "code": "def test_securedrop_application_sqlite_db(host):\n    \"\"\"\n    Ensure sqlite database exists for application. The database file should be\n    created by Ansible on first run.\n    \"\"\"\n    # sudo is necessary under the App hosts, which have restrictive file\n    # permissions on the doc root. Not technically necessary under dev host.\n    with host.sudo():\n        f = host.file(f\"{securedrop_test_vars.securedrop_data}/db.sqlite\")\n        assert f.is_file\n        assert f.user == securedrop_test_vars.securedrop_user\n        assert f.group == securedrop_test_vars.securedrop_user\n        assert f.mode == 0o640",
            "file": "test_securedrop_app_code.py"
          }
        ],
        "test_securedrop_rqrequeue.py": [
          {
            "type": "function",
            "name": "test_securedrop_rqrequeue_service",
            "code": "def test_securedrop_rqrequeue_service(host):\n    \"\"\"\n    Verify configuration of securedrop_rqrequeue systemd service.\n    \"\"\"\n    service_file = \"/lib/systemd/system/securedrop_rqrequeue.service\"\n    expected_content = \"\\n\".join(\n        [\n            \"[Unit]\",\n            \"Description=SecureDrop rqrequeue process\",\n            \"After=redis-server.service\",\n            \"Wants=redis-server.service\",\n            \"\",\n            \"[Service]\",\n            \"Type=exec\",\n            f'Environment=PYTHONPATH=\"{securedrop_test_vars.securedrop_code}:{securedrop_test_vars.securedrop_venv_site_packages}\"',\n            f\"ExecStart={securedrop_test_vars.securedrop_venv_bin}/python /var/www/securedrop/\"\n            \"scripts/rqrequeue --interval 60\",\n            \"PrivateDevices=yes\",\n            \"PrivateTmp=yes\",\n            \"ProtectSystem=full\",\n            \"ReadOnlyDirectories=/\",\n            f\"ReadWriteDirectories={securedrop_test_vars.securedrop_data}\",\n            \"Restart=always\",\n            \"RestartSec=10s\",\n            \"UMask=077\",\n            f\"User={securedrop_test_vars.securedrop_user}\",\n            f\"WorkingDirectory={securedrop_test_vars.securedrop_code}\",\n            \"\",\n            \"[Install]\",\n            \"WantedBy=multi-user.target\\n\",\n        ]\n    )\n\n    f = host.file(service_file)\n    assert f.is_file\n    assert f.mode == 0o644\n    assert f.user == \"root\"\n    assert f.group == \"root\"\n    assert f.content_string == expected_content\n\n    s = host.service(\"securedrop_rqrequeue\")\n    assert s.is_enabled\n    assert s.is_running",
            "file": "test_securedrop_rqrequeue.py"
          }
        ],
        "test_securedrop_shredder_configuration.py": [
          {
            "type": "function",
            "name": "test_securedrop_shredder_service",
            "code": "def test_securedrop_shredder_service(host):\n    \"\"\"\n    Verify configuration of securedrop_shredder systemd service.\n    \"\"\"\n    securedrop_test_vars = sdvars\n    service_file = \"/lib/systemd/system/securedrop_shredder.service\"\n    expected_content = \"\\n\".join(\n        [\n            \"[Unit]\",\n            \"Description=SecureDrop shredder\",\n            \"\",\n            \"[Service]\",\n            \"Type=exec\",\n            f'Environment=PYTHONPATH=\"{securedrop_test_vars.securedrop_code}:{securedrop_test_vars.securedrop_venv_site_packages}\"',\n            f\"ExecStart={securedrop_test_vars.securedrop_venv_bin}/python /var/www/securedrop/\"\n            \"scripts/shredder --interval 60\",\n            \"PrivateDevices=yes\",\n            \"PrivateTmp=yes\",\n            \"ProtectSystem=full\",\n            \"ReadOnlyDirectories=/\",\n            f\"ReadWriteDirectories={securedrop_test_vars.securedrop_data}\",\n            \"Restart=always\",\n            \"RestartSec=10s\",\n            \"UMask=077\",\n            f\"User={securedrop_test_vars.securedrop_user}\",\n            f\"WorkingDirectory={securedrop_test_vars.securedrop_code}\",\n            \"\",\n            \"[Install]\",\n            \"WantedBy=multi-user.target\\n\",\n        ]\n    )\n\n    f = host.file(service_file)\n    assert f.is_file\n    assert f.mode == 0o644\n    assert f.user == \"root\"\n    assert f.group == \"root\"\n    assert f.content_string == expected_content\n\n    s = host.service(\"securedrop_shredder\")\n    assert s.is_enabled\n    assert s.is_running",
            "file": "test_securedrop_shredder_configuration.py"
          }
        ]
      }
    },
    "ansible-config": {
      "tests": {
        "test_play_configuration.py": [
          {
            "type": "function",
            "name": "find_ansible_playbooks",
            "code": "def find_ansible_playbooks():\n    \"\"\"\n    Test helper to generate list of filepaths for SecureDrop\n    Ansible playbooks. All files will be validated to contain the\n    max_fail option.\n    \"\"\"\n    playbooks = []\n    # Not using os.walk since all SecureDrop playbooks are in top-level\n    # of the \"ansible-base\" directory, and *many* YAML files that are\n    # not playbooks reside in subdirectories.\n    for f in os.listdir(ANSIBLE_BASE):\n        # Assume all YAML files in directory are playbooks.\n        if f.endswith(\".yml\"):\n            # Ignore files that are deprecated or require test exceptions\n            if f not in [\"prod-specific.yml\", \"build-deb-pkgs.yml\"]:\n                playbooks.append(os.path.join(ANSIBLE_BASE, f))\n    # Sanity checking to make sure list of playbooks is not empty.\n    assert len(playbooks) > 0\n    return playbooks",
            "file": "test_play_configuration.py"
          },
          {
            "type": "function",
            "name": "test_max_fail_percentage",
            "code": "def test_max_fail_percentage(host, playbook):\n    \"\"\"\n    All SecureDrop playbooks should set `max_fail_percentage` to \"0\"\n    on each and every play. Doing so ensures that an error on a single\n    host constitutes a play failure.\n\n    In conjunction with the `any_errors_fatal` option, tested separately,\n    this will achieve a \"fail fast\" behavior from Ansible.\n\n    There's no ansible.cfg option to set for max_fail_percentage, which would\n    allow for a single DRY update that would apply automatically to all\n    invocations of `ansible-playbook`. Therefore this test, which will\n    search for the line present in all playbooks.\n\n    Technically it's only necessary that plays targeting multiple hosts use\n    the parameter, but we'll play it safe and require it everywhere,\n    to avoid mistakes down the road.\n    \"\"\"\n    with open(playbook) as f:\n        playbook_yaml = yaml.safe_load(f)\n        # Descend into playbook list structure to validate play attributes.\n        for play in playbook_yaml:\n            assert \"max_fail_percentage\" in play\n            assert play[\"max_fail_percentage\"] == 0",
            "file": "test_play_configuration.py"
          },
          {
            "type": "function",
            "name": "test_any_errors_fatal",
            "code": "def test_any_errors_fatal(host, playbook):\n    \"\"\"\n    All SecureDrop playbooks should set `any_errors_fatal` to \"yes\"\n    on each and every play. In conjunction with `max_fail_percentage` set\n    to \"0\", doing so ensures that any errors will cause an immediate failure\n    on the playbook.\n    \"\"\"\n    with open(playbook) as f:\n        playbook_yaml = yaml.safe_load(f)\n        # Descend into playbook list structure to validate play attributes.\n        for play in playbook_yaml:\n            assert \"any_errors_fatal\" in play\n            # Ansible coerces booleans, so bare assert is sufficient\n            assert play[\"any_errors_fatal\"]",
            "file": "test_play_configuration.py"
          },
          {
            "type": "function",
            "name": "test_locale",
            "code": "def test_locale(host, playbook):\n    \"\"\"\n    The securedrop-prod and securedrop-staging playbooks should\n    control the locale in the host environment by setting LC_ALL=C.\n    \"\"\"\n    with open(os.path.join(ANSIBLE_BASE, playbook)) as f:\n        playbook_yaml = yaml.safe_load(f)\n        for play in playbook_yaml:\n            assert \"environment\" in play\n            assert play[\"environment\"][\"LC_ALL\"] == \"C\"",
            "file": "test_play_configuration.py"
          }
        ]
      }
    }
  },
  "builder": {
    "tests": {
      "test_ossec_package.py": [
        {
          "type": "function",
          "name": "test_ossec_binaries_are_present_agent",
          "code": "def test_ossec_binaries_are_present_agent():\n    \"\"\"\n    Inspect the package contents to ensure all ossec agent binaries are properly\n    included in the package.\n    \"\"\"\n    wanted_files = [\n        \"/var/ossec/bin/agent-auth\",\n        \"/var/ossec/bin/ossec-syscheckd\",\n        \"/var/ossec/bin/ossec-agentd\",\n        \"/var/ossec/bin/manage_agents\",\n        \"/var/ossec/bin/ossec-control\",\n        \"/var/ossec/bin/ossec-logcollector\",\n        \"/var/ossec/bin/util.sh\",\n        \"/var/ossec/bin/ossec-execd\",\n    ]\n    path = BUILD_DIRECTORY / f\"ossec-agent_{OSSEC_VERSION}+{UBUNTU_VERSION}_amd64.deb\"\n    contents = subprocess.check_output([\"dpkg-deb\", \"-c\", str(path)]).decode()\n    for wanted_file in wanted_files:\n        assert re.search(\n            rf\"^.* .{wanted_file}$\",\n            contents,\n            re.M,\n        )",
          "file": "test_ossec_package.py"
        },
        {
          "type": "function",
          "name": "test_ossec_binaries_are_present_server",
          "code": "def test_ossec_binaries_are_present_server():\n    \"\"\"\n    Inspect the package contents to ensure all ossec server binaries are properly\n    included in the package.\n    \"\"\"\n    wanted_files = [\n        \"/var/ossec/bin/ossec-maild\",\n        \"/var/ossec/bin/ossec-remoted\",\n        \"/var/ossec/bin/ossec-syscheckd\",\n        \"/var/ossec/bin/ossec-makelists\",\n        \"/var/ossec/bin/ossec-logtest\",\n        \"/var/ossec/bin/syscheck_update\",\n        \"/var/ossec/bin/ossec-reportd\",\n        \"/var/ossec/bin/ossec-agentlessd\",\n        \"/var/ossec/bin/manage_agents\",\n        \"/var/ossec/bin/rootcheck_control\",\n        \"/var/ossec/bin/ossec-control\",\n        \"/var/ossec/bin/ossec-dbd\",\n        \"/var/ossec/bin/ossec-csyslogd\",\n        \"/var/ossec/bin/ossec-regex\",\n        \"/var/ossec/bin/agent_control\",\n        \"/var/ossec/bin/ossec-monitord\",\n        \"/var/ossec/bin/clear_stats\",\n        \"/var/ossec/bin/ossec-logcollector\",\n        \"/var/ossec/bin/list_agents\",\n        \"/var/ossec/bin/verify-agent-conf\",\n        \"/var/ossec/bin/syscheck_control\",\n        \"/var/ossec/bin/util.sh\",\n        \"/var/ossec/bin/ossec-analysisd\",\n        \"/var/ossec/bin/ossec-execd\",\n        \"/var/ossec/bin/ossec-authd\",\n    ]\n    path = BUILD_DIRECTORY / f\"ossec-server_{OSSEC_VERSION}+{UBUNTU_VERSION}_amd64.deb\"\n    contents = subprocess.check_output([\"dpkg-deb\", \"-c\", str(path)]).decode()\n    for wanted_file in wanted_files:\n        assert re.search(\n            rf\"^.* .{wanted_file}$\",\n            contents,\n            re.M,\n        )",
          "file": "test_ossec_package.py"
        }
      ],
      "test_securedrop_deb_package.py": [
        {
          "type": "function",
          "name": "securedrop_app_code_contents",
          "code": "def securedrop_app_code_contents() -> str:\n    \"\"\"\n    Returns the content listing of the securedrop-app-code Debian package.\n    \"\"\"\n    for pkg in DEB_PATHS:\n        if pkg.name.startswith(\"securedrop-app-code\") and \"dbgsym\" not in pkg.name:\n            return subprocess.check_output([\"dpkg-deb\", \"--contents\", pkg]).decode()\n\n    raise RuntimeError(\"Unable to find securedrop-app-code package in build/ folder\")",
          "file": "test_securedrop_deb_package.py"
        },
        {
          "type": "function",
          "name": "test_deb_packages_appear_installable",
          "code": "def test_deb_packages_appear_installable(deb: Path) -> None:\n    \"\"\"\n    Confirms that a dry-run of installation reports no errors.\n    Simple check for valid Debian package structure, but not thorough.\n    When run on a malformed package, `dpkg` will report:\n\n       dpkg-deb: error: `foo.deb' is not a debian format archive\n\n    Testing application behavior is left to the functional tests.\n    \"\"\"\n\n    # Normally this is called as root, but we can get away with simply\n    # adding sbin to the path\n    path = os.getenv(\"PATH\") + \":/usr/sbin:/sbin\"\n    subprocess.check_call([\"dpkg\", \"--install\", \"--dry-run\", deb], env={\"PATH\": path})",
          "file": "test_securedrop_deb_package.py"
        },
        {
          "type": "function",
          "name": "test_deb_package_contains_expected_conffiles",
          "code": "def test_deb_package_contains_expected_conffiles(deb: Path):\n    \"\"\"\n    Ensures the `securedrop-app-code` package declares only allow-listed\n    `conffiles`. Several files in `/etc/` would automatically be marked\n    conffiles, which would break unattended updates to critical package\n    functionality such as AppArmor profiles. This test validates overrides\n    in the build logic to unset those conffiles.\n\n    The same applies to `securedrop-config` too.\n    \"\"\"\n    if (\n        not deb.name.startswith((\"securedrop-app-code\", \"securedrop-config\"))\n        or \"dbgsym\" in deb.name\n    ):\n        return\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        subprocess.check_call([\"dpkg-deb\", \"--control\", deb, tmpdir])\n        conffiles_path = Path(tmpdir) / \"conffiles\"\n        assert conffiles_path.exists()\n        # No files are currently allow-listed to be conffiles\n        assert conffiles_path.read_text().rstrip() == \"\"",
          "file": "test_securedrop_deb_package.py"
        },
        {
          "type": "function",
          "name": "test_app_code_paths",
          "code": "def test_app_code_paths(securedrop_app_code_contents: str, path: str):\n    \"\"\"\n    Ensures the `securedrop-app-code` package contains the specified paths\n    \"\"\"\n    for line in securedrop_app_code_contents.splitlines():\n        if line.endswith(path):\n            assert True\n            return\n\n    pytest.fail(\"not found\")",
          "file": "test_securedrop_deb_package.py"
        },
        {
          "type": "function",
          "name": "test_app_code_paths_missing",
          "code": "def test_app_code_paths_missing(securedrop_app_code_contents: str, path: str):\n    \"\"\"\n    Ensures the `securedrop-app-code` package do *NOT* contain the specified paths\n    \"\"\"\n    for line in securedrop_app_code_contents.splitlines():\n        if line.endswith(path):\n            pytest.fail(f\"found {line}\")",
          "file": "test_securedrop_deb_package.py"
        },
        {
          "type": "function",
          "name": "test_apparmor_conditional",
          "code": "def test_apparmor_conditional():\n    try:\n        path = [pkg for pkg in DEB_PATHS if pkg.name.startswith(\"securedrop-app-code\")][0]\n    except IndexError:\n        raise RuntimeError(\"Unable to find securedrop-app-code package in build/ folder\")\n    info = subprocess.check_output([\"dpkg\", \"--info\", path]).decode()\n    found = False\n    for line in info.splitlines():\n        if line.startswith(\" Depends:\"):\n            found = True\n            if UBUNTU_VERSION == \"focal\":\n                assert \"apparmor (>=\" not in line, \"focal has no versioned apparmor dependency\"\n            else:\n                assert \"apparmor (>=\" in line, \"noble has versioned apparmor dependency\"\n\n    print(info)\n    assert found, \"Depends: line wasn't found\"",
          "file": "test_securedrop_deb_package.py"
        }
      ]
    }
  },
  "devops": {
    "scripts": {
      "verify-mo.py": [
        {
          "type": "class",
          "name": "CatalogVerifier",
          "code": "class CatalogVerifier:\n    \"\"\"Wrapper class for proving .mo → .po → .mo reproducibility.\"\"\"\n\n    def __init__(self, path: Path, domain: str):\n        \"\"\"Set up the .po/.mo pair.\"\"\"\n\n        self.path = path\n        self.po = polib.pofile(str(path / \"LC_MESSAGES\" / f\"{domain}.po\"))\n        self.mo = polib.mofile(str(path / \"LC_MESSAGES\" / f\"{domain}.mo\"))\n\n    def __enter__(self) -> \"CatalogVerifier\":\n        \"\"\"Prepare to generate the new .mo file to diff.\"\"\"\n\n        self.mo_target = Path(f\"{self.mo.fpath}.new\")\n        return self\n\n    def __exit__(\n        self,\n        exc_type: object,\n        exc_value: object,\n        traceback: object,\n    ) -> None:\n        \"\"\"Clean up.\"\"\"\n\n        self.mo_target.unlink(missing_ok=True)\n\n    @property\n    def strays(self) -> Set[str]:\n        \"\"\"Return the set of stray (fuzzy or obsolete) entries to mask when\n        diffing this catalog.\"\"\"\n\n        fuzzy = {\n            f\"^{line.replace('#| ', '')}\"  # strip fuzzy marker\n            for e in self.po.fuzzy_entries()\n            for line in str(e).splitlines()\n        }\n        obsolete = {\n            f\"^{line.replace('#~ ', '')}\"  # strip obsolete marker\n            for e in self.po.obsolete_entries()\n            for line in str(e).splitlines()\n        }\n\n        return fuzzy | obsolete\n\n    def diffoscope_args(self, a: Path, b: Path, filtered: bool = True) -> Iterator[str]:\n        \"\"\"Build up a diffoscope invocation that (with `filtered`) removes\n        false positives from the msgunfmt diff.\"\"\"\n\n        yield f\"diffoscope {a} {b}\"\n\n        if not filtered:\n            return\n\n        yield \"--diff-mask '^$'\"  # tell diffoscope to mask empty lines\n        for stray in self.strays:\n            yield f\"--diff-mask {shlex.quote(stray)}\"  # tell diffoscope to mask strays\n        yield \"| grep -Fv '[masked]'\"  # ignore things we've masked\n        yield \"| grep -E '│ (-|\\\\+)msg(id|str)'\"  # ignore context; we only care about real diffs\n\n    def diffoscope_call(\n        self, a: Path, b: Path, filtered: bool = True\n    ) -> subprocess.CompletedProcess:\n        \"\"\"Call diffoscope and return the subprocess.CompletedProcess result\n        for further processing, *without* first checking whether it was\n        succesful.\"\"\"\n\n        cmd = \" \".join(self.diffoscope_args(a, b, filtered))\n\n        # We silence Bandit and Semgrep warnings on `shell=True`\n        # because we want to inherit the Python virtual environment\n        # in which we're invoked.\n        # nosemgrep: python.lang.security.audit.subprocess-shell-true.subprocess-shell-true\n        return subprocess.run(  # noqa: S602\n            cmd,\n            capture_output=True,\n            env=os.environ,\n            shell=True,\n            check=False,\n        )\n\n    def reproduce(self) -> None:\n        \"\"\"Overwrite metadata .mo → .po.  Then rewrite the entire file .po →\n        .mo.\"\"\"\n\n        self.po.metadata = self.mo.metadata\n        self.po.save(self.po.fpath)\n\n        with open(self.mo_target, \"wb\") as mo_target:\n            convertmo(self.po.fpath, mo_target, \"\")\n\n    def verify(self) -> None:\n        \"\"\"Run diffoscope for this catalog and error if there's any unmasked\n        diff.\"\"\"\n\n        # Without filtering, diffoscope should return either 0 (no differences)\n        # or 1 (differences); anything else is an error.\n        test = self.diffoscope_call(Path(self.mo.fpath), Path(self.mo_target), filtered=False)\n        if test.returncode not in [0, 1]:\n            print(test.stdout.decode())\n            print(test.stderr.decode())\n            test.check_returncode()\n\n        # With filtering, since diffoscope will return 1 on differences\n        # (pre-filtering), and grep will return 1 on *no* differences\n        # (post-filtering), we can't count on result.returncode here.\n        result = self.diffoscope_call(Path(self.mo.fpath), Path(self.mo_target))\n        print(f\"--> Verifying {self.path}: {result.args}\")\n        if len(result.stdout) > 0:\n            raise Exception(result.stdout.decode(\"utf-8\"))",
          "file": "verify-mo.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, path: Path, domain: str):\n        \"\"\"Set up the .po/.mo pair.\"\"\"\n\n        self.path = path\n        self.po = polib.pofile(str(path / \"LC_MESSAGES\" / f\"{domain}.po\"))\n        self.mo = polib.mofile(str(path / \"LC_MESSAGES\" / f\"{domain}.mo\"))",
          "file": "verify-mo.py"
        },
        {
          "type": "function",
          "name": "__enter__",
          "code": "def __enter__(self) -> \"CatalogVerifier\":\n        \"\"\"Prepare to generate the new .mo file to diff.\"\"\"\n\n        self.mo_target = Path(f\"{self.mo.fpath}.new\")\n        return self",
          "file": "verify-mo.py"
        },
        {
          "type": "function",
          "name": "__exit__",
          "code": "def __exit__(\n        self,\n        exc_type: object,\n        exc_value: object,\n        traceback: object,\n    ) -> None:\n        \"\"\"Clean up.\"\"\"\n\n        self.mo_target.unlink(missing_ok=True)",
          "file": "verify-mo.py"
        },
        {
          "type": "function",
          "name": "strays",
          "code": "def strays(self) -> Set[str]:\n        \"\"\"Return the set of stray (fuzzy or obsolete) entries to mask when\n        diffing this catalog.\"\"\"\n\n        fuzzy = {\n            f\"^{line.replace('#| ', '')}\"  # strip fuzzy marker\n            for e in self.po.fuzzy_entries()\n            for line in str(e).splitlines()\n        }\n        obsolete = {\n            f\"^{line.replace('#~ ', '')}\"  # strip obsolete marker\n            for e in self.po.obsolete_entries()\n            for line in str(e).splitlines()\n        }\n\n        return fuzzy | obsolete",
          "file": "verify-mo.py"
        },
        {
          "type": "function",
          "name": "diffoscope_args",
          "code": "def diffoscope_args(self, a: Path, b: Path, filtered: bool = True) -> Iterator[str]:\n        \"\"\"Build up a diffoscope invocation that (with `filtered`) removes\n        false positives from the msgunfmt diff.\"\"\"\n\n        yield f\"diffoscope {a} {b}\"\n\n        if not filtered:\n            return\n\n        yield \"--diff-mask '^$'\"  # tell diffoscope to mask empty lines\n        for stray in self.strays:\n            yield f\"--diff-mask {shlex.quote(stray)}\"  # tell diffoscope to mask strays\n        yield \"| grep -Fv '[masked]'\"  # ignore things we've masked\n        yield \"| grep -E '│ (-|\\\\+)msg(id|str)'\"  # ignore context; we only care about real diffs",
          "file": "verify-mo.py"
        },
        {
          "type": "function",
          "name": "diffoscope_call",
          "code": "def diffoscope_call(\n        self, a: Path, b: Path, filtered: bool = True\n    ) -> subprocess.CompletedProcess:\n        \"\"\"Call diffoscope and return the subprocess.CompletedProcess result\n        for further processing, *without* first checking whether it was\n        succesful.\"\"\"\n\n        cmd = \" \".join(self.diffoscope_args(a, b, filtered))\n\n        # We silence Bandit and Semgrep warnings on `shell=True`\n        # because we want to inherit the Python virtual environment\n        # in which we're invoked.\n        # nosemgrep: python.lang.security.audit.subprocess-shell-true.subprocess-shell-true\n        return subprocess.run(  # noqa: S602\n            cmd,\n            capture_output=True,\n            env=os.environ,\n            shell=True,\n            check=False,\n        )",
          "file": "verify-mo.py"
        },
        {
          "type": "function",
          "name": "reproduce",
          "code": "def reproduce(self) -> None:\n        \"\"\"Overwrite metadata .mo → .po.  Then rewrite the entire file .po →\n        .mo.\"\"\"\n\n        self.po.metadata = self.mo.metadata\n        self.po.save(self.po.fpath)\n\n        with open(self.mo_target, \"wb\") as mo_target:\n            convertmo(self.po.fpath, mo_target, \"\")",
          "file": "verify-mo.py"
        },
        {
          "type": "function",
          "name": "verify",
          "code": "def verify(self) -> None:\n        \"\"\"Run diffoscope for this catalog and error if there's any unmasked\n        diff.\"\"\"\n\n        # Without filtering, diffoscope should return either 0 (no differences)\n        # or 1 (differences); anything else is an error.\n        test = self.diffoscope_call(Path(self.mo.fpath), Path(self.mo_target), filtered=False)\n        if test.returncode not in [0, 1]:\n            print(test.stdout.decode())\n            print(test.stderr.decode())\n            test.check_returncode()\n\n        # With filtering, since diffoscope will return 1 on differences\n        # (pre-filtering), and grep will return 1 on *no* differences\n        # (post-filtering), we can't count on result.returncode here.\n        result = self.diffoscope_call(Path(self.mo.fpath), Path(self.mo_target))\n        print(f\"--> Verifying {self.path}: {result.args}\")\n        if len(result.stdout) > 0:\n            raise Exception(result.stdout.decode(\"utf-8\"))",
          "file": "verify-mo.py"
        }
      ]
    },
    "demo": {
      "landing-page": {
        "webroot": {
          "assets": {
            "js": {
              "jsOTP.js": [
                {
                  "type": "function",
                  "name": "Int_64",
                  "code": "function Int_64(msint_32, lsint_32)\n\t{\n\t\tthis.highOrder = msint_32;\n\t\tthis.lowOrder = lsint_32;\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "str2binb",
                  "code": "function str2binb(str, utfType, existingBin, existingBinLen)\n\t{\n\t\tvar bin = [], codePnt, binArr = [], byteCnt = 0, i, j, existingByteLen,\n\t\t\tintOffset, byteOffset;\n\n\t\tbin = existingBin || [0];\n\t\texistingBinLen = existingBinLen || 0;\n\t\texistingByteLen = existingBinLen >>> 3;\n\n\t\tif (\"UTF8\" === utfType)\n\t\t{\n\t\t\tfor (i = 0; i < str.length; i += 1)\n\t\t\t{\n\t\t\t\tcodePnt = str.charCodeAt(i);\n\t\t\t\tbinArr = [];\n\n\t\t\t\tif (0x80 > codePnt)\n\t\t\t\t{\n\t\t\t\t\tbinArr.push(codePnt);\n\t\t\t\t}\n\t\t\t\telse if (0x800 > codePnt)\n\t\t\t\t{\n\t\t\t\t\tbinArr.push(0xC0 | (codePnt >>> 6));\n\t\t\t\t\tbinArr.push(0x80 | (codePnt & 0x3F));\n\t\t\t\t}\n\t\t\t\telse if ((0xd800 > codePnt) || (0xe000 <= codePnt)) {\n\t\t\t\t\tbinArr.push(\n\t\t\t\t\t\t0xe0 | (codePnt >>> 12),\n\t\t\t\t\t\t0x80 | ((codePnt >>> 6) & 0x3f),\n\t\t\t\t\t\t0x80 | (codePnt & 0x3f)\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\ti += 1;\n\t\t\t\t\tcodePnt = 0x10000 + (((codePnt & 0x3ff) << 10) | (str.charCodeAt(i) & 0x3ff));\n\t\t\t\t\tbinArr.push(\n\t\t\t\t\t\t0xf0 | (codePnt >>> 18),\n\t\t\t\t\t\t0x80 | ((codePnt >>> 12) & 0x3f),\n\t\t\t\t\t\t0x80 | ((codePnt >>> 6) & 0x3f),\n\t\t\t\t\t\t0x80 | (codePnt & 0x3f)\n\t\t\t\t\t);\n\t\t\t\t}\n\n\t\t\t\tfor (j = 0; j < binArr.length; j += 1)\n\t\t\t\t{\n\t\t\t\t\tbyteOffset = byteCnt + existingByteLen;\n\t\t\t\t\tintOffset = byteOffset >>> 2;\n\t\t\t\t\twhile (bin.length <= intOffset)\n\t\t\t\t\t{\n\t\t\t\t\t\tbin.push(0);\n\t\t\t\t\t}\n\t\t\t\t\t/* Known bug kicks in here */\n\t\t\t\t\tbin[intOffset] |= binArr[j] << (8 * (3 - (byteOffset % 4)));\n\t\t\t\t\tbyteCnt += 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\telse if ((\"UTF16BE\" === utfType) || \"UTF16LE\" === utfType)\n\t\t{\n\t\t\tfor (i = 0; i < str.length; i += 1)\n\t\t\t{\n\t\t\t\tcodePnt = str.charCodeAt(i);\n\t\t\t\t/* Internally strings are UTF-16BE so only change if UTF-16LE */\n\t\t\t\tif (\"UTF16LE\" === utfType)\n\t\t\t\t{\n\t\t\t\t\tj = codePnt & 0xFF;\n\t\t\t\t\tcodePnt = (j << 8) | (codePnt >>> 8);\n\t\t\t\t}\n\n\t\t\t\tbyteOffset = byteCnt + existingByteLen;\n\t\t\t\tintOffset = byteOffset >>> 2;\n\t\t\t\twhile (bin.length <= intOffset)\n\t\t\t\t{\n\t\t\t\t\tbin.push(0);\n\t\t\t\t}\n\t\t\t\tbin[intOffset] |= codePnt << (8 * (2 - (byteOffset % 4)));\n\t\t\t\tbyteCnt += 2;\n\t\t\t}\n\t\t}\n\t\treturn {\"value\" : bin, \"binLen\" : byteCnt * 8 + existingBinLen};\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "hex2binb",
                  "code": "function hex2binb(str, existingBin, existingBinLen)\n\t{\n\t\tvar bin, length = str.length, i, num, intOffset, byteOffset,\n\t\t\texistingByteLen;\n\n\t\tbin = existingBin || [0];\n\t\texistingBinLen = existingBinLen || 0;\n\t\texistingByteLen = existingBinLen >>> 3;\n\n\t\tif (0 !== (length % 2))\n\t\t{\n\t\t\tthrow new Error(\"String of HEX type must be in byte increments\");\n\t\t}\n\n\t\tfor (i = 0; i < length; i += 2)\n\t\t{\n\t\t\tnum = parseInt(str.substr(i, 2), 16);\n\t\t\tif (!isNaN(num))\n\t\t\t{\n\t\t\t\tbyteOffset = (i >>> 1) + existingByteLen;\n\t\t\t\tintOffset = byteOffset >>> 2;\n\t\t\t\twhile (bin.length <= intOffset)\n\t\t\t\t{\n\t\t\t\t\tbin.push(0);\n\t\t\t\t}\n\t\t\t\tbin[intOffset] |= num << 8 * (3 - (byteOffset % 4));\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tthrow new Error(\"String of HEX type contains invalid characters\");\n\t\t\t}\n\t\t}\n\n\t\treturn {\"value\" : bin, \"binLen\" : length * 4 + existingBinLen};\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "bytes2binb",
                  "code": "function bytes2binb(str, existingBin, existingBinLen)\n\t{\n\t\tvar bin = [], codePnt, i, existingByteLen, intOffset,\n\t\t\tbyteOffset;\n\n\t\tbin = existingBin || [0];\n\t\texistingBinLen = existingBinLen || 0;\n\t\texistingByteLen = existingBinLen >>> 3;\n\n\t\tfor (i = 0; i < str.length; i += 1)\n\t\t{\n\t\t\tcodePnt = str.charCodeAt(i);\n\n\t\t\tbyteOffset = i + existingByteLen;\n\t\t\tintOffset = byteOffset >>> 2;\n\t\t\tif (bin.length <= intOffset)\n\t\t\t{\n\t\t\t\tbin.push(0);\n\t\t\t}\n\t\t\tbin[intOffset] |= codePnt << 8 * (3 - (byteOffset % 4));\n\t\t}\n\n\t\treturn {\"value\" : bin, \"binLen\" : str.length * 8 + existingBinLen};\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "b642binb",
                  "code": "function b642binb(str, existingBin, existingBinLen)\n\t{\n\t\tvar bin = [], byteCnt = 0, index, i, j, tmpInt, strPart, firstEqual,\n\t\t\tb64Tab = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\",\n\t\t\texistingByteLen, intOffset, byteOffset;\n\n\t\tbin = existingBin || [0];\n\t\texistingBinLen = existingBinLen || 0;\n\t\texistingByteLen = existingBinLen >>> 3;\n\n\t\tif (-1 === str.search(/^[a-zA-Z0-9=+\\/]+$/))\n\t\t{\n\t\t\tthrow new Error(\"Invalid character in base-64 string\");\n\t\t}\n\t\tfirstEqual = str.indexOf('=');\n\t\tstr = str.replace(/\\=/g, '');\n\t\tif ((-1 !== firstEqual) && (firstEqual < str.length))\n\t\t{\n\t\t\tthrow new Error(\"Invalid '=' found in base-64 string\");\n\t\t}\n\n\t\tfor (i = 0; i < str.length; i += 4)\n\t\t{\n\t\t\tstrPart = str.substr(i, 4);\n\t\t\ttmpInt = 0;\n\n\t\t\tfor (j = 0; j < strPart.length; j += 1)\n\t\t\t{\n\t\t\t\tindex = b64Tab.indexOf(strPart[j]);\n\t\t\t\ttmpInt |= index << (18 - (6 * j));\n\t\t\t}\n\n\t\t\tfor (j = 0; j < strPart.length - 1; j += 1)\n\t\t\t{\n\t\t\t\tbyteOffset = byteCnt + existingByteLen;\n\t\t\t\tintOffset = byteOffset >>> 2;\n\t\t\t\twhile (bin.length <= intOffset)\n\t\t\t\t{\n\t\t\t\t\tbin.push(0);\n\t\t\t\t}\n\t\t\t\tbin[intOffset] |= ((tmpInt >>> (16 - (j * 8))) & 0xFF) <<\n\t\t\t\t\t8 * (3 - (byteOffset % 4));\n\t\t\t\tbyteCnt += 1;\n\t\t\t}\n\t\t}\n\n\t\treturn {\"value\" : bin, \"binLen\" : byteCnt * 8 + existingBinLen};\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "binb2hex",
                  "code": "function binb2hex(binarray, formatOpts)\n\t{\n\t\tvar hex_tab = \"0123456789abcdef\", str = \"\",\n\t\t\tlength = binarray.length * 4, i, srcByte;\n\n\t\tfor (i = 0; i < length; i += 1)\n\t\t{\n\t\t\t/* The below is more than a byte but it gets taken care of later */\n\t\t\tsrcByte = binarray[i >>> 2] >>> ((3 - (i % 4)) * 8);\n\t\t\tstr += hex_tab.charAt((srcByte >>> 4) & 0xF) +\n\t\t\t\thex_tab.charAt(srcByte & 0xF);\n\t\t}\n\n\t\treturn (formatOpts[\"outputUpper\"]) ? str.toUpperCase() : str;\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "binb2b64",
                  "code": "function binb2b64(binarray, formatOpts)\n\t{\n\t\tvar str = \"\", length = binarray.length * 4, i, j, triplet, offset, int1, int2,\n\t\t\tb64Tab = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\";\n\n\t\tfor (i = 0; i < length; i += 3)\n\t\t{\n\t\t\toffset = (i + 1) >>> 2;\n\t\t\tint1 = (binarray.length <= offset) ? 0 : binarray[offset];\n\t\t\toffset = (i + 2) >>> 2;\n\t\t\tint2 = (binarray.length <= offset) ? 0 : binarray[offset];\n\t\t\ttriplet = (((binarray[i >>> 2] >>> 8 * (3 - i % 4)) & 0xFF) << 16) |\n\t\t\t\t(((int1 >>> 8 * (3 - (i + 1) % 4)) & 0xFF) << 8) |\n\t\t\t\t((int2 >>> 8 * (3 - (i + 2) % 4)) & 0xFF);\n\t\t\tfor (j = 0; j < 4; j += 1)\n\t\t\t{\n\t\t\t\tif (i * 8 + j * 6 <= binarray.length * 32)\n\t\t\t\t{\n\t\t\t\t\tstr += b64Tab.charAt((triplet >>> 6 * (3 - j)) & 0x3F);\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\tstr += formatOpts[\"b64Pad\"];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn str;\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "binb2bytes",
                  "code": "function binb2bytes(binarray)\n\t{\n\t\tvar str = \"\", length = binarray.length * 4, i, srcByte;\n\n\t\tfor (i = 0; i < length; i += 1)\n\t\t{\n\t\t\tsrcByte = (binarray[i >>> 2] >>> ((3 - (i % 4)) * 8)) & 0xFF;\n\t\t\tstr += String.fromCharCode(srcByte);\n\t\t}\n\n\t\treturn str;\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "getOutputOpts",
                  "code": "function getOutputOpts(options)\n\t{\n\t\tvar retVal = {\"outputUpper\" : false, \"b64Pad\" : \"=\"}, outputOptions;\n\t\toutputOptions = options || {};\n\n\t\tretVal[\"outputUpper\"] = outputOptions[\"outputUpper\"] || false;\n\t\tretVal[\"b64Pad\"] = outputOptions[\"b64Pad\"] || \"=\";\n\n\t\tif (\"boolean\" !== typeof(retVal[\"outputUpper\"]))\n\t\t{\n\t\t\tthrow new Error(\"Invalid outputUpper formatting option\");\n\t\t}\n\n\t\tif (\"string\" !== typeof(retVal[\"b64Pad\"]))\n\t\t{\n\t\t\tthrow new Error(\"Invalid b64Pad formatting option\");\n\t\t}\n\n\t\treturn retVal;\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "getStrConverter",
                  "code": "function getStrConverter(format, utfType)\n\t{\n\t\tvar retVal;\n\n\t\t/* Validate encoding */\n\t\tswitch (utfType)\n\t\t{\n\t\tcase \"UTF8\":\n\t\t\t/* Fallthrough */\n\t\tcase \"UTF16BE\":\n\t\t\t/* Fallthrough */\n\t\tcase \"UTF16LE\":\n\t\t\t/* Fallthrough */\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tthrow new Error(\"encoding must be UTF8, UTF16BE, or UTF16LE\");\n\t\t}\n\n\t\t/* Map inputFormat to the appropriate converter */\n\t\tswitch (format)\n\t\t{\n\t\tcase \"HEX\":\n\t\t\tretVal = hex2binb;\n\t\t\tbreak;\n\t\tcase \"TEXT\":\n\t\t\tretVal = function(str, existingBin, existingBinLen)\n\t\t\t\t{\n\t\t\t\t\treturn str2binb(str, utfType, existingBin, existingBinLen);\n\t\t\t\t};\n\t\t\tbreak;\n\t\tcase \"B64\":\n\t\t\tretVal = b642binb;\n\t\t\tbreak;\n\t\tcase \"BYTES\":\n\t\t\tretVal = bytes2binb;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tthrow new Error(\"format must be HEX, TEXT, B64, or BYTES\");\n\t\t}\n\n\t\treturn retVal;\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "rotl_32",
                  "code": "function rotl_32(x, n)\n\t{\n\t\treturn (x << n) | (x >>> (32 - n));\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "rotr_32",
                  "code": "function rotr_32(x, n)\n\t{\n\t\treturn (x >>> n) | (x << (32 - n));\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "rotr_64",
                  "code": "function rotr_64(x, n)\n\t{\n\t\tvar retVal = null, tmp = new Int_64(x.highOrder, x.lowOrder);\n\n\t\tif (32 >= n)\n\t\t{\n\t\t\tretVal = new Int_64(\n\t\t\t\t\t(tmp.highOrder >>> n) | ((tmp.lowOrder << (32 - n)) & 0xFFFFFFFF),\n\t\t\t\t\t(tmp.lowOrder >>> n) | ((tmp.highOrder << (32 - n)) & 0xFFFFFFFF)\n\t\t\t\t);\n\t\t}\n\t\telse\n\t\t{\n\t\t\tretVal = new Int_64(\n\t\t\t\t\t(tmp.lowOrder >>> (n - 32)) | ((tmp.highOrder << (64 - n)) & 0xFFFFFFFF),\n\t\t\t\t\t(tmp.highOrder >>> (n - 32)) | ((tmp.lowOrder << (64 - n)) & 0xFFFFFFFF)\n\t\t\t\t);\n\t\t}\n\n\t\treturn retVal;\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "shr_32",
                  "code": "function shr_32(x, n)\n\t{\n\t\treturn x >>> n;\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "shr_64",
                  "code": "function shr_64(x, n)\n\t{\n\t\tvar retVal = null;\n\n\t\tif (32 >= n)\n\t\t{\n\t\t\tretVal = new Int_64(\n\t\t\t\t\tx.highOrder >>> n,\n\t\t\t\t\tx.lowOrder >>> n | ((x.highOrder << (32 - n)) & 0xFFFFFFFF)\n\t\t\t\t);\n\t\t}\n\t\telse\n\t\t{\n\t\t\tretVal = new Int_64(\n\t\t\t\t\t0,\n\t\t\t\t\tx.highOrder >>> (n - 32)\n\t\t\t\t);\n\t\t}\n\n\t\treturn retVal;\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "parity_32",
                  "code": "function parity_32(x, y, z)\n\t{\n\t\treturn x ^ y ^ z;\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "ch_32",
                  "code": "function ch_32(x, y, z)\n\t{\n\t\treturn (x & y) ^ (~x & z);\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "ch_64",
                  "code": "function ch_64(x, y, z)\n\t{\n\t\treturn new Int_64(\n\t\t\t\t(x.highOrder & y.highOrder) ^ (~x.highOrder & z.highOrder),\n\t\t\t\t(x.lowOrder & y.lowOrder) ^ (~x.lowOrder & z.lowOrder)\n\t\t\t);\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "maj_32",
                  "code": "function maj_32(x, y, z)\n\t{\n\t\treturn (x & y) ^ (x & z) ^ (y & z);\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "maj_64",
                  "code": "function maj_64(x, y, z)\n\t{\n\t\treturn new Int_64(\n\t\t\t\t(x.highOrder & y.highOrder) ^\n\t\t\t\t(x.highOrder & z.highOrder) ^\n\t\t\t\t(y.highOrder & z.highOrder),\n\t\t\t\t(x.lowOrder & y.lowOrder) ^\n\t\t\t\t(x.lowOrder & z.lowOrder) ^\n\t\t\t\t(y.lowOrder & z.lowOrder)\n\t\t\t);\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "sigma0_32",
                  "code": "function sigma0_32(x)\n\t{\n\t\treturn rotr_32(x, 2) ^ rotr_32(x, 13) ^ rotr_32(x, 22);\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "sigma0_64",
                  "code": "function sigma0_64(x)\n\t{\n\t\tvar rotr28 = rotr_64(x, 28), rotr34 = rotr_64(x, 34),\n\t\t\trotr39 = rotr_64(x, 39);\n\n\t\treturn new Int_64(\n\t\t\t\trotr28.highOrder ^ rotr34.highOrder ^ rotr39.highOrder,\n\t\t\t\trotr28.lowOrder ^ rotr34.lowOrder ^ rotr39.lowOrder);\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "sigma1_32",
                  "code": "function sigma1_32(x)\n\t{\n\t\treturn rotr_32(x, 6) ^ rotr_32(x, 11) ^ rotr_32(x, 25);\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "sigma1_64",
                  "code": "function sigma1_64(x)\n\t{\n\t\tvar rotr14 = rotr_64(x, 14), rotr18 = rotr_64(x, 18),\n\t\t\trotr41 = rotr_64(x, 41);\n\n\t\treturn new Int_64(\n\t\t\t\trotr14.highOrder ^ rotr18.highOrder ^ rotr41.highOrder,\n\t\t\t\trotr14.lowOrder ^ rotr18.lowOrder ^ rotr41.lowOrder);\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "gamma0_32",
                  "code": "function gamma0_32(x)\n\t{\n\t\treturn rotr_32(x, 7) ^ rotr_32(x, 18) ^ shr_32(x, 3);\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "gamma0_64",
                  "code": "function gamma0_64(x)\n\t{\n\t\tvar rotr1 = rotr_64(x, 1), rotr8 = rotr_64(x, 8), shr7 = shr_64(x, 7);\n\n\t\treturn new Int_64(\n\t\t\t\trotr1.highOrder ^ rotr8.highOrder ^ shr7.highOrder,\n\t\t\t\trotr1.lowOrder ^ rotr8.lowOrder ^ shr7.lowOrder\n\t\t\t);\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "gamma1_32",
                  "code": "function gamma1_32(x)\n\t{\n\t\treturn rotr_32(x, 17) ^ rotr_32(x, 19) ^ shr_32(x, 10);\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "gamma1_64",
                  "code": "function gamma1_64(x)\n\t{\n\t\tvar rotr19 = rotr_64(x, 19), rotr61 = rotr_64(x, 61),\n\t\t\tshr6 = shr_64(x, 6);\n\n\t\treturn new Int_64(\n\t\t\t\trotr19.highOrder ^ rotr61.highOrder ^ shr6.highOrder,\n\t\t\t\trotr19.lowOrder ^ rotr61.lowOrder ^ shr6.lowOrder\n\t\t\t);\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "safeAdd_32_2",
                  "code": "function safeAdd_32_2(a, b)\n\t{\n\t\tvar lsw = (a & 0xFFFF) + (b & 0xFFFF),\n\t\t\tmsw = (a >>> 16) + (b >>> 16) + (lsw >>> 16);\n\n\t\treturn ((msw & 0xFFFF) << 16) | (lsw & 0xFFFF);\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "safeAdd_32_4",
                  "code": "function safeAdd_32_4(a, b, c, d)\n\t{\n\t\tvar lsw = (a & 0xFFFF) + (b & 0xFFFF) + (c & 0xFFFF) + (d & 0xFFFF),\n\t\t\tmsw = (a >>> 16) + (b >>> 16) + (c >>> 16) + (d >>> 16) +\n\t\t\t\t(lsw >>> 16);\n\n\t\treturn ((msw & 0xFFFF) << 16) | (lsw & 0xFFFF);\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "safeAdd_32_5",
                  "code": "function safeAdd_32_5(a, b, c, d, e)\n\t{\n\t\tvar lsw = (a & 0xFFFF) + (b & 0xFFFF) + (c & 0xFFFF) + (d & 0xFFFF) +\n\t\t\t\t(e & 0xFFFF),\n\t\t\tmsw = (a >>> 16) + (b >>> 16) + (c >>> 16) + (d >>> 16) +\n\t\t\t\t(e >>> 16) + (lsw >>> 16);\n\n\t\treturn ((msw & 0xFFFF) << 16) | (lsw & 0xFFFF);\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "safeAdd_64_2",
                  "code": "function safeAdd_64_2(x, y)\n\t{\n\t\tvar lsw, msw, lowOrder, highOrder;\n\n\t\tlsw = (x.lowOrder & 0xFFFF) + (y.lowOrder & 0xFFFF);\n\t\tmsw = (x.lowOrder >>> 16) + (y.lowOrder >>> 16) + (lsw >>> 16);\n\t\tlowOrder = ((msw & 0xFFFF) << 16) | (lsw & 0xFFFF);\n\n\t\tlsw = (x.highOrder & 0xFFFF) + (y.highOrder & 0xFFFF) + (msw >>> 16);\n\t\tmsw = (x.highOrder >>> 16) + (y.highOrder >>> 16) + (lsw >>> 16);\n\t\thighOrder = ((msw & 0xFFFF) << 16) | (lsw & 0xFFFF);\n\n\t\treturn new Int_64(highOrder, lowOrder);\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "safeAdd_64_4",
                  "code": "function safeAdd_64_4(a, b, c, d)\n\t{\n\t\tvar lsw, msw, lowOrder, highOrder;\n\n\t\tlsw = (a.lowOrder & 0xFFFF) + (b.lowOrder & 0xFFFF) +\n\t\t\t(c.lowOrder & 0xFFFF) + (d.lowOrder & 0xFFFF);\n\t\tmsw = (a.lowOrder >>> 16) + (b.lowOrder >>> 16) +\n\t\t\t(c.lowOrder >>> 16) + (d.lowOrder >>> 16) + (lsw >>> 16);\n\t\tlowOrder = ((msw & 0xFFFF) << 16) | (lsw & 0xFFFF);\n\n\t\tlsw = (a.highOrder & 0xFFFF) + (b.highOrder & 0xFFFF) +\n\t\t\t(c.highOrder & 0xFFFF) + (d.highOrder & 0xFFFF) + (msw >>> 16);\n\t\tmsw = (a.highOrder >>> 16) + (b.highOrder >>> 16) +\n\t\t\t(c.highOrder >>> 16) + (d.highOrder >>> 16) + (lsw >>> 16);\n\t\thighOrder = ((msw & 0xFFFF) << 16) | (lsw & 0xFFFF);\n\n\t\treturn new Int_64(highOrder, lowOrder);\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "safeAdd_64_5",
                  "code": "function safeAdd_64_5(a, b, c, d, e)\n\t{\n\t\tvar lsw, msw, lowOrder, highOrder;\n\n\t\tlsw = (a.lowOrder & 0xFFFF) + (b.lowOrder & 0xFFFF) +\n\t\t\t(c.lowOrder & 0xFFFF) + (d.lowOrder & 0xFFFF) +\n\t\t\t(e.lowOrder & 0xFFFF);\n\t\tmsw = (a.lowOrder >>> 16) + (b.lowOrder >>> 16) +\n\t\t\t(c.lowOrder >>> 16) + (d.lowOrder >>> 16) + (e.lowOrder >>> 16) +\n\t\t\t(lsw >>> 16);\n\t\tlowOrder = ((msw & 0xFFFF) << 16) | (lsw & 0xFFFF);\n\n\t\tlsw = (a.highOrder & 0xFFFF) + (b.highOrder & 0xFFFF) +\n\t\t\t(c.highOrder & 0xFFFF) + (d.highOrder & 0xFFFF) +\n\t\t\t(e.highOrder & 0xFFFF) + (msw >>> 16);\n\t\tmsw = (a.highOrder >>> 16) + (b.highOrder >>> 16) +\n\t\t\t(c.highOrder >>> 16) + (d.highOrder >>> 16) +\n\t\t\t(e.highOrder >>> 16) + (lsw >>> 16);\n\t\thighOrder = ((msw & 0xFFFF) << 16) | (lsw & 0xFFFF);\n\n\t\treturn new Int_64(highOrder, lowOrder);\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "getH",
                  "code": "function getH(variant)\n\t{\n\t\tvar retVal, H_trunc, H_full;\n\n\t\tif ((\"SHA-1\" === variant) && (1 & SUPPORTED_ALGS))\n\t\t{\n\t\t\tretVal = [\n\t\t\t\t0x67452301, 0xefcdab89, 0x98badcfe, 0x10325476, 0xc3d2e1f0\n\t\t\t];\n\t\t}\n\t\telse if (6 & SUPPORTED_ALGS)\n\t\t{\n\t\t\tH_trunc = [\n\t\t\t\t0xc1059ed8, 0x367cd507, 0x3070dd17, 0xf70e5939,\n\t\t\t\t0xffc00b31, 0x68581511, 0x64f98fa7, 0xbefa4fa4\n\t\t\t];\n\t\t\tH_full = [\n\t\t\t\t0x6A09E667, 0xBB67AE85, 0x3C6EF372, 0xA54FF53A,\n\t\t\t\t0x510E527F, 0x9B05688C, 0x1F83D9AB, 0x5BE0CD19\n\t\t\t];\n\n\t\t\tswitch (variant)\n\t\t\t{\n\t\t\tcase \"SHA-224\":\n\t\t\t\tretVal = H_trunc;\n\t\t\t\tbreak;\n\t\t\tcase \"SHA-256\":\n\t\t\t\tretVal = H_full;\n\t\t\t\tbreak;\n\t\t\tcase \"SHA-384\":\n\t\t\t\tretVal = [\n\t\t\t\t\tnew Int_64(0xcbbb9d5d, H_trunc[0]),\n\t\t\t\t\tnew Int_64(0x0629a292a, H_trunc[1]),\n\t\t\t\t\tnew Int_64(0x9159015a, H_trunc[2]),\n\t\t\t\t\tnew Int_64(0x0152fecd8, H_trunc[3]),\n\t\t\t\t\tnew Int_64(0x67332667, H_trunc[4]),\n\t\t\t\t\tnew Int_64(0x98eb44a87, H_trunc[5]),\n\t\t\t\t\tnew Int_64(0xdb0c2e0d, H_trunc[6]),\n\t\t\t\t\tnew Int_64(0x047b5481d, H_trunc[7])\n\t\t\t\t];\n\t\t\t\tbreak;\n\t\t\tcase \"SHA-512\":\n\t\t\t\tretVal = [\n\t\t\t\t\tnew Int_64(H_full[0], 0xf3bcc908),\n\t\t\t\t\tnew Int_64(H_full[1], 0x84caa73b),\n\t\t\t\t\tnew Int_64(H_full[2], 0xfe94f82b),\n\t\t\t\t\tnew Int_64(H_full[3], 0x5f1d36f1),\n\t\t\t\t\tnew Int_64(H_full[4], 0xade682d1),\n\t\t\t\t\tnew Int_64(H_full[5], 0x2b3e6c1f),\n\t\t\t\t\tnew Int_64(H_full[6], 0xfb41bd6b),\n\t\t\t\t\tnew Int_64(H_full[7], 0x137e2179)\n\t\t\t\t];\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tthrow new Error(\"Unknown SHA variant\");\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\tthrow new Error(\"No SHA variants supported\");\n\t\t}\n\n\t\treturn retVal;\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "roundSHA1",
                  "code": "function roundSHA1(block, H)\n\t{\n\t\tvar W = [], a, b, c, d, e, T, ch = ch_32, parity = parity_32,\n\t\t\tmaj = maj_32, rotl = rotl_32, safeAdd_2 = safeAdd_32_2, t,\n\t\t\tsafeAdd_5 = safeAdd_32_5;\n\n\t\ta = H[0];\n\t\tb = H[1];\n\t\tc = H[2];\n\t\td = H[3];\n\t\te = H[4];\n\n\t\tfor (t = 0; t < 80; t += 1)\n\t\t{\n\t\t\tif (t < 16)\n\t\t\t{\n\t\t\t\tW[t] = block[t];\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tW[t] = rotl(W[t - 3] ^ W[t - 8] ^ W[t - 14] ^ W[t - 16], 1);\n\t\t\t}\n\n\t\t\tif (t < 20)\n\t\t\t{\n\t\t\t\tT = safeAdd_5(rotl(a, 5), ch(b, c, d), e, 0x5a827999, W[t]);\n\t\t\t}\n\t\t\telse if (t < 40)\n\t\t\t{\n\t\t\t\tT = safeAdd_5(rotl(a, 5), parity(b, c, d), e, 0x6ed9eba1, W[t]);\n\t\t\t}\n\t\t\telse if (t < 60)\n\t\t\t{\n\t\t\t\tT = safeAdd_5(rotl(a, 5), maj(b, c, d), e, 0x8f1bbcdc, W[t]);\n\t\t\t} else {\n\t\t\t\tT = safeAdd_5(rotl(a, 5), parity(b, c, d), e, 0xca62c1d6, W[t]);\n\t\t\t}\n\n\t\t\te = d;\n\t\t\td = c;\n\t\t\tc = rotl(b, 30);\n\t\t\tb = a;\n\t\t\ta = T;\n\t\t}\n\n\t\tH[0] = safeAdd_2(a, H[0]);\n\t\tH[1] = safeAdd_2(b, H[1]);\n\t\tH[2] = safeAdd_2(c, H[2]);\n\t\tH[3] = safeAdd_2(d, H[3]);\n\t\tH[4] = safeAdd_2(e, H[4]);\n\n\t\treturn H;\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "finalizeSHA1",
                  "code": "function finalizeSHA1(remainder, remainderBinLen, processedBinLen, H)\n\t{\n\t\tvar i, appendedMessageLength, offset;\n\n\t\t/* The 65 addition is a hack but it works.  The correct number is\n\t\t   actually 72 (64 + 8) but the below math fails if\n\t\t   remainderBinLen + 72 % 512 = 0. Since remainderBinLen % 8 = 0,\n\t\t   \"shorting\" the addition is OK. */\n\t\toffset = (((remainderBinLen + 65) >>> 9) << 4) + 15;\n\t\twhile (remainder.length <= offset)\n\t\t{\n\t\t\tremainder.push(0);\n\t\t}\n\t\t/* Append '1' at the end of the binary string */\n\t\tremainder[remainderBinLen >>> 5] |= 0x80 << (24 - (remainderBinLen % 32));\n\t\t/* Append length of binary string in the position such that the new\n\t\tlength is a multiple of 512.  Logic does not work for even multiples\n\t\tof 512 but there can never be even multiples of 512 */\n\t\tremainder[offset] = remainderBinLen + processedBinLen;\n\n\t\tappendedMessageLength = remainder.length;\n\n\t\t/* This will always be at least 1 full chunk */\n\t\tfor (i = 0; i < appendedMessageLength; i += 16)\n\t\t{\n\t\t\tH = roundSHA1(remainder.slice(i, i + 16), H);\n\t\t}\n\n\t\treturn H;\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "roundSHA2",
                  "code": "function roundSHA2(block, H, variant)\n\t{\n\t\tvar a, b, c, d, e, f, g, h, T1, T2, numRounds, t, binaryStringMult,\n\t\t\tsafeAdd_2, safeAdd_4, safeAdd_5, gamma0, gamma1, sigma0, sigma1,\n\t\t\tch, maj, Int, W = [], int1, int2, offset, K;\n\n\t\t/* Set up the various function handles and variable for the specific\n\t\t * variant */\n\t\tif ((variant === \"SHA-224\" || variant === \"SHA-256\") &&\n\t\t\t(2 & SUPPORTED_ALGS))\n\t\t{\n\t\t\t/* 32-bit variant */\n\t\t\tnumRounds = 64;\n\t\t\tbinaryStringMult = 1;\n\t\t\tInt = Number;\n\t\t\tsafeAdd_2 = safeAdd_32_2;\n\t\t\tsafeAdd_4 = safeAdd_32_4;\n\t\t\tsafeAdd_5 = safeAdd_32_5;\n\t\t\tgamma0 = gamma0_32;\n\t\t\tgamma1 = gamma1_32;\n\t\t\tsigma0 = sigma0_32;\n\t\t\tsigma1 = sigma1_32;\n\t\t\tmaj = maj_32;\n\t\t\tch = ch_32;\n\t\t\tK = K_sha2;\n\t\t}\n\t\telse if ((variant === \"SHA-384\" || variant === \"SHA-512\") &&\n\t\t\t(4 & SUPPORTED_ALGS))\n\t\t{\n\t\t\t/* 64-bit variant */\n\t\t\tnumRounds = 80;\n\t\t\tbinaryStringMult = 2;\n\t\t\tInt = Int_64;\n\t\t\tsafeAdd_2 = safeAdd_64_2;\n\t\t\tsafeAdd_4 = safeAdd_64_4;\n\t\t\tsafeAdd_5 = safeAdd_64_5;\n\t\t\tgamma0 = gamma0_64;\n\t\t\tgamma1 = gamma1_64;\n\t\t\tsigma0 = sigma0_64;\n\t\t\tsigma1 = sigma1_64;\n\t\t\tmaj = maj_64;\n\t\t\tch = ch_64;\n\t\t\tK = K_sha512;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tthrow new Error(\"Unexpected error in SHA-2 implementation\");\n\t\t}\n\n\t\ta = H[0];\n\t\tb = H[1];\n\t\tc = H[2];\n\t\td = H[3];\n\t\te = H[4];\n\t\tf = H[5];\n\t\tg = H[6];\n\t\th = H[7];\n\n\t\tfor (t = 0; t < numRounds; t += 1)\n\t\t{\n\t\t\tif (t < 16)\n\t\t\t{\n\t\t\t\toffset = t * binaryStringMult;\n\t\t\t\tint1 = (block.length <= offset) ? 0 : block[offset];\n\t\t\t\tint2 = (block.length <= offset + 1) ? 0 : block[offset + 1];\n\t\t\t\t/* Bit of a hack - for 32-bit, the second term is ignored */\n\t\t\t\tW[t] = new Int(int1, int2);\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tW[t] = safeAdd_4(\n\t\t\t\t\t\tgamma1(W[t - 2]), W[t - 7],\n\t\t\t\t\t\tgamma0(W[t - 15]), W[t - 16]\n\t\t\t\t\t);\n\t\t\t}\n\n\t\t\tT1 = safeAdd_5(h, sigma1(e), ch(e, f, g), K[t], W[t]);\n\t\t\tT2 = safeAdd_2(sigma0(a), maj(a, b, c));\n\t\t\th = g;\n\t\t\tg = f;\n\t\t\tf = e;\n\t\t\te = safeAdd_2(d, T1);\n\t\t\td = c;\n\t\t\tc = b;\n\t\t\tb = a;\n\t\t\ta = safeAdd_2(T1, T2);\n\t\t}\n\n\t\tH[0] = safeAdd_2(a, H[0]);\n\t\tH[1] = safeAdd_2(b, H[1]);\n\t\tH[2] = safeAdd_2(c, H[2]);\n\t\tH[3] = safeAdd_2(d, H[3]);\n\t\tH[4] = safeAdd_2(e, H[4]);\n\t\tH[5] = safeAdd_2(f, H[5]);\n\t\tH[6] = safeAdd_2(g, H[6]);\n\t\tH[7] = safeAdd_2(h, H[7]);\n\n\t\treturn H;\n\t}",
                  "file": "jsOTP.js"
                },
                {
                  "type": "function",
                  "name": "finalizeSHA2",
                  "code": "function finalizeSHA2(remainder, remainderBinLen, processedBinLen, H, variant)\n\t{\n\t\tvar i, appendedMessageLength, offset, retVal, binaryStringInc;\n\n\t\tif ((variant === \"SHA-224\" || variant === \"SHA-256\") &&\n\t\t\t(2 & SUPPORTED_ALGS))\n\t\t{\n\t\t\t/* 32-bit variant */\n\t\t\t/* The 65 addition is a hack but it works.  The correct number is\n\t\t\t   actually 72 (64 + 8) but the below math fails if\n\t\t\t   remainderBinLen + 72 % 512 = 0. Since remainderBinLen % 8 = 0,\n\t\t\t   \"shorting\" the addition is OK. */\n\t\t\toffset = (((remainderBinLen + 65) >>> 9) << 4) + 15;;\n\t\t\tbinaryStringInc = 16;\n\t\t}\n\t\telse if ((variant === \"SHA-384\" || variant === \"SHA-512\") &&\n\t\t\t(4 & SUPPORTED_ALGS))\n\t\t{\n\t\t\t/* 64-bit variant */\n\t\t\t/* The 129 addition is a hack but it works.  The correct number is\n\t\t\t   actually 136 (128 + 8) but the below math fails if\n\t\t\t   remainderBinLen + 136 % 1024 = 0. Since remainderBinLen % 8 = 0,\n\t\t\t   \"shorting\" the addition is OK. */\n\t\t\toffset = (((remainderBinLen + 129) >>> 10) << 5) + 31;\n\t\t\tbinaryStringInc = 32;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tthrow new Error(\"Unexpected error in SHA-2 implementation\");\n\t\t}\n\n\t\twhile (remainder.length <= offset)\n\t\t{\n\t\t\tremainder.push(0);\n\t\t}\n\t\t/* Append '1' at the end of the binary string */\n\t\tremainder[remainderBinLen >>> 5] |= 0x80 << (24 - remainderBinLen % 32);\n\t\t/* Append length of binary string in the position such that the new\n\t\t * length is correct */\n\t\tremainder[offset] = remainderBinLen + processedBinLen;\n\n\t\tappendedMessageLength = remainder.length;\n\n\t\t/* This will always be at least 1 full chunk */\n\t\tfor (i = 0; i < appendedMessageLength; i += binaryStringInc)\n\t\t{\n\t\t\tH = roundSHA2(remainder.slice(i, i + binaryStringInc), H, variant);\n\t\t}\n\n\t\tif ((\"SHA-224\" === variant) && (2 & SUPPORTED_ALGS))\n\t\t{\n\t\t\tretVal = [\n\t\t\t\tH[0], H[1], H[2], H[3],\n\t\t\t\tH[4], H[5], H[6]\n\t\t\t];\n\t\t}\n\t\telse if ((\"SHA-256\" === variant) && (2 & SUPPORTED_ALGS))\n\t\t{\n\t\t\tretVal = H;\n\t\t}\n\t\telse if ((\"SHA-384\" === variant) && (4 & SUPPORTED_ALGS))\n\t\t{\n\t\t\tretVal = [\n\t\t\t\tH[0].highOrder, H[0].lowOrder,\n\t\t\t\tH[1].highOrder, H[1].lowOrder,\n\t\t\t\tH[2].highOrder, H[2].lowOrder,\n\t\t\t\tH[3].highOrder, H[3].lowOrder,\n\t\t\t\tH[4].highOrder, H[4].lowOrder,\n\t\t\t\tH[5].highOrder, H[5].lowOrder\n\t\t\t];\n\t\t}\n\t\telse if ((\"SHA-512\" === variant) && (4 & SUPPORTED_ALGS))\n\t\t{\n\t\t\tretVal = [\n\t\t\t\tH[0].highOrder, H[0].lowOrder,\n\t\t\t\tH[1].highOrder, H[1].lowOrder,\n\t\t\t\tH[2].highOrder, H[2].lowOrder,\n\t\t\t\tH[3].highOrder, H[3].lowOrder,\n\t\t\t\tH[4].highOrder, H[4].lowOrder,\n\t\t\t\tH[5].highOrder, H[5].lowOrder,\n\t\t\t\tH[6].highOrder, H[6].lowOrder,\n\t\t\t\tH[7].highOrder, H[7].lowOrder\n\t\t\t];\n\t\t}\n\t\telse /* This should never be reached */\n\t\t{\n\t\t\tthrow new Error(\"Unexpected error in SHA-2 implementation\");\n\t\t}\n\n\t\treturn retVal;\n\t}",
                  "file": "jsOTP.js"
                }
              ]
            }
          }
        }
      }
    }
  },
  "install_files": {
    "ansible-base": {
      "roles": {
        "restore": {
          "files": {
            "compare_torrc.py": [
              {
                "type": "function",
                "name": "get_tor_versions",
                "code": "def get_tor_versions(path):\n    \"\"\"\n    Determine which service versions are offered in the given torrc.\n    \"\"\"\n    service_re = re.compile(r\"HiddenServiceDir\\s+(?:.*)/(.*)\")\n    versions = set()\n    with open(path) as f:\n        for line in f:\n            m = service_re.match(line)\n            if m:\n                service = m.group(1)\n                if \"v3\" in service:\n                    versions.add(3)\n                else:\n                    versions.add(2)\n\n    return versions",
                "file": "compare_torrc.py"
              },
              {
                "type": "function",
                "name": "strset",
                "code": "def strset(s):\n    \"\"\"\n    Sort the given set and join members with \"and\".\n    \"\"\"\n    return \" and \".join(str(v) for v in sorted(s))",
                "file": "compare_torrc.py"
              }
            ]
          }
        }
      },
      "callback_plugins": {
        "ansible_version_check.py": [
          {
            "type": "function",
            "name": "print_red_bold",
            "code": "def print_red_bold(text):\n    print(\"\\x1b[31;1m\" + text + \"\\x1b[0m\")",
            "file": "ansible_version_check.py"
          },
          {
            "type": "class",
            "name": "CallbackModule",
            "code": "class CallbackModule(CallbackBase):\n    def __init__(self):\n        # The acceptable version range needs to be synchronized with\n        # requirements files.\n        viable_start = [2, 13, 0]\n        viable_end = [2, 15, 10]\n        ansible_version = [int(v) for v in ansible.__version__.split(\".\")]\n        if not (viable_start <= ansible_version < viable_end):\n            print_red_bold(\n                \"SecureDrop restriction: Ansible version must be at least {viable_start} \"\n                \"and less than {viable_end}.\".format(\n                    viable_start=\".\".join(str(v) for v in viable_start),\n                    viable_end=\".\".join(str(v) for v in viable_end),\n                )\n            )\n            sys.exit(1)",
            "file": "ansible_version_check.py"
          },
          {
            "type": "function",
            "name": "__init__",
            "code": "def __init__(self):\n        # The acceptable version range needs to be synchronized with\n        # requirements files.\n        viable_start = [2, 13, 0]\n        viable_end = [2, 15, 10]\n        ansible_version = [int(v) for v in ansible.__version__.split(\".\")]\n        if not (viable_start <= ansible_version < viable_end):\n            print_red_bold(\n                \"SecureDrop restriction: Ansible version must be at least {viable_start} \"\n                \"and less than {viable_end}.\".format(\n                    viable_start=\".\".join(str(v) for v in viable_start),\n                    viable_end=\".\".join(str(v) for v in viable_end),\n                )\n            )\n            sys.exit(1)",
            "file": "ansible_version_check.py"
          }
        ]
      }
    }
  },
  "journalist_gui": {
    "test_gui.py": [
      {
        "type": "class",
        "name": "TestSecondInstancePrevention",
        "code": "class TestSecondInstancePrevention(unittest.TestCase):\n    def setUp(self):\n        self.mock_app = mock.MagicMock()\n        self.mock_app.applicationName = mock.MagicMock(return_value=\"sd\")\n\n    @staticmethod\n    def socket_mock_generator(already_bound_errno=98):\n        namespace = set()\n\n        def kernel_bind(addr):\n            if addr in namespace:\n                error = OSError()\n                error.errno = already_bound_errno\n                raise error\n            else:\n                namespace.add(addr)\n\n        socket_mock = mock.MagicMock()\n        socket_mock.socket().bind = mock.MagicMock(side_effect=kernel_bind)\n        return socket_mock\n\n    def test_diff_name(self, mock_msgbox, mock_exit):\n        mock_socket = self.socket_mock_generator()\n        with mock.patch(\"journalist_gui.SecureDropUpdater.socket\", new=mock_socket):\n            prevent_second_instance(self.mock_app, \"name1\")\n            prevent_second_instance(self.mock_app, \"name2\")\n\n            mock_exit.assert_not_called()\n\n    def test_same_name(self, mock_msgbox, mock_exit):\n        mock_socket = self.socket_mock_generator()\n        with mock.patch(\"journalist_gui.SecureDropUpdater.socket\", new=mock_socket):\n            prevent_second_instance(self.mock_app, \"name1\")\n            prevent_second_instance(self.mock_app, \"name1\")\n\n            mock_exit.assert_any_call()\n\n    def test_unknown_kernel_error(self, mock_msgbox, mock_exit):\n        mock_socket = self.socket_mock_generator(131)  # crazy unexpected error\n        with mock.patch(\"journalist_gui.SecureDropUpdater.socket\", new=mock_socket):\n            prevent_second_instance(self.mock_app, \"name1\")\n            with pytest.raises(OSError):\n                prevent_second_instance(self.mock_app, \"name1\")",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "setUp",
        "code": "def setUp(self):\n        self.mock_app = mock.MagicMock()\n        self.mock_app.applicationName = mock.MagicMock(return_value=\"sd\")",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "socket_mock_generator",
        "code": "def socket_mock_generator(already_bound_errno=98):\n        namespace = set()\n\n        def kernel_bind(addr):\n            if addr in namespace:\n                error = OSError()\n                error.errno = already_bound_errno\n                raise error\n            else:\n                namespace.add(addr)\n\n        socket_mock = mock.MagicMock()\n        socket_mock.socket().bind = mock.MagicMock(side_effect=kernel_bind)\n        return socket_mock",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "kernel_bind",
        "code": "def kernel_bind(addr):\n            if addr in namespace:\n                error = OSError()\n                error.errno = already_bound_errno\n                raise error\n            else:\n                namespace.add(addr)",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_diff_name",
        "code": "def test_diff_name(self, mock_msgbox, mock_exit):\n        mock_socket = self.socket_mock_generator()\n        with mock.patch(\"journalist_gui.SecureDropUpdater.socket\", new=mock_socket):\n            prevent_second_instance(self.mock_app, \"name1\")\n            prevent_second_instance(self.mock_app, \"name2\")\n\n            mock_exit.assert_not_called()",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_same_name",
        "code": "def test_same_name(self, mock_msgbox, mock_exit):\n        mock_socket = self.socket_mock_generator()\n        with mock.patch(\"journalist_gui.SecureDropUpdater.socket\", new=mock_socket):\n            prevent_second_instance(self.mock_app, \"name1\")\n            prevent_second_instance(self.mock_app, \"name1\")\n\n            mock_exit.assert_any_call()",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_unknown_kernel_error",
        "code": "def test_unknown_kernel_error(self, mock_msgbox, mock_exit):\n        mock_socket = self.socket_mock_generator(131)  # crazy unexpected error\n        with mock.patch(\"journalist_gui.SecureDropUpdater.socket\", new=mock_socket):\n            prevent_second_instance(self.mock_app, \"name1\")\n            with pytest.raises(OSError):\n                prevent_second_instance(self.mock_app, \"name1\")",
        "file": "test_gui.py"
      },
      {
        "type": "class",
        "name": "AppTestCase",
        "code": "class AppTestCase(unittest.TestCase):\n    def setUp(self):\n        qApp = QApplication.instance()\n        if qApp is None:\n            self.app = QApplication([\"\"])\n        else:\n            self.app = qApp",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "setUp",
        "code": "def setUp(self):\n        qApp = QApplication.instance()\n        if qApp is None:\n            self.app = QApplication([\"\"])\n        else:\n            self.app = qApp",
        "file": "test_gui.py"
      },
      {
        "type": "class",
        "name": "WindowTestCase",
        "code": "class WindowTestCase(AppTestCase):\n    def setUp(self):\n        super().setUp()\n        self.window = UpdaterApp()\n        self.window.show()\n        QTest.qWaitForWindowExposed(self.window)\n\n    def test_window_is_a_fixed_size(self):\n        # Verify the size policy is fixed\n        expected_sizePolicy = QSizePolicy(QSizePolicy.Fixed, QSizePolicy.Fixed)\n        assert self.window.sizePolicy() == expected_sizePolicy\n\n        # Verify the maximum and minimum sizes are the same as the current size\n        current_size = self.window.size()\n        assert self.window.minimumSize() == current_size\n        assert self.window.maximumSize() == current_size\n\n    def test_clicking_install_later_exits_the_application(self):\n        QTest.mouseClick(self.window.pushButton, Qt.LeftButton)\n        assert not self.window.isVisible()\n\n    def test_progress_bar_begins_at_zero(self):\n        assert self.window.progressBar.value() == 0\n\n    def test_default_tab(self):\n        assert self.window.tabWidget.currentIndex() == 0\n\n    def test_output_tab(self):\n        tab = self.window.tabWidget.tabBar()\n        QTest.mouseClick(tab, Qt.LeftButton)\n        assert self.window.tabWidget.currentIndex() == self.window.tabWidget.indexOf(\n            self.window.tab_2\n        )\n\n    @mock.patch(\"subprocess.check_output\", return_value=b\"Python dependencies for securedrop-admin\")\n    def test_setupThread(self, check_output):\n        with mock.patch.object(self.window, \"call_tailsconfig\", return_value=MagicMock()):\n            with mock.patch(\"builtins.open\") as mock_open:\n                self.window.setup_thread.run()  # Call run directly\n\n            mock_open.assert_called_once_with(FLAG_LOCATION, \"a\")\n            assert self.window.update_success == True\n            assert self.window.progressBar.value() == 70\n\n    @mock.patch(\"subprocess.check_output\", return_value=b\"Failed to install pip dependencies\")\n    def test_setupThread_failure(self, check_output):\n        with mock.patch.object(self.window, \"call_tailsconfig\", return_value=MagicMock()):\n            with mock.patch(\"builtins.open\") as mock_open:\n                self.window.setup_thread.run()  # Call run directly\n\n            mock_open.assert_called_once_with(FLAG_LOCATION, \"a\")\n            assert self.window.update_success == False\n            assert self.window.progressBar.value() == 0\n            assert self.window.failure_reason == strings.update_failed_generic_reason\n\n    @mock.patch(\"subprocess.check_output\", return_value=b\"Signature verification successful\")\n    def test_updateThread(self, check_output):\n        with mock.patch.object(self.window, \"setup_thread\", return_value=MagicMock()):\n            self.window.update_thread.run()  # Call run directly\n            assert self.window.update_success == True\n            assert self.window.progressBar.value() == 50\n\n    @mock.patch(\n        \"subprocess.check_output\",\n        side_effect=subprocess.CalledProcessError(1, \"cmd\", b\"Signature verification failed\"),\n    )\n    def test_updateThread_failure(self, check_output):\n        with mock.patch.object(self.window, \"setup_thread\", return_value=MagicMock()):\n            self.window.update_thread.run()  # Call run directly\n            assert self.window.update_success == False\n            assert self.window.failure_reason == strings.update_failed_sig_failure\n\n    @mock.patch(\n        \"subprocess.check_output\",\n        side_effect=subprocess.CalledProcessError(1, \"cmd\", b\"Generic other failure\"),\n    )\n    def test_updateThread_generic_failure(self, check_output):\n        with mock.patch.object(self.window, \"setup_thread\", return_value=MagicMock()):\n            self.window.update_thread.run()  # Call run directly\n            assert self.window.update_success == False\n            assert self.window.failure_reason == strings.update_failed_generic_reason\n\n    def test_get_sudo_password_when_password_provided(self):\n        expected_password = \"password\"\n\n        with mock.patch.object(QInputDialog, \"getText\", return_value=[expected_password, True]):\n            sudo_password = self.window.get_sudo_password()\n\n        assert sudo_password == expected_password\n\n    def test_get_sudo_password_when_password_not_provided(self):\n        test_password = \"\"\n\n        with mock.patch.object(QInputDialog, \"getText\", return_value=[test_password, False]):\n            assert self.window.get_sudo_password() is None\n\n    @mock.patch(\"pexpect.spawn\")\n    def test_tailsconfigThread_no_failures(self, pt):\n        child = pt()\n        before = MagicMock()\n\n        before.decode.side_effect = [\"SUDO: \", \"Update successful. failed=0\"]\n        child.before = before\n        child.exitstatus = 0\n        with mock.patch(\"os.remove\") as mock_remove:\n            self.window.tails_thread.run()\n\n        mock_remove.assert_called_once_with(FLAG_LOCATION)\n        assert \"failed=0\" in self.window.output\n        assert self.window.update_success == True\n\n    @mock.patch(\"pexpect.spawn\")\n    def test_tailsconfigThread_generic_failure(self, pt):\n        child = pt()\n        before = MagicMock()\n        before.decode.side_effect = [\"SUDO: \", \"failed=10 ERROR!!!!!\"]\n        child.before = before\n        self.window.tails_thread.run()\n        assert \"failed=0\" not in self.window.output\n        assert self.window.update_success == False\n        assert self.window.failure_reason == strings.tailsconfig_failed_generic_reason\n\n    @mock.patch(\"pexpect.spawn\")\n    def test_tailsconfigThread_sudo_password_is_wrong(self, pt):\n        child = pt()\n        before = MagicMock()\n        before.decode.return_value = \"stuff[sudo via ansible, key=blahblahblah\"\n        child.before = before\n        self.window.tails_thread.run()\n        assert \"failed=0\" not in self.window.output\n        assert self.window.update_success == False\n        assert self.window.failure_reason == strings.tailsconfig_failed_sudo_password\n\n    @mock.patch(\"pexpect.spawn\")\n    def test_tailsconfigThread_timeout(self, pt):\n        child = pt()\n        before = MagicMock()\n        before.decode.side_effect = [\"some data\", pexpect.exceptions.TIMEOUT(1)]\n        child.before = before\n        self.window.tails_thread.run()\n        assert \"failed=0\" not in self.window.output\n        assert self.window.update_success == False\n        assert self.window.failure_reason == strings.tailsconfig_failed_timeout\n\n    @mock.patch(\"pexpect.spawn\")\n    def test_tailsconfigThread_some_other_subprocess_error(self, pt):\n        child = pt()\n        before = MagicMock()\n        before.decode.side_effect = subprocess.CalledProcessError(\n            1, \"cmd\", b\"Generic other failure\"\n        )\n        child.before = before\n        self.window.tails_thread.run()\n        assert \"failed=0\" not in self.window.output\n        assert self.window.update_success == False\n        assert self.window.failure_reason == strings.tailsconfig_failed_generic_reason\n\n    def test_tails_status_success(self):\n        result = {\"status\": True, \"output\": \"successful.\", \"failure_reason\": \"\"}\n\n        with mock.patch(\"os.remove\") as mock_remove:\n            self.window.tails_status(result)\n\n        # We do remove the flag file if the update does finish\n        mock_remove.assert_called_once_with(FLAG_LOCATION)\n        assert self.window.progressBar.value() == 100\n\n    def test_tails_status_failure(self):\n        result = {\"status\": False, \"output\": \"successful.\", \"failure_reason\": \"42\"}\n\n        with mock.patch(\"os.remove\") as mock_remove:\n            self.window.tails_status(result)\n\n        # We do not remove the flag file if the update does not finish\n        mock_remove.assert_not_called()\n        assert self.window.progressBar.value() == 0\n\n    @mock.patch(\"journalist_gui.SecureDropUpdater.QtWidgets.QMessageBox\")\n    def test_no_update_without_password(self, mock_msgbox):\n        with mock.patch(\"journalist_gui.SecureDropUpdater.password_is_set\", return_value=False):\n            self.window.update_securedrop()\n        assert self.window.pushButton.isEnabled() == True\n        assert self.window.pushButton_2.isEnabled() == False",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "setUp",
        "code": "def setUp(self):\n        super().setUp()\n        self.window = UpdaterApp()\n        self.window.show()\n        QTest.qWaitForWindowExposed(self.window)",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_window_is_a_fixed_size",
        "code": "def test_window_is_a_fixed_size(self):\n        # Verify the size policy is fixed\n        expected_sizePolicy = QSizePolicy(QSizePolicy.Fixed, QSizePolicy.Fixed)\n        assert self.window.sizePolicy() == expected_sizePolicy\n\n        # Verify the maximum and minimum sizes are the same as the current size\n        current_size = self.window.size()\n        assert self.window.minimumSize() == current_size\n        assert self.window.maximumSize() == current_size",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_clicking_install_later_exits_the_application",
        "code": "def test_clicking_install_later_exits_the_application(self):\n        QTest.mouseClick(self.window.pushButton, Qt.LeftButton)\n        assert not self.window.isVisible()",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_progress_bar_begins_at_zero",
        "code": "def test_progress_bar_begins_at_zero(self):\n        assert self.window.progressBar.value() == 0",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_default_tab",
        "code": "def test_default_tab(self):\n        assert self.window.tabWidget.currentIndex() == 0",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_output_tab",
        "code": "def test_output_tab(self):\n        tab = self.window.tabWidget.tabBar()\n        QTest.mouseClick(tab, Qt.LeftButton)\n        assert self.window.tabWidget.currentIndex() == self.window.tabWidget.indexOf(\n            self.window.tab_2\n        )",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_setupThread",
        "code": "def test_setupThread(self, check_output):\n        with mock.patch.object(self.window, \"call_tailsconfig\", return_value=MagicMock()):\n            with mock.patch(\"builtins.open\") as mock_open:\n                self.window.setup_thread.run()  # Call run directly\n\n            mock_open.assert_called_once_with(FLAG_LOCATION, \"a\")\n            assert self.window.update_success == True\n            assert self.window.progressBar.value() == 70",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_setupThread_failure",
        "code": "def test_setupThread_failure(self, check_output):\n        with mock.patch.object(self.window, \"call_tailsconfig\", return_value=MagicMock()):\n            with mock.patch(\"builtins.open\") as mock_open:\n                self.window.setup_thread.run()  # Call run directly\n\n            mock_open.assert_called_once_with(FLAG_LOCATION, \"a\")\n            assert self.window.update_success == False\n            assert self.window.progressBar.value() == 0\n            assert self.window.failure_reason == strings.update_failed_generic_reason",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_updateThread",
        "code": "def test_updateThread(self, check_output):\n        with mock.patch.object(self.window, \"setup_thread\", return_value=MagicMock()):\n            self.window.update_thread.run()  # Call run directly\n            assert self.window.update_success == True\n            assert self.window.progressBar.value() == 50",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_updateThread_failure",
        "code": "def test_updateThread_failure(self, check_output):\n        with mock.patch.object(self.window, \"setup_thread\", return_value=MagicMock()):\n            self.window.update_thread.run()  # Call run directly\n            assert self.window.update_success == False\n            assert self.window.failure_reason == strings.update_failed_sig_failure",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_updateThread_generic_failure",
        "code": "def test_updateThread_generic_failure(self, check_output):\n        with mock.patch.object(self.window, \"setup_thread\", return_value=MagicMock()):\n            self.window.update_thread.run()  # Call run directly\n            assert self.window.update_success == False\n            assert self.window.failure_reason == strings.update_failed_generic_reason",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_get_sudo_password_when_password_provided",
        "code": "def test_get_sudo_password_when_password_provided(self):\n        expected_password = \"password\"\n\n        with mock.patch.object(QInputDialog, \"getText\", return_value=[expected_password, True]):\n            sudo_password = self.window.get_sudo_password()\n\n        assert sudo_password == expected_password",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_get_sudo_password_when_password_not_provided",
        "code": "def test_get_sudo_password_when_password_not_provided(self):\n        test_password = \"\"\n\n        with mock.patch.object(QInputDialog, \"getText\", return_value=[test_password, False]):\n            assert self.window.get_sudo_password() is None",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_tailsconfigThread_no_failures",
        "code": "def test_tailsconfigThread_no_failures(self, pt):\n        child = pt()\n        before = MagicMock()\n\n        before.decode.side_effect = [\"SUDO: \", \"Update successful. failed=0\"]\n        child.before = before\n        child.exitstatus = 0\n        with mock.patch(\"os.remove\") as mock_remove:\n            self.window.tails_thread.run()\n\n        mock_remove.assert_called_once_with(FLAG_LOCATION)\n        assert \"failed=0\" in self.window.output\n        assert self.window.update_success == True",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_tailsconfigThread_generic_failure",
        "code": "def test_tailsconfigThread_generic_failure(self, pt):\n        child = pt()\n        before = MagicMock()\n        before.decode.side_effect = [\"SUDO: \", \"failed=10 ERROR!!!!!\"]\n        child.before = before\n        self.window.tails_thread.run()\n        assert \"failed=0\" not in self.window.output\n        assert self.window.update_success == False\n        assert self.window.failure_reason == strings.tailsconfig_failed_generic_reason",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_tailsconfigThread_sudo_password_is_wrong",
        "code": "def test_tailsconfigThread_sudo_password_is_wrong(self, pt):\n        child = pt()\n        before = MagicMock()\n        before.decode.return_value = \"stuff[sudo via ansible, key=blahblahblah\"\n        child.before = before\n        self.window.tails_thread.run()\n        assert \"failed=0\" not in self.window.output\n        assert self.window.update_success == False\n        assert self.window.failure_reason == strings.tailsconfig_failed_sudo_password",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_tailsconfigThread_timeout",
        "code": "def test_tailsconfigThread_timeout(self, pt):\n        child = pt()\n        before = MagicMock()\n        before.decode.side_effect = [\"some data\", pexpect.exceptions.TIMEOUT(1)]\n        child.before = before\n        self.window.tails_thread.run()\n        assert \"failed=0\" not in self.window.output\n        assert self.window.update_success == False\n        assert self.window.failure_reason == strings.tailsconfig_failed_timeout",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_tailsconfigThread_some_other_subprocess_error",
        "code": "def test_tailsconfigThread_some_other_subprocess_error(self, pt):\n        child = pt()\n        before = MagicMock()\n        before.decode.side_effect = subprocess.CalledProcessError(\n            1, \"cmd\", b\"Generic other failure\"\n        )\n        child.before = before\n        self.window.tails_thread.run()\n        assert \"failed=0\" not in self.window.output\n        assert self.window.update_success == False\n        assert self.window.failure_reason == strings.tailsconfig_failed_generic_reason",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_tails_status_success",
        "code": "def test_tails_status_success(self):\n        result = {\"status\": True, \"output\": \"successful.\", \"failure_reason\": \"\"}\n\n        with mock.patch(\"os.remove\") as mock_remove:\n            self.window.tails_status(result)\n\n        # We do remove the flag file if the update does finish\n        mock_remove.assert_called_once_with(FLAG_LOCATION)\n        assert self.window.progressBar.value() == 100",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_tails_status_failure",
        "code": "def test_tails_status_failure(self):\n        result = {\"status\": False, \"output\": \"successful.\", \"failure_reason\": \"42\"}\n\n        with mock.patch(\"os.remove\") as mock_remove:\n            self.window.tails_status(result)\n\n        # We do not remove the flag file if the update does not finish\n        mock_remove.assert_not_called()\n        assert self.window.progressBar.value() == 0",
        "file": "test_gui.py"
      },
      {
        "type": "function",
        "name": "test_no_update_without_password",
        "code": "def test_no_update_without_password(self, mock_msgbox):\n        with mock.patch(\"journalist_gui.SecureDropUpdater.password_is_set\", return_value=False):\n            self.window.update_securedrop()\n        assert self.window.pushButton.isEnabled() == True\n        assert self.window.pushButton_2.isEnabled() == False",
        "file": "test_gui.py"
      }
    ],
    "journalist_gui": {
      "resources_rc.py": [
        {
          "type": "function",
          "name": "qInitResources",
          "code": "def qInitResources():\n    QtCore.qRegisterResourceData(\n        rcc_version, qt_resource_struct, qt_resource_name, qt_resource_data\n    )",
          "file": "resources_rc.py"
        },
        {
          "type": "function",
          "name": "qCleanupResources",
          "code": "def qCleanupResources():\n    QtCore.qUnregisterResourceData(\n        rcc_version, qt_resource_struct, qt_resource_name, qt_resource_data\n    )",
          "file": "resources_rc.py"
        }
      ],
      "updaterUI.py": [
        {
          "type": "class",
          "name": "Ui_MainWindow",
          "code": "class Ui_MainWindow:\n    def setupUi(self, MainWindow):\n        MainWindow.setObjectName(\"MainWindow\")\n        MainWindow.resize(400, 500)\n        sizePolicy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Fixed, QtWidgets.QSizePolicy.Fixed)\n        sizePolicy.setHorizontalStretch(0)\n        sizePolicy.setVerticalStretch(0)\n        sizePolicy.setHeightForWidth(MainWindow.sizePolicy().hasHeightForWidth())\n        MainWindow.setSizePolicy(sizePolicy)\n        MainWindow.setMinimumSize(QtCore.QSize(400, 500))\n        MainWindow.setMaximumSize(QtCore.QSize(400, 500))\n        self.centralwidget = QtWidgets.QWidget(MainWindow)\n        self.centralwidget.setObjectName(\"centralwidget\")\n        self.verticalLayout_3 = QtWidgets.QVBoxLayout(self.centralwidget)\n        self.verticalLayout_3.setObjectName(\"verticalLayout_3\")\n        self.label_2 = QtWidgets.QLabel(self.centralwidget)\n        sizePolicy = QtWidgets.QSizePolicy(\n            QtWidgets.QSizePolicy.Preferred, QtWidgets.QSizePolicy.Fixed\n        )\n        sizePolicy.setHorizontalStretch(0)\n        sizePolicy.setVerticalStretch(0)\n        sizePolicy.setHeightForWidth(self.label_2.sizePolicy().hasHeightForWidth())\n        self.label_2.setSizePolicy(sizePolicy)\n        self.label_2.setText(\"\")\n        self.label_2.setPixmap(QtGui.QPixmap(\"static/banner.png\"))\n        self.label_2.setScaledContents(True)\n        self.label_2.setObjectName(\"label_2\")\n        self.verticalLayout_3.addWidget(self.label_2)\n        self.tabWidget = QtWidgets.QTabWidget(self.centralwidget)\n        self.tabWidget.setObjectName(\"tabWidget\")\n        self.tab = QtWidgets.QWidget()\n        self.tab.setObjectName(\"tab\")\n        self.verticalLayout = QtWidgets.QVBoxLayout(self.tab)\n        self.verticalLayout.setObjectName(\"verticalLayout\")\n        self.label = QtWidgets.QLabel(self.tab)\n        sizePolicy = QtWidgets.QSizePolicy(\n            QtWidgets.QSizePolicy.Preferred, QtWidgets.QSizePolicy.Preferred\n        )\n        sizePolicy.setHorizontalStretch(0)\n        sizePolicy.setVerticalStretch(0)\n        sizePolicy.setHeightForWidth(self.label.sizePolicy().hasHeightForWidth())\n        self.label.setSizePolicy(sizePolicy)\n        self.label.setTextFormat(QtCore.Qt.PlainText)\n        self.label.setScaledContents(False)\n        self.label.setWordWrap(True)\n        self.label.setObjectName(\"label\")\n        self.verticalLayout.addWidget(self.label)\n        self.tabWidget.addTab(self.tab, \"\")\n        self.tab_2 = QtWidgets.QWidget()\n        self.tab_2.setObjectName(\"tab_2\")\n        self.verticalLayout_2 = QtWidgets.QVBoxLayout(self.tab_2)\n        self.verticalLayout_2.setObjectName(\"verticalLayout_2\")\n        self.plainTextEdit = QtWidgets.QPlainTextEdit(self.tab_2)\n        self.plainTextEdit.setReadOnly(True)\n        self.plainTextEdit.setObjectName(\"plainTextEdit\")\n        self.verticalLayout_2.addWidget(self.plainTextEdit)\n        self.tabWidget.addTab(self.tab_2, \"\")\n        self.verticalLayout_3.addWidget(self.tabWidget)\n        self.progressBar = QtWidgets.QProgressBar(self.centralwidget)\n        self.progressBar.setProperty(\"value\", 24)\n        self.progressBar.setObjectName(\"progressBar\")\n        self.verticalLayout_3.addWidget(self.progressBar)\n        self.horizontalLayout_2 = QtWidgets.QHBoxLayout()\n        self.horizontalLayout_2.setObjectName(\"horizontalLayout_2\")\n        self.pushButton = QtWidgets.QPushButton(self.centralwidget)\n        self.pushButton.setObjectName(\"pushButton\")\n        self.horizontalLayout_2.addWidget(self.pushButton)\n        self.pushButton_2 = QtWidgets.QPushButton(self.centralwidget)\n        self.pushButton_2.setObjectName(\"pushButton_2\")\n        self.horizontalLayout_2.addWidget(self.pushButton_2)\n        self.verticalLayout_3.addLayout(self.horizontalLayout_2)\n        MainWindow.setCentralWidget(self.centralwidget)\n        self.menubar = QtWidgets.QMenuBar(MainWindow)\n        self.menubar.setGeometry(QtCore.QRect(0, 0, 400, 22))\n        self.menubar.setObjectName(\"menubar\")\n        MainWindow.setMenuBar(self.menubar)\n        self.statusbar = QtWidgets.QStatusBar(MainWindow)\n        self.statusbar.setObjectName(\"statusbar\")\n        MainWindow.setStatusBar(self.statusbar)\n\n        self.retranslateUi(MainWindow)\n        self.tabWidget.setCurrentIndex(0)\n        QtCore.QMetaObject.connectSlotsByName(MainWindow)\n\n    def retranslateUi(self, MainWindow):\n        _translate = QtCore.QCoreApplication.translate\n        MainWindow.setWindowTitle(_translate(\"MainWindow\", \"MainWindow\"))\n        self.label.setText(\n            _translate(\n                \"MainWindow\",\n                (\n                    \"SecureDrop workstation updates are available! You should install them now. \"\n                    \"If you don't want to, you can install them the next time your system boots.\"\n                ),\n            )\n        )\n        self.tabWidget.setTabText(\n            self.tabWidget.indexOf(self.tab), _translate(\"MainWindow\", \"SecureDrop\")\n        )\n        self.tabWidget.setTabText(\n            self.tabWidget.indexOf(self.tab_2), _translate(\"MainWindow\", \"Command Output\")\n        )\n        self.pushButton.setText(_translate(\"MainWindow\", \"Install Later\"))\n        self.pushButton_2.setText(_translate(\"MainWindow\", \"Install Now\"))",
          "file": "updaterUI.py"
        },
        {
          "type": "function",
          "name": "setupUi",
          "code": "def setupUi(self, MainWindow):\n        MainWindow.setObjectName(\"MainWindow\")\n        MainWindow.resize(400, 500)\n        sizePolicy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Fixed, QtWidgets.QSizePolicy.Fixed)\n        sizePolicy.setHorizontalStretch(0)\n        sizePolicy.setVerticalStretch(0)\n        sizePolicy.setHeightForWidth(MainWindow.sizePolicy().hasHeightForWidth())\n        MainWindow.setSizePolicy(sizePolicy)\n        MainWindow.setMinimumSize(QtCore.QSize(400, 500))\n        MainWindow.setMaximumSize(QtCore.QSize(400, 500))\n        self.centralwidget = QtWidgets.QWidget(MainWindow)\n        self.centralwidget.setObjectName(\"centralwidget\")\n        self.verticalLayout_3 = QtWidgets.QVBoxLayout(self.centralwidget)\n        self.verticalLayout_3.setObjectName(\"verticalLayout_3\")\n        self.label_2 = QtWidgets.QLabel(self.centralwidget)\n        sizePolicy = QtWidgets.QSizePolicy(\n            QtWidgets.QSizePolicy.Preferred, QtWidgets.QSizePolicy.Fixed\n        )\n        sizePolicy.setHorizontalStretch(0)\n        sizePolicy.setVerticalStretch(0)\n        sizePolicy.setHeightForWidth(self.label_2.sizePolicy().hasHeightForWidth())\n        self.label_2.setSizePolicy(sizePolicy)\n        self.label_2.setText(\"\")\n        self.label_2.setPixmap(QtGui.QPixmap(\"static/banner.png\"))\n        self.label_2.setScaledContents(True)\n        self.label_2.setObjectName(\"label_2\")\n        self.verticalLayout_3.addWidget(self.label_2)\n        self.tabWidget = QtWidgets.QTabWidget(self.centralwidget)\n        self.tabWidget.setObjectName(\"tabWidget\")\n        self.tab = QtWidgets.QWidget()\n        self.tab.setObjectName(\"tab\")\n        self.verticalLayout = QtWidgets.QVBoxLayout(self.tab)\n        self.verticalLayout.setObjectName(\"verticalLayout\")\n        self.label = QtWidgets.QLabel(self.tab)\n        sizePolicy = QtWidgets.QSizePolicy(\n            QtWidgets.QSizePolicy.Preferred, QtWidgets.QSizePolicy.Preferred\n        )\n        sizePolicy.setHorizontalStretch(0)\n        sizePolicy.setVerticalStretch(0)\n        sizePolicy.setHeightForWidth(self.label.sizePolicy().hasHeightForWidth())\n        self.label.setSizePolicy(sizePolicy)\n        self.label.setTextFormat(QtCore.Qt.PlainText)\n        self.label.setScaledContents(False)\n        self.label.setWordWrap(True)\n        self.label.setObjectName(\"label\")\n        self.verticalLayout.addWidget(self.label)\n        self.tabWidget.addTab(self.tab, \"\")\n        self.tab_2 = QtWidgets.QWidget()\n        self.tab_2.setObjectName(\"tab_2\")\n        self.verticalLayout_2 = QtWidgets.QVBoxLayout(self.tab_2)\n        self.verticalLayout_2.setObjectName(\"verticalLayout_2\")\n        self.plainTextEdit = QtWidgets.QPlainTextEdit(self.tab_2)\n        self.plainTextEdit.setReadOnly(True)\n        self.plainTextEdit.setObjectName(\"plainTextEdit\")\n        self.verticalLayout_2.addWidget(self.plainTextEdit)\n        self.tabWidget.addTab(self.tab_2, \"\")\n        self.verticalLayout_3.addWidget(self.tabWidget)\n        self.progressBar = QtWidgets.QProgressBar(self.centralwidget)\n        self.progressBar.setProperty(\"value\", 24)\n        self.progressBar.setObjectName(\"progressBar\")\n        self.verticalLayout_3.addWidget(self.progressBar)\n        self.horizontalLayout_2 = QtWidgets.QHBoxLayout()\n        self.horizontalLayout_2.setObjectName(\"horizontalLayout_2\")\n        self.pushButton = QtWidgets.QPushButton(self.centralwidget)\n        self.pushButton.setObjectName(\"pushButton\")\n        self.horizontalLayout_2.addWidget(self.pushButton)\n        self.pushButton_2 = QtWidgets.QPushButton(self.centralwidget)\n        self.pushButton_2.setObjectName(\"pushButton_2\")\n        self.horizontalLayout_2.addWidget(self.pushButton_2)\n        self.verticalLayout_3.addLayout(self.horizontalLayout_2)\n        MainWindow.setCentralWidget(self.centralwidget)\n        self.menubar = QtWidgets.QMenuBar(MainWindow)\n        self.menubar.setGeometry(QtCore.QRect(0, 0, 400, 22))\n        self.menubar.setObjectName(\"menubar\")\n        MainWindow.setMenuBar(self.menubar)\n        self.statusbar = QtWidgets.QStatusBar(MainWindow)\n        self.statusbar.setObjectName(\"statusbar\")\n        MainWindow.setStatusBar(self.statusbar)\n\n        self.retranslateUi(MainWindow)\n        self.tabWidget.setCurrentIndex(0)\n        QtCore.QMetaObject.connectSlotsByName(MainWindow)",
          "file": "updaterUI.py"
        },
        {
          "type": "function",
          "name": "retranslateUi",
          "code": "def retranslateUi(self, MainWindow):\n        _translate = QtCore.QCoreApplication.translate\n        MainWindow.setWindowTitle(_translate(\"MainWindow\", \"MainWindow\"))\n        self.label.setText(\n            _translate(\n                \"MainWindow\",\n                (\n                    \"SecureDrop workstation updates are available! You should install them now. \"\n                    \"If you don't want to, you can install them the next time your system boots.\"\n                ),\n            )\n        )\n        self.tabWidget.setTabText(\n            self.tabWidget.indexOf(self.tab), _translate(\"MainWindow\", \"SecureDrop\")\n        )\n        self.tabWidget.setTabText(\n            self.tabWidget.indexOf(self.tab_2), _translate(\"MainWindow\", \"Command Output\")\n        )\n        self.pushButton.setText(_translate(\"MainWindow\", \"Install Later\"))\n        self.pushButton_2.setText(_translate(\"MainWindow\", \"Install Now\"))",
          "file": "updaterUI.py"
        }
      ],
      "SecureDropUpdater.py": [
        {
          "type": "function",
          "name": "password_is_set",
          "code": "def password_is_set():\n    pwd_flag = subprocess.check_output([\"passwd\", \"--status\"]).decode(\"utf-8\").split()[1]\n\n    return pwd_flag != \"NP\"",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "function",
          "name": "prevent_second_instance",
          "code": "def prevent_second_instance(app: QtWidgets.QApplication, name: str) -> None:\n    # Null byte triggers abstract namespace\n    IDENTIFIER = \"\\0\" + name\n    ALREADY_BOUND_ERRNO = 98\n\n    app.instance_binding = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)\n    try:\n        app.instance_binding.bind(IDENTIFIER)\n    except OSError as e:\n        if e.errno == ALREADY_BOUND_ERRNO:\n            log.syslog(log.LOG_NOTICE, name + strings.app_is_already_running)\n            sys.exit()\n        else:\n            raise",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "class",
          "name": "SetupThread",
          "code": "class SetupThread(QThread):\n    signal = pyqtSignal(\"PyQt_PyObject\")\n\n    def __init__(self):\n        QThread.__init__(self)\n        self.output = \"\"\n        self.update_success = False\n        self.failure_reason = \"\"\n\n    def run(self):\n        sdadmin_path = \"/home/amnesia/Persistent/securedrop/securedrop-admin\"\n        update_command = [sdadmin_path, \"setup\"]\n\n        # Create flag file to indicate we should resume failed updates on\n        # reboot. Don't create the flag if it already exists.\n        if not os.path.exists(FLAG_LOCATION):\n            open(FLAG_LOCATION, \"a\").close()\n\n        try:\n            self.output = subprocess.check_output(update_command, stderr=subprocess.STDOUT).decode(\n                \"utf-8\"\n            )\n            if \"Failed to install\" in self.output:\n                self.update_success = False\n                self.failure_reason = strings.update_failed_generic_reason\n            else:\n                self.update_success = True\n        except subprocess.CalledProcessError as e:\n            self.output += e.output.decode(\"utf-8\")\n            self.update_success = False\n            self.failure_reason = strings.update_failed_generic_reason\n        result = {\n            \"status\": self.update_success,\n            \"output\": self.output,\n            \"failure_reason\": self.failure_reason,\n        }\n        self.signal.emit(result)",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self):\n        QThread.__init__(self)\n        self.output = \"\"\n        self.update_success = False\n        self.failure_reason = \"\"",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "function",
          "name": "run",
          "code": "def run(self):\n        sdadmin_path = \"/home/amnesia/Persistent/securedrop/securedrop-admin\"\n        update_command = [sdadmin_path, \"setup\"]\n\n        # Create flag file to indicate we should resume failed updates on\n        # reboot. Don't create the flag if it already exists.\n        if not os.path.exists(FLAG_LOCATION):\n            open(FLAG_LOCATION, \"a\").close()\n\n        try:\n            self.output = subprocess.check_output(update_command, stderr=subprocess.STDOUT).decode(\n                \"utf-8\"\n            )\n            if \"Failed to install\" in self.output:\n                self.update_success = False\n                self.failure_reason = strings.update_failed_generic_reason\n            else:\n                self.update_success = True\n        except subprocess.CalledProcessError as e:\n            self.output += e.output.decode(\"utf-8\")\n            self.update_success = False\n            self.failure_reason = strings.update_failed_generic_reason\n        result = {\n            \"status\": self.update_success,\n            \"output\": self.output,\n            \"failure_reason\": self.failure_reason,\n        }\n        self.signal.emit(result)",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "class",
          "name": "UpdateThread",
          "code": "class UpdateThread(QThread):\n    signal = pyqtSignal(\"PyQt_PyObject\")\n\n    def __init__(self):\n        QThread.__init__(self)\n        self.output = \"\"\n        self.update_success = False\n        self.failure_reason = \"\"\n\n    def run(self):\n        sdadmin_path = \"/home/amnesia/Persistent/securedrop/securedrop-admin\"\n        update_command = [sdadmin_path, \"update\"]\n        try:\n            self.output = subprocess.check_output(update_command, stderr=subprocess.STDOUT).decode(\n                \"utf-8\"\n            )\n            if \"Signature verification successful\" in self.output:\n                self.update_success = True\n            else:\n                self.failure_reason = strings.update_failed_generic_reason\n        except subprocess.CalledProcessError as e:\n            self.update_success = False\n            self.output += e.output.decode(\"utf-8\")\n            if \"Signature verification failed\" in self.output:\n                self.failure_reason = strings.update_failed_sig_failure\n            else:\n                self.failure_reason = strings.update_failed_generic_reason\n        result = {\n            \"status\": self.update_success,\n            \"output\": self.output,\n            \"failure_reason\": self.failure_reason,\n        }\n        self.signal.emit(result)",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self):\n        QThread.__init__(self)\n        self.output = \"\"\n        self.update_success = False\n        self.failure_reason = \"\"",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "function",
          "name": "run",
          "code": "def run(self):\n        sdadmin_path = \"/home/amnesia/Persistent/securedrop/securedrop-admin\"\n        update_command = [sdadmin_path, \"update\"]\n        try:\n            self.output = subprocess.check_output(update_command, stderr=subprocess.STDOUT).decode(\n                \"utf-8\"\n            )\n            if \"Signature verification successful\" in self.output:\n                self.update_success = True\n            else:\n                self.failure_reason = strings.update_failed_generic_reason\n        except subprocess.CalledProcessError as e:\n            self.update_success = False\n            self.output += e.output.decode(\"utf-8\")\n            if \"Signature verification failed\" in self.output:\n                self.failure_reason = strings.update_failed_sig_failure\n            else:\n                self.failure_reason = strings.update_failed_generic_reason\n        result = {\n            \"status\": self.update_success,\n            \"output\": self.output,\n            \"failure_reason\": self.failure_reason,\n        }\n        self.signal.emit(result)",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "class",
          "name": "TailsconfigThread",
          "code": "class TailsconfigThread(QThread):\n    signal = pyqtSignal(\"PyQt_PyObject\")\n\n    def __init__(self):\n        QThread.__init__(self)\n        self.output = \"\"\n        self.update_success = False\n        self.failure_reason = \"\"\n        self.sudo_password = \"\"\n\n    def run(self):\n        tailsconfig_command = (\n            \"/home/amnesia/Persistent/\" \"securedrop/securedrop-admin \" \"tailsconfig\"\n        )\n        self.failure_reason = \"\"\n        try:\n            child = pexpect.spawn(tailsconfig_command)\n            child.expect(\"SUDO password:\")\n            self.output += child.before.decode(\"utf-8\")\n            child.sendline(self.sudo_password)\n            child.expect(pexpect.EOF, timeout=120)\n            self.output += child.before.decode(\"utf-8\")\n            child.close()\n\n            # For Tailsconfig to be considered a success, we expect no\n            # failures in the Ansible output.\n            if child.exitstatus:\n                self.update_success = False\n                if \"[sudo via ansible\" in self.output:\n                    self.failure_reason = strings.tailsconfig_failed_sudo_password\n                else:\n                    self.failure_reason = strings.tailsconfig_failed_generic_reason\n            else:\n                self.update_success = True\n        except pexpect.exceptions.TIMEOUT:\n            self.update_success = False\n            self.failure_reason = strings.tailsconfig_failed_timeout\n        except subprocess.CalledProcessError:\n            self.update_success = False\n            self.failure_reason = strings.tailsconfig_failed_generic_reason\n        result = {\n            \"status\": self.update_success,\n            \"output\": ESCAPE_POD.sub(\"\", self.output),\n            \"failure_reason\": self.failure_reason,\n        }\n        self.signal.emit(result)",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self):\n        QThread.__init__(self)\n        self.output = \"\"\n        self.update_success = False\n        self.failure_reason = \"\"\n        self.sudo_password = \"\"",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "function",
          "name": "run",
          "code": "def run(self):\n        tailsconfig_command = (\n            \"/home/amnesia/Persistent/\" \"securedrop/securedrop-admin \" \"tailsconfig\"\n        )\n        self.failure_reason = \"\"\n        try:\n            child = pexpect.spawn(tailsconfig_command)\n            child.expect(\"SUDO password:\")\n            self.output += child.before.decode(\"utf-8\")\n            child.sendline(self.sudo_password)\n            child.expect(pexpect.EOF, timeout=120)\n            self.output += child.before.decode(\"utf-8\")\n            child.close()\n\n            # For Tailsconfig to be considered a success, we expect no\n            # failures in the Ansible output.\n            if child.exitstatus:\n                self.update_success = False\n                if \"[sudo via ansible\" in self.output:\n                    self.failure_reason = strings.tailsconfig_failed_sudo_password\n                else:\n                    self.failure_reason = strings.tailsconfig_failed_generic_reason\n            else:\n                self.update_success = True\n        except pexpect.exceptions.TIMEOUT:\n            self.update_success = False\n            self.failure_reason = strings.tailsconfig_failed_timeout\n        except subprocess.CalledProcessError:\n            self.update_success = False\n            self.failure_reason = strings.tailsconfig_failed_generic_reason\n        result = {\n            \"status\": self.update_success,\n            \"output\": ESCAPE_POD.sub(\"\", self.output),\n            \"failure_reason\": self.failure_reason,\n        }\n        self.signal.emit(result)",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "class",
          "name": "UpdaterApp",
          "code": "class UpdaterApp(QtWidgets.QMainWindow, updaterUI.Ui_MainWindow):\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.setupUi(self)\n        self.statusbar.setSizeGripEnabled(False)\n        self.output = strings.initial_text_box\n        self.plainTextEdit.setPlainText(self.output)\n        self.update_success = False\n\n        pixmap = QtGui.QPixmap(\":/images/static/banner.png\")\n        self.label_2.setPixmap(pixmap)\n        self.label_2.setScaledContents(True)\n\n        self.progressBar.setProperty(\"value\", 0)\n        self.setWindowTitle(strings.window_title)\n        self.setWindowIcon(QtGui.QIcon(\":/images/static/securedrop_icon.png\"))\n        self.label.setText(strings.update_in_progress)\n\n        self.tabWidget.setTabText(self.tabWidget.indexOf(self.tab), strings.main_tab)\n        self.tabWidget.setTabText(self.tabWidget.indexOf(self.tab_2), strings.output_tab)\n\n        # Connect buttons to their functions.\n        self.pushButton.setText(strings.install_later_button)\n        self.pushButton.setStyleSheet(\n            \"\"\"background-color: lightgrey;\n                                      min-height: 2em;\n                                      border-radius: 10px\"\"\"\n        )\n        self.pushButton.clicked.connect(self.close)\n        self.pushButton_2.setText(strings.install_update_button)\n        self.pushButton_2.setStyleSheet(\n            \"\"\"background-color: #E6FFEB;\n                                        min-height: 2em;\n                                        border-radius: 10px;\"\"\"\n        )\n        self.pushButton_2.clicked.connect(self.update_securedrop)\n        self.update_thread = UpdateThread()\n        self.update_thread.signal.connect(self.update_status)\n        self.tails_thread = TailsconfigThread()\n        self.tails_thread.signal.connect(self.tails_status)\n        self.setup_thread = SetupThread()\n        self.setup_thread.signal.connect(self.setup_status)\n\n    # At the end of this function, we will try to do tailsconfig.\n    # A new slot will handle tailsconfig output\n    def setup_status(self, result):\n        \"This is the slot for setup thread\"\n        self.output += result[\"output\"]\n        self.update_success = result[\"status\"]\n        self.failure_reason = result[\"failure_reason\"]\n        self.progressBar.setProperty(\"value\", 60)\n        self.plainTextEdit.setPlainText(self.output)\n        self.plainTextEdit.setReadOnly = True\n        if not self.update_success:  # Failed to do setup\n            self.pushButton.setEnabled(True)\n            self.pushButton_2.setEnabled(True)\n            self.update_status_bar_and_output(self.failure_reason)\n            self.progressBar.setProperty(\"value\", 0)\n            self.alert_failure(self.failure_reason)\n            return\n        self.progressBar.setProperty(\"value\", 70)\n        self.call_tailsconfig()\n\n    # This will update the output text after the git commands.\n    def update_status(self, result):\n        \"This is the slot for update thread\"\n        self.output += result[\"output\"]\n        self.update_success = result[\"status\"]\n        self.failure_reason = result[\"failure_reason\"]\n        self.progressBar.setProperty(\"value\", 40)\n        self.plainTextEdit.setPlainText(self.output)\n        self.plainTextEdit.setReadOnly = True\n        if not self.update_success:  # Failed to do update\n            self.pushButton.setEnabled(True)\n            self.pushButton_2.setEnabled(True)\n            self.update_status_bar_and_output(self.failure_reason)\n            self.progressBar.setProperty(\"value\", 0)\n            self.alert_failure(self.failure_reason)\n            return\n        self.progressBar.setProperty(\"value\", 50)\n        self.update_status_bar_and_output(strings.doing_setup)\n        self.setup_thread.start()\n\n    def update_status_bar_and_output(self, status_message):\n        \"\"\"This method updates the status bar and the output window with the\n        status_message.\"\"\"\n        self.statusbar.showMessage(status_message)\n        self.output += status_message + \"\\n\"\n        self.plainTextEdit.setPlainText(self.output)\n\n    def call_tailsconfig(self):\n        # Now let us work on tailsconfig part\n        if self.update_success:\n            # Get sudo password and add an enter key as tailsconfig command\n            # expects\n            sudo_password = self.get_sudo_password()\n            if not sudo_password:\n                self.update_success = False\n                self.failure_reason = strings.missing_sudo_password\n                self.on_failure()\n                return\n            self.tails_thread.sudo_password = sudo_password + \"\\n\"\n            self.update_status_bar_and_output(strings.updating_tails_env)\n            self.tails_thread.start()\n        else:\n            self.on_failure()\n\n    def tails_status(self, result):\n        \"This is the slot for Tailsconfig thread\"\n        self.output += result[\"output\"]\n        self.update_success = result[\"status\"]\n        self.failure_reason = result[\"failure_reason\"]\n        self.plainTextEdit.setPlainText(self.output)\n        self.progressBar.setProperty(\"value\", 80)\n        if self.update_success:\n            # Remove flag file indicating an update is in progress\n            os.remove(FLAG_LOCATION)\n            self.update_status_bar_and_output(strings.finished)\n            self.progressBar.setProperty(\"value\", 100)\n            self.alert_success()\n        else:\n            self.on_failure()\n\n    def on_failure(self):\n        self.update_status_bar_and_output(self.failure_reason)\n        self.alert_failure(self.failure_reason)\n        # Now everything is done, enable the button.\n        self.pushButton.setEnabled(True)\n        self.pushButton_2.setEnabled(True)\n        self.progressBar.setProperty(\"value\", 0)\n\n    def update_securedrop(self):\n        if password_is_set():\n            self.pushButton_2.setEnabled(False)\n            self.pushButton.setEnabled(False)\n            self.progressBar.setProperty(\"value\", 10)\n            self.update_status_bar_and_output(strings.fetching_update)\n            self.update_thread.start()\n        else:\n            self.pushButton_2.setEnabled(False)\n            pwd_err_dialog = QtWidgets.QMessageBox()\n            pwd_err_dialog.setText(strings.no_password_set_message)\n            pwd_err_dialog.exec()\n\n    def alert_success(self):\n        self.success_dialog = QtWidgets.QMessageBox()\n        self.success_dialog.setIcon(QtWidgets.QMessageBox.Information)\n        self.success_dialog.setText(strings.finished_dialog_message)\n        self.success_dialog.setWindowTitle(strings.finished_dialog_title)\n        self.success_dialog.show()\n\n    def alert_failure(self, failure_reason):\n        self.error_dialog = QtWidgets.QMessageBox()\n        self.error_dialog.setIcon(QtWidgets.QMessageBox.Critical)\n        self.error_dialog.setText(self.failure_reason)\n        self.error_dialog.setWindowTitle(strings.update_failed_dialog_title)\n        self.error_dialog.show()\n\n    def get_sudo_password(self):\n        sudo_password, ok_is_pressed = QtWidgets.QInputDialog.getText(\n            self,\n            \"Tails Administrator password\",\n            strings.sudo_password_text,\n            QtWidgets.QLineEdit.Password,\n            \"\",\n        )\n        if ok_is_pressed and sudo_password:\n            return sudo_password\n        else:\n            return None",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, parent=None):\n        super().__init__(parent)\n        self.setupUi(self)\n        self.statusbar.setSizeGripEnabled(False)\n        self.output = strings.initial_text_box\n        self.plainTextEdit.setPlainText(self.output)\n        self.update_success = False\n\n        pixmap = QtGui.QPixmap(\":/images/static/banner.png\")\n        self.label_2.setPixmap(pixmap)\n        self.label_2.setScaledContents(True)\n\n        self.progressBar.setProperty(\"value\", 0)\n        self.setWindowTitle(strings.window_title)\n        self.setWindowIcon(QtGui.QIcon(\":/images/static/securedrop_icon.png\"))\n        self.label.setText(strings.update_in_progress)\n\n        self.tabWidget.setTabText(self.tabWidget.indexOf(self.tab), strings.main_tab)\n        self.tabWidget.setTabText(self.tabWidget.indexOf(self.tab_2), strings.output_tab)\n\n        # Connect buttons to their functions.\n        self.pushButton.setText(strings.install_later_button)\n        self.pushButton.setStyleSheet(\n            \"\"\"background-color: lightgrey;\n                                      min-height: 2em;\n                                      border-radius: 10px\"\"\"\n        )\n        self.pushButton.clicked.connect(self.close)\n        self.pushButton_2.setText(strings.install_update_button)\n        self.pushButton_2.setStyleSheet(\n            \"\"\"background-color: #E6FFEB;\n                                        min-height: 2em;\n                                        border-radius: 10px;\"\"\"\n        )\n        self.pushButton_2.clicked.connect(self.update_securedrop)\n        self.update_thread = UpdateThread()\n        self.update_thread.signal.connect(self.update_status)\n        self.tails_thread = TailsconfigThread()\n        self.tails_thread.signal.connect(self.tails_status)\n        self.setup_thread = SetupThread()\n        self.setup_thread.signal.connect(self.setup_status)",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "function",
          "name": "setup_status",
          "code": "def setup_status(self, result):\n        \"This is the slot for setup thread\"\n        self.output += result[\"output\"]\n        self.update_success = result[\"status\"]\n        self.failure_reason = result[\"failure_reason\"]\n        self.progressBar.setProperty(\"value\", 60)\n        self.plainTextEdit.setPlainText(self.output)\n        self.plainTextEdit.setReadOnly = True\n        if not self.update_success:  # Failed to do setup\n            self.pushButton.setEnabled(True)\n            self.pushButton_2.setEnabled(True)\n            self.update_status_bar_and_output(self.failure_reason)\n            self.progressBar.setProperty(\"value\", 0)\n            self.alert_failure(self.failure_reason)\n            return\n        self.progressBar.setProperty(\"value\", 70)\n        self.call_tailsconfig()",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "function",
          "name": "update_status",
          "code": "def update_status(self, result):\n        \"This is the slot for update thread\"\n        self.output += result[\"output\"]\n        self.update_success = result[\"status\"]\n        self.failure_reason = result[\"failure_reason\"]\n        self.progressBar.setProperty(\"value\", 40)\n        self.plainTextEdit.setPlainText(self.output)\n        self.plainTextEdit.setReadOnly = True\n        if not self.update_success:  # Failed to do update\n            self.pushButton.setEnabled(True)\n            self.pushButton_2.setEnabled(True)\n            self.update_status_bar_and_output(self.failure_reason)\n            self.progressBar.setProperty(\"value\", 0)\n            self.alert_failure(self.failure_reason)\n            return\n        self.progressBar.setProperty(\"value\", 50)\n        self.update_status_bar_and_output(strings.doing_setup)\n        self.setup_thread.start()",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "function",
          "name": "update_status_bar_and_output",
          "code": "def update_status_bar_and_output(self, status_message):\n        \"\"\"This method updates the status bar and the output window with the\n        status_message.\"\"\"\n        self.statusbar.showMessage(status_message)\n        self.output += status_message + \"\\n\"\n        self.plainTextEdit.setPlainText(self.output)",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "function",
          "name": "call_tailsconfig",
          "code": "def call_tailsconfig(self):\n        # Now let us work on tailsconfig part\n        if self.update_success:\n            # Get sudo password and add an enter key as tailsconfig command\n            # expects\n            sudo_password = self.get_sudo_password()\n            if not sudo_password:\n                self.update_success = False\n                self.failure_reason = strings.missing_sudo_password\n                self.on_failure()\n                return\n            self.tails_thread.sudo_password = sudo_password + \"\\n\"\n            self.update_status_bar_and_output(strings.updating_tails_env)\n            self.tails_thread.start()\n        else:\n            self.on_failure()",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "function",
          "name": "tails_status",
          "code": "def tails_status(self, result):\n        \"This is the slot for Tailsconfig thread\"\n        self.output += result[\"output\"]\n        self.update_success = result[\"status\"]\n        self.failure_reason = result[\"failure_reason\"]\n        self.plainTextEdit.setPlainText(self.output)\n        self.progressBar.setProperty(\"value\", 80)\n        if self.update_success:\n            # Remove flag file indicating an update is in progress\n            os.remove(FLAG_LOCATION)\n            self.update_status_bar_and_output(strings.finished)\n            self.progressBar.setProperty(\"value\", 100)\n            self.alert_success()\n        else:\n            self.on_failure()",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "function",
          "name": "on_failure",
          "code": "def on_failure(self):\n        self.update_status_bar_and_output(self.failure_reason)\n        self.alert_failure(self.failure_reason)\n        # Now everything is done, enable the button.\n        self.pushButton.setEnabled(True)\n        self.pushButton_2.setEnabled(True)\n        self.progressBar.setProperty(\"value\", 0)",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "function",
          "name": "update_securedrop",
          "code": "def update_securedrop(self):\n        if password_is_set():\n            self.pushButton_2.setEnabled(False)\n            self.pushButton.setEnabled(False)\n            self.progressBar.setProperty(\"value\", 10)\n            self.update_status_bar_and_output(strings.fetching_update)\n            self.update_thread.start()\n        else:\n            self.pushButton_2.setEnabled(False)\n            pwd_err_dialog = QtWidgets.QMessageBox()\n            pwd_err_dialog.setText(strings.no_password_set_message)\n            pwd_err_dialog.exec()",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "function",
          "name": "alert_success",
          "code": "def alert_success(self):\n        self.success_dialog = QtWidgets.QMessageBox()\n        self.success_dialog.setIcon(QtWidgets.QMessageBox.Information)\n        self.success_dialog.setText(strings.finished_dialog_message)\n        self.success_dialog.setWindowTitle(strings.finished_dialog_title)\n        self.success_dialog.show()",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "function",
          "name": "alert_failure",
          "code": "def alert_failure(self, failure_reason):\n        self.error_dialog = QtWidgets.QMessageBox()\n        self.error_dialog.setIcon(QtWidgets.QMessageBox.Critical)\n        self.error_dialog.setText(self.failure_reason)\n        self.error_dialog.setWindowTitle(strings.update_failed_dialog_title)\n        self.error_dialog.show()",
          "file": "SecureDropUpdater.py"
        },
        {
          "type": "function",
          "name": "get_sudo_password",
          "code": "def get_sudo_password(self):\n        sudo_password, ok_is_pressed = QtWidgets.QInputDialog.getText(\n            self,\n            \"Tails Administrator password\",\n            strings.sudo_password_text,\n            QtWidgets.QLineEdit.Password,\n            \"\",\n        )\n        if ok_is_pressed and sudo_password:\n            return sudo_password\n        else:\n            return None",
          "file": "SecureDropUpdater.py"
        }
      ]
    }
  },
  "admin": {
    "bootstrap.py": [
      {
        "type": "function",
        "name": "setup_logger",
        "code": "def setup_logger(verbose: bool = False) -> None:\n    \"\"\"Configure logging handler\"\"\"\n    # Set default level on parent\n    sdlog.setLevel(logging.DEBUG)\n    level = logging.DEBUG if verbose else logging.INFO\n\n    stdout = logging.StreamHandler(sys.stdout)\n    stdout.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n    stdout.setLevel(level)\n    sdlog.addHandler(stdout)",
        "file": "bootstrap.py"
      },
      {
        "type": "function",
        "name": "run_command",
        "code": "def run_command(command: List[str]) -> Iterator[bytes]:\n    \"\"\"\n    Wrapper function to display stdout for running command,\n    similar to how shelling out in a Bash script displays rolling output.\n\n    Yields a list of the stdout from the `command`, and raises a\n    CalledProcessError if `command` returns non-zero.\n    \"\"\"\n    popen = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    if popen.stdout is None:\n        raise OSError(\"Could not run command: None stdout\")\n    yield from iter(popen.stdout.readline, b\"\")\n    popen.stdout.close()\n    return_code = popen.wait()\n    if return_code:\n        raise subprocess.CalledProcessError(return_code, command)",
        "file": "bootstrap.py"
      },
      {
        "type": "function",
        "name": "is_tails",
        "code": "def is_tails() -> bool:\n    with open(\"/etc/os-release\") as f:\n        return 'NAME=\"Tails\"' in f.read()",
        "file": "bootstrap.py"
      },
      {
        "type": "function",
        "name": "clean_up_old_tails_venv",
        "code": "def clean_up_old_tails_venv(virtualenv_dir: str = VENV_DIR) -> None:\n    \"\"\"\n    When upgrading major Tails versions, we need to rebuild the virtualenv\n    against the correct Python version. We can detect if the Tails\n    version matches the correct Python version - if not, delete the\n    venv, so it'll get recreated.\n    \"\"\"\n    if is_tails():\n        with open(\"/etc/os-release\") as f:\n            os_release = f.readlines()\n            for line in os_release:\n                if line.startswith(\"VERSION=\"):\n                    version = line.split(\"=\")[1].strip().strip('\"')\n                    if version.startswith(\"6.\"):\n                        # Tails 6 is based on Python 3.11\n                        python_lib_path = os.path.join(virtualenv_dir, \"lib/python3.9\")\n                        if os.path.exists(python_lib_path):\n                            sdlog.info(\"Tails 5 virtualenv detected. Removing it.\")\n                            shutil.rmtree(virtualenv_dir)\n                            sdlog.info(\"Tails 5 virtualenv deleted.\")\n                    break",
        "file": "bootstrap.py"
      },
      {
        "type": "function",
        "name": "checkenv",
        "code": "def checkenv(args: argparse.Namespace) -> None:\n    clean_up_old_tails_venv(VENV_DIR)\n    if not os.path.exists(os.path.join(VENV_DIR, \"bin/activate\")):\n        sdlog.error('Please run \"securedrop-admin setup\".')\n        sys.exit(1)",
        "file": "bootstrap.py"
      },
      {
        "type": "function",
        "name": "is_missing_dependency",
        "code": "def is_missing_dependency() -> bool:\n    \"\"\"\n    Check if there are any missing apt dependencies.\n    This applies to existing Tails systems where `securedrop-setup` may not have been\n    run recently.\n    \"\"\"\n\n    apt_query = f\"apt-cache -q0 policy {APT_DEPENDENCIES_STR}\".split(\" \")\n\n    try:\n        sdlog.info(\"Checking apt dependencies are installed\")\n        apt_query_result = subprocess.run(apt_query, capture_output=True, text=True, check=True)\n\n        # If any packages are marked as not installed, we are missing dependencies\n        if \"Installed: (none)\" in apt_query_result.stdout:\n            return True\n\n        # If any packages couldn't be found, it may point to an apt cache issue\n        return \"Unable to locate package\" in apt_query_result.stderr\n\n    except subprocess.CalledProcessError as e:\n        sdlog.error(\"Error checking apt dependencies\")\n        sdlog.debug(e.output)\n        raise",
        "file": "bootstrap.py"
      },
      {
        "type": "function",
        "name": "maybe_torify",
        "code": "def maybe_torify() -> List[str]:\n    if is_tails():\n        return [\"torify\"]\n    else:\n        return []",
        "file": "bootstrap.py"
      },
      {
        "type": "function",
        "name": "install_apt_dependencies",
        "code": "def install_apt_dependencies(args: argparse.Namespace) -> None:\n    \"\"\"\n    Install apt dependencies in Tails. In order to install Ansible in\n    a virtualenv, first there are a number of Python prerequisites.\n    \"\"\"\n    sdlog.info(\"Installing SecureDrop Admin dependencies\")\n    sdlog.info(\n        \"You'll be prompted for the temporary Tails admin password,\"\n        \" which was set on Tails login screen\"\n    )\n\n    apt_command = f\"sudo apt-get -q -o=Dpkg::Use-Pty=0 install -y {APT_DEPENDENCIES_STR}\".split(\" \")\n\n    try:\n        # Print command results in real-time, to keep Admin apprised\n        # of progress during long-running command.\n        for output_line in run_command(apt_command):\n            print(output_line.decode(\"utf-8\").rstrip())\n\n    except subprocess.CalledProcessError:\n        # Tails supports apt persistence, which was used by SecureDrop\n        # under Tails 2.x. If updates are being applied, don't try to pile\n        # on with more apt requests.\n        sdlog.error(\n            \"Failed to install apt dependencies. Check network\" \" connection and try again.\"\n        )\n        raise",
        "file": "bootstrap.py"
      },
      {
        "type": "function",
        "name": "envsetup",
        "code": "def envsetup(args: argparse.Namespace, virtualenv_dir: str = VENV_DIR) -> None:\n    \"\"\"Installs Admin tooling required for managing SecureDrop. Specifically:\n\n        * updates apt-cache\n        * installs apt packages for Python virtualenv\n        * creates virtualenv\n        * installs pip packages inside virtualenv\n\n    The virtualenv is created within the Persistence volume in Tails, so that\n    Ansible is available to the Admin on subsequent boots without requiring\n    installation of packages again.\n    \"\"\"\n    # clean up old Tails venv on major upgrades\n    clean_up_old_tails_venv(virtualenv_dir)\n\n    # Check apt dependencies and ensure all are present.\n    if is_missing_dependency():\n        install_apt_dependencies(args)\n\n    # virtualenv doesnt exist? Install dependencies and create\n    if not os.path.exists(virtualenv_dir):\n        # Technically you can create a virtualenv from within python\n        # but pip can only be run over Tor on Tails, and debugging that\n        # along with instaling a third-party dependency is not worth\n        # the effort here.\n        sdlog.info(\"Setting up virtualenv\")\n        try:\n            sdlog.debug(\n                subprocess.check_output(\n                    maybe_torify() + [\"virtualenv\", \"--python=python3\", virtualenv_dir],\n                    stderr=subprocess.STDOUT,\n                )\n            )\n        except subprocess.CalledProcessError as e:\n            sdlog.debug(e.output)\n            sdlog.error(\"Unable to create virtualenv. Check network settings\" \" and try again.\")\n            sdlog.debug(\"Cleaning up virtualenv\")\n            if os.path.exists(virtualenv_dir):\n                shutil.rmtree(virtualenv_dir)\n            raise\n    else:\n        sdlog.info(\"Virtualenv already exists, not creating\")\n\n    if args.t:\n        install_pip_dependencies(\n            args,\n            requirements_file=\"requirements-testinfra.txt\",\n            desc=\"dependencies with verification support\",\n        )\n    else:\n        install_pip_dependencies(args)\n\n    if os.path.exists(os.path.join(DIR, \"setup.py\")):\n        install_pip_self(args)\n\n    sdlog.info(\"Finished installing SecureDrop dependencies\")",
        "file": "bootstrap.py"
      },
      {
        "type": "function",
        "name": "install_pip_self",
        "code": "def install_pip_self(args: argparse.Namespace) -> None:\n    pip_install_cmd = [os.path.join(VENV_DIR, \"bin\", \"pip3\"), \"install\", \"-e\", DIR]\n    try:\n        subprocess.check_output(maybe_torify() + pip_install_cmd, stderr=subprocess.STDOUT)\n    except subprocess.CalledProcessError as e:\n        sdlog.debug(e.output)\n        sdlog.error(\"Unable to install self, run with -v for more information\")\n        raise",
        "file": "bootstrap.py"
      },
      {
        "type": "function",
        "name": "install_pip_dependencies",
        "code": "def install_pip_dependencies(\n    args: argparse.Namespace,\n    requirements_file: str = \"requirements.txt\",\n    desc: str = \"Python dependencies\",\n) -> None:\n    \"\"\"\n    Install Python dependencies via pip into virtualenv.\n    \"\"\"\n\n    # Ansible version 2.9.* cannot be directly upgraded and must be removed\n    # before attempting to install a later version - so let's check for it\n    # and uninstall it if we find it!\n\n    ansible_vercheck_cmd = [\n        os.path.join(VENV_DIR, \"bin\", \"python3\"),\n        \"-c\",\n        \"from importlib.metadata import version as v; print(v('ansible'))\",\n    ]\n\n    ansible_uninstall_cmd = [\n        os.path.join(VENV_DIR, \"bin\", \"pip3\"),\n        \"uninstall\",\n        \"-y\",\n        \"ansible\",\n    ]\n\n    ansible_ver = subprocess.run(\n        maybe_torify() + ansible_vercheck_cmd, text=True, capture_output=True, check=False\n    )\n    if ansible_ver.stdout.startswith(\"2.9\"):\n        sdlog.info(\"Ansible is out-of-date, removing it.\")\n        delete_result = subprocess.run(\n            maybe_torify() + ansible_uninstall_cmd, capture_output=True, text=True, check=False\n        )\n        if delete_result.returncode != 0:\n            sdlog.error(\n                \"Failed to remove old ansible version:\\n\"\n                f\"  return num: {delete_result.returncode}\\n\"\n                f\"  error text: {delete_result.stderr}\\n\"\n                \"Attempting to continue.\"\n            )\n\n    pip_install_cmd = [\n        os.path.join(VENV_DIR, \"bin\", \"pip3\"),\n        \"install\",\n        \"--no-deps\",\n        \"-r\",\n        os.path.join(DIR, requirements_file),\n        \"--require-hashes\",\n        \"-U\",\n        \"--upgrade-strategy\",\n        \"only-if-needed\",\n    ]\n\n    sdlog.info(f\"Checking {desc} for securedrop-admin\")\n    try:\n        pip_output = subprocess.check_output(\n            maybe_torify() + pip_install_cmd, stderr=subprocess.STDOUT\n        )\n    except subprocess.CalledProcessError as e:\n        sdlog.debug(e.output)\n        sdlog.error(f\"Failed to install {desc}. Check network\" \" connection and try again.\")\n        raise\n\n    sdlog.debug(pip_output)\n    if \"Successfully installed\" in str(pip_output):\n        sdlog.info(f\"{desc} for securedrop-admin upgraded\")\n    else:\n        sdlog.info(f\"{desc} for securedrop-admin are up-to-date\")",
        "file": "bootstrap.py"
      },
      {
        "type": "function",
        "name": "parse_argv",
        "code": "def parse_argv(argv: List[str]) -> argparse.Namespace:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-v\", action=\"store_true\", default=False, help=\"Increase verbosity on output\"\n    )\n    parser.add_argument(\n        \"-t\", action=\"store_true\", default=False, help=\"Install additional test dependencies\"\n    )\n    parser.set_defaults(func=envsetup)\n\n    subparsers = parser.add_subparsers()\n\n    envsetup_parser = subparsers.add_parser(\"envsetup\", help=\"Set up the admin virtualenv.\")\n    envsetup_parser.set_defaults(func=envsetup)\n\n    checkenv_parser = subparsers.add_parser(\n        \"checkenv\", help=\"Check that the admin virtualenv is properly set up.\"\n    )\n    checkenv_parser.set_defaults(func=checkenv)\n\n    return parser.parse_args(argv)",
        "file": "bootstrap.py"
      }
    ],
    "securedrop_admin": {
      "__init__.py": [
        {
          "type": "function",
          "name": "openssh_version",
          "code": "def openssh_version() -> int:\n    try:\n        result = subprocess.run([\"ssh\", \"-V\"], capture_output=True, text=True, check=False)\n        if result.stderr.startswith(\"OpenSSH_9\"):\n            return 9\n        elif result.stderr.startswith(\"OpenSSH_8\"):\n            return 8\n        else:\n            return 0\n    except subprocess.CalledProcessError:\n        return 0\n    return 0",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "ansible_command",
          "code": "def ansible_command() -> List[str]:\n    cmd = [\"ansible-playbook\"]\n    if openssh_version() == 9:\n        cmd = [\"ansible-playbook\", \"--scp-extra-args='-O'\"]\n    return cmd",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "FingerprintException",
          "code": "class FingerprintException(Exception):\n    pass",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "JournalistAlertEmailException",
          "code": "class JournalistAlertEmailException(Exception):\n    pass",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "SiteConfig",
          "code": "class SiteConfig:\n    class ValidateNotEmpty(Validator):\n        def validate(self, document: Document) -> bool:\n            if document.text != \"\":\n                return True\n            raise ValidationError(message=\"Must not be an empty string\")\n\n    class ValidateTime(Validator):\n        def validate(self, document: Document) -> bool:\n            if document.text.isdigit() and int(document.text) in range(24):\n                return True\n            raise ValidationError(message=\"Must be an integer between 0 and 23\")\n\n    class ValidateUser(Validator):\n        def validate(self, document: Document) -> bool:\n            text = document.text\n            if text not in (\"\", \"root\", \"amnesia\"):\n                return True\n            raise ValidationError(message=\"Must not be root, amnesia or an empty string\")\n\n    class ValidateIP(Validator):\n        def validate(self, document: Document) -> bool:\n            try:\n                ipaddress.ip_address(document.text)\n                return True\n            except ValueError as e:\n                raise ValidationError(message=str(e))\n\n    class ValidateNameservers(Validator):\n        def validate(self, document: Document) -> bool:\n            candidates = LIST_SPLIT_RE.split(document.text)\n            if len(candidates) > MAX_NAMESERVERS:\n                raise ValidationError(message=\"Specify no more than three nameservers.\")\n            try:\n                all(map(ipaddress.ip_address, candidates))\n            except ValueError:\n                raise ValidationError(\n                    message=(\n                        \"DNS server(s) should be a space/comma-separated list \"\n                        f\"of up to {MAX_NAMESERVERS} IP addresses\"\n                    )\n                )\n            return True\n\n    @staticmethod\n    def split_list(text: str) -> List[str]:\n        \"\"\"\n        Splits a string containing a list of values separated by commas or whitespace.\n        \"\"\"\n        return LIST_SPLIT_RE.split(text)\n\n    class ValidatePath(Validator):\n        def __init__(self, basedir: str) -> None:\n            self.basedir = basedir\n            super().__init__()\n\n        def validate(self, document: Document) -> bool:\n            if document.text == \"\":\n                raise ValidationError(message=\"an existing file name is required\")\n            path = os.path.join(self.basedir, document.text)\n            if os.path.exists(path):\n                return True\n            raise ValidationError(message=path + \" file does not exist\")\n\n    class ValidateOptionalPath(ValidatePath):\n        def validate(self, document: Document) -> bool:\n            if document.text == \"\":\n                return True\n            return super().validate(document)\n\n    class ValidateYesNo(Validator):\n        def validate(self, document: Document) -> bool:\n            text = document.text.lower()\n            if text in (\"yes\", \"no\"):\n                return True\n            raise ValidationError(message=\"Must be either yes or no\")\n\n    class ValidateFingerprint(Validator):\n        def validate(self, document: Document) -> bool:\n            text = document.text.replace(\" \", \"\")\n            if text == \"65A1B5FF195B56353CC63DFFCC40EF1228271441\":\n                raise ValidationError(message=\"This is the TEST journalist fingerprint\")\n            if text == \"600BC6D5142C68F35DDBCEA87B597104EDDDC102\":\n                raise ValidationError(message=\"This is the TEST admin fingerprint\")\n            if not re.match(\"[a-fA-F0-9]{40}$\", text):\n                raise ValidationError(message=\"fingerprints must be 40 hexadecimal characters\")\n            return True\n\n    class ValidateOptionalFingerprint(ValidateFingerprint):\n        def validate(self, document: Document) -> bool:\n            if document.text == \"\":\n                return True\n            return super().validate(document)\n\n    class ValidateInt(Validator):\n        def validate(self, document: Document) -> bool:\n            if re.match(r\"\\d+$\", document.text):\n                return True\n            raise ValidationError(message=\"Must be an integer\")\n\n    class Locales:\n        def __init__(self, appdir: str) -> None:\n            self.translation_dir = os.path.realpath(os.path.join(appdir, \"translations\"))\n\n        def get_translations(self) -> Set[str]:\n            translations = I18N_DEFAULT_LOCALES\n            for dirname in os.listdir(self.translation_dir):\n                if dirname != \"messages.pot\":\n                    translations.add(dirname)\n            return translations\n\n    class ValidateLocales(Validator):\n        def __init__(self, basedir: str, supported: Set[str]) -> None:\n            present = SiteConfig.Locales(basedir).get_translations()\n            self.available = present & supported\n\n            super().__init__()\n\n        def validate(self, document: Document) -> bool:\n            desired = document.text.split()\n            missing = set(desired) - self.available\n            if not missing:\n                return True\n            raise ValidationError(\n                message=\"The following locales are not available \" + \" \".join(missing)\n            )\n\n    class ValidateOSSECUsername(Validator):\n        def validate(self, document: Document) -> bool:\n            text = document.text\n            if text and \"@\" not in text and text != \"test\":\n                return True\n            raise ValidationError(message=\"The SASL username should not include the domain name\")\n\n    class ValidateOSSECPassword(Validator):\n        def validate(self, document: Document) -> bool:\n            text = document.text\n            if len(text) >= 8 and text != \"password123\":\n                return True\n            raise ValidationError(message=\"Password for OSSEC email account must be strong\")\n\n    class ValidateEmail(Validator):\n        def validate(self, document: Document) -> bool:\n            text = document.text\n            if text == \"\":\n                raise ValidationError(message=(\"Must not be empty\"))\n            if \"@\" not in text:\n                raise ValidationError(message=(\"Must contain a @\"))\n            return True\n\n    class ValidateOSSECEmail(ValidateEmail):\n        def validate(self, document: Document) -> bool:\n            super().validate(document)\n            text = document.text\n            if text != \"ossec@ossec.test\":\n                return True\n            raise ValidationError(\n                message=(\"Must be set to something other than \" \"ossec@ossec.test\")\n            )\n\n    class ValidateOptionalEmail(ValidateEmail):\n        def validate(self, document: Document) -> bool:\n            if document.text == \"\":\n                return True\n            return super().validate(document)\n\n    def __init__(self, args: argparse.Namespace) -> None:\n        self.args = args\n        self.config: dict = {}\n        # Hold runtime configuration before save, to support\n        # referencing other responses during validation\n        self._config_in_progress: dict = {}\n\n        supported_locales = I18N_DEFAULT_LOCALES.copy()\n        i18n_conf_path = os.path.join(args.root, I18N_CONF)\n        if os.path.exists(i18n_conf_path):\n            with open(i18n_conf_path) as i18n_conf_file:\n                i18n_conf = json.load(i18n_conf_file)\n            supported_locales.update(set(i18n_conf[\"supported_locales\"].keys()))\n        locale_validator = SiteConfig.ValidateLocales(self.args.app_path, supported_locales)\n\n        self.desc: List[_DescEntryType] = [\n            (\n                \"ssh_users\",\n                \"sdadmin\",\n                str,\n                \"Username for SSH access to the servers\",\n                SiteConfig.ValidateUser(),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"daily_reboot_time\",\n                4,\n                int,\n                \"Daily reboot time of the server (24-hour clock)\",\n                SiteConfig.ValidateTime(),\n                int,\n                lambda config: True,\n            ),\n            (\n                \"app_ip\",\n                \"10.20.2.2\",\n                str,\n                \"Local IPv4 address for the Application Server\",\n                SiteConfig.ValidateIP(),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"monitor_ip\",\n                \"10.20.3.2\",\n                str,\n                \"Local IPv4 address for the Monitor Server\",\n                SiteConfig.ValidateIP(),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"app_hostname\",\n                \"app\",\n                str,\n                \"Hostname for Application Server\",\n                SiteConfig.ValidateNotEmpty(),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"monitor_hostname\",\n                \"mon\",\n                str,\n                \"Hostname for Monitor Server\",\n                SiteConfig.ValidateNotEmpty(),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"dns_server\",\n                [\"8.8.8.8\", \"8.8.4.4\"],\n                list,\n                \"DNS server(s)\",\n                SiteConfig.ValidateNameservers(),\n                SiteConfig.split_list,\n                lambda config: True,\n            ),\n            (\n                \"securedrop_app_gpg_public_key\",\n                \"SecureDrop.asc\",\n                str,\n                \"Local filepath to public key for \" + \"SecureDrop Application GPG public key\",\n                SiteConfig.ValidatePath(self.args.ansible_path),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"securedrop_app_pow_on_source_interface\",\n                True,\n                bool,\n                \"Enable Tor's proof-of-work defense against denial-of-service attacks for the \"\n                \"Source Interface?\",\n                SiteConfig.ValidateYesNo(),\n                lambda x: x.lower() == \"yes\",\n                lambda config: True,\n            ),\n            (\n                \"securedrop_app_https_on_source_interface\",\n                False,\n                bool,\n                \"Enable HTTPS for the Source Interface (requires EV certificate)?\",\n                SiteConfig.ValidateYesNo(),\n                lambda x: x.lower() == \"yes\",\n                lambda config: True,\n            ),\n            (\n                \"securedrop_app_https_certificate_cert_src\",\n                \"\",\n                str,\n                \"Local filepath to HTTPS certificate\",\n                SiteConfig.ValidateOptionalPath(self.args.ansible_path),\n                None,\n                lambda config: config.get(\"securedrop_app_https_on_source_interface\"),\n            ),\n            (\n                \"securedrop_app_https_certificate_key_src\",\n                \"\",\n                str,\n                \"Local filepath to HTTPS certificate key\",\n                SiteConfig.ValidateOptionalPath(self.args.ansible_path),\n                None,\n                lambda config: config.get(\"securedrop_app_https_on_source_interface\"),\n            ),\n            (\n                \"securedrop_app_https_certificate_chain_src\",\n                \"\",\n                str,\n                \"Local filepath to HTTPS certificate chain file\",\n                SiteConfig.ValidateOptionalPath(self.args.ansible_path),\n                None,\n                lambda config: config.get(\"securedrop_app_https_on_source_interface\"),\n            ),\n            (\n                \"securedrop_app_gpg_fingerprint\",\n                \"\",\n                str,\n                \"Full fingerprint for the SecureDrop Application GPG Key\",\n                SiteConfig.ValidateFingerprint(),\n                self.sanitize_fingerprint,\n                lambda config: True,\n            ),\n            (\n                \"ossec_alert_gpg_public_key\",\n                \"ossec.pub\",\n                str,\n                \"Local filepath to OSSEC alerts GPG public key\",\n                SiteConfig.ValidatePath(self.args.ansible_path),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"ossec_gpg_fpr\",\n                \"\",\n                str,\n                \"Full fingerprint for the OSSEC alerts GPG public key\",\n                SiteConfig.ValidateFingerprint(),\n                self.sanitize_fingerprint,\n                lambda config: True,\n            ),\n            (\n                \"ossec_alert_email\",\n                \"\",\n                str,\n                \"Admin email address for receiving OSSEC alerts\",\n                SiteConfig.ValidateOSSECEmail(),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"journalist_alert_gpg_public_key\",\n                \"\",\n                str,\n                \"Local filepath to journalist alerts GPG public key (optional)\",\n                SiteConfig.ValidateOptionalPath(self.args.ansible_path),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"journalist_gpg_fpr\",\n                \"\",\n                str,\n                \"Full fingerprint for the journalist alerts \" + \"GPG public key (optional)\",\n                SiteConfig.ValidateOptionalFingerprint(),\n                self.sanitize_fingerprint,\n                lambda config: config.get(\"journalist_alert_gpg_public_key\"),\n            ),\n            (\n                \"journalist_alert_email\",\n                \"\",\n                str,\n                \"Email address for receiving journalist alerts (optional)\",\n                SiteConfig.ValidateOptionalEmail(),\n                None,\n                lambda config: config.get(\"journalist_alert_gpg_public_key\"),\n            ),\n            (\n                \"smtp_relay\",\n                \"smtp.gmail.com\",\n                str,\n                \"SMTP relay for sending OSSEC alerts\",\n                SiteConfig.ValidateNotEmpty(),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"smtp_relay_port\",\n                587,\n                int,\n                \"SMTP port for sending OSSEC alerts\",\n                SiteConfig.ValidateInt(),\n                int,\n                lambda config: True,\n            ),\n            (\n                \"sasl_domain\",\n                \"gmail.com\",\n                str,\n                \"SASL domain for sending OSSEC alerts\",\n                None,\n                None,\n                lambda config: True,\n            ),\n            (\n                \"sasl_username\",\n                \"\",\n                str,\n                \"SASL username for sending OSSEC alerts\",\n                SiteConfig.ValidateOSSECUsername(),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"sasl_password\",\n                \"\",\n                str,\n                \"SASL password for sending OSSEC alerts\",\n                SiteConfig.ValidateOSSECPassword(),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"enable_ssh_over_tor\",\n                True,\n                bool,\n                \"Enable SSH over Tor (recommended, disables SSH over LAN). \"\n                + \"If you respond no, SSH will be available over LAN only\",\n                SiteConfig.ValidateYesNo(),\n                lambda x: x.lower() == \"yes\",\n                lambda config: True,\n            ),\n            (\n                \"securedrop_supported_locales\",\n                [],\n                list,\n                \"Space separated list of additional locales to support \"\n                \"(\" + \" \".join(sorted(list(locale_validator.available))) + \")\",\n                locale_validator,\n                str.split,\n                lambda config: True,\n            ),\n        ]\n\n    def load_and_update_config(self, validate: bool = True, prompt: bool = True) -> bool:\n        if self.exists():\n            self.config = self.load(validate)\n        elif not prompt:\n            sdlog.error('Please run \"securedrop-admin sdconfig\" first.')\n            sys.exit(1)\n\n        return self.update_config(prompt)\n\n    def update_config(self, prompt: bool = True) -> bool:\n        if prompt:\n            self.config.update(self.user_prompt_config())\n        self.save()\n        self.validate_gpg_keys()\n        self.validate_journalist_alert_email()\n        return True\n\n    def user_prompt_config(self) -> Dict[str, Any]:\n        self._config_in_progress = {}\n        for desc in self.desc:\n            (var, default, type, prompt, validator, transform, condition) = desc\n            if not condition(self._config_in_progress):\n                self._config_in_progress[var] = \"\"\n                continue\n            self._config_in_progress[var] = self.user_prompt_config_one(desc, self.config.get(var))\n        return self._config_in_progress\n\n    def user_prompt_config_one(self, desc: _DescEntryType, from_config: Optional[Any]) -> Any:\n        (var, default, type, prompt, validator, transform, condition) = desc\n        if from_config is not None:\n            default = from_config\n        prompt += \": \"\n\n        # The following is for the dynamic check of the user input\n        # for the previous question, as we are calling the default value\n        # function dynamically, we can get the right value based on the\n        # previous user input.\n        if callable(default):\n            default = default()\n        return self.validated_input(prompt, default, validator, transform)\n\n    def validated_input(\n        self, prompt: str, default: Any, validator: Validator, transform: Optional[Callable]\n    ) -> Any:\n        if type(default) is bool:\n            default = \"yes\" if default else \"no\"\n        if type(default) is int:\n            default = str(default)\n        if isinstance(default, list):\n            default = \" \".join(default)\n        if type(default) is not str:\n            default = str(default)\n        value = prompt_toolkit.prompt(prompt, default=default, validator=validator)\n        if transform:\n            return transform(value)\n        else:\n            return value\n\n    def sanitize_fingerprint(self, value: str) -> str:\n        return value.upper().replace(\" \", \"\")\n\n    def validate_gpg_keys(self) -> bool:\n        keys = (\n            (\"securedrop_app_gpg_public_key\", \"securedrop_app_gpg_fingerprint\"),\n            (\"ossec_alert_gpg_public_key\", \"ossec_gpg_fpr\"),\n            (\"journalist_alert_gpg_public_key\", \"journalist_gpg_fpr\"),\n        )\n        validate = os.path.join(os.path.dirname(__file__), \"..\", \"bin\", \"validate-gpg-key.sh\")\n        for public_key, fingerprint in keys:\n            if self.config[public_key] == \"\" and self.config[fingerprint] == \"\":\n                continue\n            public_key = os.path.join(self.args.ansible_path, self.config[public_key])\n            fingerprint = self.config[fingerprint]\n            try:\n                sdlog.debug(\n                    subprocess.check_output(\n                        [validate, public_key, fingerprint], stderr=subprocess.STDOUT\n                    )\n                )\n            except subprocess.CalledProcessError as e:\n                sdlog.debug(e.output)\n                message = f\"{fingerprint}: Fingerprint validation failed\"\n\n                # The validation script returns different error codes depending on what\n                # the cause of the validation failure was. See `admin/bin/validate-gpg-key.sh`\n                if e.returncode == 1:\n                    message = (\n                        f\"fingerprint {fingerprint} does not match \"\n                        + f\"the public key {public_key}\"\n                    )\n                elif e.returncode == 2:\n                    message = (\n                        f\"fingerprint {fingerprint} \"\n                        + \"failed sq-keyring-linter check. You may be using an older key that \"\n                        + \"needs to be updated. Please contact your SecureDrop administrator, or \"\n                        + \"https://support.freedom.press for assistance.\"\n                    )\n                raise FingerprintException(message)\n        return True\n\n    def validate_journalist_alert_email(self) -> bool:\n        if (\n            self.config[\"journalist_alert_gpg_public_key\"] == \"\"\n            and self.config[\"journalist_gpg_fpr\"] == \"\"\n        ):\n            return True\n\n        class Document:\n            def __init__(self, text: str) -> None:\n                self.text = text\n\n        try:\n            SiteConfig.ValidateEmail().validate(Document(self.config[\"journalist_alert_email\"]))\n        except ValidationError as e:\n            raise JournalistAlertEmailException(\"journalist alerts email: \" + e.message)\n        return True\n\n    def exists(self) -> bool:\n        return os.path.exists(self.args.site_config)\n\n    def save(self) -> None:\n        with open(self.args.site_config, \"w\") as site_config_file:\n            yaml.safe_dump(self.config, site_config_file, default_flow_style=False)\n\n    def clean_config(self, config: Dict) -> Dict:\n        \"\"\"\n        Cleans a loaded config without prompting.\n\n        For every variable defined in self.desc, validate its value in\n        the supplied configuration dictionary, run the value through\n        its defined transformer, and add the result to a clean version\n        of the configuration.\n\n        If no configuration variable triggers a ValidationError, the\n        clean configuration will be returned.\n        \"\"\"\n        clean_config = {}\n        clean_config.update(config)\n        for desc in self.desc:\n            var, default, vartype, prompt, validator, transform, condition = desc\n            if var in clean_config:\n                value = clean_config[var]\n                if isinstance(value, list):\n                    text = \" \".join(str(v) for v in value)\n                elif isinstance(value, bool):\n                    text = \"yes\" if value else \"no\"\n                else:\n                    text = str(value)\n\n                if validator is not None:\n                    try:\n                        validator.validate(Document(text))\n                    except ValidationError as e:\n                        sdlog.error(e)\n                        sdlog.error(\n                            \"Error loading configuration. \"\n                            'Please run \"securedrop-admin sdconfig\" again.'\n                        )\n                        raise\n                clean_config[var] = transform(text) if transform else text\n                if var not in self._config_in_progress:\n                    self._config_in_progress[var] = clean_config[var]\n        return clean_config\n\n    def load(self, validate: bool = True) -> Dict:\n        \"\"\"\n        Loads the site configuration file.\n\n        If validate is True, then each configuration variable that has\n        an entry in self.desc is validated and transformed according\n        to current specifications.\n        \"\"\"\n        try:\n            with open(self.args.site_config) as site_config_file:\n                c = yaml.safe_load(site_config_file)\n                return self.clean_config(c) if validate else c\n        except OSError:\n            sdlog.error(\"Config file missing, re-run with sdconfig\")\n            raise\n        except yaml.YAMLError:\n            sdlog.error(f\"There was an issue processing {self.args.site_config}\")\n            raise",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "ValidateNotEmpty",
          "code": "class ValidateNotEmpty(Validator):\n        def validate(self, document: Document) -> bool:\n            if document.text != \"\":\n                return True\n            raise ValidationError(message=\"Must not be an empty string\")",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validate",
          "code": "def validate(self, document: Document) -> bool:\n            if document.text != \"\":\n                return True\n            raise ValidationError(message=\"Must not be an empty string\")",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "ValidateTime",
          "code": "class ValidateTime(Validator):\n        def validate(self, document: Document) -> bool:\n            if document.text.isdigit() and int(document.text) in range(24):\n                return True\n            raise ValidationError(message=\"Must be an integer between 0 and 23\")",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validate",
          "code": "def validate(self, document: Document) -> bool:\n            if document.text.isdigit() and int(document.text) in range(24):\n                return True\n            raise ValidationError(message=\"Must be an integer between 0 and 23\")",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "ValidateUser",
          "code": "class ValidateUser(Validator):\n        def validate(self, document: Document) -> bool:\n            text = document.text\n            if text not in (\"\", \"root\", \"amnesia\"):\n                return True\n            raise ValidationError(message=\"Must not be root, amnesia or an empty string\")",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validate",
          "code": "def validate(self, document: Document) -> bool:\n            text = document.text\n            if text not in (\"\", \"root\", \"amnesia\"):\n                return True\n            raise ValidationError(message=\"Must not be root, amnesia or an empty string\")",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "ValidateIP",
          "code": "class ValidateIP(Validator):\n        def validate(self, document: Document) -> bool:\n            try:\n                ipaddress.ip_address(document.text)\n                return True\n            except ValueError as e:\n                raise ValidationError(message=str(e))",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validate",
          "code": "def validate(self, document: Document) -> bool:\n            try:\n                ipaddress.ip_address(document.text)\n                return True\n            except ValueError as e:\n                raise ValidationError(message=str(e))",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "ValidateNameservers",
          "code": "class ValidateNameservers(Validator):\n        def validate(self, document: Document) -> bool:\n            candidates = LIST_SPLIT_RE.split(document.text)\n            if len(candidates) > MAX_NAMESERVERS:\n                raise ValidationError(message=\"Specify no more than three nameservers.\")\n            try:\n                all(map(ipaddress.ip_address, candidates))\n            except ValueError:\n                raise ValidationError(\n                    message=(\n                        \"DNS server(s) should be a space/comma-separated list \"\n                        f\"of up to {MAX_NAMESERVERS} IP addresses\"\n                    )\n                )\n            return True",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validate",
          "code": "def validate(self, document: Document) -> bool:\n            candidates = LIST_SPLIT_RE.split(document.text)\n            if len(candidates) > MAX_NAMESERVERS:\n                raise ValidationError(message=\"Specify no more than three nameservers.\")\n            try:\n                all(map(ipaddress.ip_address, candidates))\n            except ValueError:\n                raise ValidationError(\n                    message=(\n                        \"DNS server(s) should be a space/comma-separated list \"\n                        f\"of up to {MAX_NAMESERVERS} IP addresses\"\n                    )\n                )\n            return True",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "split_list",
          "code": "def split_list(text: str) -> List[str]:\n        \"\"\"\n        Splits a string containing a list of values separated by commas or whitespace.\n        \"\"\"\n        return LIST_SPLIT_RE.split(text)",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "ValidatePath",
          "code": "class ValidatePath(Validator):\n        def __init__(self, basedir: str) -> None:\n            self.basedir = basedir\n            super().__init__()\n\n        def validate(self, document: Document) -> bool:\n            if document.text == \"\":\n                raise ValidationError(message=\"an existing file name is required\")\n            path = os.path.join(self.basedir, document.text)\n            if os.path.exists(path):\n                return True\n            raise ValidationError(message=path + \" file does not exist\")",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, basedir: str) -> None:\n            self.basedir = basedir\n            super().__init__()",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validate",
          "code": "def validate(self, document: Document) -> bool:\n            if document.text == \"\":\n                raise ValidationError(message=\"an existing file name is required\")\n            path = os.path.join(self.basedir, document.text)\n            if os.path.exists(path):\n                return True\n            raise ValidationError(message=path + \" file does not exist\")",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "ValidateOptionalPath",
          "code": "class ValidateOptionalPath(ValidatePath):\n        def validate(self, document: Document) -> bool:\n            if document.text == \"\":\n                return True\n            return super().validate(document)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validate",
          "code": "def validate(self, document: Document) -> bool:\n            if document.text == \"\":\n                return True\n            return super().validate(document)",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "ValidateYesNo",
          "code": "class ValidateYesNo(Validator):\n        def validate(self, document: Document) -> bool:\n            text = document.text.lower()\n            if text in (\"yes\", \"no\"):\n                return True\n            raise ValidationError(message=\"Must be either yes or no\")",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validate",
          "code": "def validate(self, document: Document) -> bool:\n            text = document.text.lower()\n            if text in (\"yes\", \"no\"):\n                return True\n            raise ValidationError(message=\"Must be either yes or no\")",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "ValidateFingerprint",
          "code": "class ValidateFingerprint(Validator):\n        def validate(self, document: Document) -> bool:\n            text = document.text.replace(\" \", \"\")\n            if text == \"65A1B5FF195B56353CC63DFFCC40EF1228271441\":\n                raise ValidationError(message=\"This is the TEST journalist fingerprint\")\n            if text == \"600BC6D5142C68F35DDBCEA87B597104EDDDC102\":\n                raise ValidationError(message=\"This is the TEST admin fingerprint\")\n            if not re.match(\"[a-fA-F0-9]{40}$\", text):\n                raise ValidationError(message=\"fingerprints must be 40 hexadecimal characters\")\n            return True",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validate",
          "code": "def validate(self, document: Document) -> bool:\n            text = document.text.replace(\" \", \"\")\n            if text == \"65A1B5FF195B56353CC63DFFCC40EF1228271441\":\n                raise ValidationError(message=\"This is the TEST journalist fingerprint\")\n            if text == \"600BC6D5142C68F35DDBCEA87B597104EDDDC102\":\n                raise ValidationError(message=\"This is the TEST admin fingerprint\")\n            if not re.match(\"[a-fA-F0-9]{40}$\", text):\n                raise ValidationError(message=\"fingerprints must be 40 hexadecimal characters\")\n            return True",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "ValidateOptionalFingerprint",
          "code": "class ValidateOptionalFingerprint(ValidateFingerprint):\n        def validate(self, document: Document) -> bool:\n            if document.text == \"\":\n                return True\n            return super().validate(document)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validate",
          "code": "def validate(self, document: Document) -> bool:\n            if document.text == \"\":\n                return True\n            return super().validate(document)",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "ValidateInt",
          "code": "class ValidateInt(Validator):\n        def validate(self, document: Document) -> bool:\n            if re.match(r\"\\d+$\", document.text):\n                return True\n            raise ValidationError(message=\"Must be an integer\")",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validate",
          "code": "def validate(self, document: Document) -> bool:\n            if re.match(r\"\\d+$\", document.text):\n                return True\n            raise ValidationError(message=\"Must be an integer\")",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "Locales",
          "code": "class Locales:\n        def __init__(self, appdir: str) -> None:\n            self.translation_dir = os.path.realpath(os.path.join(appdir, \"translations\"))\n\n        def get_translations(self) -> Set[str]:\n            translations = I18N_DEFAULT_LOCALES\n            for dirname in os.listdir(self.translation_dir):\n                if dirname != \"messages.pot\":\n                    translations.add(dirname)\n            return translations",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, appdir: str) -> None:\n            self.translation_dir = os.path.realpath(os.path.join(appdir, \"translations\"))",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "get_translations",
          "code": "def get_translations(self) -> Set[str]:\n            translations = I18N_DEFAULT_LOCALES\n            for dirname in os.listdir(self.translation_dir):\n                if dirname != \"messages.pot\":\n                    translations.add(dirname)\n            return translations",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "ValidateLocales",
          "code": "class ValidateLocales(Validator):\n        def __init__(self, basedir: str, supported: Set[str]) -> None:\n            present = SiteConfig.Locales(basedir).get_translations()\n            self.available = present & supported\n\n            super().__init__()\n\n        def validate(self, document: Document) -> bool:\n            desired = document.text.split()\n            missing = set(desired) - self.available\n            if not missing:\n                return True\n            raise ValidationError(\n                message=\"The following locales are not available \" + \" \".join(missing)\n            )",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, basedir: str, supported: Set[str]) -> None:\n            present = SiteConfig.Locales(basedir).get_translations()\n            self.available = present & supported\n\n            super().__init__()",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validate",
          "code": "def validate(self, document: Document) -> bool:\n            desired = document.text.split()\n            missing = set(desired) - self.available\n            if not missing:\n                return True\n            raise ValidationError(\n                message=\"The following locales are not available \" + \" \".join(missing)\n            )",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "ValidateOSSECUsername",
          "code": "class ValidateOSSECUsername(Validator):\n        def validate(self, document: Document) -> bool:\n            text = document.text\n            if text and \"@\" not in text and text != \"test\":\n                return True\n            raise ValidationError(message=\"The SASL username should not include the domain name\")",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validate",
          "code": "def validate(self, document: Document) -> bool:\n            text = document.text\n            if text and \"@\" not in text and text != \"test\":\n                return True\n            raise ValidationError(message=\"The SASL username should not include the domain name\")",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "ValidateOSSECPassword",
          "code": "class ValidateOSSECPassword(Validator):\n        def validate(self, document: Document) -> bool:\n            text = document.text\n            if len(text) >= 8 and text != \"password123\":\n                return True\n            raise ValidationError(message=\"Password for OSSEC email account must be strong\")",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validate",
          "code": "def validate(self, document: Document) -> bool:\n            text = document.text\n            if len(text) >= 8 and text != \"password123\":\n                return True\n            raise ValidationError(message=\"Password for OSSEC email account must be strong\")",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "ValidateEmail",
          "code": "class ValidateEmail(Validator):\n        def validate(self, document: Document) -> bool:\n            text = document.text\n            if text == \"\":\n                raise ValidationError(message=(\"Must not be empty\"))\n            if \"@\" not in text:\n                raise ValidationError(message=(\"Must contain a @\"))\n            return True",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validate",
          "code": "def validate(self, document: Document) -> bool:\n            text = document.text\n            if text == \"\":\n                raise ValidationError(message=(\"Must not be empty\"))\n            if \"@\" not in text:\n                raise ValidationError(message=(\"Must contain a @\"))\n            return True",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "ValidateOSSECEmail",
          "code": "class ValidateOSSECEmail(ValidateEmail):\n        def validate(self, document: Document) -> bool:\n            super().validate(document)\n            text = document.text\n            if text != \"ossec@ossec.test\":\n                return True\n            raise ValidationError(\n                message=(\"Must be set to something other than \" \"ossec@ossec.test\")\n            )",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validate",
          "code": "def validate(self, document: Document) -> bool:\n            super().validate(document)\n            text = document.text\n            if text != \"ossec@ossec.test\":\n                return True\n            raise ValidationError(\n                message=(\"Must be set to something other than \" \"ossec@ossec.test\")\n            )",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "ValidateOptionalEmail",
          "code": "class ValidateOptionalEmail(ValidateEmail):\n        def validate(self, document: Document) -> bool:\n            if document.text == \"\":\n                return True\n            return super().validate(document)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validate",
          "code": "def validate(self, document: Document) -> bool:\n            if document.text == \"\":\n                return True\n            return super().validate(document)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, args: argparse.Namespace) -> None:\n        self.args = args\n        self.config: dict = {}\n        # Hold runtime configuration before save, to support\n        # referencing other responses during validation\n        self._config_in_progress: dict = {}\n\n        supported_locales = I18N_DEFAULT_LOCALES.copy()\n        i18n_conf_path = os.path.join(args.root, I18N_CONF)\n        if os.path.exists(i18n_conf_path):\n            with open(i18n_conf_path) as i18n_conf_file:\n                i18n_conf = json.load(i18n_conf_file)\n            supported_locales.update(set(i18n_conf[\"supported_locales\"].keys()))\n        locale_validator = SiteConfig.ValidateLocales(self.args.app_path, supported_locales)\n\n        self.desc: List[_DescEntryType] = [\n            (\n                \"ssh_users\",\n                \"sdadmin\",\n                str,\n                \"Username for SSH access to the servers\",\n                SiteConfig.ValidateUser(),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"daily_reboot_time\",\n                4,\n                int,\n                \"Daily reboot time of the server (24-hour clock)\",\n                SiteConfig.ValidateTime(),\n                int,\n                lambda config: True,\n            ),\n            (\n                \"app_ip\",\n                \"10.20.2.2\",\n                str,\n                \"Local IPv4 address for the Application Server\",\n                SiteConfig.ValidateIP(),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"monitor_ip\",\n                \"10.20.3.2\",\n                str,\n                \"Local IPv4 address for the Monitor Server\",\n                SiteConfig.ValidateIP(),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"app_hostname\",\n                \"app\",\n                str,\n                \"Hostname for Application Server\",\n                SiteConfig.ValidateNotEmpty(),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"monitor_hostname\",\n                \"mon\",\n                str,\n                \"Hostname for Monitor Server\",\n                SiteConfig.ValidateNotEmpty(),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"dns_server\",\n                [\"8.8.8.8\", \"8.8.4.4\"],\n                list,\n                \"DNS server(s)\",\n                SiteConfig.ValidateNameservers(),\n                SiteConfig.split_list,\n                lambda config: True,\n            ),\n            (\n                \"securedrop_app_gpg_public_key\",\n                \"SecureDrop.asc\",\n                str,\n                \"Local filepath to public key for \" + \"SecureDrop Application GPG public key\",\n                SiteConfig.ValidatePath(self.args.ansible_path),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"securedrop_app_pow_on_source_interface\",\n                True,\n                bool,\n                \"Enable Tor's proof-of-work defense against denial-of-service attacks for the \"\n                \"Source Interface?\",\n                SiteConfig.ValidateYesNo(),\n                lambda x: x.lower() == \"yes\",\n                lambda config: True,\n            ),\n            (\n                \"securedrop_app_https_on_source_interface\",\n                False,\n                bool,\n                \"Enable HTTPS for the Source Interface (requires EV certificate)?\",\n                SiteConfig.ValidateYesNo(),\n                lambda x: x.lower() == \"yes\",\n                lambda config: True,\n            ),\n            (\n                \"securedrop_app_https_certificate_cert_src\",\n                \"\",\n                str,\n                \"Local filepath to HTTPS certificate\",\n                SiteConfig.ValidateOptionalPath(self.args.ansible_path),\n                None,\n                lambda config: config.get(\"securedrop_app_https_on_source_interface\"),\n            ),\n            (\n                \"securedrop_app_https_certificate_key_src\",\n                \"\",\n                str,\n                \"Local filepath to HTTPS certificate key\",\n                SiteConfig.ValidateOptionalPath(self.args.ansible_path),\n                None,\n                lambda config: config.get(\"securedrop_app_https_on_source_interface\"),\n            ),\n            (\n                \"securedrop_app_https_certificate_chain_src\",\n                \"\",\n                str,\n                \"Local filepath to HTTPS certificate chain file\",\n                SiteConfig.ValidateOptionalPath(self.args.ansible_path),\n                None,\n                lambda config: config.get(\"securedrop_app_https_on_source_interface\"),\n            ),\n            (\n                \"securedrop_app_gpg_fingerprint\",\n                \"\",\n                str,\n                \"Full fingerprint for the SecureDrop Application GPG Key\",\n                SiteConfig.ValidateFingerprint(),\n                self.sanitize_fingerprint,\n                lambda config: True,\n            ),\n            (\n                \"ossec_alert_gpg_public_key\",\n                \"ossec.pub\",\n                str,\n                \"Local filepath to OSSEC alerts GPG public key\",\n                SiteConfig.ValidatePath(self.args.ansible_path),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"ossec_gpg_fpr\",\n                \"\",\n                str,\n                \"Full fingerprint for the OSSEC alerts GPG public key\",\n                SiteConfig.ValidateFingerprint(),\n                self.sanitize_fingerprint,\n                lambda config: True,\n            ),\n            (\n                \"ossec_alert_email\",\n                \"\",\n                str,\n                \"Admin email address for receiving OSSEC alerts\",\n                SiteConfig.ValidateOSSECEmail(),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"journalist_alert_gpg_public_key\",\n                \"\",\n                str,\n                \"Local filepath to journalist alerts GPG public key (optional)\",\n                SiteConfig.ValidateOptionalPath(self.args.ansible_path),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"journalist_gpg_fpr\",\n                \"\",\n                str,\n                \"Full fingerprint for the journalist alerts \" + \"GPG public key (optional)\",\n                SiteConfig.ValidateOptionalFingerprint(),\n                self.sanitize_fingerprint,\n                lambda config: config.get(\"journalist_alert_gpg_public_key\"),\n            ),\n            (\n                \"journalist_alert_email\",\n                \"\",\n                str,\n                \"Email address for receiving journalist alerts (optional)\",\n                SiteConfig.ValidateOptionalEmail(),\n                None,\n                lambda config: config.get(\"journalist_alert_gpg_public_key\"),\n            ),\n            (\n                \"smtp_relay\",\n                \"smtp.gmail.com\",\n                str,\n                \"SMTP relay for sending OSSEC alerts\",\n                SiteConfig.ValidateNotEmpty(),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"smtp_relay_port\",\n                587,\n                int,\n                \"SMTP port for sending OSSEC alerts\",\n                SiteConfig.ValidateInt(),\n                int,\n                lambda config: True,\n            ),\n            (\n                \"sasl_domain\",\n                \"gmail.com\",\n                str,\n                \"SASL domain for sending OSSEC alerts\",\n                None,\n                None,\n                lambda config: True,\n            ),\n            (\n                \"sasl_username\",\n                \"\",\n                str,\n                \"SASL username for sending OSSEC alerts\",\n                SiteConfig.ValidateOSSECUsername(),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"sasl_password\",\n                \"\",\n                str,\n                \"SASL password for sending OSSEC alerts\",\n                SiteConfig.ValidateOSSECPassword(),\n                None,\n                lambda config: True,\n            ),\n            (\n                \"enable_ssh_over_tor\",\n                True,\n                bool,\n                \"Enable SSH over Tor (recommended, disables SSH over LAN). \"\n                + \"If you respond no, SSH will be available over LAN only\",\n                SiteConfig.ValidateYesNo(),\n                lambda x: x.lower() == \"yes\",\n                lambda config: True,\n            ),\n            (\n                \"securedrop_supported_locales\",\n                [],\n                list,\n                \"Space separated list of additional locales to support \"\n                \"(\" + \" \".join(sorted(list(locale_validator.available))) + \")\",\n                locale_validator,\n                str.split,\n                lambda config: True,\n            ),\n        ]",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "load_and_update_config",
          "code": "def load_and_update_config(self, validate: bool = True, prompt: bool = True) -> bool:\n        if self.exists():\n            self.config = self.load(validate)\n        elif not prompt:\n            sdlog.error('Please run \"securedrop-admin sdconfig\" first.')\n            sys.exit(1)\n\n        return self.update_config(prompt)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "update_config",
          "code": "def update_config(self, prompt: bool = True) -> bool:\n        if prompt:\n            self.config.update(self.user_prompt_config())\n        self.save()\n        self.validate_gpg_keys()\n        self.validate_journalist_alert_email()\n        return True",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "user_prompt_config",
          "code": "def user_prompt_config(self) -> Dict[str, Any]:\n        self._config_in_progress = {}\n        for desc in self.desc:\n            (var, default, type, prompt, validator, transform, condition) = desc\n            if not condition(self._config_in_progress):\n                self._config_in_progress[var] = \"\"\n                continue\n            self._config_in_progress[var] = self.user_prompt_config_one(desc, self.config.get(var))\n        return self._config_in_progress",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "user_prompt_config_one",
          "code": "def user_prompt_config_one(self, desc: _DescEntryType, from_config: Optional[Any]) -> Any:\n        (var, default, type, prompt, validator, transform, condition) = desc\n        if from_config is not None:\n            default = from_config\n        prompt += \": \"\n\n        # The following is for the dynamic check of the user input\n        # for the previous question, as we are calling the default value\n        # function dynamically, we can get the right value based on the\n        # previous user input.\n        if callable(default):\n            default = default()\n        return self.validated_input(prompt, default, validator, transform)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validated_input",
          "code": "def validated_input(\n        self, prompt: str, default: Any, validator: Validator, transform: Optional[Callable]\n    ) -> Any:\n        if type(default) is bool:\n            default = \"yes\" if default else \"no\"\n        if type(default) is int:\n            default = str(default)\n        if isinstance(default, list):\n            default = \" \".join(default)\n        if type(default) is not str:\n            default = str(default)\n        value = prompt_toolkit.prompt(prompt, default=default, validator=validator)\n        if transform:\n            return transform(value)\n        else:\n            return value",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "sanitize_fingerprint",
          "code": "def sanitize_fingerprint(self, value: str) -> str:\n        return value.upper().replace(\" \", \"\")",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validate_gpg_keys",
          "code": "def validate_gpg_keys(self) -> bool:\n        keys = (\n            (\"securedrop_app_gpg_public_key\", \"securedrop_app_gpg_fingerprint\"),\n            (\"ossec_alert_gpg_public_key\", \"ossec_gpg_fpr\"),\n            (\"journalist_alert_gpg_public_key\", \"journalist_gpg_fpr\"),\n        )\n        validate = os.path.join(os.path.dirname(__file__), \"..\", \"bin\", \"validate-gpg-key.sh\")\n        for public_key, fingerprint in keys:\n            if self.config[public_key] == \"\" and self.config[fingerprint] == \"\":\n                continue\n            public_key = os.path.join(self.args.ansible_path, self.config[public_key])\n            fingerprint = self.config[fingerprint]\n            try:\n                sdlog.debug(\n                    subprocess.check_output(\n                        [validate, public_key, fingerprint], stderr=subprocess.STDOUT\n                    )\n                )\n            except subprocess.CalledProcessError as e:\n                sdlog.debug(e.output)\n                message = f\"{fingerprint}: Fingerprint validation failed\"\n\n                # The validation script returns different error codes depending on what\n                # the cause of the validation failure was. See `admin/bin/validate-gpg-key.sh`\n                if e.returncode == 1:\n                    message = (\n                        f\"fingerprint {fingerprint} does not match \"\n                        + f\"the public key {public_key}\"\n                    )\n                elif e.returncode == 2:\n                    message = (\n                        f\"fingerprint {fingerprint} \"\n                        + \"failed sq-keyring-linter check. You may be using an older key that \"\n                        + \"needs to be updated. Please contact your SecureDrop administrator, or \"\n                        + \"https://support.freedom.press for assistance.\"\n                    )\n                raise FingerprintException(message)\n        return True",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "validate_journalist_alert_email",
          "code": "def validate_journalist_alert_email(self) -> bool:\n        if (\n            self.config[\"journalist_alert_gpg_public_key\"] == \"\"\n            and self.config[\"journalist_gpg_fpr\"] == \"\"\n        ):\n            return True\n\n        class Document:\n            def __init__(self, text: str) -> None:\n                self.text = text\n\n        try:\n            SiteConfig.ValidateEmail().validate(Document(self.config[\"journalist_alert_email\"]))\n        except ValidationError as e:\n            raise JournalistAlertEmailException(\"journalist alerts email: \" + e.message)\n        return True",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "Document",
          "code": "class Document:\n            def __init__(self, text: str) -> None:\n                self.text = text",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, text: str) -> None:\n                self.text = text",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "exists",
          "code": "def exists(self) -> bool:\n        return os.path.exists(self.args.site_config)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "save",
          "code": "def save(self) -> None:\n        with open(self.args.site_config, \"w\") as site_config_file:\n            yaml.safe_dump(self.config, site_config_file, default_flow_style=False)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "clean_config",
          "code": "def clean_config(self, config: Dict) -> Dict:\n        \"\"\"\n        Cleans a loaded config without prompting.\n\n        For every variable defined in self.desc, validate its value in\n        the supplied configuration dictionary, run the value through\n        its defined transformer, and add the result to a clean version\n        of the configuration.\n\n        If no configuration variable triggers a ValidationError, the\n        clean configuration will be returned.\n        \"\"\"\n        clean_config = {}\n        clean_config.update(config)\n        for desc in self.desc:\n            var, default, vartype, prompt, validator, transform, condition = desc\n            if var in clean_config:\n                value = clean_config[var]\n                if isinstance(value, list):\n                    text = \" \".join(str(v) for v in value)\n                elif isinstance(value, bool):\n                    text = \"yes\" if value else \"no\"\n                else:\n                    text = str(value)\n\n                if validator is not None:\n                    try:\n                        validator.validate(Document(text))\n                    except ValidationError as e:\n                        sdlog.error(e)\n                        sdlog.error(\n                            \"Error loading configuration. \"\n                            'Please run \"securedrop-admin sdconfig\" again.'\n                        )\n                        raise\n                clean_config[var] = transform(text) if transform else text\n                if var not in self._config_in_progress:\n                    self._config_in_progress[var] = clean_config[var]\n        return clean_config",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "load",
          "code": "def load(self, validate: bool = True) -> Dict:\n        \"\"\"\n        Loads the site configuration file.\n\n        If validate is True, then each configuration variable that has\n        an entry in self.desc is validated and transformed according\n        to current specifications.\n        \"\"\"\n        try:\n            with open(self.args.site_config) as site_config_file:\n                c = yaml.safe_load(site_config_file)\n                return self.clean_config(c) if validate else c\n        except OSError:\n            sdlog.error(\"Config file missing, re-run with sdconfig\")\n            raise\n        except yaml.YAMLError:\n            sdlog.error(f\"There was an issue processing {self.args.site_config}\")\n            raise",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "setup_logger",
          "code": "def setup_logger(verbose: bool = False) -> None:\n    \"\"\"Configure logging handler\"\"\"\n    # Set default level on parent\n    sdlog.setLevel(logging.DEBUG)\n    level = logging.DEBUG if verbose else logging.INFO\n\n    stdout = logging.StreamHandler(sys.stdout)\n    stdout.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n    stdout.setLevel(level)\n    sdlog.addHandler(stdout)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "update_check_required",
          "code": "def update_check_required(cmd_name: str) -> Callable[[_FuncT], _FuncT]:\n    \"\"\"\n    This decorator can be added to any subcommand that is part of securedrop-admin\n    via `@update_check_required(\"name_of_subcommand\")`. It forces a check for\n    updates, and aborts if the locally installed code is out of date. It should\n    be generally added to all subcommands that make modifications on the\n    server or on the Admin Workstation.\n\n    The user can override this check by specifying the --force argument before\n    any subcommand.\n    \"\"\"\n\n    def decorator_update_check(func: _FuncT) -> _FuncT:\n        @functools.wraps(func)\n        def wrapper(*args: Any, **kwargs: Any) -> Any:\n            cli_args = args[0]\n            if cli_args.force:\n                sdlog.info(\"Skipping update check because --force argument was provided.\")\n                return func(*args, **kwargs)\n\n            update_status, latest_tag = check_for_updates(cli_args)\n            if update_status is True:\n                # Useful for troubleshooting\n                branch_status = get_git_branch(cli_args)\n\n                sdlog.error(\n                    \"You are not running the most recent signed SecureDrop release \"\n                    \"on this workstation.\"\n                )\n                sdlog.error(f\"Latest available version: {latest_tag}\")\n\n                if branch_status is not None:\n                    sdlog.error(f\"Current branch status: {branch_status}\")\n                else:\n                    sdlog.error(\"Problem determining current branch status.\")\n\n                sdlog.error(\n                    \"Running outdated or mismatched code can cause significant \" \"technical issues.\"\n                )\n                sdlog.error(\n                    \"To display more information about your repository state, run:\\n\\n\\t\"\n                    \"git status\\n\"\n                )\n                sdlog.error(\n                    \"If you are certain you want to proceed, run:\\n\\n\\t\"\n                    f\"./securedrop-admin --force {cmd_name}\\n\"\n                )\n                sdlog.error(\"To apply the latest updates, run:\\n\\n\\t\" \"./securedrop-admin update\\n\")\n                sdlog.error(\n                    \"If this fails, see the latest upgrade guide on \"\n                    \"https://docs.securedrop.org/ for instructions.\"\n                )\n                sys.exit(1)\n            return func(*args, **kwargs)\n\n        return cast(_FuncT, wrapper)\n\n    return decorator_update_check",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "decorator_update_check",
          "code": "def decorator_update_check(func: _FuncT) -> _FuncT:\n        @functools.wraps(func)\n        def wrapper(*args: Any, **kwargs: Any) -> Any:\n            cli_args = args[0]\n            if cli_args.force:\n                sdlog.info(\"Skipping update check because --force argument was provided.\")\n                return func(*args, **kwargs)\n\n            update_status, latest_tag = check_for_updates(cli_args)\n            if update_status is True:\n                # Useful for troubleshooting\n                branch_status = get_git_branch(cli_args)\n\n                sdlog.error(\n                    \"You are not running the most recent signed SecureDrop release \"\n                    \"on this workstation.\"\n                )\n                sdlog.error(f\"Latest available version: {latest_tag}\")\n\n                if branch_status is not None:\n                    sdlog.error(f\"Current branch status: {branch_status}\")\n                else:\n                    sdlog.error(\"Problem determining current branch status.\")\n\n                sdlog.error(\n                    \"Running outdated or mismatched code can cause significant \" \"technical issues.\"\n                )\n                sdlog.error(\n                    \"To display more information about your repository state, run:\\n\\n\\t\"\n                    \"git status\\n\"\n                )\n                sdlog.error(\n                    \"If you are certain you want to proceed, run:\\n\\n\\t\"\n                    f\"./securedrop-admin --force {cmd_name}\\n\"\n                )\n                sdlog.error(\"To apply the latest updates, run:\\n\\n\\t\" \"./securedrop-admin update\\n\")\n                sdlog.error(\n                    \"If this fails, see the latest upgrade guide on \"\n                    \"https://docs.securedrop.org/ for instructions.\"\n                )\n                sys.exit(1)\n            return func(*args, **kwargs)\n\n        return cast(_FuncT, wrapper)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "wrapper",
          "code": "def wrapper(*args: Any, **kwargs: Any) -> Any:\n            cli_args = args[0]\n            if cli_args.force:\n                sdlog.info(\"Skipping update check because --force argument was provided.\")\n                return func(*args, **kwargs)\n\n            update_status, latest_tag = check_for_updates(cli_args)\n            if update_status is True:\n                # Useful for troubleshooting\n                branch_status = get_git_branch(cli_args)\n\n                sdlog.error(\n                    \"You are not running the most recent signed SecureDrop release \"\n                    \"on this workstation.\"\n                )\n                sdlog.error(f\"Latest available version: {latest_tag}\")\n\n                if branch_status is not None:\n                    sdlog.error(f\"Current branch status: {branch_status}\")\n                else:\n                    sdlog.error(\"Problem determining current branch status.\")\n\n                sdlog.error(\n                    \"Running outdated or mismatched code can cause significant \" \"technical issues.\"\n                )\n                sdlog.error(\n                    \"To display more information about your repository state, run:\\n\\n\\t\"\n                    \"git status\\n\"\n                )\n                sdlog.error(\n                    \"If you are certain you want to proceed, run:\\n\\n\\t\"\n                    f\"./securedrop-admin --force {cmd_name}\\n\"\n                )\n                sdlog.error(\"To apply the latest updates, run:\\n\\n\\t\" \"./securedrop-admin update\\n\")\n                sdlog.error(\n                    \"If this fails, see the latest upgrade guide on \"\n                    \"https://docs.securedrop.org/ for instructions.\"\n                )\n                sys.exit(1)\n            return func(*args, **kwargs)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "sdconfig",
          "code": "def sdconfig(args: argparse.Namespace) -> int:\n    \"\"\"Configure SD site settings\"\"\"\n    SiteConfig(args).load_and_update_config(validate=False)\n    return 0",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "generate_new_v3_keys",
          "code": "def generate_new_v3_keys() -> Tuple[str, str]:\n    \"\"\"This function generate new keys for Tor v3 onion\n    services and returns them as as tuple.\n\n    :returns: Tuple(public_key, private_key)\n    \"\"\"\n\n    private_key = x25519.X25519PrivateKey.generate()\n    private_bytes = private_key.private_bytes(\n        encoding=serialization.Encoding.Raw,\n        format=serialization.PrivateFormat.Raw,\n        encryption_algorithm=serialization.NoEncryption(),\n    )\n    public_key = private_key.public_key()\n    public_bytes = public_key.public_bytes(\n        encoding=serialization.Encoding.Raw, format=serialization.PublicFormat.Raw\n    )\n\n    # Base32 encode and remove base32 padding characters (`=`)\n    public = base64.b32encode(public_bytes).replace(b\"=\", b\"\").decode(\"utf-8\")\n    private = base64.b32encode(private_bytes).replace(b\"=\", b\"\").decode(\"utf-8\")\n    return public, private",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "find_or_generate_new_torv3_keys",
          "code": "def find_or_generate_new_torv3_keys(args: argparse.Namespace) -> int:\n    \"\"\"\n    This method will either read v3 Tor onion service keys if found or generate\n    a new public/private keypair.\n    \"\"\"\n    secret_key_path = os.path.join(args.ansible_path, \"tor_v3_keys.json\")\n    if os.path.exists(secret_key_path):\n        print(f\"Tor v3 onion service keys already exist in: {secret_key_path}\")\n        return 0\n    # No old keys, generate and store them first\n    app_journalist_public_key, app_journalist_private_key = generate_new_v3_keys()\n    # For app SSH service\n    app_ssh_public_key, app_ssh_private_key = generate_new_v3_keys()\n    # For mon SSH service\n    mon_ssh_public_key, mon_ssh_private_key = generate_new_v3_keys()\n    tor_v3_service_info = {\n        \"app_journalist_public_key\": app_journalist_public_key,\n        \"app_journalist_private_key\": app_journalist_private_key,\n        \"app_ssh_public_key\": app_ssh_public_key,\n        \"app_ssh_private_key\": app_ssh_private_key,\n        \"mon_ssh_public_key\": mon_ssh_public_key,\n        \"mon_ssh_private_key\": mon_ssh_private_key,\n    }\n    with open(secret_key_path, \"w\") as fobj:\n        json.dump(tor_v3_service_info, fobj, indent=4)\n    print(f\"Tor v3 onion service keys generated and stored in: {secret_key_path}\")\n    return 0",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "install_securedrop",
          "code": "def install_securedrop(args: argparse.Namespace) -> int:\n    \"\"\"Install/Update SecureDrop\"\"\"\n\n    SiteConfig(args).load_and_update_config(prompt=False)\n\n    sdlog.info(\"Now installing SecureDrop on remote servers.\")\n    sdlog.info(\"You will be prompted for the sudo password on the \" \"servers.\")\n    sdlog.info(\"The sudo password is only necessary during initial \" \"installation.\")\n    return subprocess.check_call(\n        ansible_command()\n        + [os.path.join(args.ansible_path, \"securedrop-prod.yml\"), \"--ask-become-pass\"],\n        cwd=args.ansible_path,\n    )",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "verify_install",
          "code": "def verify_install(args: argparse.Namespace) -> int:\n    \"\"\"Run configuration tests against SecureDrop servers\"\"\"\n\n    sdlog.info(\"Running configuration tests: \")\n    testinfra_cmd = [\"./devops/scripts/run_prod_testinfra\"]\n    return subprocess.check_call(testinfra_cmd, cwd=os.getcwd())",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "backup_securedrop",
          "code": "def backup_securedrop(args: argparse.Namespace) -> int:\n    \"\"\"Perform backup of the SecureDrop Application Server.\n    Creates a tarball of submissions and server config, and fetches\n    back to the Admin Workstation. Future `restore` actions can be performed\n    with the backup tarball.\"\"\"\n    sdlog.info(\"Backing up the Sec Application Server\")\n    ansible_cmd = ansible_command() + [os.path.join(args.ansible_path, \"securedrop-backup.yml\")]\n    return subprocess.check_call(ansible_cmd, cwd=args.ansible_path)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "restore_securedrop",
          "code": "def restore_securedrop(args: argparse.Namespace) -> int:\n    \"\"\"Perform restore of the SecureDrop Application Server.\n    Requires a tarball of submissions and server config, created via\n    the `backup` action.\"\"\"\n    sdlog.info(\"Restoring the SecureDrop Application Server from backup\")\n    # Canonicalize filepath to backup tarball, so Ansible sees only the\n    # basename. The files must live in args.ansible_path,\n    # but the securedrop-admin\n    # script will be invoked from the repo root, so preceding dirs are likely.\n    restore_file_basename = os.path.basename(args.restore_file)\n\n    # Would like readable output if there's a problem\n    os.environ[\"ANSIBLE_STDOUT_CALLBACK\"] = \"debug\"\n\n    ansible_cmd = ansible_command() + [\n        os.path.join(args.ansible_path, \"securedrop-restore.yml\"),\n        \"-e\",\n    ]\n\n    ansible_cmd_extras = [\n        f\"restore_file='{restore_file_basename}'\",\n    ]\n\n    if args.restore_skip_tor:\n        ansible_cmd_extras.append(\"restore_skip_tor='True'\")\n\n    if args.restore_manual_transfer:\n        ansible_cmd_extras.append(\"restore_manual_transfer='True'\")\n\n    ansible_cmd.append(\" \".join(ansible_cmd_extras))\n    return subprocess.check_call(ansible_cmd, cwd=args.ansible_path)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "run_tails_config",
          "code": "def run_tails_config(args: argparse.Namespace) -> int:\n    \"\"\"Configure Tails environment post SD install\"\"\"\n    sdlog.info(\"Configuring Tails workstation environment\")\n    sdlog.info(\n        \"You'll be prompted for the temporary Tails admin password,\"\n        \" which was set on Tails login screen\"\n    )\n    ansible_cmd = ansible_command() + [\n        os.path.join(args.ansible_path, \"securedrop-tails.yml\"),\n        \"--ask-become-pass\",\n        # Passing an empty inventory file to override the automatic dynamic\n        # inventory script, which fails if no site vars are configured.\n        \"-i\",\n        \"/dev/null\",\n    ]\n    return subprocess.check_call(ansible_cmd, cwd=args.ansible_path)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "check_for_updates_wrapper",
          "code": "def check_for_updates_wrapper(args: argparse.Namespace) -> int:\n    check_for_updates(args)\n    # Because the command worked properly exit with 0.\n    return 0",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "check_for_updates",
          "code": "def check_for_updates(args: argparse.Namespace) -> Tuple[bool, str]:\n    \"\"\"Check for SecureDrop updates\"\"\"\n    sdlog.info(\"Checking for SecureDrop updates...\")\n\n    # Determine what tag we are likely to be on. Caveat: git describe\n    # may produce very surprising results, because it will locate the most recent\n    # _reachable_ tag. However, in our current branching model, it can be\n    # relied on to determine if we're on the latest tag or not.\n    current_tag = (\n        subprocess.check_output([\"git\", \"describe\"], cwd=args.root).decode(\"utf-8\").rstrip(\"\\n\")\n    )\n\n    # Fetch all branches\n    git_fetch_cmd = [\"git\", \"fetch\", \"--all\"]\n    subprocess.check_call(git_fetch_cmd, cwd=args.root)\n\n    # Get latest tag\n    git_all_tags = [\"git\", \"tag\"]\n    all_tags = (\n        subprocess.check_output(git_all_tags, cwd=args.root)\n        .decode(\"utf-8\")\n        .rstrip(\"\\n\")\n        .split(\"\\n\")\n    )\n\n    # Do not check out any release candidate tags\n    all_prod_tags = [x for x in all_tags if \"rc\" not in x]\n\n    # We want the tags to be sorted based on semver\n    all_prod_tags.sort(key=parse_version)\n\n    latest_tag = all_prod_tags[-1]\n\n    if current_tag != latest_tag:\n        sdlog.info(\"Update needed\")\n        return True, latest_tag\n    sdlog.info(\"All updates applied\")\n    return False, latest_tag",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "get_git_branch",
          "code": "def get_git_branch(args: argparse.Namespace) -> Optional[str]:\n    \"\"\"\n    Returns the starred line of `git branch` output.\n    \"\"\"\n    git_branch_raw = subprocess.check_output([\"git\", \"branch\"], cwd=args.root).decode(\"utf-8\")\n    match = re.search(r\"\\* (.*)\\n\", git_branch_raw)\n    if match is not None and len(match.groups()) > 0:\n        return match.group(1)\n    else:\n        return None",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "get_release_key_from_keyserver",
          "code": "def get_release_key_from_keyserver(\n    args: argparse.Namespace, keyserver: Optional[str] = None, timeout: int = 45\n) -> None:\n    gpg_recv = [\"timeout\", str(timeout), \"gpg\", \"--batch\", \"--no-tty\", \"--recv-key\"]\n    for release_key in RELEASE_KEYS:\n        # We construct the gpg --recv-key command based on optional keyserver arg.\n        if keyserver:\n            get_key_cmd = gpg_recv + [\"--keyserver\", keyserver] + [release_key]\n        else:\n            get_key_cmd = gpg_recv + [release_key]\n\n        subprocess.check_call(get_key_cmd, cwd=args.root)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "update",
          "code": "def update(args: argparse.Namespace) -> int:\n    \"\"\"Verify, and apply latest SecureDrop workstation update\"\"\"\n    sdlog.info(\"Applying SecureDrop updates...\")\n\n    update_status, latest_tag = check_for_updates(args)\n\n    if not update_status:\n        # Exit if we're up to date\n        return 0\n\n    sdlog.info(\"Verifying signature on latest update...\")\n\n    # Retrieve key from openpgp.org keyserver\n    get_release_key_from_keyserver(args, keyserver=DEFAULT_KEYSERVER)\n\n    git_verify_tag_cmd = [\"git\", \"tag\", \"-v\", latest_tag]\n    try:\n        sig_result = subprocess.check_output(\n            git_verify_tag_cmd, stderr=subprocess.STDOUT, cwd=args.root\n        ).decode(\"utf-8\")\n\n        good_sig_text = [\n            'Good signature from \"SecureDrop Release Signing '\n            + 'Key <securedrop-release-key-2021@freedom.press>\"',\n        ]\n        bad_sig_text = \"BAD signature\"\n        gpg_lines = sig_result.split(\"\\n\")\n\n        # Check if any strings in good_sig_text match against gpg_lines[]\n        good_sig_matches = [s for s in gpg_lines if any(xs in s for xs in good_sig_text)]\n\n        # To ensure that an adversary cannot name a malicious key good_sig_text\n        # we check that bad_sig_text does not appear, that the release key\n        # appears on the second line of the output, and that there is a single\n        # match from good_sig_text[]\n        if (\n            any(key in gpg_lines[1] for key in RELEASE_KEYS)\n            and len(good_sig_matches) == 1\n            and bad_sig_text not in sig_result\n        ):\n            # Check for duplicate branch name\n            cmd = [\"git\", \"show-ref\", \"--heads\", \"--verify\", f\"refs/heads/{latest_tag}\"]\n            try:\n                subprocess.check_output(cmd, stderr=subprocess.STDOUT, cwd=args.root)\n                sdlog.error(\"Update failed: Branch name collision detected\")\n                return 1\n            except subprocess.CalledProcessError as e:\n                if \"not a valid ref\" in e.output.decode(\"utf-8\"):\n                    sdlog.info(\"Signature verification successful.\")\n                else:\n                    sdlog.error(\"Update failed: Git command error\")\n                    return 1\n        else:\n            sdlog.error(\"Update failed: Invalid signature format\")\n            return 1\n\n    except subprocess.CalledProcessError:\n        sdlog.error(\"Update failed: Missing or invalid signature\")\n        return 1\n\n    # Only if the proper signature verifies do we check out the latest\n    git_checkout_cmd = [\"git\", \"checkout\", latest_tag]\n    subprocess.check_call(git_checkout_cmd, cwd=args.root)\n\n    sdlog.info(f\"Updated to SecureDrop {latest_tag}.\")\n    return 0",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "get_logs",
          "code": "def get_logs(args: argparse.Namespace) -> int:\n    \"\"\"Get logs for forensics and debugging purposes\"\"\"\n    sdlog.info(\"Gathering logs for forensics and debugging\")\n    ansible_cmd = ansible_command() + [\n        os.path.join(args.ansible_path, \"securedrop-logs.yml\"),\n    ]\n\n    subprocess.check_call(ansible_cmd, cwd=args.ansible_path)\n    sdlog.info(\n        \"Please send the encrypted logs to securedrop@freedom.press or \"\n        \"upload them to the SecureDrop support portal: \" + SUPPORT_URL\n    )\n    return 0",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "noble_migration",
          "code": "def noble_migration(args: argparse.Namespace) -> int:\n    \"\"\"Upgrade to Ubuntu Noble\"\"\"\n    sdlog.info(\"Beginning the upgrade to Ubuntu Noble\")\n    ansible_cmd = ansible_command() + [\n        os.path.join(args.ansible_path, \"securedrop-noble-migration.yml\"),\n    ]\n\n    subprocess.check_call(ansible_cmd, cwd=args.ansible_path)\n    sdlog.info(\"Upgrade to Ubuntu Noble complete!\")\n    return 0",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "set_default_paths",
          "code": "def set_default_paths(args: argparse.Namespace) -> argparse.Namespace:\n    if not args.ansible_path:\n        args.ansible_path = args.root + \"/install_files/ansible-base\"\n    args.ansible_path = os.path.realpath(args.ansible_path)\n    if not args.site_config:\n        args.site_config = args.ansible_path + \"/group_vars/all/site-specific\"\n    args.site_config = os.path.realpath(args.site_config)\n    if not args.app_path:\n        args.app_path = args.root + \"/securedrop\"\n    args.app_path = os.path.realpath(args.app_path)\n    return args",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "reset_admin_access",
          "code": "def reset_admin_access(args: argparse.Namespace) -> int:\n    \"\"\"Resets SSH access to the SecureDrop servers, locking it to\n    this Admin Workstation.\"\"\"\n    sdlog.info(\"Resetting SSH access to the SecureDrop servers\")\n    ansible_cmd = ansible_command() + [\n        os.path.join(args.ansible_path, \"securedrop-reset-ssh-key.yml\"),\n    ]\n    return subprocess.check_call(ansible_cmd, cwd=args.ansible_path)",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "parse_argv",
          "code": "def parse_argv(argv: List[str]) -> argparse.Namespace:\n    class ArgParseFormatterCombo(\n        argparse.ArgumentDefaultsHelpFormatter, argparse.RawTextHelpFormatter\n    ):\n        \"\"\"Needed to combine formatting classes for help output\"\"\"\n\n    parser = argparse.ArgumentParser(description=__doc__, formatter_class=ArgParseFormatterCombo)\n    parser.add_argument(\n        \"-v\", action=\"store_true\", default=False, help=\"Increase verbosity on output\"\n    )\n    parser.add_argument(\n        \"-d\",\n        action=\"store_true\",\n        default=False,\n        help=\"Developer mode. Not to be used in production.\",\n    )\n    parser.add_argument(\n        \"--force\",\n        action=\"store_true\",\n        required=False,\n        help=\"force command execution without update check\",\n    )\n    parser.add_argument(\n        \"--root\", required=True, help=\"path to the root of the SecureDrop repository\"\n    )\n    parser.add_argument(\"--site-config\", help=\"path to the YAML site configuration file\")\n    parser.add_argument(\"--ansible-path\", help=\"path to the Ansible root\")\n    parser.add_argument(\"--app-path\", help=\"path to the SecureDrop application root\")\n    subparsers = parser.add_subparsers()\n\n    parse_sdconfig = subparsers.add_parser(\"sdconfig\", help=sdconfig.__doc__)\n    parse_sdconfig.set_defaults(func=sdconfig)\n\n    parse_install = subparsers.add_parser(\"install\", help=install_securedrop.__doc__)\n    parse_install.set_defaults(func=install_securedrop)\n\n    parse_tailsconfig = subparsers.add_parser(\"tailsconfig\", help=run_tails_config.__doc__)\n    parse_tailsconfig.set_defaults(func=run_tails_config)\n\n    parse_generate_tor_keys = subparsers.add_parser(\n        \"generate_v3_keys\", help=find_or_generate_new_torv3_keys.__doc__\n    )\n    parse_generate_tor_keys.set_defaults(func=find_or_generate_new_torv3_keys)\n\n    parse_backup = subparsers.add_parser(\"backup\", help=backup_securedrop.__doc__)\n    parse_backup.set_defaults(func=backup_securedrop)\n\n    parse_restore = subparsers.add_parser(\"restore\", help=restore_securedrop.__doc__)\n    parse_restore.set_defaults(func=restore_securedrop)\n    parse_restore.add_argument(\"restore_file\")\n    parse_restore.add_argument(\n        \"--preserve-tor-config\",\n        default=False,\n        action=\"store_true\",\n        dest=\"restore_skip_tor\",\n        help=\"Preserve the server's current Tor config\",\n    )\n\n    parse_restore.add_argument(\n        \"--no-transfer\",\n        default=False,\n        action=\"store_true\",\n        dest=\"restore_manual_transfer\",\n        help=\"Restore using a backup file already present on the server\",\n    )\n\n    parse_update = subparsers.add_parser(\"update\", help=update.__doc__)\n    parse_update.set_defaults(func=update)\n\n    parse_check_updates = subparsers.add_parser(\"check_for_updates\", help=check_for_updates.__doc__)\n    parse_check_updates.set_defaults(func=check_for_updates_wrapper)\n\n    parse_logs = subparsers.add_parser(\"logs\", help=get_logs.__doc__)\n    parse_logs.set_defaults(func=get_logs)\n\n    parse_noble_migration = subparsers.add_parser(\"noble_migration\", help=noble_migration.__doc__)\n    parse_noble_migration.set_defaults(func=noble_migration)\n\n    parse_reset_ssh = subparsers.add_parser(\"reset_admin_access\", help=reset_admin_access.__doc__)\n    parse_reset_ssh.set_defaults(func=reset_admin_access)\n\n    parse_verify = subparsers.add_parser(\"verify\", help=verify_install.__doc__)\n    parse_verify.set_defaults(func=verify_install)\n\n    args = parser.parse_args(argv)\n    if getattr(args, \"func\", None) is None:\n        print(\"Please specify an operation.\\n\")\n        parser.print_help()\n        sys.exit(1)\n    return set_default_paths(args)",
          "file": "__init__.py"
        },
        {
          "type": "class",
          "name": "ArgParseFormatterCombo",
          "code": "class ArgParseFormatterCombo(\n        argparse.ArgumentDefaultsHelpFormatter, argparse.RawTextHelpFormatter\n    ):\n        \"\"\"Needed to combine formatting classes for help output\"\"\"",
          "file": "__init__.py"
        },
        {
          "type": "function",
          "name": "main",
          "code": "def main(argv: List[str]) -> None:\n    args = parse_argv(argv)\n    setup_logger(args.v)\n    if args.v:\n        return_code = args.func(args)\n        if return_code != 0:\n            sys.exit(EXIT_SUBPROCESS_ERROR)\n    else:\n        try:\n            return_code = args.func(args)\n        except KeyboardInterrupt:\n            print(\"Process was interrupted.\")\n            sys.exit(EXIT_INTERRUPT)\n        except subprocess.CalledProcessError as e:\n            print(f\"ERROR (run with -v for more): {e}\", file=sys.stderr)\n            sys.exit(EXIT_SUBPROCESS_ERROR)\n        except Exception as e:\n            raise SystemExit(f\"ERROR (run with -v for more): {e}\")\n    if return_code == 0:\n        sys.exit(EXIT_SUCCESS)\n    else:\n        sys.exit(EXIT_SUBPROCESS_ERROR)",
          "file": "__init__.py"
        }
      ]
    },
    "tests": {
      "test_securedrop-admin-setup.py": [
        {
          "type": "class",
          "name": "TestSecureDropAdmin",
          "code": "class TestSecureDropAdmin:\n    def test_verbose(self, capsys):\n        bootstrap.setup_logger(verbose=True)\n        bootstrap.sdlog.debug(\"VISIBLE\")\n        out, err = capsys.readouterr()\n        assert \"VISIBLE\" in out\n\n    def test_not_verbose(self, capsys):\n        bootstrap.setup_logger(verbose=False)\n        bootstrap.sdlog.debug(\"HIDDEN\")\n        bootstrap.sdlog.info(\"VISIBLE\")\n        out, err = capsys.readouterr()\n        assert \"HIDDEN\" not in out\n        assert \"VISIBLE\" in out\n\n    def test_run_command(self):\n        for output_line in bootstrap.run_command([\"/bin/echo\", \"something\"]):\n            assert output_line.strip() == b\"something\"\n\n        lines = []\n        with pytest.raises(subprocess.CalledProcessError):  # noqa: PT012\n            for output_line in bootstrap.run_command(\n                [\"sh\", \"-c\", \"echo in stdout ; echo in stderr >&2 ; false\"]\n            ):\n                lines.append(output_line.strip())\n        assert lines[0] == b\"in stdout\"\n        assert lines[1] == b\"in stderr\"\n\n    def test_install_pip_dependencies_up_to_date(self, caplog):\n        args = argparse.Namespace()\n        with mock.patch.object(subprocess, \"check_output\", return_value=b\"up to date\"):\n            bootstrap.install_pip_dependencies(args)\n        assert \"securedrop-admin are up-to-date\" in caplog.text\n\n    def test_install_pip_dependencies_upgraded(self, caplog):\n        args = argparse.Namespace()\n        with mock.patch.object(subprocess, \"check_output\", return_value=b\"Successfully installed\"):\n            bootstrap.install_pip_dependencies(args)\n        assert \"securedrop-admin upgraded\" in caplog.text\n\n    def test_install_pip_dependencies_fail(self, caplog):\n        args = argparse.Namespace()\n        with mock.patch.object(\n            subprocess,\n            \"check_output\",\n            side_effect=subprocess.CalledProcessError(returncode=2, cmd=\"\", output=b\"failed\"),\n        ), pytest.raises(subprocess.CalledProcessError):\n            bootstrap.install_pip_dependencies(args)\n        assert \"Failed to install\" in caplog.text\n\n    def test_python3_bullseye_venv_deleted_in_bookworm(self, tmpdir, caplog):\n        venv_path = str(tmpdir)\n        python_lib_path = os.path.join(str(tmpdir), \"lib/python3.9\")\n        os.makedirs(python_lib_path)\n        with mock.patch(\"bootstrap.is_tails\", return_value=True):\n            with mock.patch(\"builtins.open\", mock.mock_open(read_data='VERSION=\"6.0\"')):\n                bootstrap.clean_up_old_tails_venv(venv_path)\n                assert \"Tails 5 virtualenv detected.\" in caplog.text\n                assert \"Tails 5 virtualenv deleted.\" in caplog.text\n                assert not os.path.exists(venv_path)\n\n    def test_python3_bookworm_venv_not_deleted_in_bookworm(self, tmpdir, caplog):\n        venv_path = str(tmpdir)\n        python_lib_path = os.path.join(venv_path, \"lib/python3.11\")\n        os.makedirs(python_lib_path)\n        with mock.patch(\"bootstrap.is_tails\", return_value=True):\n            with mock.patch(\"subprocess.check_output\", return_value=\"bookworm\"):\n                bootstrap.clean_up_old_tails_venv(venv_path)\n                assert \"Tails 5 virtualenv detected\" not in caplog.text\n                assert os.path.exists(venv_path)\n\n    def test_python3_buster_venv_not_deleted_in_buster(self, tmpdir, caplog):\n        venv_path = str(tmpdir)\n        python_lib_path = os.path.join(venv_path, \"lib/python3.9\")\n        os.makedirs(python_lib_path)\n        with mock.patch(\"bootstrap.is_tails\", return_value=True):\n            with mock.patch(\"subprocess.check_output\", return_value=\"bullseye\"):\n                bootstrap.clean_up_old_tails_venv(venv_path)\n                assert os.path.exists(venv_path)\n\n    def test_venv_cleanup_subprocess_exception(self, tmpdir, caplog):\n        venv_path = str(tmpdir)\n        python_lib_path = os.path.join(venv_path, \"lib/python3.9\")\n        os.makedirs(python_lib_path)\n        with mock.patch(\"bootstrap.is_tails\", return_value=True), mock.patch(\n            \"subprocess.check_output\", side_effect=subprocess.CalledProcessError(1, \":o\")\n        ):\n            bootstrap.clean_up_old_tails_venv(venv_path)\n            assert os.path.exists(venv_path)\n\n    def test_envsetup_cleanup(self, tmpdir, caplog):\n        venv = os.path.join(str(tmpdir), \"empty_dir\")\n        args = \"\"\n        with pytest.raises(subprocess.CalledProcessError), mock.patch(\n            \"subprocess.check_output\", side_effect=self.side_effect_venv_bootstrap(venv)\n        ):\n            bootstrap.envsetup(args, venv)\n        assert not os.path.exists(venv)\n        assert \"Cleaning up virtualenv\" in caplog.text\n\n    def side_effect_venv_bootstrap(self, venv_path):\n        # emulate the venv being created, and raise exception to simulate\n        # failure in virtualenv creation\n        def func(*args, **kwargs):\n            os.makedirs(venv_path)\n            raise subprocess.CalledProcessError(1, \":o\")\n\n        return func",
          "file": "test_securedrop-admin-setup.py"
        },
        {
          "type": "function",
          "name": "test_verbose",
          "code": "def test_verbose(self, capsys):\n        bootstrap.setup_logger(verbose=True)\n        bootstrap.sdlog.debug(\"VISIBLE\")\n        out, err = capsys.readouterr()\n        assert \"VISIBLE\" in out",
          "file": "test_securedrop-admin-setup.py"
        },
        {
          "type": "function",
          "name": "test_not_verbose",
          "code": "def test_not_verbose(self, capsys):\n        bootstrap.setup_logger(verbose=False)\n        bootstrap.sdlog.debug(\"HIDDEN\")\n        bootstrap.sdlog.info(\"VISIBLE\")\n        out, err = capsys.readouterr()\n        assert \"HIDDEN\" not in out\n        assert \"VISIBLE\" in out",
          "file": "test_securedrop-admin-setup.py"
        },
        {
          "type": "function",
          "name": "test_run_command",
          "code": "def test_run_command(self):\n        for output_line in bootstrap.run_command([\"/bin/echo\", \"something\"]):\n            assert output_line.strip() == b\"something\"\n\n        lines = []\n        with pytest.raises(subprocess.CalledProcessError):  # noqa: PT012\n            for output_line in bootstrap.run_command(\n                [\"sh\", \"-c\", \"echo in stdout ; echo in stderr >&2 ; false\"]\n            ):\n                lines.append(output_line.strip())\n        assert lines[0] == b\"in stdout\"\n        assert lines[1] == b\"in stderr\"",
          "file": "test_securedrop-admin-setup.py"
        },
        {
          "type": "function",
          "name": "test_install_pip_dependencies_up_to_date",
          "code": "def test_install_pip_dependencies_up_to_date(self, caplog):\n        args = argparse.Namespace()\n        with mock.patch.object(subprocess, \"check_output\", return_value=b\"up to date\"):\n            bootstrap.install_pip_dependencies(args)\n        assert \"securedrop-admin are up-to-date\" in caplog.text",
          "file": "test_securedrop-admin-setup.py"
        },
        {
          "type": "function",
          "name": "test_install_pip_dependencies_upgraded",
          "code": "def test_install_pip_dependencies_upgraded(self, caplog):\n        args = argparse.Namespace()\n        with mock.patch.object(subprocess, \"check_output\", return_value=b\"Successfully installed\"):\n            bootstrap.install_pip_dependencies(args)\n        assert \"securedrop-admin upgraded\" in caplog.text",
          "file": "test_securedrop-admin-setup.py"
        },
        {
          "type": "function",
          "name": "test_install_pip_dependencies_fail",
          "code": "def test_install_pip_dependencies_fail(self, caplog):\n        args = argparse.Namespace()\n        with mock.patch.object(\n            subprocess,\n            \"check_output\",\n            side_effect=subprocess.CalledProcessError(returncode=2, cmd=\"\", output=b\"failed\"),\n        ), pytest.raises(subprocess.CalledProcessError):\n            bootstrap.install_pip_dependencies(args)\n        assert \"Failed to install\" in caplog.text",
          "file": "test_securedrop-admin-setup.py"
        },
        {
          "type": "function",
          "name": "test_python3_bullseye_venv_deleted_in_bookworm",
          "code": "def test_python3_bullseye_venv_deleted_in_bookworm(self, tmpdir, caplog):\n        venv_path = str(tmpdir)\n        python_lib_path = os.path.join(str(tmpdir), \"lib/python3.9\")\n        os.makedirs(python_lib_path)\n        with mock.patch(\"bootstrap.is_tails\", return_value=True):\n            with mock.patch(\"builtins.open\", mock.mock_open(read_data='VERSION=\"6.0\"')):\n                bootstrap.clean_up_old_tails_venv(venv_path)\n                assert \"Tails 5 virtualenv detected.\" in caplog.text\n                assert \"Tails 5 virtualenv deleted.\" in caplog.text\n                assert not os.path.exists(venv_path)",
          "file": "test_securedrop-admin-setup.py"
        },
        {
          "type": "function",
          "name": "test_python3_bookworm_venv_not_deleted_in_bookworm",
          "code": "def test_python3_bookworm_venv_not_deleted_in_bookworm(self, tmpdir, caplog):\n        venv_path = str(tmpdir)\n        python_lib_path = os.path.join(venv_path, \"lib/python3.11\")\n        os.makedirs(python_lib_path)\n        with mock.patch(\"bootstrap.is_tails\", return_value=True):\n            with mock.patch(\"subprocess.check_output\", return_value=\"bookworm\"):\n                bootstrap.clean_up_old_tails_venv(venv_path)\n                assert \"Tails 5 virtualenv detected\" not in caplog.text\n                assert os.path.exists(venv_path)",
          "file": "test_securedrop-admin-setup.py"
        },
        {
          "type": "function",
          "name": "test_python3_buster_venv_not_deleted_in_buster",
          "code": "def test_python3_buster_venv_not_deleted_in_buster(self, tmpdir, caplog):\n        venv_path = str(tmpdir)\n        python_lib_path = os.path.join(venv_path, \"lib/python3.9\")\n        os.makedirs(python_lib_path)\n        with mock.patch(\"bootstrap.is_tails\", return_value=True):\n            with mock.patch(\"subprocess.check_output\", return_value=\"bullseye\"):\n                bootstrap.clean_up_old_tails_venv(venv_path)\n                assert os.path.exists(venv_path)",
          "file": "test_securedrop-admin-setup.py"
        },
        {
          "type": "function",
          "name": "test_venv_cleanup_subprocess_exception",
          "code": "def test_venv_cleanup_subprocess_exception(self, tmpdir, caplog):\n        venv_path = str(tmpdir)\n        python_lib_path = os.path.join(venv_path, \"lib/python3.9\")\n        os.makedirs(python_lib_path)\n        with mock.patch(\"bootstrap.is_tails\", return_value=True), mock.patch(\n            \"subprocess.check_output\", side_effect=subprocess.CalledProcessError(1, \":o\")\n        ):\n            bootstrap.clean_up_old_tails_venv(venv_path)\n            assert os.path.exists(venv_path)",
          "file": "test_securedrop-admin-setup.py"
        },
        {
          "type": "function",
          "name": "test_envsetup_cleanup",
          "code": "def test_envsetup_cleanup(self, tmpdir, caplog):\n        venv = os.path.join(str(tmpdir), \"empty_dir\")\n        args = \"\"\n        with pytest.raises(subprocess.CalledProcessError), mock.patch(\n            \"subprocess.check_output\", side_effect=self.side_effect_venv_bootstrap(venv)\n        ):\n            bootstrap.envsetup(args, venv)\n        assert not os.path.exists(venv)\n        assert \"Cleaning up virtualenv\" in caplog.text",
          "file": "test_securedrop-admin-setup.py"
        },
        {
          "type": "function",
          "name": "side_effect_venv_bootstrap",
          "code": "def side_effect_venv_bootstrap(self, venv_path):\n        # emulate the venv being created, and raise exception to simulate\n        # failure in virtualenv creation\n        def func(*args, **kwargs):\n            os.makedirs(venv_path)\n            raise subprocess.CalledProcessError(1, \":o\")\n\n        return func",
          "file": "test_securedrop-admin-setup.py"
        },
        {
          "type": "function",
          "name": "func",
          "code": "def func(*args, **kwargs):\n            os.makedirs(venv_path)\n            raise subprocess.CalledProcessError(1, \":o\")",
          "file": "test_securedrop-admin-setup.py"
        }
      ],
      "test_integration.py": [
        {
          "type": "function",
          "name": "setup_function",
          "code": "def setup_function(function):\n    global SD_DIR\n    SD_DIR = tempfile.mkdtemp()\n    ANSIBLE_BASE = f\"{SD_DIR}/install_files/ansible-base\"\n\n    for name in [\"roles\", \"tasks\"]:\n        shutil.copytree(\n            os.path.join(CURRENT_DIR, \"../../install_files/ansible-base\", name),\n            os.path.join(ANSIBLE_BASE, name),\n        )\n\n    for name in [\"ansible.cfg\", \"securedrop-prod.yml\"]:\n        shutil.copy(\n            os.path.join(CURRENT_DIR, \"../../install_files/ansible-base\", name), ANSIBLE_BASE\n        )\n\n    cmd = f\"mkdir -p {ANSIBLE_BASE}/group_vars/all\".split()\n    subprocess.check_call(cmd)\n    for name in [\"sd_admin_test.pub\", \"ca.crt\", \"sd.crt\", \"key.asc\"]:\n        subprocess.check_call(f\"cp -r {CURRENT_DIR}/files/{name} {ANSIBLE_BASE}\".split())\n    for name in [\"de_DE\", \"es_ES\", \"fr_FR\", \"pt_BR\"]:\n        dircmd = f\"mkdir -p {SD_DIR}/securedrop/translations/{name}\"\n        subprocess.check_call(dircmd.split())\n    subprocess.check_call(\n        f\"cp {CURRENT_DIR}/files/securedrop/i18n.json {SD_DIR}/securedrop\".split()\n    )",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "teardown_function",
          "code": "def teardown_function(function):\n    subprocess.check_call(f\"rm -rf {SD_DIR}\".split())",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_username_prompt",
          "code": "def verify_username_prompt(child):\n    child.expect(b\"Username for SSH access to the servers:\")",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_reboot_prompt",
          "code": "def verify_reboot_prompt(child):\n    child.expect(rb\"Daily reboot time of the server \\(24\\-hour clock\\):\", timeout=2)\n    assert ANSI_ESCAPE.sub(\"\", child.buffer.decode(\"utf-8\")).strip() == \"4\"",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_ipv4_appserver_prompt",
          "code": "def verify_ipv4_appserver_prompt(child):\n    child.expect(rb\"Local IPv4 address for the Application Server\\:\", timeout=2)\n    # Expected default\n    assert ANSI_ESCAPE.sub(\"\", child.buffer.decode(\"utf-8\")).strip() == \"10.20.2.2\"",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_ipv4_monserver_prompt",
          "code": "def verify_ipv4_monserver_prompt(child):\n    child.expect(rb\"Local IPv4 address for the Monitor Server\\:\", timeout=2)\n    # Expected default\n    assert ANSI_ESCAPE.sub(\"\", child.buffer.decode(\"utf-8\")).strip() == \"10.20.3.2\"",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_hostname_app_prompt",
          "code": "def verify_hostname_app_prompt(child):\n    child.expect(rb\"Hostname for Application Server\\:\", timeout=2)\n    assert ANSI_ESCAPE.sub(\"\", child.buffer.decode(\"utf-8\")).strip() == \"app\"",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_hostname_mon_prompt",
          "code": "def verify_hostname_mon_prompt(child):\n    child.expect(rb\"Hostname for Monitor Server\\:\", timeout=2)\n    assert ANSI_ESCAPE.sub(\"\", child.buffer.decode(\"utf-8\")).strip() == \"mon\"",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_dns_prompt",
          "code": "def verify_dns_prompt(child):\n    child.expect(rb\"DNS server\\(s\\):\", timeout=2)\n    assert ANSI_ESCAPE.sub(\"\", child.buffer.decode(\"utf-8\")).strip() == \"8.8.8.8 8.8.4.4\"",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_app_gpg_key_prompt",
          "code": "def verify_app_gpg_key_prompt(child):\n    child.expect(\n        rb\"Local filepath to public key for SecureDrop Application GPG public key\\:\", timeout=2\n    )",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_tor_pow_prompt",
          "code": "def verify_tor_pow_prompt(child):\n    # We don't need child.expect()'s regex matching, but the prompt is too long\n    # to match on the whole thing.\n    child.expect_exact(\"Enable Tor's proof-of-work defense\", timeout=2)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_https_prompt",
          "code": "def verify_https_prompt(child):\n    # We don't need child.expect()'s regex matching.\n    child.expect_exact(\n        \"Enable HTTPS for the Source Interface (requires EV certificate)?:\", timeout=2\n    )",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_https_cert_prompt",
          "code": "def verify_https_cert_prompt(child):\n    child.expect(rb\"Local filepath to HTTPS certificate\\:\", timeout=2)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_https_cert_key_prompt",
          "code": "def verify_https_cert_key_prompt(child):\n    child.expect(rb\"Local filepath to HTTPS certificate key\\:\", timeout=2)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_https_cert_chain_file_prompt",
          "code": "def verify_https_cert_chain_file_prompt(child):\n    child.expect(rb\"Local filepath to HTTPS certificate chain file\\:\", timeout=2)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_app_gpg_fingerprint_prompt",
          "code": "def verify_app_gpg_fingerprint_prompt(child):\n    child.expect(rb\"Full fingerprint for the SecureDrop Application GPG Key\\:\", timeout=2)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_ossec_gpg_key_prompt",
          "code": "def verify_ossec_gpg_key_prompt(child):\n    child.expect(rb\"Local filepath to OSSEC alerts GPG public key\\:\", timeout=2)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_ossec_gpg_fingerprint_prompt",
          "code": "def verify_ossec_gpg_fingerprint_prompt(child):\n    child.expect(rb\"Full fingerprint for the OSSEC alerts GPG public key\\:\", timeout=2)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_admin_email_prompt",
          "code": "def verify_admin_email_prompt(child):\n    child.expect(rb\"Admin email address for receiving OSSEC alerts\\:\", timeout=2)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_journalist_gpg_key_prompt",
          "code": "def verify_journalist_gpg_key_prompt(child):\n    child.expect(rb\"Local filepath to journalist alerts GPG public key \\(optional\\)\\:\", timeout=2)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_journalist_fingerprint_prompt",
          "code": "def verify_journalist_fingerprint_prompt(child):\n    child.expect(\n        rb\"Full fingerprint for the journalist alerts GPG public key \\(optional\\)\\:\", timeout=2\n    )",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_journalist_email_prompt",
          "code": "def verify_journalist_email_prompt(child):\n    child.expect(rb\"Email address for receiving journalist alerts \\(optional\\)\\:\", timeout=2)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_smtp_relay_prompt",
          "code": "def verify_smtp_relay_prompt(child):\n    child.expect(rb\"SMTP relay for sending OSSEC alerts\\:\", timeout=2)\n    # Expected default\n    assert ANSI_ESCAPE.sub(\"\", child.buffer.decode(\"utf-8\")).strip() == \"smtp.gmail.com\"",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_smtp_port_prompt",
          "code": "def verify_smtp_port_prompt(child):\n    child.expect(rb\"SMTP port for sending OSSEC alerts\\:\", timeout=2)\n    assert ANSI_ESCAPE.sub(\"\", child.buffer.decode(\"utf-8\")).strip() == \"587\"",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_sasl_domain_prompt",
          "code": "def verify_sasl_domain_prompt(child):\n    child.expect(rb\"SASL domain for sending OSSEC alerts\\:\", timeout=2)\n    # Expected default\n    assert ANSI_ESCAPE.sub(\"\", child.buffer.decode(\"utf-8\")).strip() == \"gmail.com\"",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_sasl_username_prompt",
          "code": "def verify_sasl_username_prompt(child):\n    child.expect(rb\"SASL username for sending OSSEC alerts\\:\", timeout=2)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_sasl_password_prompt",
          "code": "def verify_sasl_password_prompt(child):\n    child.expect(rb\"SASL password for sending OSSEC alerts\\:\", timeout=2)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_ssh_over_lan_prompt",
          "code": "def verify_ssh_over_lan_prompt(child):\n    child.expect(rb\"will be available over LAN only\\:\", timeout=2)\n    assert ANSI_ESCAPE.sub(\"\", child.buffer.decode(\"utf-8\")).strip() == \"yes\"",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_locales_prompt",
          "code": "def verify_locales_prompt(child):\n    child.expect(rb\"Space separated list of additional locales to support\")",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "verify_install_has_valid_config",
          "code": "def verify_install_has_valid_config():\n    \"\"\"\n    Checks that securedrop-admin install validates the configuration.\n    \"\"\"\n    cmd = os.path.join(os.path.dirname(CURRENT_DIR), \"securedrop_admin/__init__.py\")\n    child = pexpect.spawn(f\"python {cmd} --force --root {SD_DIR} install\")\n    child.expect(b\"SUDO password:\", timeout=5)\n    child.close()",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "test_install_with_no_config",
          "code": "def test_install_with_no_config():\n    \"\"\"\n    Checks that securedrop-admin install complains about a missing config file.\n    \"\"\"\n    cmd = os.path.join(os.path.dirname(CURRENT_DIR), \"securedrop_admin/__init__.py\")\n    child = pexpect.spawn(f\"python {cmd} --force --root {SD_DIR} install\")\n    child.expect(b'ERROR: Please run \"securedrop-admin sdconfig\" first.', timeout=5)\n    child.expect(pexpect.EOF, timeout=5)\n    child.close()\n    assert child.exitstatus == 1\n    assert child.signalstatus is None",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "test_sdconfig_on_first_run",
          "code": "def test_sdconfig_on_first_run():\n    cmd = os.path.join(os.path.dirname(CURRENT_DIR), \"securedrop_admin/__init__.py\")\n    child = pexpect.spawn(f\"python {cmd} --force --root {SD_DIR} sdconfig\")\n    verify_username_prompt(child)\n    child.sendline(\"\")\n    verify_reboot_prompt(child)\n    child.sendline(\"\\b5\")  # backspace and put 5\n    verify_ipv4_appserver_prompt(child)\n    child.sendline(\"\")\n    verify_ipv4_monserver_prompt(child)\n    child.sendline(\"\")\n    verify_hostname_app_prompt(child)\n    child.sendline(\"\")\n    verify_hostname_mon_prompt(child)\n    child.sendline(\"\")\n    verify_dns_prompt(child)\n    child.sendline(\"\")\n    verify_app_gpg_key_prompt(child)\n    child.sendline(\"\\b\" * 14 + \"sd_admin_test.pub\")\n    verify_tor_pow_prompt(child)\n    # Default answer is yes\n    child.sendline(\"\")\n    verify_https_prompt(child)\n    # Default answer is no\n    child.sendline(\"\")\n    verify_app_gpg_fingerprint_prompt(child)\n    child.sendline(\"1F544B31C845D698EB31F2FF364F1162D32E7E58\")\n    verify_ossec_gpg_key_prompt(child)\n    child.sendline(\"\\b\" * 9 + \"sd_admin_test.pub\")\n    verify_ossec_gpg_fingerprint_prompt(child)\n    child.sendline(\"1F544B31C845D698EB31F2FF364F1162D32E7E58\")\n    verify_admin_email_prompt(child)\n    child.sendline(\"test@gmail.com\")\n    verify_journalist_gpg_key_prompt(child)\n    child.sendline(\"\")\n    verify_smtp_relay_prompt(child)\n    child.sendline(\"\")\n    verify_smtp_port_prompt(child)\n    child.sendline(\"\")\n    verify_sasl_domain_prompt(child)\n    child.sendline(\"\")\n    verify_sasl_username_prompt(child)\n    child.sendline(\"testuser\")\n    verify_sasl_password_prompt(child)\n    child.sendline(\"testpassword\")\n    verify_ssh_over_lan_prompt(child)\n    child.sendline(\"\")\n    verify_locales_prompt(child)\n    child.sendline(\"de_DE es_ES\")\n    child.sendline(\"\\b\" * 3 + \"no\")\n    child.sendline(\"\\b\" * 4 + \"yes\")\n\n    child.expect(pexpect.EOF, timeout=10)  # Wait for validation to occur\n    child.close()\n    assert child.exitstatus == 0\n    assert child.signalstatus is None\n\n    with open(\n        os.path.join(SD_DIR, \"install_files/ansible-base/group_vars/all/site-specific\")\n    ) as fobj:\n        data = fobj.read()\n    assert data == OUTPUT1\n\n    verify_install_has_valid_config()",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "test_sdconfig_enable_journalist_alerts",
          "code": "def test_sdconfig_enable_journalist_alerts():\n    cmd = os.path.join(os.path.dirname(CURRENT_DIR), \"securedrop_admin/__init__.py\")\n    child = pexpect.spawn(f\"python {cmd} --force --root {SD_DIR} sdconfig\")\n    verify_username_prompt(child)\n    child.sendline(\"\")\n    verify_reboot_prompt(child)\n    child.sendline(\"\\b5\")  # backspace and put 5\n    verify_ipv4_appserver_prompt(child)\n    child.sendline(\"\")\n    verify_ipv4_monserver_prompt(child)\n    child.sendline(\"\")\n    verify_hostname_app_prompt(child)\n    child.sendline(\"\")\n    verify_hostname_mon_prompt(child)\n    child.sendline(\"\")\n    verify_dns_prompt(child)\n    child.sendline(\"\")\n    verify_app_gpg_key_prompt(child)\n    child.sendline(\"\\b\" * 14 + \"sd_admin_test.pub\")\n    verify_tor_pow_prompt(child)\n    # Default answer is yes\n    child.sendline(\"\")\n    verify_https_prompt(child)\n    child.sendline(\"\")\n    # Default answer is no\n    child.sendline(\"\")\n    verify_app_gpg_fingerprint_prompt(child)\n    child.sendline(\"1F544B31C845D698EB31F2FF364F1162D32E7E58\")\n    verify_ossec_gpg_key_prompt(child)\n    child.sendline(\"\\b\" * 9 + \"sd_admin_test.pub\")\n    verify_ossec_gpg_fingerprint_prompt(child)\n    child.sendline(\"1F544B31C845D698EB31F2FF364F1162D32E7E58\")\n    verify_admin_email_prompt(child)\n    child.sendline(\"test@gmail.com\")\n    # We will provide a key for this question\n    verify_journalist_gpg_key_prompt(child)\n    child.sendline(\"sd_admin_test.pub\")\n    verify_journalist_fingerprint_prompt(child)\n    child.sendline(\"1F544B31C845D698EB31F2FF364F1162D32E7E58\")\n    verify_journalist_email_prompt(child)\n    child.sendline(\"test@gmail.com\")\n    verify_smtp_relay_prompt(child)\n    child.sendline(\"\")\n    verify_smtp_port_prompt(child)\n    child.sendline(\"\")\n    verify_sasl_domain_prompt(child)\n    child.sendline(\"\")\n    verify_sasl_username_prompt(child)\n    child.sendline(\"testuser\")\n    verify_sasl_password_prompt(child)\n    child.sendline(\"testpassword\")\n    verify_ssh_over_lan_prompt(child)\n    child.sendline(\"\")\n    verify_locales_prompt(child)\n    child.sendline(\"de_DE es_ES\")\n\n    child.expect(pexpect.EOF, timeout=10)  # Wait for validation to occur\n    child.close()\n    assert child.exitstatus == 0\n    assert child.signalstatus is None\n\n    with open(\n        os.path.join(SD_DIR, \"install_files/ansible-base/group_vars/all/site-specific\")\n    ) as fobj:\n        data = fobj.read()\n    assert data == JOURNALIST_ALERT_OUTPUT\n\n    verify_install_has_valid_config()",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "test_sdconfig_enable_https_disable_pow_on_source_interface",
          "code": "def test_sdconfig_enable_https_disable_pow_on_source_interface():\n    cmd = os.path.join(os.path.dirname(CURRENT_DIR), \"securedrop_admin/__init__.py\")\n    child = pexpect.spawn(f\"python {cmd} --force --root {SD_DIR} sdconfig\")\n    verify_username_prompt(child)\n    child.sendline(\"\")\n    verify_reboot_prompt(child)\n    child.sendline(\"\\b5\")  # backspace and put 5\n    verify_ipv4_appserver_prompt(child)\n    child.sendline(\"\")\n    verify_ipv4_monserver_prompt(child)\n    child.sendline(\"\")\n    verify_hostname_app_prompt(child)\n    child.sendline(\"\")\n    verify_hostname_mon_prompt(child)\n    child.sendline(\"\")\n    verify_dns_prompt(child)\n    child.sendline(\"\")\n    verify_app_gpg_key_prompt(child)\n    child.sendline(\"\\b\" * 14 + \"sd_admin_test.pub\")\n    verify_tor_pow_prompt(child)\n    # Default answer is yes\n    # We will press backspace thrice and type no\n    child.sendline(\"\\b\\b\\bno\")\n    verify_https_prompt(child)\n    # Default answer is no\n    # We will press backspace twice and type yes\n    child.sendline(\"\\b\\byes\")\n    verify_https_cert_prompt(child)\n    child.sendline(\"sd.crt\")\n    verify_https_cert_key_prompt(child)\n    child.sendline(\"key.asc\")\n    verify_https_cert_chain_file_prompt(child)\n    child.sendline(\"ca.crt\")\n    verify_app_gpg_fingerprint_prompt(child)\n    child.sendline(\"1F544B31C845D698EB31F2FF364F1162D32E7E58\")\n    verify_ossec_gpg_key_prompt(child)\n    child.sendline(\"\\b\" * 9 + \"sd_admin_test.pub\")\n    verify_ossec_gpg_fingerprint_prompt(child)\n    child.sendline(\"1F544B31C845D698EB31F2FF364F1162D32E7E58\")\n    verify_admin_email_prompt(child)\n    child.sendline(\"test@gmail.com\")\n    # We will provide a key for this question\n    verify_journalist_gpg_key_prompt(child)\n    child.sendline(\"sd_admin_test.pub\")\n    verify_journalist_fingerprint_prompt(child)\n    child.sendline(\"1F544B31C845D698EB31F2FF364F1162D32E7E58\")\n    verify_journalist_email_prompt(child)\n    child.sendline(\"test@gmail.com\")\n    verify_smtp_relay_prompt(child)\n    child.sendline(\"\")\n    verify_smtp_port_prompt(child)\n    child.sendline(\"\")\n    verify_sasl_domain_prompt(child)\n    child.sendline(\"\")\n    verify_sasl_username_prompt(child)\n    child.sendline(\"testuser\")\n    verify_sasl_password_prompt(child)\n    child.sendline(\"testpassword\")\n    verify_ssh_over_lan_prompt(child)\n    child.sendline(\"\")\n    verify_locales_prompt(child)\n    child.sendline(\"de_DE es_ES\")\n\n    child.expect(pexpect.EOF, timeout=10)  # Wait for validation to occur\n    child.close()\n    assert child.exitstatus == 0\n    assert child.signalstatus is None\n\n    with open(\n        os.path.join(SD_DIR, \"install_files/ansible-base/group_vars/all/site-specific\")\n    ) as fobj:\n        data = fobj.read()\n    assert data == HTTPS_OUTPUT_NO_POW\n\n    verify_install_has_valid_config()",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "securedrop_git_repo",
          "code": "def securedrop_git_repo(tmpdir):\n    cwd = os.getcwd()\n    os.chdir(str(tmpdir))\n    # Clone the SecureDrop repository into the temp directory.\n    cmd = [\"git\", \"clone\", \"https://github.com/freedomofpress/securedrop.git\"]\n    subprocess.check_call(cmd)\n    os.chdir(os.path.join(str(tmpdir), \"securedrop/admin\"))\n    subprocess.check_call(\"git reset --hard\".split())\n    # Now we will put in our own git configuration\n    with open(\"../.git/config\", \"w\") as fobj:\n        fobj.write(GIT_CONFIG)\n    # Let us move to an older tag\n    subprocess.check_call(\"git checkout 0.6\".split())\n    yield tmpdir\n\n    # Save coverage information in same directory as unit test coverage\n    test_name = str(tmpdir).split(\"/\")[-1]\n    try:\n        subprocess.check_call(\n            [\n                \"cp\",\n                f\"{str(tmpdir)}/securedrop/admin/.coverage\",\n                f\"{CURRENT_DIR}/../.coverage.{test_name}\",\n            ]\n        )\n    except subprocess.CalledProcessError:\n        # It means the coverage file may not exist, don't error\n        pass\n\n    os.chdir(cwd)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "set_reliable_keyserver",
          "code": "def set_reliable_keyserver(gpgdir):\n    # If gpg.conf doesn't exist, create it and set a reliable default\n    # keyserver for the tests.\n    gpgconf_path = os.path.join(gpgdir, \"gpg.conf\")\n    if not os.path.exists(gpgconf_path):\n        os.mkdir(gpgdir)\n        with open(gpgconf_path, \"a\") as f:\n            f.write(\"keyserver hkps://keys.openpgp.org\")\n\n        # Ensure correct permissions on .gnupg home directory.\n        os.chmod(gpgdir, 0o0700)",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "test_check_for_update_when_updates_needed",
          "code": "def test_check_for_update_when_updates_needed(securedrop_git_repo):\n    cmd = os.path.join(os.path.dirname(CURRENT_DIR), \"securedrop_admin/__init__.py\")\n    ansible_base = os.path.join(str(securedrop_git_repo), \"securedrop/install_files/ansible-base\")\n    fullcmd = f\"coverage run {cmd} --root {ansible_base} check_for_updates\"\n    child = pexpect.spawn(fullcmd)\n    child.expect(b\"Update needed\", timeout=20)\n\n    child.expect(pexpect.EOF, timeout=10)  # Wait for CLI to exit\n    child.close()\n    assert child.exitstatus == 0\n    assert child.signalstatus is None",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "test_check_for_update_when_updates_not_needed",
          "code": "def test_check_for_update_when_updates_not_needed(securedrop_git_repo):\n    # Determine latest production tag using GitHub release object\n    github_url = \"https://api.github.com/repos/freedomofpress/securedrop/releases/latest\"\n    latest_release = requests.get(github_url, timeout=60).json()\n    latest_tag = str(latest_release[\"tag_name\"])\n\n    subprocess.check_call([\"git\", \"checkout\", latest_tag])\n\n    cmd = os.path.join(os.path.dirname(CURRENT_DIR), \"securedrop_admin/__init__.py\")\n    ansible_base = os.path.join(str(securedrop_git_repo), \"securedrop/install_files/ansible-base\")\n    fullcmd = f\"coverage run {cmd} --root {ansible_base} check_for_updates\"\n    child = pexpect.spawn(fullcmd)\n    child.expect(b\"All updates applied\", timeout=20)\n\n    child.expect(pexpect.EOF, timeout=10)  # Wait for CLI to exit\n    child.close()\n    assert child.exitstatus == 0\n    assert child.signalstatus is None",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "test_update",
          "code": "def test_update(securedrop_git_repo):\n    gpgdir = os.path.join(os.path.expanduser(\"~\"), \".gnupg\")\n    set_reliable_keyserver(gpgdir)\n\n    cmd = os.path.join(os.path.dirname(CURRENT_DIR), \"securedrop_admin/__init__.py\")\n    ansible_base = os.path.join(str(securedrop_git_repo), \"securedrop/install_files/ansible-base\")\n    child = pexpect.spawn(f\"coverage run {cmd} --root {ansible_base} update\")\n\n    output = child.read()\n    assert b\"Updated to SecureDrop\" in output\n    assert b\"Signature verification successful\" in output\n\n    child.expect(pexpect.EOF, timeout=10)  # Wait for CLI to exit\n    child.close()\n    assert child.exitstatus == 0\n    assert child.signalstatus is None",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "test_update_fails_when_no_signature_present",
          "code": "def test_update_fails_when_no_signature_present(securedrop_git_repo):\n    gpgdir = os.path.join(os.path.expanduser(\"~\"), \".gnupg\")\n    set_reliable_keyserver(gpgdir)\n\n    # First we make a very high version tag of SecureDrop so that the\n    # updater will try to update to it. Since the tag is unsigned, it\n    # should fail.\n    subprocess.check_call(\"git checkout develop\".split())\n    subprocess.check_call(\"git tag 9999999.0.0\".split())\n\n    # Switch back to an older branch for the test\n    subprocess.check_call(\"git checkout 0.6\".split())\n\n    cmd = os.path.join(os.path.dirname(CURRENT_DIR), \"securedrop_admin/__init__.py\")\n    ansible_base = os.path.join(str(securedrop_git_repo), \"securedrop/install_files/ansible-base\")\n    child = pexpect.spawn(f\"coverage run {cmd} --root {ansible_base} update\")\n    output = child.read()\n    assert b\"Updated to SecureDrop\" not in output\n    assert b\"Update failed: Missing or invalid signature\" in output\n\n    child.expect(pexpect.EOF, timeout=10)  # Wait for CLI to exit\n    child.close()\n\n    # Failures should eventually exit non-zero.\n    assert child.exitstatus != 0\n    assert child.signalstatus != 0",
          "file": "test_integration.py"
        },
        {
          "type": "function",
          "name": "test_update_with_duplicate_branch_and_tag",
          "code": "def test_update_with_duplicate_branch_and_tag(securedrop_git_repo):\n    gpgdir = os.path.join(os.path.expanduser(\"~\"), \".gnupg\")\n    set_reliable_keyserver(gpgdir)\n\n    github_url = \"https://api.github.com/repos/freedomofpress/securedrop/releases/latest\"\n    latest_release = requests.get(github_url, timeout=60).json()\n    latest_tag = str(latest_release[\"tag_name\"])\n\n    # Create a branch with the same name as a tag.\n    subprocess.check_call([\"git\", \"checkout\", \"-b\", latest_tag])\n    # Checkout the older tag again in preparation for the update.\n    subprocess.check_call(\"git checkout 0.6\".split())\n\n    cmd = os.path.join(os.path.dirname(CURRENT_DIR), \"securedrop_admin/__init__.py\")\n    ansible_base = os.path.join(str(securedrop_git_repo), \"securedrop/install_files/ansible-base\")\n\n    child = pexpect.spawn(f\"coverage run {cmd} --root {ansible_base} update\")\n    output = child.read()\n    # Verify that we do not falsely check out a branch instead of a tag.\n    assert b\"Switched to branch\" not in output\n    assert b\"Updated to SecureDrop\" not in output\n    assert b\"Update failed: Branch name collision detected\" in output\n\n    child.expect(pexpect.EOF, timeout=10)  # Wait for CLI to exit\n    child.close()\n    assert child.exitstatus != 0\n    assert child.signalstatus != 0",
          "file": "test_integration.py"
        }
      ],
      "test_securedrop-admin.py": [
        {
          "type": "class",
          "name": "Document",
          "code": "class Document:\n    def __init__(self, text):\n        self.text = text",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "__init__",
          "code": "def __init__(self, text):\n        self.text = text",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "class",
          "name": "TestSecureDropAdmin",
          "code": "class TestSecureDropAdmin:\n    def test_verbose(self, capsys):\n        securedrop_admin.setup_logger(verbose=True)\n        securedrop_admin.sdlog.debug(\"VISIBLE\")\n        out, err = capsys.readouterr()\n        assert \"VISIBLE\" in out\n\n    def test_not_verbose(self, capsys):\n        securedrop_admin.setup_logger(verbose=False)\n        securedrop_admin.sdlog.debug(\"HIDDEN\")\n        securedrop_admin.sdlog.info(\"VISIBLE\")\n        out, err = capsys.readouterr()\n        assert \"HIDDEN\" not in out\n        assert \"VISIBLE\" in out\n\n    def test_openssh_detection(self):\n        with mock.patch(\"securedrop_admin.openssh_version\", side_effect=[9]):\n            assert securedrop_admin.ansible_command() == [\n                \"ansible-playbook\",\n                \"--scp-extra-args='-O'\",\n            ]\n        with mock.patch(\"securedrop_admin.openssh_version\", side_effect=[8]):\n            assert securedrop_admin.ansible_command() == [\"ansible-playbook\"]\n\n    def test_update_check_decorator_when_no_update_needed(self, caplog):\n        \"\"\"\n        When a function decorated with `@update_check_required` is run\n          And the `--force` argument was not given\n          And no update is required\n        Then the update check should run to completion\n          And no errors should be displayed\n          And the program should not exit\n          And the decorated function should be run\n        \"\"\"\n        with mock.patch(\n            \"securedrop_admin.check_for_updates\", side_effect=[[False, \"1.5.0\"]]\n        ) as mocked_check, mock.patch(\n            \"securedrop_admin.get_git_branch\", side_effect=[\"develop\"]\n        ), mock.patch(\"sys.exit\") as mocked_exit:\n            # The decorator itself interprets --force\n            args = argparse.Namespace(force=False)\n            rv = securedrop_admin.update_check_required(\"update_check_test\")(lambda _: 100)(args)\n            assert mocked_check.called\n            assert not mocked_exit.called\n            assert rv == 100\n            assert caplog.text == \"\"\n\n    def test_update_check_decorator_when_update_needed(self, caplog):\n        \"\"\"\n        When a function decorated with `@update_check_required` is run\n          And the `--force` argument was not given\n          And an update is required\n        Then the update check should run to completion\n          And an error referencing the command should be displayed\n          And the current branch state should be included in the output\n          And the program should exit\n        \"\"\"\n        with mock.patch(\n            \"securedrop_admin.check_for_updates\", side_effect=[[True, \"1.5.0\"]]\n        ) as mocked_check, mock.patch(\n            \"securedrop_admin.get_git_branch\", side_effect=[\"bad_branch\"]\n        ), mock.patch(\"sys.exit\") as mocked_exit:\n            # The decorator itself interprets --force\n            args = argparse.Namespace(force=False)\n            securedrop_admin.update_check_required(\"update_check_test\")(lambda _: _)(args)\n            assert mocked_check.called\n            assert mocked_exit.called\n            assert \"update_check_test\" in caplog.text\n            assert \"bad_branch\" in caplog.text\n\n    def test_update_check_decorator_when_skipped(self, caplog):\n        \"\"\"\n        When a function decorated with `@update_check_required` is run\n          And the `--force` argument was given\n        Then the update check should not run\n          And a message should be displayed acknowledging this\n          And the program should not exit\n          And the decorated function should be run\n        \"\"\"\n        with mock.patch(\n            \"securedrop_admin.check_for_updates\", side_effect=[[True, \"1.5.0\"]]\n        ) as mocked_check, mock.patch(\n            \"securedrop_admin.get_git_branch\", side_effect=[\"develop\"]\n        ), mock.patch(\"sys.exit\") as mocked_exit:\n            # The decorator itself interprets --force\n            args = argparse.Namespace(force=True)\n            rv = securedrop_admin.update_check_required(\"update_check_test\")(lambda _: 100)(args)\n            assert not mocked_check.called\n            assert not mocked_exit.called\n            assert \"--force\" in caplog.text\n            assert rv == 100\n\n    def test_check_for_updates_update_needed(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n        current_tag = b\"0.6\"\n        tags_available = b\"0.6\\n0.6-rc1\\n0.6.1\\n\"\n\n        with mock.patch(\"subprocess.check_call\"):\n            with mock.patch(\"subprocess.check_output\", side_effect=[current_tag, tags_available]):\n                update_status, tag = securedrop_admin.check_for_updates(args)\n                assert \"Update needed\" in caplog.text\n                assert update_status is True\n                assert tag == \"0.6.1\"\n\n    def test_check_for_updates_higher_version(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n        current_tag = b\"0.6\"\n        tags_available = b\"0.1\\n0.10.0\\n0.6.2\\n0.6\\n0.6-rc1\\n0.9.0\\n\"\n\n        with mock.patch(\"subprocess.check_call\"):\n            with mock.patch(\"subprocess.check_output\", side_effect=[current_tag, tags_available]):\n                update_status, tag = securedrop_admin.check_for_updates(args)\n                assert \"Update needed\" in caplog.text\n                assert update_status is True\n                assert tag == \"0.10.0\"\n\n    def test_check_for_updates_ensure_newline_stripped(self, tmpdir, caplog):\n        \"\"\"Regression test for #3426\"\"\"\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n        current_tag = b\"0.6.1\\n\"\n        tags_available = b\"0.6\\n0.6-rc1\\n0.6.1\\n\"\n\n        with mock.patch(\"subprocess.check_call\"):\n            with mock.patch(\"subprocess.check_output\", side_effect=[current_tag, tags_available]):\n                update_status, tag = securedrop_admin.check_for_updates(args)\n                assert \"All updates applied\" in caplog.text\n                assert update_status is False\n                assert tag == \"0.6.1\"\n\n    def test_check_for_updates_update_not_needed(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n        current_tag = b\"0.6.1\"\n        tags_available = b\"0.6\\n0.6-rc1\\n0.6.1\\n\"\n\n        with mock.patch(\"subprocess.check_call\"):\n            with mock.patch(\"subprocess.check_output\", side_effect=[current_tag, tags_available]):\n                update_status, tag = securedrop_admin.check_for_updates(args)\n                assert \"All updates applied\" in caplog.text\n                assert update_status is False\n                assert tag == \"0.6.1\"\n\n    def test_check_for_updates_if_most_recent_tag_is_rc(self, tmpdir, caplog):\n        \"\"\"During pre-release QA, the most recent tag ends in *-rc. Let's\n        verify that users will not accidentally check out this tag.\"\"\"\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n        current_tag = b\"0.6.1\"\n        tags_available = b\"0.6\\n0.6-rc1\\n0.6.1\\n0.6.1-rc1\\n\"\n\n        with mock.patch(\"subprocess.check_call\"):\n            with mock.patch(\"subprocess.check_output\", side_effect=[current_tag, tags_available]):\n                update_status, tag = securedrop_admin.check_for_updates(args)\n                assert \"All updates applied\" in caplog.text\n                assert update_status is False\n                assert tag == \"0.6.1\"\n\n    @pytest.mark.parametrize(\n        (\"git_output\", \"expected_rv\"),\n        [\n            (b\"* develop\\n\", \"develop\"),\n            (b\" develop\\n\" b\"* release/1.7.0\\n\", \"release/1.7.0\"),\n            (\n                b\"* (HEAD detached at 1.7.0)\\n\" b\"  develop\\n\" b\"  release/1.7.0\\n\",\n                \"(HEAD detached at 1.7.0)\",\n            ),\n            (b\"  main\\n\" b\"* valid_+!@#$%&_branch_name\\n\", \"valid_+!@#$%&_branch_name\"),\n            (b\"Unrecognized output.\", None),\n        ],\n    )\n    def test_get_git_branch(self, git_output, expected_rv):\n        \"\"\"\n        When `git branch` completes with exit code 0\n          And the output conforms to the expected format\n          Then `get_git_branch` should return a description of the current HEAD\n\n        When `git branch` completes with exit code 0\n          And the output does not conform to the expected format\n          Then `get_git_branch` should return `None`\n        \"\"\"\n        args = argparse.Namespace(root=None)\n        with mock.patch(\"subprocess.check_output\", side_effect=[git_output]):\n            rv = securedrop_admin.get_git_branch(args)\n            assert rv == expected_rv\n\n    def test_update_exits_if_not_needed(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n\n        with mock.patch(\"securedrop_admin.check_for_updates\", return_value=(False, \"0.6.1\")):\n            ret_code = securedrop_admin.update(args)\n            assert \"Applying SecureDrop updates...\" in caplog.text\n            assert \"Updated to SecureDrop\" not in caplog.text\n            assert ret_code == 0\n\n    def test_get_release_key_from_valid_keyserver(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n        with mock.patch(\"subprocess.check_call\"):\n            # Check that no exception is raised when the process is fast\n            securedrop_admin.get_release_key_from_keyserver(args)\n\n            # Check that use of the keyword arg also raises no exception\n            securedrop_admin.get_release_key_from_keyserver(args, keyserver=\"test.com\")\n\n    @pytest.mark.parametrize(\n        \"git_output\",\n        [\n            b\"gpg: Signature made Thu 20 Jul \"\n            b\"2022 08:12:25 PM EDT\\n\"\n            b\"gpg:                using RSA key \"\n            b\"2359E6538C0613E652955E6C188EDD3B7B22E6A3\\n\"\n            b'gpg: Good signature from \"SecureDrop Release '\n            b\"Signing Key \"\n            b'<securedrop-release-key-2021@freedom.press>\" [unknown]\\n',\n        ],\n    )\n    def test_update_signature_verifies(self, tmpdir, caplog, git_output):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n        patchers = [\n            mock.patch(\"securedrop_admin.check_for_updates\", return_value=(True, \"0.6.1\")),\n            mock.patch(\"subprocess.check_call\"),\n            # securedrop-admin checks if there is a branch with the same name as a tag\n            # that is being verified, and bails if there is. To ensure the verification\n            # succeeds, we have to mock the \"not a valid ref\" output it looks for.\n            mock.patch(\n                \"subprocess.check_output\",\n                side_effect=[\n                    git_output,\n                    subprocess.CalledProcessError(1, \"cmd\", b\"not a valid ref\"),\n                ],\n            ),\n        ]\n\n        for patcher in patchers:\n            patcher.start()\n\n        try:\n            ret_code = securedrop_admin.update(args)\n            assert \"Applying SecureDrop updates...\" in caplog.text\n            assert \"Signature verification successful.\" in caplog.text\n            assert \"Updated to SecureDrop\" in caplog.text\n            assert ret_code == 0\n        finally:\n            for patcher in patchers:\n                patcher.stop()\n\n    def test_update_unexpected_exception_git_refs(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n\n        git_output = (\n            b\"gpg: Signature made Tue 13 Mar 2022 01:14:11 AM UTC\\n\"\n            b\"gpg:                using RSA key \"\n            b\"2359E6538C0613E652955E6C188EDD3B7B22E6A3\\n\"\n            b'gpg: Good signature from \"SecureDrop Release '\n            b'Signing Key <securedrop-release-key-2021@freedom.press>\" [unknown]\\n'\n        )\n\n        patchers = [\n            mock.patch(\"securedrop_admin.check_for_updates\", return_value=(True, \"0.6.1\")),\n            mock.patch(\"subprocess.check_call\"),\n            mock.patch(\n                \"subprocess.check_output\",\n                side_effect=[\n                    git_output,\n                    subprocess.CalledProcessError(1, \"cmd\", b\"a random error\"),\n                ],\n            ),\n        ]\n\n        for patcher in patchers:\n            patcher.start()\n\n        try:\n            ret_code = securedrop_admin.update(args)\n            assert \"Applying SecureDrop updates...\" in caplog.text\n            assert \"Signature verification successful.\" not in caplog.text\n            assert \"Updated to SecureDrop\" not in caplog.text\n            assert ret_code == 1\n        finally:\n            for patcher in patchers:\n                patcher.stop()\n\n    def test_outdated_signature_does_not_verify(self, tmpdir, caplog):\n        \"\"\"\n        When a tag is signed with a release key that is no longer valid\n            Then the signature of a current tag should not verify\n        \"\"\"\n\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n\n        git_output = (\n            b\"gpg: Signature made Tue 13 Mar 2022 01:14:11 AM UTC\\n\"\n            b\"gpg:                using RSA key \"\n            b\"22245C81E3BAEB4138B36061310F561200F4AD77\\n\"\n            b'gpg: Good signature from \"SecureDrop Release '\n            b'Signing Key\" [unknown]\\n'\n        )\n\n        patchers = [\n            mock.patch(\"securedrop_admin.check_for_updates\", return_value=(True, \"0.6.1\")),\n            mock.patch(\"subprocess.check_call\"),\n            mock.patch(\n                \"subprocess.check_output\",\n                side_effect=[\n                    git_output,\n                    subprocess.CalledProcessError(1, \"cmd\", b\"not a valid ref\"),\n                ],\n            ),\n        ]\n\n        for patcher in patchers:\n            patcher.start()\n\n        try:\n            ret_code = securedrop_admin.update(args)\n            assert \"Applying SecureDrop updates...\" in caplog.text\n            assert \"Signature verification successful.\" not in caplog.text\n            assert \"Updated to SecureDrop\" not in caplog.text\n            assert ret_code == 1\n        finally:\n            for patcher in patchers:\n                patcher.stop()\n\n    def test_update_signature_does_not_verify(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n\n        git_output = (\n            b\"gpg: Signature made Tue 13 Mar 2022 01:14:11 AM UTC\\n\"\n            b\"gpg:                using RSA key \"\n            b\"2359E6538C0613E652955E6C188EDD3B7B22E6A3\\n\"\n            b'gpg: BAD signature from \"SecureDrop Release '\n            b'Signing Key <securedrop-release-key-2021@freedom.press>\" [unknown]\\n'\n        )\n\n        with mock.patch(\"securedrop_admin.check_for_updates\", return_value=(True, \"0.6.1\")):\n            with mock.patch(\"subprocess.check_call\"):\n                with mock.patch(\"subprocess.check_output\", return_value=git_output):\n                    ret_code = securedrop_admin.update(args)\n                    assert \"Applying SecureDrop updates...\" in caplog.text\n                    assert \"Update failed: Invalid signature format\" in caplog.text\n                    assert \"Updated to SecureDrop\" not in caplog.text\n                    assert ret_code != 0\n\n    def test_update_malicious_key_named_fingerprint(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n\n        git_output = (\n            b\"gpg: Signature made Tue 13 Mar 2022 01:14:11 AM UTC\\n\"\n            b\"gpg:                using RSA key \"\n            b\"1234567812345678123456781234567812345678\\n\"\n            b'gpg: Good signature from \"2359E6538C0613E652'\n            b'955E6C188EDD3B7B22E6A3\" [unknown]\\n'\n        )\n\n        with mock.patch(\"securedrop_admin.check_for_updates\", return_value=(True, \"0.6.1\")):\n            with mock.patch(\"subprocess.check_call\"):\n                with mock.patch(\"subprocess.check_output\", return_value=git_output):\n                    ret_code = securedrop_admin.update(args)\n                    assert \"Applying SecureDrop updates...\" in caplog.text\n                    assert \"Update failed: Invalid signature format\" in caplog.text\n                    assert \"Updated to SecureDrop\" not in caplog.text\n                    assert ret_code != 0\n\n    def test_update_malicious_key_named_good_sig(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n\n        git_output = (\n            b\"gpg: Signature made Tue 13 Mar 2022 01:14:11 AM UTC\\n\"\n            b\"gpg:                using RSA key \"\n            b\"1234567812345678123456781234567812345678\\n\"\n            b\"gpg: Good signature from Good signature from \"\n            b'\"SecureDrop Release Signing Key <securedrop-release-key-2021@freedom.press>\" '\n            b\"[unknown]\\n\"\n        )\n\n        with mock.patch(\"securedrop_admin.check_for_updates\", return_value=(True, \"0.6.1\")):\n            with mock.patch(\"subprocess.check_call\"):\n                with mock.patch(\"subprocess.check_output\", return_value=git_output):\n                    ret_code = securedrop_admin.update(args)\n                    assert \"Applying SecureDrop updates...\" in caplog.text\n                    assert \"Update failed: Invalid signature format\" in caplog.text\n                    assert \"Updated to SecureDrop\" not in caplog.text\n                    assert ret_code != 0\n\n    def test_update_malicious_key_named_good_sig_fingerprint(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n\n        git_output = (\n            b\"gpg: Signature made Tue 13 Mar 2022 01:14:11 AM UTC\\n\"\n            b\"gpg:                using RSA key \"\n            b\"1234567812345678123456781234567812345678\\n\"\n            b\"gpg: Good signature from 22245C81E3BAEB4138\"\n            b\"955E6C188EDD3B7B22E6A3 Good signature from \"\n            b'\"SecureDrop Release Signing Key <securedrop-release-key-2021@freedom.press>\" '\n            b\"[unknown]\\n\"\n        )\n\n        with mock.patch(\"securedrop_admin.check_for_updates\", return_value=(True, \"0.6.1\")):\n            with mock.patch(\"subprocess.check_call\"):\n                with mock.patch(\"subprocess.check_output\", return_value=git_output):\n                    ret_code = securedrop_admin.update(args)\n                    assert \"Applying SecureDrop updates...\" in caplog.text\n                    assert \"Update failed: Invalid signature format\" in caplog.text\n                    assert \"Updated to SecureDrop\" not in caplog.text\n                    assert ret_code != 0\n\n    def test_no_signature_on_update(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n\n        with mock.patch(\"securedrop_admin.check_for_updates\", return_value=(True, \"0.6.1\")):\n            with mock.patch(\"subprocess.check_call\"):\n                with mock.patch(\n                    \"subprocess.check_output\",\n                    side_effect=subprocess.CalledProcessError(\n                        1, \"git\", \"error: no signature found\"\n                    ),\n                ):\n                    ret_code = securedrop_admin.update(args)\n                    assert \"Applying SecureDrop updates...\" in caplog.text\n                    assert \"Update failed: Missing or invalid signature\" in caplog.text\n                    assert \"Updated to SecureDrop\" not in caplog.text\n                    assert ret_code != 0\n\n    def test_exit_codes(self, tmpdir):\n        \"\"\"Ensure that securedrop-admin returns the correct\n        exit codes for success or failure.\"\"\"\n        with mock.patch(\"securedrop_admin.install_securedrop\", return_value=0):\n            with pytest.raises(SystemExit) as e:\n                securedrop_admin.main([\"--root\", str(tmpdir), \"install\"])\n            assert e.value.code == securedrop_admin.EXIT_SUCCESS\n\n        with mock.patch(\n            \"securedrop_admin.install_securedrop\",\n            side_effect=subprocess.CalledProcessError(1, \"TestError\"),\n        ):\n            with pytest.raises(SystemExit) as e:\n                securedrop_admin.main([\"--root\", str(tmpdir), \"install\"])\n            assert e.value.code == securedrop_admin.EXIT_SUBPROCESS_ERROR\n\n        with mock.patch(\"securedrop_admin.install_securedrop\", side_effect=KeyboardInterrupt):\n            with pytest.raises(SystemExit) as e:\n                securedrop_admin.main([\"--root\", str(tmpdir), \"install\"])\n            assert e.value.code == securedrop_admin.EXIT_INTERRUPT",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_verbose",
          "code": "def test_verbose(self, capsys):\n        securedrop_admin.setup_logger(verbose=True)\n        securedrop_admin.sdlog.debug(\"VISIBLE\")\n        out, err = capsys.readouterr()\n        assert \"VISIBLE\" in out",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_not_verbose",
          "code": "def test_not_verbose(self, capsys):\n        securedrop_admin.setup_logger(verbose=False)\n        securedrop_admin.sdlog.debug(\"HIDDEN\")\n        securedrop_admin.sdlog.info(\"VISIBLE\")\n        out, err = capsys.readouterr()\n        assert \"HIDDEN\" not in out\n        assert \"VISIBLE\" in out",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_openssh_detection",
          "code": "def test_openssh_detection(self):\n        with mock.patch(\"securedrop_admin.openssh_version\", side_effect=[9]):\n            assert securedrop_admin.ansible_command() == [\n                \"ansible-playbook\",\n                \"--scp-extra-args='-O'\",\n            ]\n        with mock.patch(\"securedrop_admin.openssh_version\", side_effect=[8]):\n            assert securedrop_admin.ansible_command() == [\"ansible-playbook\"]",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_update_check_decorator_when_no_update_needed",
          "code": "def test_update_check_decorator_when_no_update_needed(self, caplog):\n        \"\"\"\n        When a function decorated with `@update_check_required` is run\n          And the `--force` argument was not given\n          And no update is required\n        Then the update check should run to completion\n          And no errors should be displayed\n          And the program should not exit\n          And the decorated function should be run\n        \"\"\"\n        with mock.patch(\n            \"securedrop_admin.check_for_updates\", side_effect=[[False, \"1.5.0\"]]\n        ) as mocked_check, mock.patch(\n            \"securedrop_admin.get_git_branch\", side_effect=[\"develop\"]\n        ), mock.patch(\"sys.exit\") as mocked_exit:\n            # The decorator itself interprets --force\n            args = argparse.Namespace(force=False)\n            rv = securedrop_admin.update_check_required(\"update_check_test\")(lambda _: 100)(args)\n            assert mocked_check.called\n            assert not mocked_exit.called\n            assert rv == 100\n            assert caplog.text == \"\"",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_update_check_decorator_when_update_needed",
          "code": "def test_update_check_decorator_when_update_needed(self, caplog):\n        \"\"\"\n        When a function decorated with `@update_check_required` is run\n          And the `--force` argument was not given\n          And an update is required\n        Then the update check should run to completion\n          And an error referencing the command should be displayed\n          And the current branch state should be included in the output\n          And the program should exit\n        \"\"\"\n        with mock.patch(\n            \"securedrop_admin.check_for_updates\", side_effect=[[True, \"1.5.0\"]]\n        ) as mocked_check, mock.patch(\n            \"securedrop_admin.get_git_branch\", side_effect=[\"bad_branch\"]\n        ), mock.patch(\"sys.exit\") as mocked_exit:\n            # The decorator itself interprets --force\n            args = argparse.Namespace(force=False)\n            securedrop_admin.update_check_required(\"update_check_test\")(lambda _: _)(args)\n            assert mocked_check.called\n            assert mocked_exit.called\n            assert \"update_check_test\" in caplog.text\n            assert \"bad_branch\" in caplog.text",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_update_check_decorator_when_skipped",
          "code": "def test_update_check_decorator_when_skipped(self, caplog):\n        \"\"\"\n        When a function decorated with `@update_check_required` is run\n          And the `--force` argument was given\n        Then the update check should not run\n          And a message should be displayed acknowledging this\n          And the program should not exit\n          And the decorated function should be run\n        \"\"\"\n        with mock.patch(\n            \"securedrop_admin.check_for_updates\", side_effect=[[True, \"1.5.0\"]]\n        ) as mocked_check, mock.patch(\n            \"securedrop_admin.get_git_branch\", side_effect=[\"develop\"]\n        ), mock.patch(\"sys.exit\") as mocked_exit:\n            # The decorator itself interprets --force\n            args = argparse.Namespace(force=True)\n            rv = securedrop_admin.update_check_required(\"update_check_test\")(lambda _: 100)(args)\n            assert not mocked_check.called\n            assert not mocked_exit.called\n            assert \"--force\" in caplog.text\n            assert rv == 100",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_check_for_updates_update_needed",
          "code": "def test_check_for_updates_update_needed(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n        current_tag = b\"0.6\"\n        tags_available = b\"0.6\\n0.6-rc1\\n0.6.1\\n\"\n\n        with mock.patch(\"subprocess.check_call\"):\n            with mock.patch(\"subprocess.check_output\", side_effect=[current_tag, tags_available]):\n                update_status, tag = securedrop_admin.check_for_updates(args)\n                assert \"Update needed\" in caplog.text\n                assert update_status is True\n                assert tag == \"0.6.1\"",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_check_for_updates_higher_version",
          "code": "def test_check_for_updates_higher_version(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n        current_tag = b\"0.6\"\n        tags_available = b\"0.1\\n0.10.0\\n0.6.2\\n0.6\\n0.6-rc1\\n0.9.0\\n\"\n\n        with mock.patch(\"subprocess.check_call\"):\n            with mock.patch(\"subprocess.check_output\", side_effect=[current_tag, tags_available]):\n                update_status, tag = securedrop_admin.check_for_updates(args)\n                assert \"Update needed\" in caplog.text\n                assert update_status is True\n                assert tag == \"0.10.0\"",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_check_for_updates_ensure_newline_stripped",
          "code": "def test_check_for_updates_ensure_newline_stripped(self, tmpdir, caplog):\n        \"\"\"Regression test for #3426\"\"\"\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n        current_tag = b\"0.6.1\\n\"\n        tags_available = b\"0.6\\n0.6-rc1\\n0.6.1\\n\"\n\n        with mock.patch(\"subprocess.check_call\"):\n            with mock.patch(\"subprocess.check_output\", side_effect=[current_tag, tags_available]):\n                update_status, tag = securedrop_admin.check_for_updates(args)\n                assert \"All updates applied\" in caplog.text\n                assert update_status is False\n                assert tag == \"0.6.1\"",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_check_for_updates_update_not_needed",
          "code": "def test_check_for_updates_update_not_needed(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n        current_tag = b\"0.6.1\"\n        tags_available = b\"0.6\\n0.6-rc1\\n0.6.1\\n\"\n\n        with mock.patch(\"subprocess.check_call\"):\n            with mock.patch(\"subprocess.check_output\", side_effect=[current_tag, tags_available]):\n                update_status, tag = securedrop_admin.check_for_updates(args)\n                assert \"All updates applied\" in caplog.text\n                assert update_status is False\n                assert tag == \"0.6.1\"",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_check_for_updates_if_most_recent_tag_is_rc",
          "code": "def test_check_for_updates_if_most_recent_tag_is_rc(self, tmpdir, caplog):\n        \"\"\"During pre-release QA, the most recent tag ends in *-rc. Let's\n        verify that users will not accidentally check out this tag.\"\"\"\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n        current_tag = b\"0.6.1\"\n        tags_available = b\"0.6\\n0.6-rc1\\n0.6.1\\n0.6.1-rc1\\n\"\n\n        with mock.patch(\"subprocess.check_call\"):\n            with mock.patch(\"subprocess.check_output\", side_effect=[current_tag, tags_available]):\n                update_status, tag = securedrop_admin.check_for_updates(args)\n                assert \"All updates applied\" in caplog.text\n                assert update_status is False\n                assert tag == \"0.6.1\"",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_get_git_branch",
          "code": "def test_get_git_branch(self, git_output, expected_rv):\n        \"\"\"\n        When `git branch` completes with exit code 0\n          And the output conforms to the expected format\n          Then `get_git_branch` should return a description of the current HEAD\n\n        When `git branch` completes with exit code 0\n          And the output does not conform to the expected format\n          Then `get_git_branch` should return `None`\n        \"\"\"\n        args = argparse.Namespace(root=None)\n        with mock.patch(\"subprocess.check_output\", side_effect=[git_output]):\n            rv = securedrop_admin.get_git_branch(args)\n            assert rv == expected_rv",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_update_exits_if_not_needed",
          "code": "def test_update_exits_if_not_needed(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n\n        with mock.patch(\"securedrop_admin.check_for_updates\", return_value=(False, \"0.6.1\")):\n            ret_code = securedrop_admin.update(args)\n            assert \"Applying SecureDrop updates...\" in caplog.text\n            assert \"Updated to SecureDrop\" not in caplog.text\n            assert ret_code == 0",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_get_release_key_from_valid_keyserver",
          "code": "def test_get_release_key_from_valid_keyserver(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n        with mock.patch(\"subprocess.check_call\"):\n            # Check that no exception is raised when the process is fast\n            securedrop_admin.get_release_key_from_keyserver(args)\n\n            # Check that use of the keyword arg also raises no exception\n            securedrop_admin.get_release_key_from_keyserver(args, keyserver=\"test.com\")",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_update_signature_verifies",
          "code": "def test_update_signature_verifies(self, tmpdir, caplog, git_output):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n        patchers = [\n            mock.patch(\"securedrop_admin.check_for_updates\", return_value=(True, \"0.6.1\")),\n            mock.patch(\"subprocess.check_call\"),\n            # securedrop-admin checks if there is a branch with the same name as a tag\n            # that is being verified, and bails if there is. To ensure the verification\n            # succeeds, we have to mock the \"not a valid ref\" output it looks for.\n            mock.patch(\n                \"subprocess.check_output\",\n                side_effect=[\n                    git_output,\n                    subprocess.CalledProcessError(1, \"cmd\", b\"not a valid ref\"),\n                ],\n            ),\n        ]\n\n        for patcher in patchers:\n            patcher.start()\n\n        try:\n            ret_code = securedrop_admin.update(args)\n            assert \"Applying SecureDrop updates...\" in caplog.text\n            assert \"Signature verification successful.\" in caplog.text\n            assert \"Updated to SecureDrop\" in caplog.text\n            assert ret_code == 0\n        finally:\n            for patcher in patchers:\n                patcher.stop()",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_update_unexpected_exception_git_refs",
          "code": "def test_update_unexpected_exception_git_refs(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n\n        git_output = (\n            b\"gpg: Signature made Tue 13 Mar 2022 01:14:11 AM UTC\\n\"\n            b\"gpg:                using RSA key \"\n            b\"2359E6538C0613E652955E6C188EDD3B7B22E6A3\\n\"\n            b'gpg: Good signature from \"SecureDrop Release '\n            b'Signing Key <securedrop-release-key-2021@freedom.press>\" [unknown]\\n'\n        )\n\n        patchers = [\n            mock.patch(\"securedrop_admin.check_for_updates\", return_value=(True, \"0.6.1\")),\n            mock.patch(\"subprocess.check_call\"),\n            mock.patch(\n                \"subprocess.check_output\",\n                side_effect=[\n                    git_output,\n                    subprocess.CalledProcessError(1, \"cmd\", b\"a random error\"),\n                ],\n            ),\n        ]\n\n        for patcher in patchers:\n            patcher.start()\n\n        try:\n            ret_code = securedrop_admin.update(args)\n            assert \"Applying SecureDrop updates...\" in caplog.text\n            assert \"Signature verification successful.\" not in caplog.text\n            assert \"Updated to SecureDrop\" not in caplog.text\n            assert ret_code == 1\n        finally:\n            for patcher in patchers:\n                patcher.stop()",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_outdated_signature_does_not_verify",
          "code": "def test_outdated_signature_does_not_verify(self, tmpdir, caplog):\n        \"\"\"\n        When a tag is signed with a release key that is no longer valid\n            Then the signature of a current tag should not verify\n        \"\"\"\n\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n\n        git_output = (\n            b\"gpg: Signature made Tue 13 Mar 2022 01:14:11 AM UTC\\n\"\n            b\"gpg:                using RSA key \"\n            b\"22245C81E3BAEB4138B36061310F561200F4AD77\\n\"\n            b'gpg: Good signature from \"SecureDrop Release '\n            b'Signing Key\" [unknown]\\n'\n        )\n\n        patchers = [\n            mock.patch(\"securedrop_admin.check_for_updates\", return_value=(True, \"0.6.1\")),\n            mock.patch(\"subprocess.check_call\"),\n            mock.patch(\n                \"subprocess.check_output\",\n                side_effect=[\n                    git_output,\n                    subprocess.CalledProcessError(1, \"cmd\", b\"not a valid ref\"),\n                ],\n            ),\n        ]\n\n        for patcher in patchers:\n            patcher.start()\n\n        try:\n            ret_code = securedrop_admin.update(args)\n            assert \"Applying SecureDrop updates...\" in caplog.text\n            assert \"Signature verification successful.\" not in caplog.text\n            assert \"Updated to SecureDrop\" not in caplog.text\n            assert ret_code == 1\n        finally:\n            for patcher in patchers:\n                patcher.stop()",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_update_signature_does_not_verify",
          "code": "def test_update_signature_does_not_verify(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n\n        git_output = (\n            b\"gpg: Signature made Tue 13 Mar 2022 01:14:11 AM UTC\\n\"\n            b\"gpg:                using RSA key \"\n            b\"2359E6538C0613E652955E6C188EDD3B7B22E6A3\\n\"\n            b'gpg: BAD signature from \"SecureDrop Release '\n            b'Signing Key <securedrop-release-key-2021@freedom.press>\" [unknown]\\n'\n        )\n\n        with mock.patch(\"securedrop_admin.check_for_updates\", return_value=(True, \"0.6.1\")):\n            with mock.patch(\"subprocess.check_call\"):\n                with mock.patch(\"subprocess.check_output\", return_value=git_output):\n                    ret_code = securedrop_admin.update(args)\n                    assert \"Applying SecureDrop updates...\" in caplog.text\n                    assert \"Update failed: Invalid signature format\" in caplog.text\n                    assert \"Updated to SecureDrop\" not in caplog.text\n                    assert ret_code != 0",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_update_malicious_key_named_fingerprint",
          "code": "def test_update_malicious_key_named_fingerprint(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n\n        git_output = (\n            b\"gpg: Signature made Tue 13 Mar 2022 01:14:11 AM UTC\\n\"\n            b\"gpg:                using RSA key \"\n            b\"1234567812345678123456781234567812345678\\n\"\n            b'gpg: Good signature from \"2359E6538C0613E652'\n            b'955E6C188EDD3B7B22E6A3\" [unknown]\\n'\n        )\n\n        with mock.patch(\"securedrop_admin.check_for_updates\", return_value=(True, \"0.6.1\")):\n            with mock.patch(\"subprocess.check_call\"):\n                with mock.patch(\"subprocess.check_output\", return_value=git_output):\n                    ret_code = securedrop_admin.update(args)\n                    assert \"Applying SecureDrop updates...\" in caplog.text\n                    assert \"Update failed: Invalid signature format\" in caplog.text\n                    assert \"Updated to SecureDrop\" not in caplog.text\n                    assert ret_code != 0",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_update_malicious_key_named_good_sig",
          "code": "def test_update_malicious_key_named_good_sig(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n\n        git_output = (\n            b\"gpg: Signature made Tue 13 Mar 2022 01:14:11 AM UTC\\n\"\n            b\"gpg:                using RSA key \"\n            b\"1234567812345678123456781234567812345678\\n\"\n            b\"gpg: Good signature from Good signature from \"\n            b'\"SecureDrop Release Signing Key <securedrop-release-key-2021@freedom.press>\" '\n            b\"[unknown]\\n\"\n        )\n\n        with mock.patch(\"securedrop_admin.check_for_updates\", return_value=(True, \"0.6.1\")):\n            with mock.patch(\"subprocess.check_call\"):\n                with mock.patch(\"subprocess.check_output\", return_value=git_output):\n                    ret_code = securedrop_admin.update(args)\n                    assert \"Applying SecureDrop updates...\" in caplog.text\n                    assert \"Update failed: Invalid signature format\" in caplog.text\n                    assert \"Updated to SecureDrop\" not in caplog.text\n                    assert ret_code != 0",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_update_malicious_key_named_good_sig_fingerprint",
          "code": "def test_update_malicious_key_named_good_sig_fingerprint(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n\n        git_output = (\n            b\"gpg: Signature made Tue 13 Mar 2022 01:14:11 AM UTC\\n\"\n            b\"gpg:                using RSA key \"\n            b\"1234567812345678123456781234567812345678\\n\"\n            b\"gpg: Good signature from 22245C81E3BAEB4138\"\n            b\"955E6C188EDD3B7B22E6A3 Good signature from \"\n            b'\"SecureDrop Release Signing Key <securedrop-release-key-2021@freedom.press>\" '\n            b\"[unknown]\\n\"\n        )\n\n        with mock.patch(\"securedrop_admin.check_for_updates\", return_value=(True, \"0.6.1\")):\n            with mock.patch(\"subprocess.check_call\"):\n                with mock.patch(\"subprocess.check_output\", return_value=git_output):\n                    ret_code = securedrop_admin.update(args)\n                    assert \"Applying SecureDrop updates...\" in caplog.text\n                    assert \"Update failed: Invalid signature format\" in caplog.text\n                    assert \"Updated to SecureDrop\" not in caplog.text\n                    assert ret_code != 0",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_no_signature_on_update",
          "code": "def test_no_signature_on_update(self, tmpdir, caplog):\n        git_repo_path = str(tmpdir)\n        args = argparse.Namespace(root=git_repo_path)\n\n        with mock.patch(\"securedrop_admin.check_for_updates\", return_value=(True, \"0.6.1\")):\n            with mock.patch(\"subprocess.check_call\"):\n                with mock.patch(\n                    \"subprocess.check_output\",\n                    side_effect=subprocess.CalledProcessError(\n                        1, \"git\", \"error: no signature found\"\n                    ),\n                ):\n                    ret_code = securedrop_admin.update(args)\n                    assert \"Applying SecureDrop updates...\" in caplog.text\n                    assert \"Update failed: Missing or invalid signature\" in caplog.text\n                    assert \"Updated to SecureDrop\" not in caplog.text\n                    assert ret_code != 0",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_exit_codes",
          "code": "def test_exit_codes(self, tmpdir):\n        \"\"\"Ensure that securedrop-admin returns the correct\n        exit codes for success or failure.\"\"\"\n        with mock.patch(\"securedrop_admin.install_securedrop\", return_value=0):\n            with pytest.raises(SystemExit) as e:\n                securedrop_admin.main([\"--root\", str(tmpdir), \"install\"])\n            assert e.value.code == securedrop_admin.EXIT_SUCCESS\n\n        with mock.patch(\n            \"securedrop_admin.install_securedrop\",\n            side_effect=subprocess.CalledProcessError(1, \"TestError\"),\n        ):\n            with pytest.raises(SystemExit) as e:\n                securedrop_admin.main([\"--root\", str(tmpdir), \"install\"])\n            assert e.value.code == securedrop_admin.EXIT_SUBPROCESS_ERROR\n\n        with mock.patch(\"securedrop_admin.install_securedrop\", side_effect=KeyboardInterrupt):\n            with pytest.raises(SystemExit) as e:\n                securedrop_admin.main([\"--root\", str(tmpdir), \"install\"])\n            assert e.value.code == securedrop_admin.EXIT_INTERRUPT",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "class",
          "name": "TestSiteConfig",
          "code": "class TestSiteConfig:\n    def test_exists(self, tmpdir):\n        args = argparse.Namespace(\n            site_config=\"DOES_NOT_EXIST\",\n            ansible_path=\".\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        assert not securedrop_admin.SiteConfig(args).exists()\n        args = argparse.Namespace(\n            site_config=__file__, ansible_path=\".\", app_path=dirname(__file__), root=tmpdir\n        )\n        assert securedrop_admin.SiteConfig(args).exists()\n\n    def test_validate_not_empty(self):\n        validator = securedrop_admin.SiteConfig.ValidateNotEmpty()\n\n        assert validator.validate(Document(\"something\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"\"))\n\n    def test_validate_time(self):\n        validator = securedrop_admin.SiteConfig.ValidateTime()\n\n        assert validator.validate(Document(\"4\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"four\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"4.30\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"25\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"-4\"))\n\n    def test_validate_ossec_username(self):\n        validator = securedrop_admin.SiteConfig.ValidateOSSECUsername()\n\n        assert validator.validate(Document(\"username\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"bad@user\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"test\"))\n\n    def test_validate_ossec_password(self):\n        validator = securedrop_admin.SiteConfig.ValidateOSSECPassword()\n\n        assert validator.validate(Document(\"goodpassword\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"password123\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"short\"))\n\n    def test_validate_email(self):\n        validator = securedrop_admin.SiteConfig.ValidateEmail()\n\n        assert validator.validate(Document(\"good@mail.com\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"badmail\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"\"))\n\n    def test_validate_ossec_email(self):\n        validator = securedrop_admin.SiteConfig.ValidateOSSECEmail()\n\n        assert validator.validate(Document(\"good@mail.com\"))\n        with pytest.raises(ValidationError) as e:\n            validator.validate(Document(\"ossec@ossec.test\"))\n        assert \"something other than ossec@ossec.test\" in str(e)\n\n    def test_validate_optional_email(self):\n        validator = securedrop_admin.SiteConfig.ValidateOptionalEmail()\n\n        assert validator.validate(Document(\"good@mail.com\"))\n        assert validator.validate(Document(\"\"))\n\n    def test_validate_user(self):\n        validator = securedrop_admin.SiteConfig.ValidateUser()\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"amnesia\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"root\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"\"))\n        assert validator.validate(Document(\"gooduser\"))\n\n    def test_validate_ip(self):\n        validator = securedrop_admin.SiteConfig.ValidateIP()\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"599.20\"))\n        assert validator.validate(Document(\"192.168.1.1\"))\n\n    def test_validate_path(self):\n        mydir = dirname(__file__)\n        myfile = basename(__file__)\n        validator = securedrop_admin.SiteConfig.ValidatePath(mydir)\n        assert validator.validate(Document(myfile))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"NONEXIST\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"\"))\n\n    def test_validate_optional_path(self):\n        mydir = dirname(__file__)\n        myfile = basename(__file__)\n        validator = securedrop_admin.SiteConfig.ValidateOptionalPath(mydir)\n        assert validator.validate(Document(myfile))\n        assert validator.validate(Document(\"\"))\n\n    def test_validate_yes_no(self):\n        validator = securedrop_admin.SiteConfig.ValidateYesNo()\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"something\"))\n        assert validator.validate(Document(\"yes\"))\n        assert validator.validate(Document(\"YES\"))\n        assert validator.validate(Document(\"no\"))\n        assert validator.validate(Document(\"NO\"))\n\n    def test_validate_fingerprint(self):\n        validator = securedrop_admin.SiteConfig.ValidateFingerprint()\n        assert validator.validate(Document(\"012345678901234567890123456789ABCDEFABCD\"))\n        assert validator.validate(Document(\"01234 5678901234567890123456789ABCDE   FABCD\"))\n\n        with pytest.raises(ValidationError) as e:\n            validator.validate(Document(\"65A1B5FF195B56353CC63DFFCC40EF1228271441\"))\n        assert \"TEST journalist\" in str(e)\n\n        with pytest.raises(ValidationError) as e:\n            validator.validate(Document(\"600BC6D5142C68F35DDBCEA87B597104EDDDC102\"))\n        assert \"TEST admin\" in str(e)\n\n        with pytest.raises(ValidationError) as e:\n            validator.validate(Document(\"0000\"))\n        assert \"40 hexadecimal\" in str(e)\n\n        with pytest.raises(ValidationError) as e:\n            validator.validate(Document(\"zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\"))\n        assert \"40 hexadecimal\" in str(e)\n\n    def test_validate_optional_fingerprint(self):\n        validator = securedrop_admin.SiteConfig.ValidateOptionalFingerprint()\n        assert validator.validate(Document(\"012345678901234567890123456789ABCDEFABCD\"))\n        assert validator.validate(Document(\"\"))\n\n    def test_sanitize_fingerprint(self, tmpdir):\n        args = argparse.Namespace(\n            site_config=\"DOES_NOT_EXIST\",\n            ansible_path=\".\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        assert site_config.sanitize_fingerprint(\"    A bc\") == \"ABC\"\n\n    def test_validate_int(self):\n        validator = securedrop_admin.SiteConfig.ValidateInt()\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"123X\"))\n        assert validator.validate(Document(\"192\"))\n\n    def test_locales(self):\n        locales = securedrop_admin.SiteConfig.Locales(dirname(__file__))\n        translations = locales.get_translations()\n        assert \"en_US\" in translations\n        assert \"fr_FR\" in translations\n\n    def test_validate_locales(self):\n        validator = securedrop_admin.SiteConfig.ValidateLocales(\n            dirname(__file__), {\"en_US\", \"fr_FR\"}\n        )\n        assert validator.validate(Document(\"en_US  fr_FR \"))\n        with pytest.raises(ValidationError) as e:\n            validator.validate(Document(\"BAD\"))\n        assert \"BAD\" in str(e)\n\n    def test_save(self, tmpdir):\n        site_config_path = join(str(tmpdir), \"site_config\")\n        args = argparse.Namespace(\n            site_config=site_config_path,\n            ansible_path=\".\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        site_config.config = {\"var1\": \"val1\", \"var2\": \"val2\"}\n        site_config.save()\n        expected = textwrap.dedent(\n            \"\"\"\\\n        var1: val1\n        var2: val2\n        \"\"\"\n        )\n        assert expected == open(site_config_path).read()\n\n    def test_validate_gpg_key(self, tmpdir, caplog):\n        args = argparse.Namespace(\n            site_config=\"INVALID\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        good_config = {\n            \"securedrop_app_gpg_public_key\": \"test_journalist_key.pub\",\n            \"securedrop_app_gpg_fingerprint\": \"65A1B5FF195B56353CC63DFFCC40EF1228271441\",\n            \"ossec_alert_gpg_public_key\": \"test_journalist_key.pub\",\n            \"ossec_gpg_fpr\": \"65A1B5FF195B56353CC63DFFCC40EF1228271441\",\n            \"journalist_alert_gpg_public_key\": \"test_journalist_key.pub\",\n            \"journalist_gpg_fpr\": \"65A1B5FF195B56353CC63DFFCC40EF1228271441\",\n        }\n        site_config = securedrop_admin.SiteConfig(args)\n        site_config.config = good_config\n        assert site_config.validate_gpg_keys()\n\n        for key in (\"securedrop_app_gpg_fingerprint\", \"ossec_gpg_fpr\", \"journalist_gpg_fpr\"):\n            bad_config = good_config.copy()\n            bad_config[key] = \"FAIL\"\n            site_config.config = bad_config\n            with pytest.raises(securedrop_admin.FingerprintException) as e:\n                site_config.validate_gpg_keys()\n            assert \"FAIL does not match\" in str(e)\n\n        # Test a key with matching fingerprint but that fails sq-keyring-linter\n        invalid_config = {\n            # Correct key fingerprint but weak 1024-bit RSA key with SHA-1 self signature\n            \"securedrop_app_gpg_public_key\": \"weak_test_key_should_fail_sqlinter.asc\",\n            \"securedrop_app_gpg_fingerprint\": \"40F1C17B7E7826DAB40B14AE7786B000E6D0A76E\",\n            \"ossec_alert_gpg_public_key\": \"test_journalist_key.pub\",\n            \"ossec_gpg_fpr\": \"65A1B5FF195B56353CC63DFFCC40EF1228271441\",\n            \"journalist_alert_gpg_public_key\": \"test_journalist_key.pub\",\n            \"journalist_gpg_fpr\": \"65A1B5FF195B56353CC63DFFCC40EF1228271441\",\n        }\n        site_config.config = invalid_config\n        with pytest.raises(securedrop_admin.FingerprintException) as e:\n            site_config.validate_gpg_keys()\n        assert \"failed sq-keyring-linter check\" in str(e)\n\n    def test_journalist_alert_email(self, tmpdir):\n        args = argparse.Namespace(\n            site_config=\"INVALID\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        site_config.config = {\n            \"journalist_alert_gpg_public_key\": \"\",\n            \"journalist_gpg_fpr\": \"\",\n        }\n        assert site_config.validate_journalist_alert_email()\n        site_config.config = {\n            \"journalist_alert_gpg_public_key\": \"test_journalist_key.pub\",\n            \"journalist_gpg_fpr\": \"65A1B5FF195B56353CC63DFFCC40EF1228271441\",\n        }\n        site_config.config[\"journalist_alert_email\"] = \"\"\n        with pytest.raises(securedrop_admin.JournalistAlertEmailException) as e:\n            site_config.validate_journalist_alert_email()\n        assert \"not be empty\" in str(e)\n\n        site_config.config[\"journalist_alert_email\"] = \"bademail\"\n        with pytest.raises(securedrop_admin.JournalistAlertEmailException) as e:\n            site_config.validate_journalist_alert_email()\n        assert \"Must contain a @\" in str(e)\n\n        site_config.config[\"journalist_alert_email\"] = \"good@email.com\"\n        assert site_config.validate_journalist_alert_email()\n\n    @mock.patch(\"securedrop_admin.SiteConfig.validated_input\", side_effect=lambda p, d, v, t: d)\n    @mock.patch(\"securedrop_admin.SiteConfig.save\")\n    def test_update_config(self, mock_save, mock_validate_input):\n        args = argparse.Namespace(\n            site_config=\"tests/files/site-specific\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=\"tests/files\",\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n\n        assert site_config.load_and_update_config()\n        assert \"user_defined_variable\" in site_config.config\n        mock_save.assert_called_once()\n        mock_validate_input.assert_called()\n\n    @mock.patch(\"securedrop_admin.SiteConfig.validated_input\", side_effect=lambda p, d, v, t: d)\n    @mock.patch(\"securedrop_admin.SiteConfig.validate_gpg_keys\")\n    def test_update_config_no_site_specific(self, validate_gpg_keys, mock_validate_input, tmpdir):\n        site_config_path = join(str(tmpdir), \"site_config\")\n        args = argparse.Namespace(\n            site_config=site_config_path,\n            ansible_path=\".\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        assert site_config.load_and_update_config()\n        mock_validate_input.assert_called()\n        validate_gpg_keys.assert_called_once()\n        assert exists(site_config_path)\n\n    def test_load_and_update_config(self, tmpdir):\n        args = argparse.Namespace(\n            site_config=\"tests/files/site-specific\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        with mock.patch(\"securedrop_admin.SiteConfig.update_config\"):\n            site_config.load_and_update_config()\n            assert site_config.config != {}\n\n        args = argparse.Namespace(\n            site_config=\"tests/files/site-specific-missing-entries\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        with mock.patch(\"securedrop_admin.SiteConfig.update_config\"):\n            site_config.load_and_update_config()\n            assert site_config.config != {}\n\n        args = argparse.Namespace(\n            site_config=\"UNKNOWN\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        with mock.patch(\"securedrop_admin.SiteConfig.update_config\"):\n            site_config.load_and_update_config()\n            assert site_config.config == {}\n\n    def get_desc(self, site_config, var):\n        for desc in site_config.desc:\n            if desc[0] == var:\n                return desc\n\n    def verify_desc_consistency_optional(self, site_config, desc):\n        (var, default, etype, prompt, validator, transform, condition) = desc\n        # verify the default passes validation\n        if callable(default):\n            default = default()\n        assert site_config.user_prompt_config_one(desc, None) == default\n        assert type(default) is etype\n\n    def verify_desc_consistency(self, site_config, desc):\n        self.verify_desc_consistency_optional(site_config, desc)\n\n    def verify_prompt_boolean(self, site_config, desc):\n        self.verify_desc_consistency(site_config, desc)\n        (var, default, etype, prompt, validator, transform, condition) = desc\n        assert site_config.user_prompt_config_one(desc, True) is True\n        assert site_config.user_prompt_config_one(desc, False) is False\n        assert site_config.user_prompt_config_one(desc, \"YES\") is True\n        assert site_config.user_prompt_config_one(desc, \"NO\") is False\n\n    def test_desc_conditional(self, tmpdir):\n        \"\"\"Ensure that conditional prompts behave correctly.\n\n        Prompts which depend on another question should only be\n        asked if the prior question was answered appropriately.\"\"\"\n\n        questions = [\n            (\n                \"first_question\",\n                False,\n                bool,\n                \"Test Question 1\",\n                None,\n                lambda x: x.lower() == \"yes\",\n                lambda config: True,\n            ),\n            (\n                \"dependent_question\",\n                \"default_value\",\n                str,\n                \"Test Question 2\",\n                None,\n                None,\n                lambda config: config.get(\"first_question\", False),\n            ),\n        ]\n        args = argparse.Namespace(\n            site_config=\"tests/files/site-specific\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        site_config.desc = questions\n\n        def auto_prompt(prompt, default, **kwargs):\n            return default\n\n        with mock.patch(\"prompt_toolkit.prompt\", side_effect=auto_prompt):\n            config = site_config.user_prompt_config()\n            assert config[\"dependent_question\"] != \"default_value\"\n\n            edited_first_question = list(site_config.desc[0])\n            edited_first_question[1] = True\n            site_config.desc[0] = tuple(edited_first_question)\n\n            config = site_config.user_prompt_config()\n            assert config[\"dependent_question\"] == \"default_value\"\n\n    verify_prompt_ssh_users = verify_desc_consistency\n    verify_prompt_app_ip = verify_desc_consistency\n    verify_prompt_monitor_ip = verify_desc_consistency\n    verify_prompt_app_hostname = verify_desc_consistency\n    verify_prompt_monitor_hostname = verify_desc_consistency\n    verify_prompt_dns_server = verify_desc_consistency\n\n    verify_prompt_securedrop_app_pow_on_source_interface = verify_prompt_boolean\n    verify_prompt_securedrop_app_https_on_source_interface = verify_prompt_boolean\n    verify_prompt_enable_ssh_over_tor = verify_prompt_boolean\n\n    verify_prompt_securedrop_app_gpg_public_key = verify_desc_consistency\n\n    def verify_prompt_not_empty(self, site_config, desc):\n        with pytest.raises(ValidationError):\n            site_config.user_prompt_config_one(desc, \"\")\n\n    def verify_prompt_fingerprint_optional(self, site_config, desc):\n        fpr = \"0123456 789012 34567890123456789ABCDEFABCD\"\n        clean_fpr = site_config.sanitize_fingerprint(fpr)\n        assert site_config.user_prompt_config_one(desc, fpr) == clean_fpr\n\n    def verify_desc_consistency_allow_empty(self, site_config, desc):\n        (var, default, etype, prompt, validator, transform, condition) = desc\n        # verify the default passes validation\n        assert site_config.user_prompt_config_one(desc, None) == default\n        assert type(default) is etype\n\n    def verify_prompt_fingerprint(self, site_config, desc):\n        self.verify_prompt_not_empty(site_config, desc)\n        self.verify_prompt_fingerprint_optional(site_config, desc)\n\n    verify_prompt_securedrop_app_gpg_fingerprint = verify_prompt_fingerprint\n    verify_prompt_ossec_alert_gpg_public_key = verify_desc_consistency\n    verify_prompt_ossec_gpg_fpr = verify_prompt_fingerprint\n    verify_prompt_ossec_alert_email = verify_prompt_not_empty\n    verify_prompt_journalist_alert_gpg_public_key = verify_desc_consistency_optional\n    verify_prompt_journalist_gpg_fpr = verify_prompt_fingerprint_optional\n    verify_prompt_journalist_alert_email = verify_desc_consistency_optional\n    verify_prompt_securedrop_app_https_certificate_chain_src = verify_desc_consistency_optional\n    verify_prompt_securedrop_app_https_certificate_key_src = verify_desc_consistency_optional\n    verify_prompt_securedrop_app_https_certificate_cert_src = verify_desc_consistency_optional\n    verify_prompt_smtp_relay = verify_prompt_not_empty\n    verify_prompt_smtp_relay_port = verify_desc_consistency\n    verify_prompt_daily_reboot_time = verify_desc_consistency\n    verify_prompt_sasl_domain = verify_desc_consistency_allow_empty\n    verify_prompt_sasl_username = verify_prompt_not_empty\n    verify_prompt_sasl_password = verify_prompt_not_empty\n\n    def verify_prompt_securedrop_supported_locales(self, site_config, desc):\n        (var, default, etype, prompt, validator, transform, condition) = desc\n        # verify the default passes validation\n        assert site_config.user_prompt_config_one(desc, None) == default\n        assert type(default) is etype\n        assert site_config.user_prompt_config_one(desc, \"fr_FR en_US\") == [\"fr_FR\", \"en_US\"]\n        assert site_config.user_prompt_config_one(desc, [\"fr_FR\", \"en_US\"]) == [\"fr_FR\", \"en_US\"]\n        assert site_config.user_prompt_config_one(desc, \"\") == []\n        with pytest.raises(ValidationError):\n            site_config.user_prompt_config_one(desc, \"wrong\")\n\n    def test_user_prompt_config_one(self, tmpdir):\n        args = argparse.Namespace(\n            site_config=\"UNKNOWN\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n\n        def auto_prompt(prompt, default, **kwargs):\n            if \"validator\" in kwargs and kwargs[\"validator\"]:\n                assert kwargs[\"validator\"].validate(Document(default))\n            return default\n\n        with mock.patch(\"prompt_toolkit.prompt\", side_effect=auto_prompt):\n            for desc in site_config.desc:\n                (var, default, etype, prompt, validator, transform, condition) = desc\n                method = \"verify_prompt_\" + var\n                print(\"checking \" + method)\n                getattr(self, method)(site_config, desc)\n\n    def test_validated_input(self, tmpdir):\n        args = argparse.Namespace(\n            site_config=\"UNKNOWN\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n\n        def auto_prompt(prompt, default, **kwargs):\n            return default\n\n        with mock.patch(\"prompt_toolkit.prompt\", side_effect=auto_prompt):\n            value = \"VALUE\"\n            assert value == site_config.validated_input(\"\", value, lambda: True, None)\n            assert value.lower() == site_config.validated_input(\"\", value, lambda: True, str.lower)\n            assert site_config.validated_input(\"\", True, lambda: True, None) == \"yes\"\n            assert site_config.validated_input(\"\", False, lambda: True, None) == \"no\"\n            assert site_config.validated_input(\"\", 1234, lambda: True, None) == \"1234\"\n            assert site_config.validated_input(\"\", [\"a\", \"b\"], lambda: True, None) == \"a b\"\n            assert site_config.validated_input(\"\", {}, lambda: True, None) == \"{}\"\n\n    def test_load(self, tmpdir, caplog):\n        args = argparse.Namespace(\n            site_config=\"tests/files/site-specific\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        assert \"app_hostname\" in site_config.load()\n\n        args = argparse.Namespace(\n            site_config=\"UNKNOWN\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        with pytest.raises(IOError) as e:\n            site_config.load()\n        assert \"No such file\" in e.value.strerror\n        assert \"Config file missing\" in caplog.text\n\n        args = argparse.Namespace(\n            site_config=\"tests/files/corrupted\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        with pytest.raises(yaml.YAMLError) as e:\n            site_config.load()\n        assert \"issue processing\" in caplog.text",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_exists",
          "code": "def test_exists(self, tmpdir):\n        args = argparse.Namespace(\n            site_config=\"DOES_NOT_EXIST\",\n            ansible_path=\".\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        assert not securedrop_admin.SiteConfig(args).exists()\n        args = argparse.Namespace(\n            site_config=__file__, ansible_path=\".\", app_path=dirname(__file__), root=tmpdir\n        )\n        assert securedrop_admin.SiteConfig(args).exists()",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_validate_not_empty",
          "code": "def test_validate_not_empty(self):\n        validator = securedrop_admin.SiteConfig.ValidateNotEmpty()\n\n        assert validator.validate(Document(\"something\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"\"))",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_validate_time",
          "code": "def test_validate_time(self):\n        validator = securedrop_admin.SiteConfig.ValidateTime()\n\n        assert validator.validate(Document(\"4\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"four\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"4.30\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"25\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"-4\"))",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_validate_ossec_username",
          "code": "def test_validate_ossec_username(self):\n        validator = securedrop_admin.SiteConfig.ValidateOSSECUsername()\n\n        assert validator.validate(Document(\"username\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"bad@user\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"test\"))",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_validate_ossec_password",
          "code": "def test_validate_ossec_password(self):\n        validator = securedrop_admin.SiteConfig.ValidateOSSECPassword()\n\n        assert validator.validate(Document(\"goodpassword\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"password123\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"short\"))",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_validate_email",
          "code": "def test_validate_email(self):\n        validator = securedrop_admin.SiteConfig.ValidateEmail()\n\n        assert validator.validate(Document(\"good@mail.com\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"badmail\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"\"))",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_validate_ossec_email",
          "code": "def test_validate_ossec_email(self):\n        validator = securedrop_admin.SiteConfig.ValidateOSSECEmail()\n\n        assert validator.validate(Document(\"good@mail.com\"))\n        with pytest.raises(ValidationError) as e:\n            validator.validate(Document(\"ossec@ossec.test\"))\n        assert \"something other than ossec@ossec.test\" in str(e)",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_validate_optional_email",
          "code": "def test_validate_optional_email(self):\n        validator = securedrop_admin.SiteConfig.ValidateOptionalEmail()\n\n        assert validator.validate(Document(\"good@mail.com\"))\n        assert validator.validate(Document(\"\"))",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_validate_user",
          "code": "def test_validate_user(self):\n        validator = securedrop_admin.SiteConfig.ValidateUser()\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"amnesia\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"root\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"\"))\n        assert validator.validate(Document(\"gooduser\"))",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_validate_ip",
          "code": "def test_validate_ip(self):\n        validator = securedrop_admin.SiteConfig.ValidateIP()\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"599.20\"))\n        assert validator.validate(Document(\"192.168.1.1\"))",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_validate_path",
          "code": "def test_validate_path(self):\n        mydir = dirname(__file__)\n        myfile = basename(__file__)\n        validator = securedrop_admin.SiteConfig.ValidatePath(mydir)\n        assert validator.validate(Document(myfile))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"NONEXIST\"))\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"\"))",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_validate_optional_path",
          "code": "def test_validate_optional_path(self):\n        mydir = dirname(__file__)\n        myfile = basename(__file__)\n        validator = securedrop_admin.SiteConfig.ValidateOptionalPath(mydir)\n        assert validator.validate(Document(myfile))\n        assert validator.validate(Document(\"\"))",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_validate_yes_no",
          "code": "def test_validate_yes_no(self):\n        validator = securedrop_admin.SiteConfig.ValidateYesNo()\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"something\"))\n        assert validator.validate(Document(\"yes\"))\n        assert validator.validate(Document(\"YES\"))\n        assert validator.validate(Document(\"no\"))\n        assert validator.validate(Document(\"NO\"))",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_validate_fingerprint",
          "code": "def test_validate_fingerprint(self):\n        validator = securedrop_admin.SiteConfig.ValidateFingerprint()\n        assert validator.validate(Document(\"012345678901234567890123456789ABCDEFABCD\"))\n        assert validator.validate(Document(\"01234 5678901234567890123456789ABCDE   FABCD\"))\n\n        with pytest.raises(ValidationError) as e:\n            validator.validate(Document(\"65A1B5FF195B56353CC63DFFCC40EF1228271441\"))\n        assert \"TEST journalist\" in str(e)\n\n        with pytest.raises(ValidationError) as e:\n            validator.validate(Document(\"600BC6D5142C68F35DDBCEA87B597104EDDDC102\"))\n        assert \"TEST admin\" in str(e)\n\n        with pytest.raises(ValidationError) as e:\n            validator.validate(Document(\"0000\"))\n        assert \"40 hexadecimal\" in str(e)\n\n        with pytest.raises(ValidationError) as e:\n            validator.validate(Document(\"zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\"))\n        assert \"40 hexadecimal\" in str(e)",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_validate_optional_fingerprint",
          "code": "def test_validate_optional_fingerprint(self):\n        validator = securedrop_admin.SiteConfig.ValidateOptionalFingerprint()\n        assert validator.validate(Document(\"012345678901234567890123456789ABCDEFABCD\"))\n        assert validator.validate(Document(\"\"))",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_sanitize_fingerprint",
          "code": "def test_sanitize_fingerprint(self, tmpdir):\n        args = argparse.Namespace(\n            site_config=\"DOES_NOT_EXIST\",\n            ansible_path=\".\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        assert site_config.sanitize_fingerprint(\"    A bc\") == \"ABC\"",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_validate_int",
          "code": "def test_validate_int(self):\n        validator = securedrop_admin.SiteConfig.ValidateInt()\n        with pytest.raises(ValidationError):\n            validator.validate(Document(\"123X\"))\n        assert validator.validate(Document(\"192\"))",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_locales",
          "code": "def test_locales(self):\n        locales = securedrop_admin.SiteConfig.Locales(dirname(__file__))\n        translations = locales.get_translations()\n        assert \"en_US\" in translations\n        assert \"fr_FR\" in translations",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_validate_locales",
          "code": "def test_validate_locales(self):\n        validator = securedrop_admin.SiteConfig.ValidateLocales(\n            dirname(__file__), {\"en_US\", \"fr_FR\"}\n        )\n        assert validator.validate(Document(\"en_US  fr_FR \"))\n        with pytest.raises(ValidationError) as e:\n            validator.validate(Document(\"BAD\"))\n        assert \"BAD\" in str(e)",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_save",
          "code": "def test_save(self, tmpdir):\n        site_config_path = join(str(tmpdir), \"site_config\")\n        args = argparse.Namespace(\n            site_config=site_config_path,\n            ansible_path=\".\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        site_config.config = {\"var1\": \"val1\", \"var2\": \"val2\"}\n        site_config.save()\n        expected = textwrap.dedent(\n            \"\"\"\\\n        var1: val1\n        var2: val2\n        \"\"\"\n        )\n        assert expected == open(site_config_path).read()",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_validate_gpg_key",
          "code": "def test_validate_gpg_key(self, tmpdir, caplog):\n        args = argparse.Namespace(\n            site_config=\"INVALID\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        good_config = {\n            \"securedrop_app_gpg_public_key\": \"test_journalist_key.pub\",\n            \"securedrop_app_gpg_fingerprint\": \"65A1B5FF195B56353CC63DFFCC40EF1228271441\",\n            \"ossec_alert_gpg_public_key\": \"test_journalist_key.pub\",\n            \"ossec_gpg_fpr\": \"65A1B5FF195B56353CC63DFFCC40EF1228271441\",\n            \"journalist_alert_gpg_public_key\": \"test_journalist_key.pub\",\n            \"journalist_gpg_fpr\": \"65A1B5FF195B56353CC63DFFCC40EF1228271441\",\n        }\n        site_config = securedrop_admin.SiteConfig(args)\n        site_config.config = good_config\n        assert site_config.validate_gpg_keys()\n\n        for key in (\"securedrop_app_gpg_fingerprint\", \"ossec_gpg_fpr\", \"journalist_gpg_fpr\"):\n            bad_config = good_config.copy()\n            bad_config[key] = \"FAIL\"\n            site_config.config = bad_config\n            with pytest.raises(securedrop_admin.FingerprintException) as e:\n                site_config.validate_gpg_keys()\n            assert \"FAIL does not match\" in str(e)\n\n        # Test a key with matching fingerprint but that fails sq-keyring-linter\n        invalid_config = {\n            # Correct key fingerprint but weak 1024-bit RSA key with SHA-1 self signature\n            \"securedrop_app_gpg_public_key\": \"weak_test_key_should_fail_sqlinter.asc\",\n            \"securedrop_app_gpg_fingerprint\": \"40F1C17B7E7826DAB40B14AE7786B000E6D0A76E\",\n            \"ossec_alert_gpg_public_key\": \"test_journalist_key.pub\",\n            \"ossec_gpg_fpr\": \"65A1B5FF195B56353CC63DFFCC40EF1228271441\",\n            \"journalist_alert_gpg_public_key\": \"test_journalist_key.pub\",\n            \"journalist_gpg_fpr\": \"65A1B5FF195B56353CC63DFFCC40EF1228271441\",\n        }\n        site_config.config = invalid_config\n        with pytest.raises(securedrop_admin.FingerprintException) as e:\n            site_config.validate_gpg_keys()\n        assert \"failed sq-keyring-linter check\" in str(e)",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_journalist_alert_email",
          "code": "def test_journalist_alert_email(self, tmpdir):\n        args = argparse.Namespace(\n            site_config=\"INVALID\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        site_config.config = {\n            \"journalist_alert_gpg_public_key\": \"\",\n            \"journalist_gpg_fpr\": \"\",\n        }\n        assert site_config.validate_journalist_alert_email()\n        site_config.config = {\n            \"journalist_alert_gpg_public_key\": \"test_journalist_key.pub\",\n            \"journalist_gpg_fpr\": \"65A1B5FF195B56353CC63DFFCC40EF1228271441\",\n        }\n        site_config.config[\"journalist_alert_email\"] = \"\"\n        with pytest.raises(securedrop_admin.JournalistAlertEmailException) as e:\n            site_config.validate_journalist_alert_email()\n        assert \"not be empty\" in str(e)\n\n        site_config.config[\"journalist_alert_email\"] = \"bademail\"\n        with pytest.raises(securedrop_admin.JournalistAlertEmailException) as e:\n            site_config.validate_journalist_alert_email()\n        assert \"Must contain a @\" in str(e)\n\n        site_config.config[\"journalist_alert_email\"] = \"good@email.com\"\n        assert site_config.validate_journalist_alert_email()",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_update_config",
          "code": "def test_update_config(self, mock_save, mock_validate_input):\n        args = argparse.Namespace(\n            site_config=\"tests/files/site-specific\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=\"tests/files\",\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n\n        assert site_config.load_and_update_config()\n        assert \"user_defined_variable\" in site_config.config\n        mock_save.assert_called_once()\n        mock_validate_input.assert_called()",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_update_config_no_site_specific",
          "code": "def test_update_config_no_site_specific(self, validate_gpg_keys, mock_validate_input, tmpdir):\n        site_config_path = join(str(tmpdir), \"site_config\")\n        args = argparse.Namespace(\n            site_config=site_config_path,\n            ansible_path=\".\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        assert site_config.load_and_update_config()\n        mock_validate_input.assert_called()\n        validate_gpg_keys.assert_called_once()\n        assert exists(site_config_path)",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_load_and_update_config",
          "code": "def test_load_and_update_config(self, tmpdir):\n        args = argparse.Namespace(\n            site_config=\"tests/files/site-specific\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        with mock.patch(\"securedrop_admin.SiteConfig.update_config\"):\n            site_config.load_and_update_config()\n            assert site_config.config != {}\n\n        args = argparse.Namespace(\n            site_config=\"tests/files/site-specific-missing-entries\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        with mock.patch(\"securedrop_admin.SiteConfig.update_config\"):\n            site_config.load_and_update_config()\n            assert site_config.config != {}\n\n        args = argparse.Namespace(\n            site_config=\"UNKNOWN\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        with mock.patch(\"securedrop_admin.SiteConfig.update_config\"):\n            site_config.load_and_update_config()\n            assert site_config.config == {}",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "get_desc",
          "code": "def get_desc(self, site_config, var):\n        for desc in site_config.desc:\n            if desc[0] == var:\n                return desc",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "verify_desc_consistency_optional",
          "code": "def verify_desc_consistency_optional(self, site_config, desc):\n        (var, default, etype, prompt, validator, transform, condition) = desc\n        # verify the default passes validation\n        if callable(default):\n            default = default()\n        assert site_config.user_prompt_config_one(desc, None) == default\n        assert type(default) is etype",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "verify_desc_consistency",
          "code": "def verify_desc_consistency(self, site_config, desc):\n        self.verify_desc_consistency_optional(site_config, desc)",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "verify_prompt_boolean",
          "code": "def verify_prompt_boolean(self, site_config, desc):\n        self.verify_desc_consistency(site_config, desc)\n        (var, default, etype, prompt, validator, transform, condition) = desc\n        assert site_config.user_prompt_config_one(desc, True) is True\n        assert site_config.user_prompt_config_one(desc, False) is False\n        assert site_config.user_prompt_config_one(desc, \"YES\") is True\n        assert site_config.user_prompt_config_one(desc, \"NO\") is False",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_desc_conditional",
          "code": "def test_desc_conditional(self, tmpdir):\n        \"\"\"Ensure that conditional prompts behave correctly.\n\n        Prompts which depend on another question should only be\n        asked if the prior question was answered appropriately.\"\"\"\n\n        questions = [\n            (\n                \"first_question\",\n                False,\n                bool,\n                \"Test Question 1\",\n                None,\n                lambda x: x.lower() == \"yes\",\n                lambda config: True,\n            ),\n            (\n                \"dependent_question\",\n                \"default_value\",\n                str,\n                \"Test Question 2\",\n                None,\n                None,\n                lambda config: config.get(\"first_question\", False),\n            ),\n        ]\n        args = argparse.Namespace(\n            site_config=\"tests/files/site-specific\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        site_config.desc = questions\n\n        def auto_prompt(prompt, default, **kwargs):\n            return default\n\n        with mock.patch(\"prompt_toolkit.prompt\", side_effect=auto_prompt):\n            config = site_config.user_prompt_config()\n            assert config[\"dependent_question\"] != \"default_value\"\n\n            edited_first_question = list(site_config.desc[0])\n            edited_first_question[1] = True\n            site_config.desc[0] = tuple(edited_first_question)\n\n            config = site_config.user_prompt_config()\n            assert config[\"dependent_question\"] == \"default_value\"",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "auto_prompt",
          "code": "def auto_prompt(prompt, default, **kwargs):\n            return default",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "verify_prompt_not_empty",
          "code": "def verify_prompt_not_empty(self, site_config, desc):\n        with pytest.raises(ValidationError):\n            site_config.user_prompt_config_one(desc, \"\")",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "verify_prompt_fingerprint_optional",
          "code": "def verify_prompt_fingerprint_optional(self, site_config, desc):\n        fpr = \"0123456 789012 34567890123456789ABCDEFABCD\"\n        clean_fpr = site_config.sanitize_fingerprint(fpr)\n        assert site_config.user_prompt_config_one(desc, fpr) == clean_fpr",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "verify_desc_consistency_allow_empty",
          "code": "def verify_desc_consistency_allow_empty(self, site_config, desc):\n        (var, default, etype, prompt, validator, transform, condition) = desc\n        # verify the default passes validation\n        assert site_config.user_prompt_config_one(desc, None) == default\n        assert type(default) is etype",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "verify_prompt_fingerprint",
          "code": "def verify_prompt_fingerprint(self, site_config, desc):\n        self.verify_prompt_not_empty(site_config, desc)\n        self.verify_prompt_fingerprint_optional(site_config, desc)",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "verify_prompt_securedrop_supported_locales",
          "code": "def verify_prompt_securedrop_supported_locales(self, site_config, desc):\n        (var, default, etype, prompt, validator, transform, condition) = desc\n        # verify the default passes validation\n        assert site_config.user_prompt_config_one(desc, None) == default\n        assert type(default) is etype\n        assert site_config.user_prompt_config_one(desc, \"fr_FR en_US\") == [\"fr_FR\", \"en_US\"]\n        assert site_config.user_prompt_config_one(desc, [\"fr_FR\", \"en_US\"]) == [\"fr_FR\", \"en_US\"]\n        assert site_config.user_prompt_config_one(desc, \"\") == []\n        with pytest.raises(ValidationError):\n            site_config.user_prompt_config_one(desc, \"wrong\")",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_user_prompt_config_one",
          "code": "def test_user_prompt_config_one(self, tmpdir):\n        args = argparse.Namespace(\n            site_config=\"UNKNOWN\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n\n        def auto_prompt(prompt, default, **kwargs):\n            if \"validator\" in kwargs and kwargs[\"validator\"]:\n                assert kwargs[\"validator\"].validate(Document(default))\n            return default\n\n        with mock.patch(\"prompt_toolkit.prompt\", side_effect=auto_prompt):\n            for desc in site_config.desc:\n                (var, default, etype, prompt, validator, transform, condition) = desc\n                method = \"verify_prompt_\" + var\n                print(\"checking \" + method)\n                getattr(self, method)(site_config, desc)",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "auto_prompt",
          "code": "def auto_prompt(prompt, default, **kwargs):\n            if \"validator\" in kwargs and kwargs[\"validator\"]:\n                assert kwargs[\"validator\"].validate(Document(default))\n            return default",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_validated_input",
          "code": "def test_validated_input(self, tmpdir):\n        args = argparse.Namespace(\n            site_config=\"UNKNOWN\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n\n        def auto_prompt(prompt, default, **kwargs):\n            return default\n\n        with mock.patch(\"prompt_toolkit.prompt\", side_effect=auto_prompt):\n            value = \"VALUE\"\n            assert value == site_config.validated_input(\"\", value, lambda: True, None)\n            assert value.lower() == site_config.validated_input(\"\", value, lambda: True, str.lower)\n            assert site_config.validated_input(\"\", True, lambda: True, None) == \"yes\"\n            assert site_config.validated_input(\"\", False, lambda: True, None) == \"no\"\n            assert site_config.validated_input(\"\", 1234, lambda: True, None) == \"1234\"\n            assert site_config.validated_input(\"\", [\"a\", \"b\"], lambda: True, None) == \"a b\"\n            assert site_config.validated_input(\"\", {}, lambda: True, None) == \"{}\"",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "auto_prompt",
          "code": "def auto_prompt(prompt, default, **kwargs):\n            return default",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_load",
          "code": "def test_load(self, tmpdir, caplog):\n        args = argparse.Namespace(\n            site_config=\"tests/files/site-specific\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        assert \"app_hostname\" in site_config.load()\n\n        args = argparse.Namespace(\n            site_config=\"UNKNOWN\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        with pytest.raises(IOError) as e:\n            site_config.load()\n        assert \"No such file\" in e.value.strerror\n        assert \"Config file missing\" in caplog.text\n\n        args = argparse.Namespace(\n            site_config=\"tests/files/corrupted\",\n            ansible_path=\"tests/files\",\n            app_path=dirname(__file__),\n            root=tmpdir,\n        )\n        site_config = securedrop_admin.SiteConfig(args)\n        with pytest.raises(yaml.YAMLError) as e:\n            site_config.load()\n        assert \"issue processing\" in caplog.text",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_generate_new_v3_keys",
          "code": "def test_generate_new_v3_keys():\n    public, private = securedrop_admin.generate_new_v3_keys()\n\n    for key in [public, private]:\n        # base32 padding characters should be removed\n        assert \"=\" not in key\n        assert len(key) == 52",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_find_or_generate_new_torv3_keys_first_run",
          "code": "def test_find_or_generate_new_torv3_keys_first_run(tmpdir, capsys):\n    args = argparse.Namespace(ansible_path=str(tmpdir))\n\n    return_code = securedrop_admin.find_or_generate_new_torv3_keys(args)\n\n    out, err = capsys.readouterr()\n    assert \"Tor v3 onion service keys generated\" in out\n    assert return_code == 0\n\n    secret_key_path = os.path.join(args.ansible_path, \"tor_v3_keys.json\")\n\n    with open(secret_key_path) as f:\n        v3_onion_service_keys = json.load(f)\n\n    expected_keys = [\n        \"app_journalist_public_key\",\n        \"app_journalist_private_key\",\n        \"app_ssh_public_key\",\n        \"app_ssh_private_key\",\n        \"mon_ssh_public_key\",\n        \"mon_ssh_private_key\",\n    ]\n    for key in expected_keys:\n        assert key in v3_onion_service_keys",
          "file": "test_securedrop-admin.py"
        },
        {
          "type": "function",
          "name": "test_find_or_generate_new_torv3_keys_subsequent_run",
          "code": "def test_find_or_generate_new_torv3_keys_subsequent_run(tmpdir, capsys):\n    args = argparse.Namespace(ansible_path=str(tmpdir))\n\n    secret_key_path = os.path.join(args.ansible_path, \"tor_v3_keys.json\")\n    old_keys = {\"foo\": \"bar\"}\n    with open(secret_key_path, \"w\") as f:\n        json.dump(old_keys, f)\n\n    return_code = securedrop_admin.find_or_generate_new_torv3_keys(args)\n\n    out, err = capsys.readouterr()\n    assert \"Tor v3 onion service keys already exist\" in out\n    assert return_code == 0\n\n    with open(secret_key_path) as f:\n        v3_onion_service_keys = json.load(f)\n\n    assert v3_onion_service_keys == old_keys",
          "file": "test_securedrop-admin.py"
        }
      ]
    }
  }
}