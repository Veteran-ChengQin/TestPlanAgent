PR_TEST_PLAN_EDIT_SYSTEM_PROMPT = """
You are a software test manager. Your task is to write the test execution steps in the test plan for the pull request (PR).

You can use the following tools to help you get useful information for writing test execution steps:

# Tools

## Tool_1: search_entity_in_CKG
You are allowed to search for information about an entity(class or method) from the code knowledge graph. The entity information includes entity name, entity type, file to which it belongs, number of lines in the file and codes.
When you search for information about an entity(class or method), please format your arguments as a JSON according to the following schema:
{"entity_name": "the name of the class/method"}
e.g. {"entity_name": "get_user_name"}

## Tool_2: search_neighbors_of_entity_in_CKG
You are allowed to search for other entities (classes or methods) referenced by the target entity(class or method) or other entities (classes or methods) that reference the target entity (class or method) from the code knowledge graph.
When you search for neighbors of an entity(class or method), please format your arguments as a JSON according to the following schema:
{"entity_name": "the name of the class/method"}
e.g. {"entity_name": "get_user_id"}

## Tool_3: find_file
You are allowed to finds files or directories matching the given pattern in the workspace.
When you find files, please format your arguments as a JSON according to the following schema:
{"pattern": "the pattern of a file"}
You can use absolute paths or paths containing wildcards. Here are two examples:
{"pattern": "/home/user/project/main.py"}
{"pattern": "*main.py}

## Tool_4: view_file
You are allowed to view a file based on the provided file path.
When you view a file, please format your arguments as a JSON according to the following schema:
{"file_path": "the path of the file"}
e.g. {"file_path: "/home/user/project/test.py"}

The tool names, descriptions of their functions, and a JSON scheme for passing parameters to the tool are provided. 
When you decide you need to use a tool, give the tool name on the first line, followed by the JSON scheme for the tool's input. For example:
```
find_file
{
    "path": "*main.py
}
```

You should call above tools with thought, and the thought serve as a guide for which tool to call, e.g.

### Thought: I need to see information about the get_user_name method because it is closely related to the subject of the PR change.

### Action:
```
search_entity_in_CKG
{
    "entity_name": "get_user_name"
}
```

When you feel you have gathered enough information to complete the test steps, please provide your conclusion in the following format:

### Thought: I have gathered enough information. The information in the conversation is enough to complete the writing of the test steps.

### Test Steps:
```
<test steps>
```

# TIPS:
- Please read the information in the conversation carefully and use a tool `ONLY` when you are sure you need it.
- Please think and decide step by step which tool to use to get enough information, and only include `ONE` tool in your answer round!
- Please do not use any tools other than those mentioned above!
- Please follow the format I provided for your answer. Do not put some reasoning before `Thought:`, but put it all after `Thought:`, as I said above.
- Please think hard and do not stop acquiring information until you have enough information. Write test steps only when you think you have collected enough information.
- When writing test steps, strive to make the content accurate, concise and neat.

"""

PR_TEST_PLAN_EDIT_USER_PROMPT = """
Please help me complete the test steps for this pull request (PR). The body of the pull request and the changed files are as follows:

Body of the current pull request (PR):
feat(migrations): add migration order validator SNS-1831. Adds a script to validate the order of migration and introduce a test in CI that checks existing migrations\r\n\r\n## Context\r\n#3324 added the option to set flags to specify order, this PR introduces a checker to validate the specified order does not introduce errors. The checker uses the following spec:\r\n\r\n1. For two `AddColumn` OPs with the same target table and target column, the local OP must be applied first\r\n2. For two `CreateTable` OPs with the same target table name, the local OP must be applied first\r\n3. For two `DropColumn` OPs with the same target table and target column, the dist OP must be applied first\r\n\r\nThe target table for distributed ops is then extracted either by looking at the engine attribute in the case of `CreateTable`, or by parsing the the local table name from the table engine in Clickhouse via querying `SELECT engine_full FROM system.tables WHERE name = table_name` for `AddColumn`,`DropColumn` ops. For distributed tables, the target is the third argument in the `Distributed()` engine_full value.\r\n\r\n## Before\r\nWe wouldn't be able to detect if the wrong order was specified in migrations\r\n\r\n## After\r\nA checker script is run that can catch some errors when the wrong migration operation order is specified\r\n\r\n##  Blast Radius\r\nThe validator is an independent script in the migrations package. It doesn't affect the migrations themselves, but this PR add a test to check the existing migrations. \r\n\r\nTo make it easier to implement, we make public some attributes of `SqlOperations` such as the table name and column.\r\n\r\nFor migrations `0004_drop_profile_column.py`,  `0016_drop_legacy_events` and `0014_transactions_remove_flattened_columns.py` we corrected the order flags as the checker detected errors.\r\n\r\n"

Diff of the current pull request (PR):
\nFile: snuba/migrations/operations.py\nMetadata: index aefff164d7a..2b8380cbae9 100644\n\n\nChunk @@ -60,16 +60,18 @@ def __init__(\n  60:         engine: TableEngine,\n  61:     ):\n  62:         super().__init__(storage_set)\n-   :         self.__table_name = table_name\n+ 63:         self.table_name = table_name\n  64:         self.__columns = columns\n-   :         self.__engine = engine\n+ 65:         self.engine = engine\n  66: \n  67:     def format_sql(self) -> str:\n  68:         columns = \", \".join([col.for_schema() for col in self.__columns])\n  69:         cluster = get_cluster(self._storage_set)\n-   :         engine = self.__engine.get_sql(cluster, self.__table_name)\n+ 70:         engine = self.engine.get_sql(cluster, self.table_name)\n  71: \n-   :         return f\"CREATE TABLE IF NOT EXISTS {self.__table_name} ({columns}) ENGINE {engine};\"\n+ 72:         return (\n+ 73:             f\"CREATE TABLE IF NOT EXISTS {self.table_name} ({columns}) ENGINE {engine};\"\n+ 74:         )\n  75: \n  76: \n  77: class CreateMaterializedView(SqlOperation):\n\nChunk @@ -188,14 +190,14 @@ def __init__(\n  190:         after: Optional[str] = None,\n  191:     ):\n  192:         super().__init__(storage_set)\n-    :         self.__table_name = table_name\n-    :         self.__column = column\n+ 193:         self.table_name = table_name\n+ 194:         self.column = column\n  195:         self.__after = after\n  196: \n  197:     def format_sql(self) -> str:\n-    :         column = self.__column.for_schema()\n+ 198:         column = self.column.for_schema()\n  199:         optional_after_clause = f\" AFTER {self.__after}\" if self.__after else \"\"\n-    :         return f\"ALTER TABLE {self.__table_name} ADD COLUMN IF NOT EXISTS {column}{optional_after_clause};\"\n+ 200:         return f\"ALTER TABLE {self.table_name} ADD COLUMN IF NOT EXISTS {column}{optional_after_clause};\"\n  201: \n  202: \n  203: class DropColumn(SqlOperation):\n\nChunk @@ -211,11 +213,13 @@ class DropColumn(SqlOperation):\n  213: \n  214:     def __init__(self, storage_set: StorageSetKey, table_name: str, column_name: str):\n  215:         super().__init__(storage_set)\n-    :         self.__table_name = table_name\n-    :         self.__column_name = column_name\n+ 216:         self.table_name = table_name\n+ 217:         self.column_name = column_name\n  218: \n  219:     def format_sql(self) -> str:\n-    :         return f\"ALTER TABLE {self.__table_name} DROP COLUMN IF EXISTS {self.__column_name};\"\n+ 220:         return (\n+ 221:             f\"ALTER TABLE {self.table_name} DROP COLUMN IF EXISTS {self.column_name};\"\n+ 222:         )\n  223: \n  224: \n  225: class ModifyColumn(SqlOperation):\n-    : -- /dev/null\n+ 226: ++ b/snuba/migrations/validator.py\n\nFile: snuba/migrations/validator.py\nMetadata: index 00000000000..44552af22e0\n\n\nChunk @@ -0,0 +1,199 @@\n  + 1: import re\n  + 2: from typing import Sequence, Union\n  + 3: \n  + 4: from snuba.clickhouse.native import ClickhousePool\n  + 5: from snuba.clusters.cluster import ClickhouseClientSettings, get_cluster\n  + 6: from snuba.migrations.migration import ClickhouseNodeMigration\n  + 7: from snuba.migrations.operations import AddColumn, CreateTable, DropColumn, SqlOperation\n  + 8: \n  + 9: ENGINE_REGEX = r\"^Distributed(\\(.+\\))$\"\n + 10: \n + 11: \n + 12: class InvalidMigrationOrderError(Exception):\n + 13:     \"\"\"\n + 14:     Raised when a migration ops are not in the correct order.\n + 15:     \"\"\"\n + 16: \n + 17:     pass\n + 18: \n + 19: \n + 20: class NoDistClusterNodes(Exception):\n + 21:     \"\"\"\n + 22:     The migration has no distributed cluster nodes, so we can't validate the order.\n + 23:     \"\"\"\n + 24: \n + 25:     pass\n + 26: \n + 27: \n + 28: class DistributedEngineParseError(Exception):\n + 29:     \"\"\"\n + 30:     Cant parser the distributed engine string value\n + 31:     \"\"\"\n + 32: \n + 33:     pass\n + 34: \n + 35: \n + 36: def validate_migration_order(migration: ClickhouseNodeMigration) -> None:\n + 37:     \"\"\"\n + 38:     Validates that the migration order is correct. Checks that the order of\n + 39:     AddColumn, CreateTable and DropColumn operations are correct with regards to being\n + 40:     applied on local and distributed tables.\n + 41:     \"\"\"\n + 42: \n + 43:     def conflicts_ops(local_op: SqlOperation, dist_op: SqlOperation) -> bool:\n + 44:         if isinstance(local_op, CreateTable) and isinstance(dist_op, CreateTable):\n + 45:             return conflicts_create_table_op(local_op, dist_op)\n + 46:         elif isinstance(local_op, AddColumn) and isinstance(dist_op, AddColumn):\n + 47:             return conflicts_add_column_op(local_op, dist_op)\n + 48:         elif isinstance(local_op, DropColumn) and isinstance(dist_op, DropColumn):\n + 49:             return conflicts_drop_column_op(local_op, dist_op)\n + 50:         return False\n + 51: \n + 52:     def validate_add_col_or_create_table(\n + 53:         local_op: SqlOperation, dist_ops: Sequence[SqlOperation]\n + 54:     ) -> None:\n + 55:         if isinstance(local_op, (CreateTable, AddColumn)):\n + 56:             if any(conflicts_ops(local_op, dist_op) for dist_op in dist_ops):\n + 57:                 op_name = (\n + 58:                     f\"{local_op.table_name}.{local_op.column.name}\"\n + 59:                     if isinstance(local_op, AddColumn)\n + 60:                     else local_op.table_name\n + 61:                 )\n + 62:                 raise InvalidMigrationOrderError(\n + 63:                     f\"{type(local_op).__name__} {op_name} operation \"\n + 64:                     \"must be applied on local table before dist\"\n + 65:                 )\n + 66: \n + 67:     def validate_drop(dist_op: SqlOperation, local_ops: Sequence[SqlOperation]) -> None:\n + 68:         if isinstance(dist_op, (DropColumn)):\n + 69:             if any(conflicts_ops(local_op, dist_op) for local_op in local_ops):\n + 70:                 raise InvalidMigrationOrderError(\n + 71:                     f\"{type(dist_op).__name__} {dist_op.table_name}.{dist_op.column_name} \"\n + 72:                     \"operation must be applied on dist table before local\"\n + 73:                 )\n + 74: \n + 75:     def validate_order(\n + 76:         local_ops: Sequence[SqlOperation],\n + 77:         dist_ops: Sequence[SqlOperation],\n + 78:         local_first: bool,\n + 79:     ) -> None:\n + 80:         if local_first:\n + 81:             for dist_op in dist_ops:\n + 82:                 validate_drop(dist_op, local_ops)\n + 83:         else:\n + 84:             for local_op in local_ops:\n + 85:                 validate_add_col_or_create_table(local_op, dist_ops)\n + 86: \n + 87:     validate_order(\n + 88:         migration.forwards_local(),\n + 89:         migration.forwards_dist(),\n + 90:         migration.forwards_local_first,\n + 91:     )\n + 92:     validate_order(\n + 93:         migration.backwards_local(),\n + 94:         migration.backwards_dist(),\n + 95:         migration.backwards_local_first,\n + 96:     )\n + 97: \n + 98: \n + 99: def conflicts_create_table_op(\n+ 100:     local_create: CreateTable, dist_create: CreateTable\n+ 101: ) -> bool:\n+ 102:     \"\"\"\n+ 103:     Returns True if create table operation and local create table operation\n+ 104:     target same underlying local table.\n+ 105:     \"\"\"\n+ 106:     if local_create._storage_set != dist_create._storage_set:\n+ 107:         return False\n+ 108:     try:\n+ 109:         cluster = get_cluster(dist_create._storage_set)\n+ 110:         if cluster.is_single_node():\n+ 111:             raise NoDistClusterNodes\n+ 112:         dist_engine_str = dist_create.engine.get_sql(cluster, dist_create.table_name)\n+ 113:         if local_create.table_name == _extract_local_table_name(dist_engine_str):\n+ 114:             return True\n+ 115:     except NoDistClusterNodes:\n+ 116:         # If there are no distributed nodes, we can't validate the order.\n+ 117:         pass\n+ 118: \n+ 119:     return False\n+ 120: \n+ 121: \n+ 122: def conflicts_add_column_op(local_add: AddColumn, dist_add: AddColumn) -> bool:\n+ 123:     \"\"\"\n+ 124:     Returns True if distributed add column operation and local add column operation\n+ 125:     target the same column and same underlying local table.\n+ 126:     \"\"\"\n+ 127:     if (\n+ 128:         local_add.column != dist_add.column\n+ 129:         or local_add._storage_set != dist_add._storage_set\n+ 130:     ):\n+ 131:         return False\n+ 132:     try:\n+ 133:         if local_add.table_name == _get_local_table_name(dist_add):\n+ 134:             return True\n+ 135:     except NoDistClusterNodes:\n+ 136:         # If there are no distributed nodes, we can't validate the order.\n+ 137:         pass\n+ 138: \n+ 139:     return False\n+ 140: \n+ 141: \n+ 142: def conflicts_drop_column_op(local_drop: DropColumn, dist_drop: DropColumn) -> bool:\n+ 143:     \"\"\"\n+ 144:     Returns True if distributed drop column operation and local drop column operation\n+ 145:     target the same column and same underlying local table.\n+ 146:     \"\"\"\n+ 147: \n+ 148:     if (\n+ 149:         local_drop.column_name != dist_drop.column_name\n+ 150:         or local_drop._storage_set != dist_drop._storage_set\n+ 151:     ):\n+ 152:         return False\n+ 153:     try:\n+ 154:         if local_drop.table_name == _get_local_table_name(dist_drop):\n+ 155:             return True\n+ 156:     except NoDistClusterNodes:\n+ 157:         # If there are no distributed nodes, we can't validate the order.\n+ 158:         pass\n+ 159: \n+ 160:     return False\n+ 161: \n+ 162: \n+ 163: def _get_dist_connection(dist_op: SqlOperation) -> ClickhousePool:\n+ 164:     cluster = get_cluster(dist_op._storage_set)\n+ 165:     nodes = cluster.get_distributed_nodes()\n+ 166:     if not nodes:\n+ 167:         raise NoDistClusterNodes(f\"No distributed nodes for {dist_op._storage_set}\")\n+ 168:     node = nodes[0]\n+ 169:     connection = cluster.get_node_connection(ClickhouseClientSettings.MIGRATE, node)\n+ 170:     return connection\n+ 171: \n+ 172: \n+ 173: def _get_local_table_name(dist_op: Union[CreateTable, AddColumn, DropColumn]) -> str:\n+ 174:     \"\"\"\n+ 175:     Returns the local table name for a distributed table.\n+ 176:     \"\"\"\n+ 177:     clickhouse = _get_dist_connection(dist_op)\n+ 178:     dist_table_name = dist_op.table_name\n+ 179:     engine = clickhouse.execute(\n+ 180:         f\"SELECT engine_full FROM system.tables WHERE name = '{dist_table_name}' AND database = '{clickhouse.database}'\"\n+ 181:     ).results\n+ 182:     if not engine:\n+ 183:         raise DistributedEngineParseError(\n+ 184:             f\"No engine found for table {dist_table_name}\"\n+ 185:         )\n+ 186:     engine_str = engine[0][0]\n+ 187:     return _extract_local_table_name(engine_str)\n+ 188: \n+ 189: \n+ 190: def _extract_local_table_name(engine_str: str) -> str:\n+ 191:     params = re.match(ENGINE_REGEX, engine_str)\n+ 192:     if not params:\n+ 193:         raise DistributedEngineParseError(\n+ 194:             f\"Cannot match engine string {engine_str} for distributed table\"\n+ 195:         )\n+ 196: \n+ 197:     dist_engine_args = params.group(1)[1:-1].split(\",\")\n+ 198:     local_table_name = dist_engine_args[2].strip()\n+ 199:     return local_table_name.strip(\"'\")\n-    : -- a/snuba/snuba_migrations/events/0016_drop_legacy_events.py\n+ 200: ++ b/snuba/snuba_migrations/events/0016_drop_legacy_events.py\n\nFile: snuba/snuba_migrations/events/0016_drop_legacy_events.py\nMetadata: index de36baec6a3..f1ed130ca47 100644\n\n\nChunk @@ -123,6 +123,8 @@ class Migration(migration.ClickhouseNodeMigration):\n  123:     \"\"\"\n  124: \n  125:     blocking = False\n+ 126:     forwards_local_first: bool = False\n+ 127:     backwards_local_first: bool = True\n  128: \n  129:     def forwards_local(self) -> Sequence[operations.SqlOperation]:\n  130:         return [\n-    : -- a/snuba/snuba_migrations/profiles/0004_drop_profile_column.py\n+ 131: ++ b/snuba/snuba_migrations/profiles/0004_drop_profile_column.py\n\nFile: snuba/snuba_migrations/profiles/0004_drop_profile_column.py\nMetadata: index f4f2ededcbf..aac677aae2c 100644\n\n\nChunk @@ -8,6 +8,8 @@\n   8: \n   9: class Migration(migration.ClickhouseNodeMigration):\n  10:     blocking = False\n+ 11:     forwards_local_first: bool = False\n+ 12:     backwards_local_first: bool = True\n  13: \n  14:     def forwards_local(self) -> Sequence[operations.SqlOperation]:\n  15:         return [\n-   : -- a/snuba/snuba_migrations/transactions/0014_transactions_remove_flattened_columns.py\n+ 16: ++ b/snuba/snuba_migrations/transactions/0014_transactions_remove_flattened_columns.py\n\nFile: snuba/snuba_migrations/transactions/0014_transactions_remove_flattened_columns.py\nMetadata: index 4a16dbc8991..545e4b6fd7f 100644\n\n\nChunk @@ -8,6 +8,8 @@\n   8: class Migration(migration.ClickhouseNodeMigration):\n   9: \n  10:     blocking = False\n+ 11:     forwards_local_first: bool = False\n+ 12:     backwards_local_first: bool = True\n  13: \n  14:     def forwards_local(self) -> Sequence[operations.SqlOperation]:\n  15:         return [\n-   : -- /dev/null\n+ 16: ++ b/tests/migrations/test_validator.py\n\nFile: tests/migrations/test_validator.py\nMetadata: index 00000000000..9b3094db475\n\n\nChunk @@ -0,0 +1,386 @@\n  + 1: from contextlib import contextmanager\n  + 2: from typing import Any, Iterator, Sequence, Union\n  + 3: from unittest.mock import Mock, patch\n  + 4: \n  + 5: import pytest\n  + 6: \n  + 7: from snuba.clickhouse.columns import Column, String, UInt\n  + 8: from snuba.clusters.cluster import ClickhouseClientSettings, get_cluster\n  + 9: from snuba.clusters.storage_sets import StorageSetKey\n + 10: from snuba.migrations import migration, validator\n + 11: from snuba.migrations.columns import MigrationModifiers as Modifiers\n + 12: from snuba.migrations.groups import MigrationGroup, get_group_loader\n + 13: from snuba.migrations.operations import AddColumn, CreateTable, DropColumn, SqlOperation\n + 14: from snuba.migrations.table_engines import Distributed, ReplacingMergeTree\n + 15: from snuba.migrations.validator import (\n + 16:     DistributedEngineParseError,\n + 17:     InvalidMigrationOrderError,\n + 18:     _get_local_table_name,\n + 19:     conflicts_add_column_op,\n + 20:     conflicts_create_table_op,\n + 21:     conflicts_drop_column_op,\n + 22:     validate_migration_order,\n + 23: )\n + 24: \n + 25: all_migrations = []\n + 26: for group in MigrationGroup:\n + 27:     group_loader = get_group_loader(group)\n + 28:     for migration_id in group_loader.get_migrations():\n + 29:         snuba_migration = group_loader.load_migration(migration_id)\n + 30:         if isinstance(snuba_migration, migration.ClickhouseNodeMigration):\n + 31:             all_migrations.append((migration_id, snuba_migration))\n + 32: \n + 33: \n + 34: @pytest.mark.parametrize(\n + 35:     \"snuba_migration\",\n + 36:     [\n + 37:         pytest.param(snuba_migration, id=migration_id)\n + 38:         for migration_id, snuba_migration in all_migrations\n + 39:     ],\n + 40: )\n + 41: def test_validate_all_migrations(\n + 42:     snuba_migration: migration.ClickhouseNodeMigration,\n + 43: ) -> None:\n + 44:     \"\"\"\n + 45:     Runs the migration validator on all existing migrations.\n + 46:     \"\"\"\n + 47:     validate_migration_order(snuba_migration)\n + 48: \n + 49: \n + 50: @contextmanager\n + 51: def does_not_raise() -> Iterator[None]:\n + 52:     yield\n + 53: \n + 54: \n + 55: class TestValidateMigrations:\n + 56:     columns: Sequence[Column[Modifiers]] = [\n + 57:         Column(\"col1\", String()),\n + 58:     ]\n + 59:     storage = StorageSetKey.EVENTS\n + 60:     create_local_op = CreateTable(\n + 61:         storage,\n + 62:         \"test_local_table\",\n + 63:         columns,\n + 64:         ReplacingMergeTree(\n + 65:             storage_set=storage,\n + 66:             order_by=\"version\",\n + 67:         ),\n + 68:     )\n + 69:     create_dist_op = CreateTable(\n + 70:         storage,\n + 71:         \"test_dist_table\",\n + 72:         columns,\n + 73:         Distributed(\"test_local_table\", None),\n + 74:     )\n + 75:     col: Column[Modifiers] = Column(\"col\", String())\n + 76:     add_col_local_op = AddColumn(storage, \"test_local_table\", col, None)\n + 77:     add_col_dist_op = AddColumn(storage, \"test_dist_table\", col, None)\n + 78:     drop_col_local_op = DropColumn(storage, \"test_local_table\", \"col\")\n + 79:     drop_col_dist_op = DropColumn(storage, \"test_dist_table\", \"col\")\n + 80: \n + 81:     def _dist_to_local(self, op: Union[CreateTable, AddColumn, DropColumn]) -> str:\n + 82:         if op.table_name == \"test_dist_table\":\n + 83:             return \"test_local_table\"\n + 84:         if op.table_name == \"test_dist_table2\":\n + 85:             return \"test_local_table2\"\n + 86:         return op.table_name\n + 87: \n + 88:     test_data = [\n + 89:         (True, False, [], [], [], [], does_not_raise(), \"\"),\n + 90:         (\n + 91:             True,\n + 92:             False,\n + 93:             [create_local_op],\n + 94:             [],\n + 95:             [create_dist_op],\n + 96:             [],\n + 97:             does_not_raise(),\n + 98:             \"\",\n + 99:         ),\n+ 100:         (\n+ 101:             False,\n+ 102:             False,\n+ 103:             [create_local_op],\n+ 104:             [create_dist_op],\n+ 105:             [],\n+ 106:             [],\n+ 107:             pytest.raises(InvalidMigrationOrderError),\n+ 108:             \"CreateTable test_local_table operation must be applied on local table before dist\",\n+ 109:         ),\n+ 110:         (\n+ 111:             True,\n+ 112:             False,\n+ 113:             [create_local_op, add_col_local_op],\n+ 114:             [create_dist_op, add_col_dist_op],\n+ 115:             [],\n+ 116:             [],\n+ 117:             does_not_raise(),\n+ 118:             \"\",\n+ 119:         ),\n+ 120:         (\n+ 121:             False,\n+ 122:             True,\n+ 123:             [add_col_local_op],\n+ 124:             [add_col_dist_op],\n+ 125:             [],\n+ 126:             [],\n+ 127:             pytest.raises(InvalidMigrationOrderError),\n+ 128:             \"AddColumn test_local_table.col operation must be applied on local table before dist\",\n+ 129:         ),\n+ 130:         (\n+ 131:             True,\n+ 132:             False,\n+ 133:             [create_local_op, add_col_local_op],\n+ 134:             [create_dist_op, add_col_dist_op],\n+ 135:             [drop_col_local_op],\n+ 136:             [drop_col_dist_op],\n+ 137:             does_not_raise(),\n+ 138:             \"\",\n+ 139:         ),\n+ 140:         (\n+ 141:             True,\n+ 142:             True,\n+ 143:             [create_local_op, add_col_local_op],\n+ 144:             [create_dist_op, add_col_dist_op],\n+ 145:             [drop_col_local_op],\n+ 146:             [drop_col_dist_op],\n+ 147:             pytest.raises(InvalidMigrationOrderError),\n+ 148:             \"DropColumn test_dist_table.col operation must be applied on dist table before local\",\n+ 149:         ),\n+ 150:         (\n+ 151:             True,\n+ 152:             False,\n+ 153:             [create_local_op, drop_col_local_op],\n+ 154:             [create_dist_op, drop_col_dist_op],\n+ 155:             [add_col_local_op],\n+ 156:             [add_col_dist_op],\n+ 157:             pytest.raises(InvalidMigrationOrderError),\n+ 158:             \"DropColumn test_dist_table.col operation must be applied on dist table before local\",\n+ 159:         ),\n+ 160:         (\n+ 161:             False,\n+ 162:             False,\n+ 163:             [drop_col_local_op],\n+ 164:             [drop_col_dist_op],\n+ 165:             [add_col_local_op],\n+ 166:             [add_col_dist_op],\n+ 167:             pytest.raises(InvalidMigrationOrderError),\n+ 168:             \"AddColumn test_local_table.col operation must be applied on local table before dist\",\n+ 169:         ),\n+ 170:         (\n+ 171:             False,\n+ 172:             True,\n+ 173:             [create_local_op, drop_col_local_op],\n+ 174:             [create_dist_op, drop_col_dist_op],\n+ 175:             [add_col_local_op],\n+ 176:             [add_col_dist_op],\n+ 177:             pytest.raises(InvalidMigrationOrderError),\n+ 178:             \"CreateTable test_local_table operation must be applied on local table before dist\",\n+ 179:         ),\n+ 180:     ]\n+ 181: \n+ 182:     @pytest.mark.parametrize(\n+ 183:         \"forwards_local_first_val, backwards_local_first_val,forwards_local,forwards_dist,\"\n+ 184:         \"backwards_local, backwards_dist, expectation, err_msg\",\n+ 185:         test_data,\n+ 186:     )\n+ 187:     @patch.object(validator, \"get_cluster\")\n+ 188:     @patch.object(validator, \"_get_local_table_name\")\n+ 189:     def test_validate_migration_order(\n+ 190:         self,\n+ 191:         mock_get_local_table_name: Mock,\n+ 192:         mock_get_cluster: Mock,\n+ 193:         forwards_local_first_val: bool,\n+ 194:         backwards_local_first_val: bool,\n+ 195:         forwards_local: Sequence[SqlOperation],\n+ 196:         forwards_dist: Sequence[SqlOperation],\n+ 197:         backwards_local: Sequence[SqlOperation],\n+ 198:         backwards_dist: Sequence[SqlOperation],\n+ 199:         expectation: Any,\n+ 200:         err_msg: str,\n+ 201:     ) -> None:\n+ 202: \n+ 203:         mock_get_local_table_name.side_effect = self._dist_to_local\n+ 204: \n+ 205:         storage = StorageSetKey.EVENTS\n+ 206:         cluster = get_cluster(storage)\n+ 207: \n+ 208:         mock_cluster = Mock(spec=cluster)\n+ 209:         mock_cluster.is_single_node.return_value = False\n+ 210:         mock_cluster.get_database.return_value = cluster.get_database()\n+ 211:         mock_cluster.get_clickhouse_cluster_name.return_value = (\n+ 212:             cluster.get_clickhouse_cluster_name() or \"test_cluster\"\n+ 213:         )\n+ 214:         mock_get_cluster.return_value = mock_cluster\n+ 215: \n+ 216:         class TestMigration(migration.ClickhouseNodeMigration):\n+ 217:             blocking = False\n+ 218:             backwards_local_first: bool = backwards_local_first_val\n+ 219:             forwards_local_first: bool = forwards_local_first_val\n+ 220: \n+ 221:             def forwards_local(self) -> Sequence[SqlOperation]:\n+ 222:                 return forwards_local\n+ 223: \n+ 224:             def backwards_local(self) -> Sequence[SqlOperation]:\n+ 225:                 return backwards_local\n+ 226: \n+ 227:             def forwards_dist(self) -> Sequence[SqlOperation]:\n+ 228:                 return forwards_dist\n+ 229: \n+ 230:             def backwards_dist(self) -> Sequence[SqlOperation]:\n+ 231:                 return backwards_dist\n+ 232: \n+ 233:         with expectation as err:\n+ 234:             validate_migration_order(TestMigration())\n+ 235:         if err_msg:\n+ 236:             assert str(err.value) == err_msg\n+ 237: \n+ 238: \n+ 239: @patch.object(validator, \"get_cluster\")\n+ 240: @patch.object(validator, \"_get_local_table_name\")\n+ 241: def test_conflicts(mock_get_local_table_name: Mock, mock_get_cluster: Mock) -> None:\n+ 242:     \"\"\"\n+ 243:     Test that the conlicts functions detect conflicting SQL operations that target the same table.\n+ 244:     \"\"\"\n+ 245:     storage = StorageSetKey.EVENTS\n+ 246:     cluster = get_cluster(storage)\n+ 247:     mock_cluster = Mock(spec=cluster)\n+ 248:     mock_cluster.is_single_node.return_value = False\n+ 249:     mock_cluster.get_database.return_value = cluster.get_database()\n+ 250:     mock_cluster.get_clickhouse_cluster_name.return_value = (\n+ 251:         cluster.get_clickhouse_cluster_name() or \"test_cluster\"\n+ 252:     )\n+ 253:     mock_get_cluster.return_value = mock_cluster\n+ 254: \n+ 255:     def _dist_to_local(op: Union[CreateTable, AddColumn, DropColumn]) -> str:\n+ 256:         if op.table_name == \"test_dist_table\":\n+ 257:             return \"test_local_table\"\n+ 258:         if op.table_name == \"test_dist_table2\":\n+ 259:             return \"test_local_table2\"\n+ 260:         return op.table_name\n+ 261: \n+ 262:     mock_get_local_table_name.side_effect = _dist_to_local\n+ 263: \n+ 264:     columns: Sequence[Column[Modifiers]] = [\n+ 265:         Column(\"id\", String()),\n+ 266:         Column(\"name\", String(Modifiers(nullable=True))),\n+ 267:         Column(\"version\", UInt(64)),\n+ 268:     ]\n+ 269: \n+ 270:     create_local_op = CreateTable(\n+ 271:         storage,\n+ 272:         \"test_local_table\",\n+ 273:         columns,\n+ 274:         ReplacingMergeTree(\n+ 275:             storage_set=storage,\n+ 276:             order_by=\"version\",\n+ 277:         ),\n+ 278:     )\n+ 279: \n+ 280:     create_local_op_table2 = CreateTable(\n+ 281:         storage,\n+ 282:         \"test_local_table2\",\n+ 283:         columns,\n+ 284:         ReplacingMergeTree(\n+ 285:             storage_set=storage,\n+ 286:             order_by=\"version\",\n+ 287:         ),\n+ 288:     )\n+ 289: \n+ 290:     create_dist_op = CreateTable(\n+ 291:         storage,\n+ 292:         \"test_dist_table\",\n+ 293:         columns,\n+ 294:         Distributed(\"test_local_table\", None),\n+ 295:     )\n+ 296: \n+ 297:     assert conflicts_create_table_op(create_local_op, create_dist_op)\n+ 298:     assert not conflicts_create_table_op(create_local_op_table2, create_dist_op)\n+ 299: \n+ 300:     add_col1: Column[Modifiers] = Column(\"col1\", String())\n+ 301:     add_col2: Column[Modifiers] = Column(\"col2\", String())\n+ 302:     add_col_local_op = AddColumn(storage, \"test_local_table\", add_col1, None)\n+ 303:     add_col_dist_op = AddColumn(storage, \"test_dist_table\", add_col1, None)\n+ 304:     add_col_dist_op_col_2 = AddColumn(storage, \"test_dist_table\", add_col2, None)\n+ 305:     add_col_local_op_col_2 = AddColumn(storage, \"test_local_table\", add_col2, None)\n+ 306:     add_col_dist_op_table_2 = AddColumn(storage, \"test_dist_table2\", add_col1, None)\n+ 307:     add_col_local_op_table_2 = AddColumn(storage, \"test_local_table2\", add_col1, None)\n+ 308: \n+ 309:     assert conflicts_add_column_op(add_col_local_op, add_col_dist_op)\n+ 310:     assert conflicts_add_column_op(add_col_local_op_table_2, add_col_dist_op_table_2)\n+ 311:     assert not conflicts_add_column_op(add_col_local_op_col_2, add_col_dist_op)\n+ 312:     assert not conflicts_add_column_op(add_col_local_op, add_col_dist_op_col_2)\n+ 313:     assert not conflicts_add_column_op(add_col_local_op, add_col_dist_op_table_2)\n+ 314:     assert not conflicts_add_column_op(add_col_local_op_table_2, add_col_dist_op)\n+ 315:     assert not conflicts_add_column_op(add_col_local_op, add_col_dist_op_col_2)\n+ 316: \n+ 317:     drop_col1, drop_col2 = \"col1\", \"col2\"\n+ 318:     drop_col_local_op = DropColumn(storage, \"test_local_table\", drop_col1)\n+ 319:     drop_col_dist_op = DropColumn(storage, \"test_dist_table\", drop_col1)\n+ 320:     drop_col_dist_op_col_2 = DropColumn(storage, \"test_dist_table\", drop_col2)\n+ 321:     drop_col_local_op_col_2 = DropColumn(storage, \"test_local_table\", drop_col2)\n+ 322:     drop_col_dist_op_table_2 = DropColumn(storage, \"test_dist_table2\", drop_col1)\n+ 323:     drop_col_local_op_table_2 = DropColumn(storage, \"test_local_table2\", drop_col1)\n+ 324: \n+ 325:     assert conflicts_drop_column_op(drop_col_local_op, drop_col_dist_op)\n+ 326:     assert conflicts_drop_column_op(drop_col_local_op_table_2, drop_col_dist_op_table_2)\n+ 327:     assert not conflicts_drop_column_op(drop_col_local_op_col_2, drop_col_dist_op)\n+ 328:     assert not conflicts_drop_column_op(drop_col_local_op, drop_col_dist_op_col_2)\n+ 329:     assert not conflicts_drop_column_op(drop_col_local_op, drop_col_dist_op_table_2)\n+ 330:     assert not conflicts_drop_column_op(drop_col_local_op_table_2, drop_col_dist_op)\n+ 331:     assert not conflicts_drop_column_op(drop_col_local_op, drop_col_dist_op_col_2)\n+ 332: \n+ 333: \n+ 334: @patch.object(validator, \"_get_dist_connection\")\n+ 335: def test_parse_engine(mock_get_dist_connection: Mock) -> None:\n+ 336:     cluster = get_cluster(StorageSetKey.MIGRATIONS)\n+ 337:     connection = cluster.get_query_connection(ClickhouseClientSettings.MIGRATE)\n+ 338:     database = connection.database\n+ 339:     mock_get_dist_connection.return_value = connection\n+ 340: \n+ 341:     # setup\n+ 342:     connection.execute(f\"DROP TABLE IF EXISTS {database}.test_local_table\")\n+ 343:     connection.execute(f\"DROP TABLE IF EXISTS {database}.test_dist_table\")\n+ 344:     connection.execute(f\"DROP TABLE IF EXISTS {database}.test_sharded_dist_table\")\n+ 345: \n+ 346:     local_table_engine = f\"Merge('{database}', 'test_local_table')\"\n+ 347:     connection.execute(\n+ 348:         f\"CREATE TABLE {database}.test_local_table (id String) ENGINE = {local_table_engine}\"\n+ 349:     )\n+ 350:     connection.execute(\n+ 351:         f\"CREATE TABLE {database}.test_dist_table (id String)\"\n+ 352:         f\"ENGINE = Distributed(test_shard_localhost, {database}, test_local_table)\"\n+ 353:     )\n+ 354:     mock_sql_op = Mock(spec=SqlOperation)\n+ 355:     mock_dist_op = mock_sql_op()\n+ 356: \n+ 357:     connection.execute(\n+ 358:         f\"CREATE TABLE {database}.test_sharded_dist_table (id String)\"\n+ 359:         f\"ENGINE = Distributed(test_shard_localhost, {database}, test_local_table, rand())\"\n+ 360:     )\n+ 361: \n+ 362:     # test parsing the local table name from engine\n+ 363:     mock_dist_op.table_name = \"test_dist_table\"\n+ 364:     assert _get_local_table_name(mock_dist_op) == \"test_local_table\"\n+ 365:     mock_dist_op.table_name = \"test_sharded_dist_table\"\n+ 366:     assert _get_local_table_name(mock_dist_op) == \"test_local_table\"\n+ 367: \n+ 368:     # test on not existing table\n+ 369:     mock_dist_op.table_name = \"not_exists_table\"\n+ 370:     with pytest.raises(DistributedEngineParseError) as parse_error:\n+ 371:         _get_local_table_name(mock_dist_op)\n+ 372:     assert str(parse_error.value) == \"No engine found for table not_exists_table\"\n+ 373: \n+ 374:     # test on not distributed table\n+ 375:     mock_dist_op.table_name = \"test_local_table\"\n+ 376:     with pytest.raises(DistributedEngineParseError) as parse_error:\n+ 377:         _get_local_table_name(mock_dist_op)\n+ 378:     assert (\n+ 379:         str(parse_error.value)\n+ 380:         == f\"Cannot match engine string {local_table_engine} for distributed table\"\n+ 381:     )\n+ 382: \n+ 383:     # cleanup\n+ 384:     connection.execute(f\"DROP TABLE {database}.test_local_table\")\n+ 385:     connection.execute(f\"DROP TABLE {database}.test_dist_table\")\n+ 386:     connection.execute(f\"DROP TABLE {database}.test_sharded_dist_table\")\n  387:

"""

